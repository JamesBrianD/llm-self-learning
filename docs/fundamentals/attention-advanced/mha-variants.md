# å¤šå¤´æ³¨æ„åŠ›å˜ä½“

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

ç†è§£ä»MHAåˆ°MLAçš„æŠ€æœ¯æ¼”è¿›ï¼ŒæŒæ¡ä¸åŒæ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–åŸç†å’Œåº”ç”¨åœºæ™¯ã€‚

## ğŸ“– é˜…è¯»ææ–™

### æ ¸å¿ƒæŠ€æœ¯æ–‡ç« 
1. [Transformerçš„AttentionåŠå…¶å„ç§å˜ä½“](https://lengm.cn/post/20250226_attention/) - è¯¦ç»†å¯¹æ¯”åˆ†æ
2. [ç¼“å­˜ä¸æ•ˆæœçš„æé™æ‹‰æ‰¯ï¼šä»MHAã€MQAã€GQAåˆ°MLA](https://spaces.ac.cn/archives/10091) - ç§‘å­¦ç©ºé—´æ·±åº¦è§£æ

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æŠ€æœ¯æ¼”è¿›è·¯å¾„

```
MHA (æ ‡å‡†å¤šå¤´) â†’ MQA (å…±äº«KV) â†’ GQA (åˆ†ç»„å…±äº«) â†’ MLA (æ½œåœ¨ç©ºé—´)
```

### å„å˜ä½“è¯¦ç»†å¯¹æ¯”

| å˜ä½“ | KV Cacheéœ€æ±‚ | è®¡ç®—å¤æ‚åº¦ | æ€§èƒ½è¡¨ç° | ä¸»è¦åº”ç”¨ |
|------|-------------|-----------|----------|----------|
| **MHA** | O(hÃ—dÃ—L) | O(H x N^2 x D) | åŸºå‡†æ€§èƒ½ | æ ‡å‡†Transformer |
| **MQA** | O(dÃ—L) | O(N^2Ã—D) | è½»å¾®ä¸‹é™ | èµ„æºå—é™åœºæ™¯ |
| **GQA** | O(gÃ—dÃ—L) | O(GÃ—N^2Ã—D) | å¹³è¡¡ä¼˜ç§€ | ä¸»æµå¤§æ¨¡å‹ |
| **MLA** | æœ€ä¼˜åŒ– | O(RÃ—N^2Ã—D) | æ¥è¿‘MHA | é•¿ä¸Šä¸‹æ–‡ |

> å…¶ä¸­ï¼šh=å¤´æ•°ï¼Œd=ç»´åº¦ï¼ŒL=åºåˆ—é•¿åº¦ï¼Œg=ç»„æ•°

### æ ¸å¿ƒæŠ€æœ¯ç»†èŠ‚

#### 1. MHA (Multi-Head Attention)
```python
# æ¯ä¸ªå¤´éƒ½æœ‰ç‹¬ç«‹çš„Qã€Kã€V
for i in range(num_heads):
    Q_i = input @ W_Q_i  # æ¯ä¸ªå¤´ç‹¬ç«‹çš„æŸ¥è¯¢çŸ©é˜µ
    K_i = input @ W_K_i  # æ¯ä¸ªå¤´ç‹¬ç«‹çš„é”®çŸ©é˜µ  
    V_i = input @ W_V_i  # æ¯ä¸ªå¤´ç‹¬ç«‹çš„å€¼çŸ©é˜µ
    head_i = attention(Q_i, K_i, V_i)
```

#### 2. MQA (Multi-Query Attention)
```python
# æ‰€æœ‰å¤´å…±äº«Kã€Vï¼Œåªæœ‰Qç‹¬ç«‹
K_shared = input @ W_K  # å…±äº«çš„é”®çŸ©é˜µ
V_shared = input @ W_V  # å…±äº«çš„å€¼çŸ©é˜µ

for i in range(num_heads):
    Q_i = input @ W_Q_i  # æ¯ä¸ªå¤´ç‹¬ç«‹çš„æŸ¥è¯¢çŸ©é˜µ
    head_i = attention(Q_i, K_shared, V_shared)
```

#### 3. GQA (Grouped-Query Attention)
```python
# åˆ†ç»„å…±äº«ï¼šæ¯ç»„å†…å…±äº«Kã€V
num_groups = num_heads // group_size

for g in range(num_groups):
    K_g = input @ W_K_g  # ç»„å…±äº«çš„é”®çŸ©é˜µ
    V_g = input @ W_V_g  # ç»„å…±äº«çš„å€¼çŸ©é˜µ
    
    for i in range(group_size):
        Q_i = input @ W_Q_i
        head_i = attention(Q_i, K_g, V_g)
```

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: MHAã€MQAã€GQAã€MLAéƒ½æ˜¯ä»€ä¹ˆï¼Ÿ

**ç®€æ´å›ç­”ï¼š**
è¿™æ˜¯Transformeræ³¨æ„åŠ›æœºåˆ¶çš„å››ä¸ªæ¼”è¿›é˜¶æ®µï¼Œä¸»è¦ä¼˜åŒ–KV Cacheçš„å­˜å‚¨éœ€æ±‚ï¼š

- **MHA**: æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹QKV
- **MQA**: å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼Œæ‰€æœ‰å¤´å…±äº«KV
- **GQA**: åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼Œåˆ†ç»„å†…å…±äº«KV  
- **MLA**: å¤šå¤´æ½œåœ¨æ³¨æ„åŠ›ï¼Œé€šè¿‡ä½ç§©åˆ†è§£ä¼˜åŒ–

**æŠ€æœ¯ç»†èŠ‚ï¼š**

**MHAé—®é¢˜**: KV Cacheéšå¤´æ•°çº¿æ€§å¢é•¿ï¼Œå†…å­˜å¼€é”€å¤§
```
å†…å­˜éœ€æ±‚ = å¤´æ•° Ã— ç»´åº¦ Ã— åºåˆ—é•¿åº¦
```

**MQAè§£å†³æ–¹æ¡ˆ**: å…±äº«KVçŸ©é˜µï¼Œå†…å­˜éœ€æ±‚é™ä½hå€
```python
# ä» hÃ—(d_k + d_v) é™ä½åˆ° (d_k + d_v)
```

**GQAå¹³è¡¡æ–¹æ¡ˆ**: åˆ†ç»„å…±äº«ï¼Œå…¼é¡¾æ€§èƒ½å’Œæ•ˆç‡
```python
# å†…å­˜éœ€æ±‚ = ç»„æ•° Ã— ç»´åº¦ Ã— åºåˆ—é•¿åº¦  
# å…¶ä¸­ï¼šç»„æ•° = å¤´æ•° / æ¯ç»„å¤´æ•°
```

**MLAç»ˆæä¼˜åŒ–**: æ½œåœ¨ç©ºé—´æŠ•å½±ï¼Œæœ€å°åŒ–KV Cache

## ğŸ”¬ MLAæŠ€æœ¯æ·±åº¦è§£æ

### MLAæ ¸å¿ƒåˆ›æ–°

**Multi-head Latent Attention (MLA)** æ˜¯DeepSeekå›¢é˜Ÿæå‡ºçš„é©å‘½æ€§æ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡ä¸‰å¤§åˆ›æ–°æ˜¾è‘—é™ä½KV Cacheå†…å­˜éœ€æ±‚ï¼š

#### 1. ä½ç§©KVè”åˆå‹ç¼©

**æ ¸å¿ƒæ€æƒ³**: å°†é«˜ç»´çš„Keyå’ŒValueçŸ©é˜µè”åˆå‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´

```python
# ä¼ ç»Ÿæ–¹å¼ï¼šæ¯ä¸ªå¤´ç‹¬ç«‹å­˜å‚¨KV
traditional_kv_cache = num_heads Ã— head_dim Ã— seq_len Ã— 2  # Kå’ŒV

# MLAæ–¹å¼ï¼šå‹ç¼©åçš„æ½œåœ¨å‘é‡
mla_kv_cache = compressed_dim Ã— seq_len + rope_dim Ã— seq_len
```

**å‹ç¼©è¿‡ç¨‹**:
$$c_t^{KV} = x_t W^{DKV}$$

å…¶ä¸­ $W^{DKV} \in \mathbb{R}^{d \times d_c}$ï¼Œ$d_c \ll h \cdot d_h$

#### 2. RoPEè§£è€¦æœºåˆ¶

**é—®é¢˜**: ä½ç½®ç¼–ç ä¸å‹ç¼©æœºåˆ¶çš„å†²çª
- ä¼ ç»ŸRoPEéœ€è¦åœ¨åŸå§‹QKç©ºé—´ä¸­åº”ç”¨
- å‹ç¼©ç ´åäº†ä½ç½®ä¿¡æ¯çš„æ­£ç¡®ä¼ é€’

**è§£å†³æ–¹æ¡ˆ**: å°†Queryå’ŒKeyåˆ†ä¸ºä¸¤éƒ¨åˆ†
- **è¯­ä¹‰éƒ¨åˆ†** ($q^C, k^C$): æºå¸¦ä¸»è¦è¯­ä¹‰ä¿¡æ¯ï¼Œå¯ä»¥å‹ç¼©
- **ä½ç½®éƒ¨åˆ†** ($q^R, k^R$): æºå¸¦ä½ç½®ä¿¡æ¯ï¼Œä¿æŒåŸç»´åº¦

```python
def mla_with_rope_decoupling(x, position):
    # 1. ç”Ÿæˆæ½œåœ¨å‘é‡
    c_kv = x @ W_down_kv  # å‹ç¼©
    
    # 2. Queryåˆ†ç¦»
    q = x @ W_q
    q_c, q_r = q[:, :d_c], q[:, d_c:]  # è¯­ä¹‰ + ä½ç½®
    
    # 3. Keyåˆ†ç¦»å’Œæ¢å¤
    k_c = c_kv @ W_up_k   # ä»æ½œåœ¨ç©ºé—´æ¢å¤è¯­ä¹‰Key
    k_r = x @ W_k_r       # ç›´æ¥ç”Ÿæˆä½ç½®Key
    
    # 4. åˆ†åˆ«åº”ç”¨RoPE
    q_r = apply_rope(q_r, position)
    k_r = apply_rope(k_r, position)
    
    # 5. ç»„åˆè®¡ç®—
    q_combined = concat([q_c, q_r])
    k_combined = concat([k_c, k_r])
    v = c_kv @ W_up_v
    
    return attention(q_combined, k_combined, v)
```

#### 3. æƒé‡å¸æ”¶ä¼˜åŒ–

**ç›®æ ‡**: å‡å°‘æ¨ç†æ—¶çš„çŸ©é˜µä¹˜æ³•æ“ä½œ

**æŠ€æœ¯**: åˆ©ç”¨çŸ©é˜µä¹˜æ³•ç»“åˆå¾‹ï¼Œé¢„å…ˆåˆå¹¶æƒé‡çŸ©é˜µ

```python
# åŸå§‹è®¡ç®—ï¼šä¸¤æ¬¡çŸ©é˜µä¹˜æ³•
c_kv = x @ W_down_kv
k = c_kv @ W_up_k

# æƒé‡å¸æ”¶ï¼šåˆå¹¶ä¸ºä¸€æ¬¡ä¹˜æ³•
W_combined = W_down_kv @ W_up_k
k = x @ W_combined
```

### å†…å­˜æ•ˆç‡å¯¹æ¯”

| æ–¹æ³• | KV Cacheå¤§å° | å‹ç¼©æ¯” |
|------|-------------|--------|
| **MHA** | $2 h \cdot d_h \cdot L$ | 1.0Ã— (åŸºå‡†) |
| **MQA** | $2 d_h \cdot L$ | $h$Ã— |
| **GQA** | $2 g \cdot d_h \cdot L$ | $h/g$Ã— |
| **MLA** | $(d_c + d_h^R) \cdot L$ | ~10-20Ã— |

**å…·ä½“ä¾‹å­** (LLaMA-7Bè§„æ¨¡):
- åŸå§‹MHA: 32å¤´ Ã— 128ç»´ Ã— 2 = 8192ç»´/token
- MLAå‹ç¼©: 512ç»´ + 128ç»´ = 640ç»´/token
- **å‹ç¼©æ¯”**: 12.8Ã—

### æ€§èƒ½ä¿æŒæœºåˆ¶

å°½ç®¡å¤§å¹…å‹ç¼©ï¼ŒMLAé€šè¿‡å·§å¦™è®¾è®¡ä¿æŒäº†æ¥è¿‘MHAçš„æ€§èƒ½ï¼š

#### 1. è¡¨è¾¾èƒ½åŠ›ä¿æŒ
- ä½ç§©å‡è®¾ï¼šå¤§éƒ¨åˆ†æ³¨æ„åŠ›æ¨¡å¼å¯ä»¥ç”¨ä½ç§©çŸ©é˜µè¿‘ä¼¼
- å…³é”®ä¿¡æ¯ä¿ç•™ï¼šä½ç½®ä¿¡æ¯é€šè¿‡è§£è€¦æœºåˆ¶å®Œæ•´ä¿ç•™
- æ¸è¿›æ¢å¤ï¼šå¤šå±‚å †å é€æ­¥æ¢å¤å®Œæ•´ä¿¡æ¯

#### 2. è®­ç»ƒç¨³å®šæ€§
```python
# æ®‹å·®è¿æ¥ç¡®ä¿è®­ç»ƒç¨³å®š
def mla_block(x):
    # MLAæ³¨æ„åŠ›
    attn_out = mla_attention(x)
    x = x + attn_out  # æ®‹å·®è¿æ¥
    
    # FFN
    ffn_out = feed_forward(x)
    x = x + ffn_out   # æ®‹å·®è¿æ¥
    
    return x
```

#### 3. ä½ç½®æ•æ„Ÿæ€§
- RoPEè§£è€¦ç¡®ä¿ä½ç½®ä¿¡æ¯ä¸ä¸¢å¤±
- ä½ç½®ç¼–ç ç»´åº¦å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚è°ƒæ•´
- é•¿åºåˆ—å¤–æ¨èƒ½åŠ›å¾—åˆ°ä¿æŒ

### Q2: ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›ä¼˜åŒ–ï¼Ÿ

**æ ¸å¿ƒåŠ¨æœºï¼š**

1. **å†…å­˜ç“¶é¢ˆ**
   - é•¿åºåˆ—æ¨ç†æ—¶KV Cacheå ç”¨å¤§é‡æ˜¾å­˜
   - é™åˆ¶äº†æ¨¡å‹çš„éƒ¨ç½²å’Œæ‰©å±•èƒ½åŠ›

2. **æ¨ç†é€Ÿåº¦**
   - å‡å°‘å†…å­˜è®¿é—®ï¼Œæé«˜è®¡ç®—æ•ˆç‡
   - æ”¯æŒæ›´å¤§çš„batch size

3. **æˆæœ¬è€ƒè™‘**
   - é™ä½ç¡¬ä»¶è¦æ±‚
   - æé«˜æœåŠ¡å¹¶å‘èƒ½åŠ›

### Q3: å„å˜ä½“çš„ä¼˜ç¼ºç‚¹å¯¹æ¯”ï¼Ÿ

| ç»´åº¦ | MHA | MQA | GQA | MLA |
|------|-----|-----|-----|-----|
| **æ€§èƒ½** | ğŸŸ¢ åŸºå‡†æœ€å¥½ | ğŸŸ¡ è½»å¾®ä¸‹é™ | ğŸŸ¢ æ¥è¿‘MHA | ğŸŸ¢ è¶…è¶ŠMHA |
| **å†…å­˜** | ğŸ”´ éœ€æ±‚æœ€é«˜ | ğŸŸ¢ æ˜¾è‘—é™ä½ | ğŸŸ¡ é€‚ä¸­ | ğŸŸ¢ æœ€ä¼˜ |
| **é€Ÿåº¦** | ğŸŸ¡ æ ‡å‡† | ğŸŸ¢ æœ€å¿« | ğŸŸ¢ è¾ƒå¿« | ğŸŸ¢ ä¼˜ç§€ |
| **å®ç°** | ğŸŸ¢ ç®€å• | ğŸŸ¢ ç®€å• | ğŸŸ¡ ä¸­ç­‰ | ğŸ”´ å¤æ‚ |

### Q4: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Ÿ

**é€‰æ‹©ç­–ç•¥ï¼š**

```python
if èµ„æºå……è¶³ and è¿½æ±‚æœ€ä½³æ€§èƒ½:
    é€‰æ‹© MHA
elif èµ„æºä¸¥é‡å—é™ and å¯æ¥å—æ€§èƒ½æŸå¤±:
    é€‰æ‹© MQA  
elif éœ€è¦å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡:
    é€‰æ‹© GQA  # ä¸»æµé€‰æ‹©
elif é•¿ä¸Šä¸‹æ–‡ and å†…å­˜æ•æ„Ÿ:
    é€‰æ‹© MLA
```

**å®é™…è€ƒè™‘å› ç´ ï¼š**
- ç¡¬ä»¶å†…å­˜é™åˆ¶
- åºåˆ—é•¿åº¦éœ€æ±‚  
- å»¶è¿Ÿè¦æ±‚
- å¼€å‘å¤æ‚åº¦

## ğŸ’» ä»£ç å®ç°

### ç»ƒä¹ 1: å®ç°Softmaxå‡½æ•°
**å¹³å°**: [Deep-ML Softmax](https://www.deep-ml.com/problems/23)

### ç»ƒä¹ 2: MHAåˆ°GQAçš„é€‚é…

```python
class GroupedQueryAttention(nn.Module):
    def __init__(self, d_model, num_heads, num_groups):
        super().__init__()
        assert num_heads % num_groups == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.num_groups = num_groups
        self.heads_per_group = num_heads // num_groups
        self.d_k = d_model // num_heads
        
        # QçŸ©é˜µï¼šæ¯ä¸ªå¤´ç‹¬ç«‹
        self.W_q = nn.Linear(d_model, d_model)
        
        # K,VçŸ©é˜µï¼šæŒ‰ç»„å…±äº«
        self.W_k = nn.Linear(d_model, num_groups * self.d_k)
        self.W_v = nn.Linear(d_model, num_groups * self.d_k)
        
        self.W_o = nn.Linear(d_model, d_model)
    
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        
        # ç”ŸæˆQï¼šæ¯ä¸ªå¤´ç‹¬ç«‹
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k)
        
        # ç”ŸæˆK,Vï¼šæŒ‰ç»„å…±äº«
        K = self.W_k(x).view(batch_size, seq_len, self.num_groups, self.d_k)
        V = self.W_v(x).view(batch_size, seq_len, self.num_groups, self.d_k)
        
        # é‡å¤K,Vä»¥åŒ¹é…Qçš„å¤´æ•°
        K = K.repeat_interleave(self.heads_per_group, dim=2)
        V = V.repeat_interleave(self.heads_per_group, dim=2)
        
        # è®¡ç®—æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)
        attn_weights = F.softmax(scores, dim=-1)
        out = torch.matmul(attn_weights, V)
        
        # åˆå¹¶å¤šå¤´è¾“å‡º
        out = out.view(batch_size, seq_len, d_model)
        return self.W_o(out)
```

### ç»ƒä¹ 3: KV Cacheå®ç°é¢„è§ˆ

```python
class KVCache:
    def __init__(self, max_seq_len, num_heads, d_k):
        self.max_seq_len = max_seq_len
        self.cache_k = torch.zeros(max_seq_len, num_heads, d_k)
        self.cache_v = torch.zeros(max_seq_len, num_heads, d_k) 
        self.current_len = 0
    
    def update(self, new_k, new_v):
        """æ›´æ–°ç¼“å­˜å¹¶è¿”å›å®Œæ•´çš„K,V"""
        seq_len = new_k.size(0)
        
        # å­˜å‚¨æ–°çš„K,V
        self.cache_k[self.current_len:self.current_len+seq_len] = new_k
        self.cache_v[self.current_len:self.current_len+seq_len] = new_v
        
        self.current_len += seq_len
        
        # è¿”å›åˆ°ç›®å‰ä¸ºæ­¢çš„å®Œæ•´K,V
        return (self.cache_k[:self.current_len], 
                self.cache_v[:self.current_len])
```

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] èƒ½è§£é‡Šå„å˜ä½“çš„æ ¸å¿ƒåŒºåˆ«
- [ ] ç†è§£KV Cacheä¼˜åŒ–çš„åŸç†
- [ ] å®ŒæˆGQAä»£ç å®ç°
- [ ] èƒ½æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„æ³¨æ„åŠ›æœºåˆ¶

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸‹ä¸€èŠ‚ï¼šKV CacheæŠ€æœ¯](kv-cache.md)
- [è¿”å›ï¼šAttentionå‡çº§æ¦‚è§ˆ](index.md)