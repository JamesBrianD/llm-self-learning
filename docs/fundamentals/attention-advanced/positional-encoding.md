# ä½ç½®ç¼–ç 

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

ç†è§£ä½ç½®ç¼–ç åœ¨Transformerä¸­çš„ä½œç”¨ï¼ŒæŒæ¡ä»ç»å¯¹ä½ç½®ç¼–ç åˆ°RoPEçš„æŠ€æœ¯æ¼”è¿›ï¼Œèƒ½å¤Ÿæ¨å¯¼RoPEçš„æ•°å­¦åŸç†ã€‚

## ğŸ“– é˜…è¯»ææ–™

### æ ¸å¿ƒæŠ€æœ¯æ–‡ç« 
1. [Sinusoidalä½ç½®ç¼–ç è¿½æ ¹æº¯æº](https://kexue.fm/archives/8231) - ç§‘å­¦ç©ºé—´
2. [åšé‡‡ä¼—é•¿çš„æ—‹è½¬å¼ä½ç½®ç¼–ç ](https://kexue.fm/archives/8265) - ç§‘å­¦ç©ºé—´
3. [è®©ç ”ç©¶äººå‘˜ç»å°½è„‘æ±çš„Transformerä½ç½®ç¼–ç ](https://kexue.fm/archives/8130) - ç§‘å­¦ç©ºé—´

## ğŸ“ çŸ¥è¯†æ€»ç»“

### ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ

**æ ¸å¿ƒé—®é¢˜**: Transformerçš„Self-Attentionæœºåˆ¶æ˜¯ç½®æ¢ä¸å˜çš„ï¼ˆpermutation invariantï¼‰ï¼Œæ— æ³•åŒºåˆ†tokençš„é¡ºåºã€‚

```python
# æ²¡æœ‰ä½ç½®ä¿¡æ¯æ—¶ï¼Œè¿™ä¸¤ä¸ªåºåˆ—æ˜¯ç­‰ä»·çš„
sequence1 = ["æˆ‘", "çˆ±", "åŒ—äº¬"]
sequence2 = ["çˆ±", "åŒ—äº¬", "æˆ‘"]
# Self-Attentionä¼šç»™å‡ºç›¸åŒçš„ç»“æœï¼
```

**è§£å†³æ–¹æ¡ˆ**: åœ¨è¾“å…¥ä¸­æ³¨å…¥ä½ç½®ä¿¡æ¯ï¼Œè®©æ¨¡å‹èƒ½å¤Ÿç†è§£tokenä¹‹é—´çš„ç›¸å¯¹æˆ–ç»å¯¹ä½ç½®å…³ç³»ã€‚

### ä½ç½®ç¼–ç åˆ†ç±»

```
ä½ç½®ç¼–ç 
â”œâ”€â”€ ç»å¯¹ä½ç½®ç¼–ç  (APE)
â”‚   â”œâ”€â”€ å¯è®­ç»ƒä½ç½®ç¼–ç  (Learned PE)
â”‚   â””â”€â”€ å›ºå®šä½ç½®ç¼–ç  (Sinusoidal PE)
â””â”€â”€ ç›¸å¯¹ä½ç½®ç¼–ç  (RPE)
    â”œâ”€â”€ ç»å…¸ç›¸å¯¹ä½ç½®ç¼–ç 
    â”œâ”€â”€ æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)
    â””â”€â”€ å…¶ä»–å˜ä½“ (ALiBiç­‰)
```

### ç»å¯¹ä½ç½®ç¼–ç  vs ç›¸å¯¹ä½ç½®ç¼–ç 

| ç»´åº¦ | ç»å¯¹ä½ç½®ç¼–ç  | ç›¸å¯¹ä½ç½®ç¼–ç  |
|------|-------------|-------------|
| **ç¼–ç å¯¹è±¡** | tokençš„ç»å¯¹ä½ç½® | tokenä¹‹é—´çš„ç›¸å¯¹è·ç¦» |
| **æ“ä½œä½ç½®** | è¾“å…¥å±‚æ·»åŠ ä½ç½®å‘é‡ | æ³¨æ„åŠ›å±‚ä¿®æ”¹è®¡ç®—æ–¹å¼ |
| **å®ç°å¤æ‚åº¦** | ç®€å• | ç›¸å¯¹å¤æ‚ |
| **é•¿åº¦å¤–æ¨** | è¾ƒå·® | è¾ƒå¥½ |
| **æ€§èƒ½è¡¨ç°** | çŸ­åºåˆ—è¶³å¤Ÿ | é•¿åºåˆ—æ›´ä¼˜ |

### æ ¸å¿ƒæŠ€æœ¯è¯¦è§£

#### 1. Sinusoidalä½ç½®ç¼–ç  (åŸå§‹Transformer)

**æ•°å­¦å…¬å¼**:
```math
PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}})
PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}})
```

**æ ¸å¿ƒç‰¹ç‚¹**:
- ä½¿ç”¨æ­£å¼¦ä½™å¼¦å‡½æ•°ç”Ÿæˆä½ç½®ç¼–ç 
- ä¸åŒç»´åº¦ä½¿ç”¨ä¸åŒçš„é¢‘ç‡
- å›ºå®šç¼–ç ï¼Œä¸éœ€è¦è®­ç»ƒ
- ç†è®ºä¸Šæ”¯æŒä»»æ„é•¿åº¦åºåˆ—

**ä¼˜åŠ¿**:
- è®¡ç®—ç®€å•ï¼Œä¸å ç”¨å‚æ•°
- å…·æœ‰ä¸€å®šçš„å¤–æ¨èƒ½åŠ›
- ç›¸å¯¹ä½ç½®æœ‰ä¸€å®šçš„è§„å¾‹æ€§

**ç¼ºç‚¹**:
- ä½ç½®ä¿¡æ¯åœ¨æ·±å±‚å¯èƒ½è¡°å‡
- å¯¹ç›¸å¯¹ä½ç½®çš„å»ºæ¨¡ä¸å¤Ÿç›´æ¥

#### 2. å¯è®­ç»ƒä½ç½®ç¼–ç 

**å®ç°æ–¹å¼**:
```python
# ä¸ºæ¯ä¸ªä½ç½®å­¦ä¹ ä¸€ä¸ªå‘é‡
position_embeddings = nn.Embedding(max_seq_len, d_model)
pos_emb = position_embeddings(position_ids)
input_emb = token_emb + pos_emb
```

**ç‰¹ç‚¹**:
- æ¯ä¸ªä½ç½®å¯¹åº”ä¸€ä¸ªå¯å­¦ä¹ çš„å‘é‡
- é€šè¿‡è®­ç»ƒä¼˜åŒ–ä½ç½®è¡¨ç¤º
- åœ¨è®­ç»ƒé•¿åº¦èŒƒå›´å†…æ•ˆæœé€šå¸¸æ›´å¥½

#### 3. RoPE (æ—‹è½¬ä½ç½®ç¼–ç )

**æ ¸å¿ƒæ€æƒ³**: é€šè¿‡æ—‹è½¬å˜æ¢å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ°æŸ¥è¯¢å’Œé”®å‘é‡ä¸­ï¼Œä½¿å¾—æ³¨æ„åŠ›åˆ†æ•°è‡ªç„¶åœ°ä¾èµ–äºç›¸å¯¹ä½ç½®ã€‚

**æ•°å­¦æ¨å¯¼**:

**æ­¥éª¤1**: å°†ç‰¹å¾åˆ†ä¸ºpairsï¼Œæ¯å¯¹ç‰¹å¾çœ‹ä½œ2Då¹³é¢çš„åæ ‡
```math
x = [x_1, x_2, x_3, x_4, ...] â†’ [(x_1, x_2), (x_3, x_4), ...]
```

**æ­¥éª¤2**: å¯¹æ¯ä¸€å¯¹ç‰¹å¾åº”ç”¨æ—‹è½¬çŸ©é˜µ
```math
\begin{pmatrix}
x_{m}^{(1)} \\
x_{m}^{(2)}
\end{pmatrix}
â†’
\begin{pmatrix}
\cos(m\theta) & -\sin(m\theta) \\
\sin(m\theta) & \cos(m\theta)
\end{pmatrix}
\begin{pmatrix}
x_{m}^{(1)} \\
x_{m}^{(2)}
\end{pmatrix}
```

**æ­¥éª¤3**: æ—‹è½¬åçš„å‘é‡
```math
\begin{pmatrix}
x_{m}^{(1)} \cos(m\theta) - x_{m}^{(2)} \sin(m\theta) \\
x_{m}^{(2)} \cos(m\theta) + x_{m}^{(1)} \sin(m\theta)
\end{pmatrix}
```

**æ ¸å¿ƒæ€§è´¨**: ç›¸å¯¹ä½ç½®ä¾èµ–
```math
\langle RoPE(q_m), RoPE(k_n) \rangle = \langle q_m, k_n \rangle \cos((m-n)\theta) + \text{å…¶ä»–é¡¹}
```

æ³¨æ„åŠ›åˆ†æ•°åªä¾èµ–äºç›¸å¯¹è·ç¦» (m-n)ï¼

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: ä»€ä¹ˆæ˜¯ç»å¯¹ä½ç½®ç¼–ç ï¼Œç›¸å¯¹ä½ç½®ç¼–ç ï¼Ÿ

**ç»å¯¹ä½ç½®ç¼–ç  (APE)**:
- **å®šä¹‰**: ä¸ºæ¯ä¸ªtokençš„ç»å¯¹ä½ç½®åˆ†é…ä¸€ä¸ªä½ç½®å‘é‡
- **å®ç°**: åœ¨è¾“å…¥å±‚å°†ä½ç½®å‘é‡åŠ åˆ°token embeddingä¸Š
- **ç‰¹ç‚¹**: ç®€å•ç›´æ¥ï¼Œæ¯ä¸ªä½ç½®æœ‰å›ºå®šçš„ç¼–ç 

**ç›¸å¯¹ä½ç½®ç¼–ç  (RPE)**:
- **å®šä¹‰**: åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶è€ƒè™‘tokenä¹‹é—´çš„ç›¸å¯¹è·ç¦»
- **å®ç°**: ä¿®æ”¹æ³¨æ„åŠ›è®¡ç®—å…¬å¼ï¼ŒåŠ å…¥ç›¸å¯¹ä½ç½®åç½®
- **ç‰¹ç‚¹**: æ›´ç¬¦åˆç›´è§‰ï¼Œå¤–æ¨èƒ½åŠ›æ›´å¼º

**æŠ€æœ¯ç»†èŠ‚å¯¹æ¯”**:
```python
# ç»å¯¹ä½ç½®ç¼–ç 
input_emb = token_emb + position_emb[pos]

# ç›¸å¯¹ä½ç½®ç¼–ç   
attention_score = QK^T + relative_position_bias[i-j]
```

### Q2: æ¨å¯¼RoPEçš„æ•°å­¦åŸç†

**æ¨å¯¼æ­¥éª¤**:

**ç›®æ ‡**: è®¾è®¡ä¸€ä¸ªå‡½æ•°fï¼Œä½¿å¾—ï¼š
```math
\langle f(q, m), f(k, n) \rangle = g(q, k, m-n)
```
å³æ³¨æ„åŠ›åˆ†æ•°åªä¾èµ–ç›¸å¯¹ä½ç½® m-nã€‚

**è§£å†³æ–¹æ¡ˆ**: å¤æ•°åŸŸçš„æ—‹è½¬å˜æ¢

**æ­¥éª¤1**: å°†å®æ•°å‘é‡æ˜ å°„åˆ°å¤æ•°
```math
q_{1} + i q_{2} â†’ q_{complex}
```

**æ­¥éª¤2**: åº”ç”¨å¤æ•°æ—‹è½¬
```math
f(q, m) = q_{complex} \cdot e^{im\theta} = q_{complex} \cdot (\cos(m\theta) + i\sin(m\theta))
```

**æ­¥éª¤3**: éªŒè¯ç›¸å¯¹ä½ç½®æ€§è´¨
```math
\langle f(q,m), f(k,n) \rangle^* = \langle q \cdot e^{im\theta}, k \cdot e^{in\theta} \rangle
= \langle q, k \rangle \cdot e^{i(m-n)\theta}
```

åªä¾èµ–äº (m-n)ï¼

**æ­¥éª¤4**: è½¬æ¢å›å®æ•°åŸŸ
```math
\begin{pmatrix}
q_1 \cos(m\theta) - q_2 \sin(m\theta) \\
q_1 \sin(m\theta) + q_2 \cos(m\theta)
\end{pmatrix}
```

**å…³é”®æ´å¯Ÿ**: é€šè¿‡æ—‹è½¬å˜æ¢ï¼Œç›¸å¯¹ä½ç½®ä¿¡æ¯è‡ªç„¶åœ°ç¼–ç åœ¨äº†å‘é‡çš„å‡ ä½•å…³ç³»ä¸­ã€‚

### Q3: RoPEç›¸æ¯”ä¼ ç»Ÿä½ç½®ç¼–ç çš„ä¼˜åŠ¿ï¼Ÿ

**æ ¸å¿ƒä¼˜åŠ¿**:

1. **è‡ªç„¶çš„ç›¸å¯¹ä½ç½®ä¾èµ–**
   - æ³¨æ„åŠ›åˆ†æ•°ç›´æ¥ä¾èµ–ç›¸å¯¹è·ç¦»
   - æ— éœ€é¢å¤–çš„ç›¸å¯¹ä½ç½®åç½®é¡¹

2. **ä¼˜ç§€çš„å¤–æ¨èƒ½åŠ›**
   - è®­ç»ƒæ—¶çš„ç›¸å¯¹ä½ç½®æ¨¡å¼å¯ä»¥æ³›åŒ–åˆ°æ›´é•¿åºåˆ—
   - ç†è®ºä¸Šæ”¯æŒæ— é™é•¿åº¦å¤–æ¨

3. **è®¡ç®—é«˜æ•ˆ**
   - æ— éœ€å­˜å‚¨ä½ç½®åµŒå…¥è¡¨
   - æ—‹è½¬æ“ä½œå¯ä»¥é«˜æ•ˆå®ç°

4. **ç†è®ºä¼˜é›…**
   - æ•°å­¦åŸºç¡€æ‰å®
   - åŸºäºå¤æ•°æ—‹è½¬çš„å‡ ä½•ç›´è§‰

**å®éªŒéªŒè¯**:
- åœ¨å¤šä¸ªNLPä»»åŠ¡ä¸Šè¶…è¶Šä¼ ç»Ÿä½ç½®ç¼–ç 
- é•¿åºåˆ—ä»»åŠ¡ä¸Šè¡¨ç°ç‰¹åˆ«çªå‡º
- å·²è¢«å¤šä¸ªå¤§æ¨¡å‹é‡‡ç”¨(LLaMAã€PaLMç­‰)

### Q4: RoPEåœ¨å®é™…å®ç°ä¸­æœ‰ä»€ä¹ˆæŠ€å·§ï¼Ÿ

**å®ç°ä¼˜åŒ–**:

1. **é¢‘ç‡é€‰æ‹©**
   ```python
   # ä¸åŒç»´åº¦ä½¿ç”¨ä¸åŒé¢‘ç‡
   theta = 10000 ** (-2 * torch.arange(0, dim, 2) / dim)
   ```

2. **é¢„è®¡ç®—æ—‹è½¬çŸ©é˜µ**
   ```python
   # é¿å…é‡å¤è®¡ç®—sin/cos
   cos_cached = torch.cos(position * theta)
   sin_cached = torch.sin(position * theta)
   ```

3. **å‘é‡åŒ–å®ç°**
   ```python
   # åŒæ—¶å¤„ç†æ‰€æœ‰ä½ç½®å’Œç»´åº¦
   q_rot = q * cos_cached - q_shifted * sin_cached
   ```

## ğŸ’» ä»£ç å®ç°

### RoPEå®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import math

class RotaryPositionalEmbedding(nn.Module):
    """RoPE (Rotary Position Embedding) å®ç°"""
    
    def __init__(self, dim, max_seq_len=2048, base=10000):
        super().__init__()
        self.dim = dim
        self.max_seq_len = max_seq_len
        self.base = base
        
        # è®¡ç®—æ—‹è½¬é¢‘ç‡
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
        
        # é¢„è®¡ç®—ä½ç½®ç¼–ç 
        self._build_cache(max_seq_len)
    
    def _build_cache(self, seq_len):
        """é¢„è®¡ç®—å¹¶ç¼“å­˜æ—‹è½¬çŸ©é˜µ"""
        # ç”Ÿæˆä½ç½®åºåˆ—
        position = torch.arange(seq_len).float()
        
        # è®¡ç®—è§’åº¦: position * inv_freq
        freqs = torch.outer(position, self.inv_freq)  # [seq_len, dim//2]
        
        # æ‹¼æ¥ï¼Œå½¢æˆå®Œæ•´çš„é¢‘ç‡çŸ©é˜µ
        emb = torch.cat([freqs, freqs], dim=-1)  # [seq_len, dim]
        
        # è®¡ç®—coså’Œsin
        cos_cached = emb.cos()
        sin_cached = emb.sin()
        
        self.register_buffer('cos_cached', cos_cached)
        self.register_buffer('sin_cached', sin_cached)
    
    def rotate_half(self, x):
        """å°†è¾“å…¥çš„ååŠéƒ¨åˆ†å–è´Ÿå·å¹¶ç§»åˆ°å‰é¢"""
        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]
        return torch.cat([-x2, x1], dim=-1)
    
    def forward(self, q, k, seq_len=None):
        """
        å¯¹æŸ¥è¯¢å’Œé”®å‘é‡åº”ç”¨RoPE
        
        Args:
            q: æŸ¥è¯¢çŸ©é˜µ [batch, heads, seq_len, dim]
            k: é”®çŸ©é˜µ [batch, heads, seq_len, dim]
            seq_len: åºåˆ—é•¿åº¦
        """
        if seq_len is None:
            seq_len = q.shape[-2]
        
        # å¦‚æœåºåˆ—é•¿åº¦è¶…å‡ºç¼“å­˜ï¼Œé‡æ–°æ„å»º
        if seq_len > self.max_seq_len:
            self._build_cache(seq_len)
        
        # è·å–å¯¹åº”é•¿åº¦çš„coså’Œsin
        cos = self.cos_cached[:seq_len]  # [seq_len, dim]
        sin = self.sin_cached[:seq_len]  # [seq_len, dim]
        
        # åº”ç”¨æ—‹è½¬å˜æ¢
        q_rot = q * cos + self.rotate_half(q) * sin
        k_rot = k * cos + self.rotate_half(k) * sin
        
        return q_rot, k_rot

class MultiHeadAttentionWithRoPE(nn.Module):
    """å¸¦RoPEçš„å¤šå¤´æ³¨æ„åŠ›"""
    
    def __init__(self, d_model, num_heads, max_seq_len=2048):
        super().__init__()
        assert d_model % num_heads == 0
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        # RoPEåªåº”ç”¨åˆ°éƒ¨åˆ†ç»´åº¦ï¼ˆé€šå¸¸æ˜¯å‰åŠéƒ¨åˆ†ï¼‰
        self.rope = RotaryPositionalEmbedding(
            dim=self.head_dim, 
            max_seq_len=max_seq_len
        )
    
    def forward(self, x, mask=None):
        batch_size, seq_len, d_model = x.shape
        
        # è®¡ç®—Q, K, V
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # è½¬ç½®ä»¥ç¬¦åˆæ³¨æ„åŠ›è®¡ç®—çš„ç»´åº¦è¦æ±‚
        Q = Q.transpose(1, 2)  # [batch, heads, seq_len, head_dim]
        K = K.transpose(1, 2)
        V = V.transpose(1, 2)
        
        # åº”ç”¨RoPE
        Q, K = self.rope(Q, K, seq_len)
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        
        # åº”ç”¨æ©ç 
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmaxå½’ä¸€åŒ–
        attn_weights = torch.softmax(scores, dim=-1)
        
        # è®¡ç®—è¾“å‡º
        out = torch.matmul(attn_weights, V)
        
        # é‡å¡‘å¹¶åˆå¹¶å¤šå¤´
        out = out.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        return self.W_o(out)

# ä¸åŒä½ç½®ç¼–ç çš„å¯¹æ¯”æµ‹è¯•
def compare_position_encodings():
    """å¯¹æ¯”ä¸åŒä½ç½®ç¼–ç çš„æ•ˆæœ"""
    
    d_model, seq_len = 512, 64
    batch_size, num_heads = 2, 8
    
    print("=== ä½ç½®ç¼–ç å¯¹æ¯”æµ‹è¯• ===")
    
    # æµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, d_model)
    
    # 1. æ— ä½ç½®ç¼–ç çš„æ³¨æ„åŠ›
    attn_no_pos = MultiHeadAttentionWithRoPE(d_model, num_heads)
    # ä¸´æ—¶ç§»é™¤RoPE
    attn_no_pos.rope = lambda q, k, seq_len: (q, k)
    out_no_pos = attn_no_pos(x)
    
    # 2. å¸¦RoPEçš„æ³¨æ„åŠ›
    attn_with_rope = MultiHeadAttentionWithRoPE(d_model, num_heads)
    out_with_rope = attn_with_rope(x)
    
    print(f"æ— ä½ç½®ç¼–ç è¾“å‡ºæ ‡å‡†å·®: {out_no_pos.std():.4f}")
    print(f"RoPEä½ç½®ç¼–ç è¾“å‡ºæ ‡å‡†å·®: {out_with_rope.std():.4f}")
    
    # 3. æµ‹è¯•å¤–æ¨èƒ½åŠ›
    print("\n=== å¤–æ¨èƒ½åŠ›æµ‹è¯• ===")
    
    # çŸ­åºåˆ—è®­ç»ƒ
    short_len = 32
    x_short = torch.randn(1, short_len, d_model)
    
    # é•¿åºåˆ—æ¨ç†
    long_len = 128
    x_long = torch.randn(1, long_len, d_model)
    
    try:
        out_short = attn_with_rope(x_short)
        out_long = attn_with_rope(x_long)
        print(f"çŸ­åºåˆ—({short_len})å¤„ç†æˆåŠŸ")
        print(f"é•¿åºåˆ—({long_len})å¤„ç†æˆåŠŸ - RoPEæ”¯æŒå¤–æ¨")
    except Exception as e:
        print(f"å¤–æ¨å¤±è´¥: {e}")

# æ‰‹åŠ¨éªŒè¯RoPEçš„ç›¸å¯¹ä½ç½®æ€§è´¨
def verify_rope_property():
    """éªŒè¯RoPEçš„ç›¸å¯¹ä½ç½®ä¾èµ–æ€§è´¨"""
    
    print("=== éªŒè¯RoPEç›¸å¯¹ä½ç½®æ€§è´¨ ===")
    
    dim = 64
    rope = RotaryPositionalEmbedding(dim, max_seq_len=10)
    
    # åˆ›å»ºä¸¤ä¸ªä½ç½®çš„æŸ¥è¯¢å’Œé”®
    q = torch.randn(1, 1, 1, dim)  # ä½ç½®0çš„æŸ¥è¯¢
    k = torch.randn(1, 1, 1, dim)  # ä½ç½®0çš„é”®
    
    # åœ¨ä¸åŒç›¸å¯¹è·ç¦»ä¸‹æµ‹è¯•
    distances = [1, 2, 3]
    
    for dist in distances:
        # è®¡ç®—ä½ç½®(0, dist)çš„ç›¸å¯¹æ³¨æ„åŠ›
        q_pos0, k_pos_dist = rope(q, k, seq_len=dist+1)
        score1 = torch.matmul(q_pos0[:,:,0:1], k_pos_dist[:,:,dist:dist+1].transpose(-2,-1))
        
        # è®¡ç®—ä½ç½®(1, 1+dist)çš„ç›¸å¯¹æ³¨æ„åŠ›  
        q_pos1, k_pos1_dist = rope(q, k, seq_len=dist+2)
        score2 = torch.matmul(q_pos1[:,:,1:2], k_pos1_dist[:,:,1+dist:2+dist].transpose(-2,-1))
        
        print(f"ç›¸å¯¹è·ç¦»{dist}: åˆ†æ•°å·®å¼‚ = {abs(score1.item() - score2.item()):.6f}")

if __name__ == "__main__":
    compare_position_encodings()
    print()
    verify_rope_property()
```

### Sinusoidalä½ç½®ç¼–ç å®ç°

```python
class SinusoidalPositionalEncoding(nn.Module):
    """åŸå§‹Transformerçš„æ­£å¼¦ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, max_seq_len=5000):
        super().__init__()
        
        pe = torch.zeros(max_seq_len, d_model)
        position = torch.arange(0, max_seq_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
    
    def forward(self, x):
        seq_len = x.size(1)
        return x + self.pe[:, :seq_len]
```

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] ç†è§£ç»å¯¹ä½ç½®ç¼–ç å’Œç›¸å¯¹ä½ç½®ç¼–ç çš„åŒºåˆ«
- [ ] èƒ½æ¨å¯¼RoPEçš„æ•°å­¦åŸç†
- [ ] æŒæ¡RoPEçš„å®ç°ç»†èŠ‚å’Œä¼˜åŒ–æŠ€å·§
- [ ] å®Œæˆä½ç½®ç¼–ç çš„ä»£ç å®ç°å’Œæ•ˆæœéªŒè¯

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸Šä¸€èŠ‚ï¼šå½’ä¸€åŒ–æŠ€æœ¯](normalization.md)
- [ä¸‹ä¸€èŠ‚ï¼šLLMå‡çº§æŠ€æœ¯](../llm-advanced/index.md)
- [è¿”å›ï¼šAttentionå‡çº§æ¦‚è§ˆ](index.md)