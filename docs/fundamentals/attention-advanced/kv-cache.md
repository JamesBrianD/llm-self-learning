# KV CacheæŠ€æœ¯

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æ·±å…¥ç†è§£KV Cacheçš„å·¥ä½œåŸç†ï¼ŒæŒæ¡è¿™ä¸ªè®©å¤§æ¨¡å‹æ¨ç†æé€Ÿæ•°å€çš„å…³é”®æŠ€æœ¯ã€‚

## ğŸ“ çŸ¥è¯†æ€»ç»“

### KV Cacheæ˜¯ä»€ä¹ˆï¼Ÿ

**å®šä¹‰**: KV Cacheæ˜¯ä¸€ç§æ¨ç†ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ç¼“å­˜ä¹‹å‰è®¡ç®—è¿‡çš„Keyå’ŒValueçŸ©é˜µï¼Œé¿å…é‡å¤è®¡ç®—ï¼Œå¤§å¹…æå‡ç”Ÿæˆé€Ÿåº¦ã€‚

### ä¸ºä»€ä¹ˆéœ€è¦KV Cacheï¼Ÿ

**é—®é¢˜èƒŒæ™¯**: è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é‡å¤è®¡ç®—

```python
# ç”Ÿæˆ"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨"çš„è¿‡ç¨‹
Step 1: è¾“å…¥["æˆ‘"]        â†’ é¢„æµ‹"çˆ±" 
Step 2: è¾“å…¥["æˆ‘","çˆ±"]    â†’ é¢„æµ‹"åŒ—"
Step 3: è¾“å…¥["æˆ‘","çˆ±","åŒ—"] â†’ é¢„æµ‹"äº¬"
...
```

**é‡å¤è®¡ç®—é—®é¢˜**:
- æ¯ä¸€æ­¥éƒ½è¦é‡æ–°è®¡ç®—æ‰€æœ‰previous tokensçš„K,VçŸ©é˜µ
- è®¡ç®—å¤æ‚åº¦: O(nÂ²)ï¼Œå…¶ä¸­næ˜¯åºåˆ—é•¿åº¦
- å¤§é‡é‡å¤è®¡ç®—å¯¼è‡´æ¨ç†é€Ÿåº¦æ…¢

### KV Cacheå·¥ä½œåŸç†

#### 1. ä¼ ç»Ÿæ–¹å¼ (æ— ç¼“å­˜)

```python
# æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—å…¨éƒ¨K,V
def generate_token_naive(tokens):
    # é‡æ–°è®¡ç®—æ‰€æœ‰tokençš„K,V - éå¸¸ä½æ•ˆï¼
    K = compute_K(tokens)  # åŒ…å«æ‰€æœ‰å†å²token
    V = compute_V(tokens)  # åŒ…å«æ‰€æœ‰å†å²token
    Q = compute_Q(tokens[-1])  # åªéœ€è¦æœ€åä¸€ä¸ªtokençš„Q
    
    attention_output = attention(Q, K, V)
    return next_token
```

#### 2. KV Cacheä¼˜åŒ–æ–¹å¼

```python
# åªè®¡ç®—æ–°tokençš„K,Vï¼Œå¤ç”¨å†å²ç¼“å­˜
def generate_token_with_cache(new_token, kv_cache):
    # åªè®¡ç®—æ–°tokençš„K,V
    new_K = compute_K(new_token)  
    new_V = compute_V(new_token)
    
    # æ›´æ–°ç¼“å­˜
    kv_cache.append(new_K, new_V)
    
    # ä½¿ç”¨å®Œæ•´çš„K,V (å†å²+æ–°å¢)
    Q = compute_Q(new_token)
    attention_output = attention(Q, kv_cache.K, kv_cache.V)
    
    return next_token
```

### åŠ é€Ÿæ•ˆæœåˆ†æ

**æ—¶é—´å¤æ‚åº¦å¯¹æ¯”**:

| ç”Ÿæˆæ­¥éª¤ | æ— Cache | æœ‰Cache | åŠ é€Ÿæ¯” |
|----------|---------|---------|--------|
| ç¬¬1æ­¥ | O(1) | O(1) | 1x |
| ç¬¬2æ­¥ | O(4) | O(1) | 4x |
| ç¬¬3æ­¥ | O(9) | O(1) | 9x |
| ç¬¬næ­¥ | O(nÂ²) | O(1) | nÂ²x |

**å†…å­˜ä½¿ç”¨**:
- ç©ºé—´æ¢æ—¶é—´çš„ç­–ç•¥
- éœ€è¦å­˜å‚¨: `seq_len Ã— num_heads Ã— head_dim Ã— 2` (Kå’ŒV)
- é•¿åºåˆ—æ—¶å†…å­˜éœ€æ±‚æ˜¾è‘—å¢åŠ 

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: KV Cacheæ˜¯ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆKV Cacheèƒ½åŠ é€Ÿæ¨¡å‹æ¨ç†ï¼Ÿ

**æ ¸å¿ƒç­”æ¡ˆ**: 
KV Cacheæ˜¯ç¼“å­˜æ³¨æ„åŠ›æœºåˆ¶ä¸­Keyå’ŒValueçŸ©é˜µçš„æŠ€æœ¯ï¼Œé€šè¿‡é¿å…é‡å¤è®¡ç®—å†å²tokençš„K,Væ¥åŠ é€Ÿæ¨ç†ã€‚

**è¯¦ç»†è§£é‡Š**:

1. **é—®é¢˜æ ¹æº**: 
   - è‡ªå›å½’ç”Ÿæˆæ¯æ­¥éƒ½éœ€è¦å®Œæ•´çš„attentionè®¡ç®—
   - å†å²tokençš„K,VçŸ©é˜µåœ¨æ¯æ­¥ä¸­ä¿æŒä¸å˜
   - é‡å¤è®¡ç®—é€ æˆO(nÂ²)çš„æ—¶é—´å¤æ‚åº¦

2. **è§£å†³æ–¹æ¡ˆ**:
   - ç¼“å­˜å·²è®¡ç®—çš„K,VçŸ©é˜µ
   - æ–°tokenåªéœ€è®¡ç®—è‡ªå·±çš„K,Vå¹¶è¿½åŠ åˆ°ç¼“å­˜
   - å°†æ—¶é—´å¤æ‚åº¦ä»O(nÂ²)é™ä½åˆ°O(1)

3. **åŠ é€ŸåŸç†**:
   ```
   ä¼ ç»Ÿæ–¹å¼: æ¯æ­¥è®¡ç®—å®Œæ•´åºåˆ—çš„K,V
   ç¼“å­˜æ–¹å¼: åªè®¡ç®—æ–°å¢tokençš„K,V
   ```

### Q2: KV Cacheçš„å†…å­˜å¼€é”€å¦‚ä½•ï¼Ÿ

**å†…å­˜éœ€æ±‚è®¡ç®—**:
```python
memory_per_token = num_layers Ã— num_heads Ã— head_dim Ã— 2 Ã— dtype_size
total_memory = memory_per_token Ã— max_seq_length
```

**å…·ä½“ä¾‹å­** (LLaMA-7B):
```
å‚æ•°: 32å±‚, 32å¤´, 128ç»´åº¦, FP16
æ¯ä¸ªtoken: 32 Ã— 32 Ã— 128 Ã— 2 Ã— 2 bytes = 524KB
2048é•¿åº¦: 524KB Ã— 2048 â‰ˆ 1GB
```

**å†…å­˜ä¼˜åŒ–ç­–ç•¥**:
- ä½¿ç”¨æ›´ä½ç²¾åº¦(FP16/INT8)
- åˆ†å±‚ç¼“å­˜ï¼Œåªä¿ç•™æœ€è¿‘çš„token
- æ»‘åŠ¨çª—å£ï¼Œä¸¢å¼ƒè¿‡æ—§çš„ç¼“å­˜

### Q3: KV Cacheåœ¨ä¸åŒæ³¨æ„åŠ›å˜ä½“ä¸­çš„è¡¨ç°ï¼Ÿ

**å„å˜ä½“çš„KV Cacheéœ€æ±‚**:

| æ³¨æ„åŠ›ç±»å‹ | KV Cacheå¤§å° | è¯´æ˜ |
|-----------|-------------|------|
| **MHA** | `h Ã— d Ã— L` | æ¯ä¸ªå¤´ç‹¬ç«‹å­˜å‚¨K,V |
| **MQA** | `d Ã— L` | æ‰€æœ‰å¤´å…±äº«K,Vï¼Œå†…å­˜å‡å°‘hå€ |
| **GQA** | `g Ã— d Ã— L` | åˆ†ç»„å…±äº«ï¼Œå†…å­˜éœ€æ±‚åœ¨MHAå’ŒMQAä¹‹é—´ |
| **MLA** | æœ€å° | é€šè¿‡ä½ç§©åˆ†è§£è¿›ä¸€æ­¥å‹ç¼© |

> h=å¤´æ•°, d=ç»´åº¦, L=åºåˆ—é•¿åº¦, g=ç»„æ•°

## ğŸ’» ä»£ç å®ç°

### å®Œæ•´KV Cacheæ¼”ç¤º

```python
import torch
import torch.nn as nn

class KVCache:
    """ç®€åŒ–çš„KV Cacheå®ç°"""
    
    def __init__(self, max_seq_len, num_heads, head_dim, device='cpu'):
        self.max_seq_len = max_seq_len
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.device = device
        
        # é¢„åˆ†é…ç¼“å­˜ç©ºé—´
        self.cache_k = torch.zeros(
            max_seq_len, num_heads, head_dim, 
            device=device, dtype=torch.float16
        )
        self.cache_v = torch.zeros(
            max_seq_len, num_heads, head_dim,
            device=device, dtype=torch.float16
        )
        
        self.current_length = 0
    
    def update_cache(self, new_k, new_v):
        """æ›´æ–°ç¼“å­˜å¹¶è¿”å›å®Œæ•´çš„K,V"""
        batch_size, seq_len, num_heads, head_dim = new_k.shape
        
        # æ£€æŸ¥æ˜¯å¦è¶…å‡ºç¼“å­˜å®¹é‡
        if self.current_length + seq_len > self.max_seq_len:
            raise ValueError("Sequence length exceeds cache capacity")
        
        # æ›´æ–°ç¼“å­˜
        end_pos = self.current_length + seq_len
        self.cache_k[self.current_length:end_pos] = new_k[0]  # å‡è®¾batch_size=1
        self.cache_v[self.current_length:end_pos] = new_v[0]
        
        self.current_length = end_pos
        
        # è¿”å›åˆ°ç›®å‰ä¸ºæ­¢çš„å®Œæ•´K,V
        return (
            self.cache_k[:self.current_length].unsqueeze(0),  # æ·»åŠ batchç»´åº¦
            self.cache_v[:self.current_length].unsqueeze(0)
        )
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.current_length = 0
    
    def get_cache_info(self):
        """è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯"""
        return {
            'current_length': self.current_length,
            'capacity': self.max_seq_len,
            'usage_ratio': self.current_length / self.max_seq_len,
            'memory_mb': self.cache_k.numel() * 2 * 2 / 1024 / 1024  # FP16
        }


class AttentionWithKVCache(nn.Module):
    """å¸¦KV Cacheçš„æ³¨æ„åŠ›å±‚"""
    
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
        self.kv_cache = None
    
    def setup_cache(self, max_seq_len, device):
        """åˆå§‹åŒ–KV Cache"""
        self.kv_cache = KVCache(max_seq_len, self.num_heads, self.head_dim, device)
    
    def forward(self, x, use_cache=False):
        batch_size, seq_len, d_model = x.shape
        
        # è®¡ç®—Q, K, V
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        if use_cache and self.kv_cache is not None:
            # ä½¿ç”¨ç¼“å­˜æ¨¡å¼ï¼šæ›´æ–°ç¼“å­˜å¹¶è·å–å®Œæ•´çš„K,V
            K, V = self.kv_cache.update_cache(K, V)
        
        # è®¡ç®—æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn_weights = torch.softmax(scores, dim=-1)
        out = torch.matmul(attn_weights, V)
        
        # åˆå¹¶å¤šå¤´è¾“å‡º
        out = out.view(batch_size, seq_len, d_model)
        return self.W_o(out)


# ä½¿ç”¨ç¤ºä¾‹
def demo_kv_cache():
    """KV Cacheä½¿ç”¨æ¼”ç¤º"""
    
    # åˆå§‹åŒ–æ¨¡å‹
    attention = AttentionWithKVCache(d_model=512, num_heads=8)
    attention.setup_cache(max_seq_len=1024, device='cpu')
    
    print("=== KV Cacheæ¼”ç¤º ===")
    
    # æ¨¡æ‹Ÿç”Ÿæˆè¿‡ç¨‹
    vocab_size = 1000
    sequence = []
    
    for step in range(5):
        if step == 0:
            # ç¬¬ä¸€æ­¥ï¼šè¾“å…¥å®Œæ•´çš„prompt
            current_input = torch.randint(0, vocab_size, (1, 3, 512))  # 3ä¸ªtokençš„prompt
            print(f"Step {step}: è¾“å…¥prompt (3 tokens)")
        else:
            # åç»­æ­¥éª¤ï¼šåªè¾“å…¥æ–°ç”Ÿæˆçš„token
            current_input = torch.randint(0, vocab_size, (1, 1, 512))  # 1ä¸ªæ–°token
            print(f"Step {step}: è¾“å…¥æ–°token (1 token)")
        
        # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
        output = attention(current_input, use_cache=True)
        
        # æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
        cache_info = attention.kv_cache.get_cache_info()
        print(f"  ç¼“å­˜é•¿åº¦: {cache_info['current_length']}")
        print(f"  å†…å­˜ä½¿ç”¨: {cache_info['memory_mb']:.2f} MB")
        print()

if __name__ == "__main__":
    demo_kv_cache()
```

### æ€§èƒ½å¯¹æ¯”æµ‹è¯•

```python
import time

def benchmark_with_without_cache():
    """å¯¹æ¯”æœ‰æ— KV Cacheçš„æ€§èƒ½"""
    
    d_model, num_heads = 768, 12
    max_seq_len = 512
    
    # åˆå§‹åŒ–æ¨¡å‹
    attention_with_cache = AttentionWithKVCache(d_model, num_heads)
    attention_with_cache.setup_cache(max_seq_len, 'cpu')
    
    attention_without_cache = AttentionWithKVCache(d_model, num_heads)
    
    # æ¨¡æ‹Ÿåºåˆ—ç”Ÿæˆ
    prompt_len = 50
    generate_len = 100
    
    print("=== æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===")
    
    # æµ‹è¯•æ— ç¼“å­˜ç‰ˆæœ¬
    start_time = time.time()
    sequence_input = torch.randn(1, prompt_len, d_model)
    
    for i in range(generate_len):
        # æ¯æ¬¡éƒ½è¾“å…¥å®Œæ•´åºåˆ—ï¼ˆæ— ç¼“å­˜ï¼‰
        full_input = torch.randn(1, prompt_len + i + 1, d_model)
        _ = attention_without_cache(full_input, use_cache=False)
    
    no_cache_time = time.time() - start_time
    print(f"æ— ç¼“å­˜ç”Ÿæˆæ—¶é—´: {no_cache_time:.3f}ç§’")
    
    # æµ‹è¯•æœ‰ç¼“å­˜ç‰ˆæœ¬  
    start_time = time.time()
    
    # å¤„ç†prompt
    _ = attention_with_cache(sequence_input, use_cache=True)
    
    # é€æ­¥ç”Ÿæˆ
    for i in range(generate_len):
        # æ¯æ¬¡åªè¾“å…¥æ–°tokenï¼ˆæœ‰ç¼“å­˜ï¼‰
        new_token = torch.randn(1, 1, d_model)
        _ = attention_with_cache(new_token, use_cache=True)
    
    with_cache_time = time.time() - start_time
    print(f"æœ‰ç¼“å­˜ç”Ÿæˆæ—¶é—´: {with_cache_time:.3f}ç§’")
    
    speedup = no_cache_time / with_cache_time
    print(f"åŠ é€Ÿå€æ•°: {speedup:.1f}x")

if __name__ == "__main__":
    benchmark_with_without_cache()
```

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] ç†è§£KV Cacheçš„å·¥ä½œåŸç†å’ŒåŠ é€Ÿæœºåˆ¶
- [ ] èƒ½è®¡ç®—KV Cacheçš„å†…å­˜éœ€æ±‚
- [ ] å®ŒæˆKV Cacheæ¼”ç¤ºä»£ç çš„ç¼–å†™å’Œæµ‹è¯•
- [ ] ç†è§£ä¸åŒæ³¨æ„åŠ›å˜ä½“å¯¹KV Cacheçš„å½±å“

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸Šä¸€èŠ‚ï¼šå¤šå¤´æ³¨æ„åŠ›å˜ä½“](mha-variants.md)
- [ä¸‹ä¸€èŠ‚ï¼šå½’ä¸€åŒ–æŠ€æœ¯](normalization.md)
- [è¿”å›ï¼šAttentionå‡çº§æ¦‚è§ˆ](index.md)