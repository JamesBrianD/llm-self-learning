# å½’ä¸€åŒ–æŠ€æœ¯

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æŒæ¡æ·±åº¦å­¦ä¹ ä¸­ä¸‰å¤§å½’ä¸€åŒ–æŠ€æœ¯çš„åŸç†å’Œåº”ç”¨åœºæ™¯ï¼Œç†è§£Pre-Normä¸Post-Normåœ¨Transformerä¸­çš„å½±å“ã€‚

## ğŸ“– é˜…è¯»ææ–™

### æ ¸å¿ƒæŠ€æœ¯æ–‡ç« 
1. [å¤§æ¨¡å‹ä¸­å¸¸è§çš„3ç§Norm](https://zhuanlan.zhihu.com/p/648987575) - çŸ¥ä¹
2. [ä¸ºä»€ä¹ˆå½“å‰ä¸»æµçš„å¤§æ¨¡å‹éƒ½ä½¿ç”¨RMS-Normï¼Ÿ](https://zhuanlan.zhihu.com/p/12392406696) - çŸ¥ä¹  
3. [ä¸ºä»€ä¹ˆPre Normçš„æ•ˆæœä¸å¦‚Post Normï¼Ÿ](https://spaces.ac.cn/archives/9009) - ç§‘å­¦ç©ºé—´

### é€‰è¯»æ·±å…¥ææ–™
- [BNç©¶ç«Ÿèµ·äº†ä»€ä¹ˆä½œç”¨ï¼Ÿ](https://spaces.ac.cn/archives/6992) - ç§‘å­¦ç©ºé—´

## ğŸ“ çŸ¥è¯†æ€»ç»“

### ä¸‰å¤§å½’ä¸€åŒ–æŠ€æœ¯å¯¹æ¯”

| æŠ€æœ¯ | å½’ä¸€åŒ–ç»´åº¦ | é€‚ç”¨åœºæ™¯ | ä¸»è¦ä¼˜åŠ¿ | è®¡ç®—æˆæœ¬ |
|------|----------|----------|----------|----------|
| **BatchNorm** | è·¨æ ·æœ¬ç‰¹å¾ç»´åº¦ | CNNã€å¤§batch | è®­ç»ƒåŠ é€Ÿã€é˜²è¿‡æ‹Ÿåˆ | ä¸­ç­‰ |
| **LayerNorm** | å•æ ·æœ¬æ‰€æœ‰ç‰¹å¾ | RNNã€Transformer | ä¸ä¾èµ–batchå¤§å° | è¾ƒé«˜ |
| **RMSNorm** | å•æ ·æœ¬RMSå½’ä¸€åŒ– | å¤§å‹è¯­è¨€æ¨¡å‹ | è®¡ç®—é«˜æ•ˆã€æ•ˆæœç›¸å½“ | æœ€ä½ |

### æ•°å­¦å…¬å¼è¯¦è§£

#### 1. Batch Normalization
```math
BN(x) = Î³ Ã— \frac{x - Î¼_B}{\sqrt{Ïƒ_B^2 + Îµ}} + Î²
```

**æ ¸å¿ƒç‰¹ç‚¹**:
- Î¼_B, Ïƒ_B: åœ¨batchç»´åº¦è®¡ç®—å‡å€¼å’Œæ–¹å·®
- è®­ç»ƒæ—¶ä½¿ç”¨å½“å‰batchç»Ÿè®¡é‡ï¼Œæ¨ç†æ—¶ä½¿ç”¨ç§»åŠ¨å¹³å‡
- éœ€è¦Î³(ç¼©æ”¾)å’ŒÎ²(åç§»)å¯å­¦ä¹ å‚æ•°

**é—®é¢˜**:
- ä¾èµ–batchå¤§å°ï¼Œå°batchæ•ˆæœå·®
- è®­ç»ƒå’Œæ¨ç†ä¸ä¸€è‡´
- åœ¨åºåˆ—æ¨¡å‹ä¸­æ•ˆæœä¸ä½³

#### 2. Layer Normalization  
```math
LN(x) = Î³ Ã— \frac{x - Î¼_L}{\sqrt{Ïƒ_L^2 + Îµ}} + Î²
```

**æ ¸å¿ƒç‰¹ç‚¹**:
- Î¼_L, Ïƒ_L: åœ¨ç‰¹å¾ç»´åº¦è®¡ç®—å‡å€¼å’Œæ–¹å·®
- æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹å½’ä¸€åŒ–ï¼Œä¸ä¾èµ–å…¶ä»–æ ·æœ¬
- è®­ç»ƒå’Œæ¨ç†ä¸€è‡´

**ä¼˜åŠ¿**:
- é€‚åˆå˜é•¿åºåˆ—
- ä¸å—batchå¤§å°å½±å“
- Transformerçš„æ ‡å‡†é€‰æ‹©

#### 3. RMS Normalization
```math
RMSNorm(x) = Î³ Ã— \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^d x_i^2 + Îµ}}
```

**æ ¸å¿ƒç‰¹ç‚¹**:
- åªè®¡ç®—RMSï¼Œä¸å‡å»å‡å€¼
- ç®€åŒ–äº†LayerNormçš„è®¡ç®—
- åªéœ€è¦Î³å‚æ•°ï¼Œæ— éœ€Î²

**ä¼˜åŠ¿**:
- è®¡ç®—æˆæœ¬æ›´ä½
- åœ¨å¤§æ¨¡å‹ä¸­æ•ˆæœä¸è¾“LayerNorm
- å†…å­˜å‹å¥½

### Pre-Norm vs Post-Norm

#### Post-Norm (åŸå§‹Transformer)
```
Input â†’ Attention â†’ Add â†’ LayerNorm â†’ FFN â†’ Add â†’ LayerNorm â†’ Output
```

**ç‰¹ç‚¹**:
- å½’ä¸€åŒ–åœ¨æ®‹å·®è¿æ¥ä¹‹å
- éœ€è¦å­¦ä¹ ç‡warmupæ‰èƒ½ç¨³å®šè®­ç»ƒ
- æµ…å±‚æ¨¡å‹(â‰¤6å±‚)æ•ˆæœæ›´å¥½
- æ¢¯åº¦ä¼ æ’­å¯èƒ½ä¸ç¨³å®š

#### Pre-Norm (ç°ä»£ä¸»æµ)
```  
Input â†’ LayerNorm â†’ Attention â†’ Add â†’ LayerNorm â†’ FFN â†’ Add â†’ Output
```

**ç‰¹ç‚¹**:
- å½’ä¸€åŒ–åœ¨æ®‹å·®è¿æ¥ä¹‹å‰
- è®­ç»ƒæ›´ç¨³å®šï¼Œæ— éœ€warmup
- æ·±å±‚æ¨¡å‹è®­ç»ƒæ›´å®¹æ˜“
- ç°ä»£å¤§æ¨¡å‹çš„æ ‡å‡†é€‰æ‹©

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: Batch Normå’ŒLayer Normçš„åŒºåˆ«ï¼Ÿ

**æ ¸å¿ƒåŒºåˆ«**:

1. **å½’ä¸€åŒ–ç»´åº¦ä¸åŒ**:
   - BatchNorm: åœ¨batchç»´åº¦å½’ä¸€åŒ–ï¼Œæ¯ä¸ªç‰¹å¾ç‹¬ç«‹
   - LayerNorm: åœ¨ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–ï¼Œæ¯ä¸ªæ ·æœ¬ç‹¬ç«‹

2. **åº”ç”¨åœºæ™¯**:
   - BatchNorm: CNNã€è§†è§‰ä»»åŠ¡ã€å¤§batchè®­ç»ƒ
   - LayerNorm: NLPã€åºåˆ—æ¨¡å‹ã€å°batchæˆ–å˜é•¿åºåˆ—

3. **ä¾èµ–æ€§**:
   - BatchNorm: ä¾èµ–batchå¤§å°å’Œå…¶ä»–æ ·æœ¬
   - LayerNorm: åªä¾èµ–å½“å‰æ ·æœ¬ï¼Œæ›´ç¨³å®š

**æŠ€æœ¯ç»†èŠ‚**:
```python
# BatchNorm: åœ¨batchç»´åº¦è®¡ç®—ç»Ÿè®¡é‡
batch_mean = x.mean(dim=0)  # [features]
batch_var = x.var(dim=0)    # [features]

# LayerNorm: åœ¨ç‰¹å¾ç»´åº¦è®¡ç®—ç»Ÿè®¡é‡  
layer_mean = x.mean(dim=-1, keepdim=True)  # [batch, 1]
layer_var = x.var(dim=-1, keepdim=True)    # [batch, 1]
```

### Q2: ä¸ºä»€ä¹ˆç°åœ¨ç”¨RMSNormï¼Ÿ

**ä¸»è¦åŸå› **:

1. **è®¡ç®—æ•ˆç‡**:
   - çœç•¥äº†å‡å€¼è®¡ç®—ï¼Œå‡å°‘äº†çº¦15%çš„è®¡ç®—é‡
   - å†…å­˜è®¿é—®æ›´å°‘ï¼Œå¯¹GPUæ›´å‹å¥½

2. **æ•ˆæœç›¸å½“**:
   - å¤§é‡å®éªŒè¯æ˜RMSNormæ•ˆæœä¸è¾“LayerNorm
   - åœ¨å¤§æ¨¡å‹ä¸­è¡¨ç°ç”šè‡³æ›´å¥½

3. **ç®€åŒ–å®ç°**:
   - ä¸éœ€è¦Î²å‚æ•°ï¼Œå‡å°‘äº†å‚æ•°é‡
   - æ•°å€¼ç¨³å®šæ€§æ›´å¥½

**æŠ€æœ¯åŸç†**:
```python
# LayerNorméœ€è¦è®¡ç®—å‡å€¼å’Œæ–¹å·®
mean = x.mean(dim=-1, keepdim=True)
var = x.var(dim=-1, keepdim=True)  
ln_out = gamma * (x - mean) / sqrt(var + eps) + beta

# RMSNormåªéœ€è¦è®¡ç®—RMS
rms = sqrt(x.pow(2).mean(dim=-1, keepdim=True) + eps)
rms_out = gamma * x / rms
```

### Q3: Pre-Normå’ŒPost-Normçš„ä½ç½®åŒºåˆ«ï¼Ÿ

**æ¶æ„å¯¹æ¯”**:

**Post-Norm (åŸå§‹)**:
```
x â†’ Attention â†’ (+) â†’ LayerNorm â†’ FFN â†’ (+) â†’ LayerNorm
    â†‘_______________|              â†‘_______|
```

**Pre-Norm (ç°ä»£)**:
```
x â†’ LayerNorm â†’ Attention â†’ (+) â†’ LayerNorm â†’ FFN â†’ (+)
    â†‘________________________|              â†‘______|
```

**è®­ç»ƒç¨³å®šæ€§å·®å¼‚**:

| æ–¹é¢ | Post-Norm | Pre-Norm |
|------|-----------|----------|
| **å­¦ä¹ ç‡warmup** | å¿…éœ€ | å¯é€‰ |
| **æ·±å±‚è®­ç»ƒ** | å®¹æ˜“å¤±è´¥ | ç¨³å®š |
| **æ¢¯åº¦ä¼ æ’­** | å¯èƒ½ä¸ç¨³å®š | æ›´å¹³æ»‘ |
| **æ”¶æ•›é€Ÿåº¦** | è¾ƒæ…¢ | è¾ƒå¿« |
| **æœ€ç»ˆæ€§èƒ½** | æµ…å±‚æ›´å¥½ | æ·±å±‚æ›´ä¼˜ |

### Q4: ä¸ºä»€ä¹ˆPre-Normè®­ç»ƒæ›´ç¨³å®šï¼Ÿ

**æ¢¯åº¦ä¼ æ’­åˆ†æ**:

1. **Post-Normé—®é¢˜**:
   - æ¢¯åº¦éœ€è¦ç»è¿‡LayerNormçš„åå‘ä¼ æ’­
   - LayerNormçš„å¯¼æ•°å¯èƒ½æ”¾å¤§æˆ–ç¼©å°æ¢¯åº¦
   - æ·±å±‚ç½‘ç»œå®¹æ˜“å‡ºç°æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±

2. **Pre-Normä¼˜åŠ¿**:
   - æä¾›äº†æ›´å¼ºçš„æ’ç­‰è·¯å¾„(identity path)
   - æ¢¯åº¦å¯ä»¥æ›´ç›´æ¥åœ°åå‘ä¼ æ’­
   - åœ¨æ·±å±‚ç½‘ç»œä¸­æ¢¯åº¦çš„æ•°é‡çº§ä¸ºâˆšL (Lä¸ºå±‚æ•°)

**æ•°å­¦ç›´è§‰**:
```
Post-Norm: æ¢¯åº¦éœ€è¦ç©¿è¿‡LayerNorm
âˆ‡L/âˆ‚x = âˆ‡L/âˆ‚norm Ã— âˆ‚norm/âˆ‚x  (ä¸ç¨³å®š)

Pre-Norm: æ’ç­‰è·¯å¾„æ›´å¼º  
âˆ‡L/âˆ‚x = âˆ‡L/âˆ‚residual + âˆ‡L/âˆ‚processed  (æ›´ç¨³å®š)
```

## ğŸ’» ä»£ç å®ç°

### ä¸‰ç§Normçš„PyTorchå®ç°

```python
import torch
import torch.nn as nn
import math

class BatchNorm1d(nn.Module):
    """è‡ªå®ç°BatchNorm"""
    def __init__(self, num_features, eps=1e-5, momentum=0.9):
        super().__init__()
        self.num_features = num_features
        self.eps = eps
        self.momentum = momentum
        
        # å¯å­¦ä¹ å‚æ•°
        self.gamma = nn.Parameter(torch.ones(num_features))
        self.beta = nn.Parameter(torch.zeros(num_features))
        
        # ç§»åŠ¨å¹³å‡ç»Ÿè®¡é‡
        self.register_buffer('running_mean', torch.zeros(num_features))
        self.register_buffer('running_var', torch.ones(num_features))
    
    def forward(self, x):
        # x shape: [batch_size, num_features]
        if self.training:
            # è®¡ç®—å½“å‰batchçš„ç»Ÿè®¡é‡
            batch_mean = x.mean(dim=0)
            batch_var = x.var(dim=0, unbiased=False)
            
            # æ›´æ–°ç§»åŠ¨å¹³å‡
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var
            
            # ä½¿ç”¨å½“å‰batchç»Ÿè®¡é‡å½’ä¸€åŒ–
            mean, var = batch_mean, batch_var
        else:
            # æ¨ç†æ—¶ä½¿ç”¨ç§»åŠ¨å¹³å‡
            mean, var = self.running_mean, self.running_var
        
        # å½’ä¸€åŒ–
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        return self.gamma * x_norm + self.beta

class LayerNorm(nn.Module):
    """è‡ªå®ç°LayerNorm"""
    def __init__(self, normalized_shape, eps=1e-5):
        super().__init__()
        self.normalized_shape = normalized_shape
        self.eps = eps
        
        self.gamma = nn.Parameter(torch.ones(normalized_shape))
        self.beta = nn.Parameter(torch.zeros(normalized_shape))
    
    def forward(self, x):
        # åœ¨æœ€åä¸€ä¸ªç»´åº¦è®¡ç®—ç»Ÿè®¡é‡
        mean = x.mean(dim=-1, keepdim=True)
        var = x.var(dim=-1, keepdim=True, unbiased=False)
        
        # å½’ä¸€åŒ–
        x_norm = (x - mean) / torch.sqrt(var + self.eps)
        return self.gamma * x_norm + self.beta

class RMSNorm(nn.Module):
    """è‡ªå®ç°RMSNorm"""
    def __init__(self, normalized_shape, eps=1e-5):
        super().__init__()
        self.normalized_shape = normalized_shape
        self.eps = eps
        
        self.gamma = nn.Parameter(torch.ones(normalized_shape))
    
    def forward(self, x):
        # è®¡ç®—RMS
        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        
        # RMSå½’ä¸€åŒ–
        x_norm = x / rms
        return self.gamma * x_norm

# ä½¿ç”¨ç¤ºä¾‹å’Œæ€§èƒ½å¯¹æ¯”
def compare_normalization():
    """å¯¹æ¯”ä¸‰ç§å½’ä¸€åŒ–çš„è®¡ç®—æˆæœ¬"""
    
    batch_size, seq_len, d_model = 32, 512, 768
    x = torch.randn(batch_size, seq_len, d_model)
    
    # åˆå§‹åŒ–ä¸‰ç§norm
    bn = BatchNorm1d(d_model)
    ln = LayerNorm(d_model)
    rms = RMSNorm(d_model)
    
    print("=== å½’ä¸€åŒ–æŠ€æœ¯å¯¹æ¯” ===")
    
    # æµ‹è¯•LayerNorm
    import time
    start_time = time.time()
    for _ in range(1000):
        _ = ln(x)
    ln_time = time.time() - start_time
    print(f"LayerNormè€—æ—¶: {ln_time:.4f}ç§’")
    
    # æµ‹è¯•RMSNorm
    start_time = time.time()
    for _ in range(1000):
        _ = rms(x)
    rms_time = time.time() - start_time
    print(f"RMSNormè€—æ—¶: {rms_time:.4f}ç§’")
    
    speedup = ln_time / rms_time
    print(f"RMSNormåŠ é€Ÿå€æ•°: {speedup:.2f}x")
    
    # å‚æ•°é‡å¯¹æ¯”
    ln_params = sum(p.numel() for p in ln.parameters())
    rms_params = sum(p.numel() for p in rms.parameters())
    print(f"LayerNormå‚æ•°é‡: {ln_params}")
    print(f"RMSNormå‚æ•°é‡: {rms_params}")
    print(f"å‚æ•°å‡å°‘: {(ln_params - rms_params) / ln_params * 100:.1f}%")

# Pre-Norm vs Post-Normå®ç°
class PostNormBlock(nn.Module):
    """Post-Norm Transformer Block"""
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.ReLU(),
            nn.Linear(4 * d_model, d_model)
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
    
    def forward(self, x):
        # Post-Norm: Attention â†’ Add â†’ Norm
        attn_out, _ = self.attention(x, x, x)
        x = self.norm1(x + attn_out)
        
        # FFN â†’ Add â†’ Norm
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)
        return x

class PreNormBlock(nn.Module):
    """Pre-Norm Transformer Block"""
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
        self.ffn = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.ReLU(),
            nn.Linear(4 * d_model, d_model)
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
    
    def forward(self, x):
        # Pre-Norm: Norm â†’ Attention â†’ Add
        norm_x = self.norm1(x)
        attn_out, _ = self.attention(norm_x, norm_x, norm_x)
        x = x + attn_out
        
        # Norm â†’ FFN â†’ Add
        norm_x = self.norm2(x)
        ffn_out = self.ffn(norm_x)
        x = x + ffn_out
        return x

if __name__ == "__main__":
    compare_normalization()
```

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] ç†è§£ä¸‰ç§å½’ä¸€åŒ–çš„æ•°å­¦åŸç†å’Œè®¡ç®—æ–¹å¼
- [ ] èƒ½è§£é‡Šä¸ºä»€ä¹ˆTransformeré€‰æ‹©LayerNormè€ŒéBatchNorm
- [ ] æŒæ¡Pre-Normç›¸æ¯”Post-Normçš„è®­ç»ƒä¼˜åŠ¿
- [ ] å®Œæˆå½’ä¸€åŒ–æŠ€æœ¯çš„ä»£ç å®ç°å’Œæ€§èƒ½å¯¹æ¯”

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸Šä¸€èŠ‚ï¼šKV CacheæŠ€æœ¯](kv-cache.md)
- [ä¸‹ä¸€èŠ‚ï¼šä½ç½®ç¼–ç ](positional-encoding.md)
- [è¿”å›ï¼šAttentionå‡çº§æ¦‚è§ˆ](index.md)