# MOE架构

## 🎯 本节目标

理解专家混合模型(Mixture of Experts)的核心原理和应用优势。

## 📝 知识总结

### MOE是什么？

**Mixture of Experts (MOE)** 是一种稀疏激活的神经网络架构，通过路由机制将不同的输入分配给不同的"专家"子网络处理。

### 核心优势

1. **参数效率**: 虽然总参数量大，但每次前向传播只激活部分参数
2. **扩展能力**: 可以通过增加专家数量来扩展模型容量
3. **专业分工**: 不同专家可以学习处理不同类型的输入

## 💬 面试问题解答

### Q1: MOE是什么，它有什么好处呢？

**简洁回答**: MOE是专家混合模型，通过稀疏激活机制让不同的子网络(专家)处理不同的输入，在保持计算量相对稳定的情况下大幅增加模型容量。

**详细解释**:
- **工作原理**: 输入通过门控网络选择激活少数几个专家
- **核心优势**: 参数量大但计算量可控
- **实际应用**: Google的Switch Transformer等大模型

## ✅ 学习检验

- [ ] 理解MOE的稀疏激活原理
- [ ] 能解释MOE相比稠密模型的优势

## 🔗 相关链接

- [下一节：分布式训练](distributed.md)
- [返回：LLM升级技术概览](index.md)