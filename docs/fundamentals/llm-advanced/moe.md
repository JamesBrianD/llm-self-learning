# MOEæ¶æ„

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æ·±å…¥ç†è§£ä¸“å®¶æ··åˆæ¨¡å‹(Mixture of Experts)çš„æ ¸å¿ƒåŸç†ã€è·¯ç”±æœºåˆ¶å’Œå·¥ç¨‹å®ç°æŒ‘æˆ˜ã€‚

## ğŸ“ çŸ¥è¯†æ€»ç»“

### MOEåŸºæœ¬æ¦‚å¿µ

**Mixture of Experts (MOE)** æ˜¯ä¸€ç§ç¨€ç–æ¿€æ´»çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œé€šè¿‡åŠ¨æ€è·¯ç”±æœºåˆ¶å°†ä¸åŒè¾“å…¥åˆ†é…ç»™ä¸“é—¨çš„"ä¸“å®¶"å­ç½‘ç»œå¤„ç†ã€‚

#### æ ¸å¿ƒæ€æƒ³
- **æ¡ä»¶è®¡ç®—**: æ ¹æ®è¾“å…¥å†…å®¹åŠ¨æ€é€‰æ‹©è®¡ç®—è·¯å¾„
- **ä¸“å®¶åˆ†å·¥**: ä¸åŒä¸“å®¶å­¦ä¹ å¤„ç†ä¸åŒç±»å‹çš„æ¨¡å¼
- **ç¨€ç–æ¿€æ´»**: æ¯æ¬¡åªæ¿€æ´»å°‘æ•°ä¸“å®¶ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦

### MOEæ ¸å¿ƒç»„ä»¶

#### 1. ä¸“å®¶ç½‘ç»œ (Experts)
```python
class Expert(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.w1 = nn.Linear(d_model, d_ff)
        self.w2 = nn.Linear(d_ff, d_model)
        self.activation = nn.ReLU()
    
    def forward(self, x):
        return self.w2(self.activation(self.w1(x)))

# å¤šä¸ªä¸“å®¶ç»„æˆä¸“å®¶æ± 
experts = nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_experts)])
```

#### 2. è·¯ç”±ç½‘ç»œ (Router/Gate)
```python
class Router(nn.Module):
    def __init__(self, d_model, num_experts):
        super().__init__()
        self.gate = nn.Linear(d_model, num_experts)
    
    def forward(self, x):
        # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„é—¨æ§åˆ†æ•°
        logits = self.gate(x)  # [batch, seq_len, num_experts]
        
        # é€‰æ‹©Top-Kä¸“å®¶
        top_k_logits, top_k_indices = torch.topk(logits, k=2, dim=-1)
        top_k_probs = F.softmax(top_k_logits, dim=-1)
        
        return top_k_probs, top_k_indices
```

#### 3. èšåˆæœºåˆ¶
```python
def moe_forward(x, experts, router):
    # è·å–è·¯ç”±ä¿¡æ¯
    probs, indices = router(x)  # [batch, seq_len, k], [batch, seq_len, k]
    
    # åˆå§‹åŒ–è¾“å‡º
    output = torch.zeros_like(x)
    
    # å¯¹æ¯ä¸ªé€‰ä¸­çš„ä¸“å®¶è®¡ç®—è¾“å‡º
    for i in range(k):
        expert_idx = indices[:, :, i]
        expert_prob = probs[:, :, i]
        
        # è·å–å¯¹åº”ä¸“å®¶çš„è¾“å‡º
        expert_output = experts[expert_idx](x)
        
        # æŒ‰æ¦‚ç‡åŠ æƒ
        output += expert_prob.unsqueeze(-1) * expert_output
    
    return output
```

### è·¯ç”±ç­–ç•¥è¯¦è§£

#### 1. Token-Choiceè·¯ç”±
**æœºåˆ¶**: æ¯ä¸ªtokené€‰æ‹©top-kä¸ªä¸“å®¶
```python
def token_choice_routing(x, num_experts, k=2):
    """æ¯ä¸ªtokené€‰æ‹©kä¸ªä¸“å®¶"""
    batch_size, seq_len, d_model = x.shape
    
    # è·¯ç”±æ‰“åˆ†
    router_logits = router(x)  # [batch, seq_len, num_experts]
    
    # é€‰æ‹©top-kä¸“å®¶
    top_k_probs, top_k_indices = torch.topk(router_logits, k, dim=-1)
    top_k_probs = F.softmax(top_k_probs, dim=-1)
    
    return top_k_probs, top_k_indices
```

**ä¼˜åŠ¿**: 
- ä¿è¯æ¯ä¸ªtokenéƒ½è¢«å¤„ç†
- æ§åˆ¶è®¡ç®—å¤æ‚åº¦ç¨³å®š

**åŠ£åŠ¿**:
- å¯èƒ½å¯¼è‡´ä¸“å®¶è´Ÿè½½ä¸å‡è¡¡
- éƒ¨åˆ†ä¸“å®¶å¯èƒ½å¾—ä¸åˆ°è®­ç»ƒ

#### 2. Expert-Choiceè·¯ç”±
**æœºåˆ¶**: æ¯ä¸ªä¸“å®¶é€‰æ‹©top-kä¸ªtoken
```python
def expert_choice_routing(x, num_experts, capacity):
    """æ¯ä¸ªä¸“å®¶é€‰æ‹©å›ºå®šæ•°é‡çš„token"""
    batch_size, seq_len, d_model = x.shape
    
    # è·¯ç”±æ‰“åˆ†
    router_logits = router(x)  # [batch, seq_len, num_experts]
    
    # ä¸ºæ¯ä¸ªä¸“å®¶é€‰æ‹©top tokens
    expert_assignments = {}
    for expert_id in range(num_experts):
        expert_scores = router_logits[:, :, expert_id]
        top_tokens = torch.topk(expert_scores.flatten(), capacity).indices
        expert_assignments[expert_id] = top_tokens
    
    return expert_assignments
```

**ä¼˜åŠ¿**:
- è‡ªç„¶çš„è´Ÿè½½å‡è¡¡
- ä¸“å®¶èƒ½å¤Ÿé€‰æ‹©æœ€ç›¸å…³çš„è¾“å…¥

**åŠ£åŠ¿**:
- å¯èƒ½æœ‰tokenè¢«ä¸¢å¼ƒ
- å®ç°æ›´å¤æ‚

### è´Ÿè½½å‡è¡¡æŠ€æœ¯

#### 1. è¾…åŠ©æŸå¤±å‡½æ•°
```python
def load_balancing_loss(router_probs, expert_indices, num_experts):
    """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±"""
    # è®¡ç®—æ¯ä¸ªä¸“å®¶è¢«é€‰æ‹©çš„é¢‘ç‡
    expert_counts = torch.zeros(num_experts)
    for expert_id in range(num_experts):
        expert_counts[expert_id] = (expert_indices == expert_id).float().sum()
    
    # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä¸“å®¶å¤„ç†ç›¸åŒæ•°é‡çš„token
    ideal_count = expert_indices.numel() / num_experts
    
    # è®¡ç®—è´Ÿè½½ä¸å‡è¡¡æŸå¤±
    load_loss = torch.var(expert_counts) / (ideal_count ** 2)
    
    return load_loss

# æ€»æŸå¤± = ä¸»ä»»åŠ¡æŸå¤± + Î» * è´Ÿè½½å‡è¡¡æŸå¤±
total_loss = task_loss + lambda_balance * load_balancing_loss(probs, indices, num_experts)
```

#### 2. ä¸“å®¶å®¹é‡é™åˆ¶
```python
def capacity_limited_routing(router_logits, capacity_factor=1.25):
    """é™åˆ¶æ¯ä¸ªä¸“å®¶çš„å¤„ç†å®¹é‡"""
    num_tokens = router_logits.shape[0] * router_logits.shape[1]
    expert_capacity = int(capacity_factor * num_tokens / num_experts)
    
    # ä¸ºæ¯ä¸ªä¸“å®¶åˆ†é…å›ºå®šå®¹é‡
    expert_assignments = []
    expert_counts = torch.zeros(num_experts)
    
    for token_idx in range(num_tokens):
        # è·å–å½“å‰tokençš„ä¸“å®¶åå¥½
        token_probs = F.softmax(router_logits.flatten()[token_idx], dim=-1)
        
        # é€‰æ‹©å®¹é‡æœªæ»¡çš„æœ€ä¼˜ä¸“å®¶
        for expert_id in torch.argsort(token_probs, descending=True):
            if expert_counts[expert_id] < expert_capacity:
                expert_assignments.append((token_idx, expert_id))
                expert_counts[expert_id] += 1
                break
    
    return expert_assignments
```

### MOEå˜ä½“å’Œä¼˜åŒ–

#### 1. ç¨€ç–MOE vs å¯†é›†MOE
| ç‰¹æ€§ | ç¨€ç–MOE | å¯†é›†MOE |
|------|---------|---------|
| **æ¿€æ´»ä¸“å®¶æ•°** | Top-K (K<<N) | å…¨éƒ¨ä¸“å®¶ |
| **è®¡ç®—å¤æ‚åº¦** | O(K) | O(N) |
| **å‚æ•°åˆ©ç”¨ç‡** | ä½ | é«˜ |
| **æ‰©å±•æ€§** | å¥½ | å·® |

#### 2. å±‚çº§MOE
```python
class HierarchicalMoE(nn.Module):
    """å±‚çº§ä¸“å®¶æ··åˆæ¨¡å‹"""
    
    def __init__(self, d_model, num_coarse_experts, num_fine_experts):
        super().__init__()
        # ç²—ç²’åº¦ä¸“å®¶é€‰æ‹©
        self.coarse_router = Router(d_model, num_coarse_experts)
        
        # ç»†ç²’åº¦ä¸“å®¶ç»„
        self.fine_routers = nn.ModuleList([
            Router(d_model, num_fine_experts) 
            for _ in range(num_coarse_experts)
        ])
        
        # ä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_fine_experts)])
            for _ in range(num_coarse_experts)
        ])
    
    def forward(self, x):
        # ç¬¬ä¸€å±‚ï¼šé€‰æ‹©ç²—ç²’åº¦ä¸“å®¶
        coarse_probs, coarse_indices = self.coarse_router(x)
        
        output = torch.zeros_like(x)
        
        # ç¬¬äºŒå±‚ï¼šåœ¨é€‰ä¸­çš„ç²—ç²’åº¦ä¸“å®¶å†…é€‰æ‹©ç»†ç²’åº¦ä¸“å®¶
        for i, coarse_idx in enumerate(coarse_indices[0, 0]):  # ç®€åŒ–å¤„ç†
            fine_probs, fine_indices = self.fine_routers[coarse_idx](x)
            
            # è®¡ç®—ç»†ç²’åº¦ä¸“å®¶è¾“å‡º
            for j, fine_idx in enumerate(fine_indices[0, 0]):
                expert_output = self.experts[coarse_idx][fine_idx](x)
                weight = coarse_probs[0, 0, i] * fine_probs[0, 0, j]
                output += weight * expert_output
        
        return output
```

### åˆ†å¸ƒå¼è®­ç»ƒæŒ‘æˆ˜

#### 1. é€šä¿¡æ¨¡å¼
```python
# All-to-Allé€šä¿¡æ¨¡å¼
def all_to_all_communication(tokens, expert_assignments):
    """
    å°†tokensåˆ†å‘åˆ°ä¸åŒè®¾å¤‡ä¸Šçš„ä¸“å®¶
    """
    # Token dispatch: æ ¹æ®è·¯ç”±ç»“æœé‡æ–°åˆ†å¸ƒtoken
    expert_inputs = {}
    for expert_id, token_list in expert_assignments.items():
        device_id = expert_id % world_size
        expert_inputs[device_id] = expert_inputs.get(device_id, []) + token_list
    
    # è·¨è®¾å¤‡é€šä¿¡
    for device_id, tokens in expert_inputs.items():
        send_to_device(tokens, device_id)
    
    # Expert processing
    expert_outputs = process_on_experts(expert_inputs)
    
    # Token combine: æ”¶é›†ä¸“å®¶è¾“å‡º
    final_outputs = all_gather(expert_outputs)
    
    return final_outputs
```

#### 2. å†…å­˜ä¼˜åŒ–
```python
class MemoryEfficientMoE(nn.Module):
    """å†…å­˜é«˜æ•ˆçš„MOEå®ç°"""
    
    def __init__(self, d_model, num_experts, expert_capacity):
        super().__init__()
        self.num_experts = num_experts
        self.expert_capacity = expert_capacity
        
        # å…±äº«ä¸“å®¶å‚æ•°å­˜å‚¨
        self.expert_weights = nn.Parameter(torch.randn(num_experts, d_model, d_ff))
        self.router = Router(d_model, num_experts)
    
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        
        # è·å–è·¯ç”±å†³ç­–
        probs, indices = self.router(x)
        
        # é‡å¡‘ä¸ºä¸“å®¶æ‰¹å¤„ç†æ ¼å¼
        flat_x = x.view(-1, d_model)
        flat_probs = probs.view(-1, 2)
        flat_indices = indices.view(-1, 2)
        
        # æ‰¹é‡å¤„ç†å‡å°‘å†…å­˜å ç”¨
        outputs = []
        for batch_start in range(0, flat_x.shape[0], self.expert_capacity):
            batch_end = min(batch_start + self.expert_capacity, flat_x.shape[0])
            batch_output = self._process_batch(
                flat_x[batch_start:batch_end],
                flat_probs[batch_start:batch_end],
                flat_indices[batch_start:batch_end]
            )
            outputs.append(batch_output)
        
        # é‡ç»„è¾“å‡º
        final_output = torch.cat(outputs, dim=0)
        return final_output.view(batch_size, seq_len, d_model)
```

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: MOEæ˜¯ä»€ä¹ˆï¼Œå®ƒæœ‰ä»€ä¹ˆå¥½å¤„å‘¢ï¼Ÿ

**ç®€æ´å›ç­”**: MOEæ˜¯ä¸“å®¶æ··åˆæ¨¡å‹ï¼Œé€šè¿‡ç¨€ç–æ¿€æ´»æœºåˆ¶è®©ä¸åŒçš„å­ç½‘ç»œ(ä¸“å®¶)å¤„ç†ä¸åŒçš„è¾“å…¥ï¼Œåœ¨ä¿æŒè®¡ç®—é‡ç›¸å¯¹ç¨³å®šçš„æƒ…å†µä¸‹å¤§å¹…å¢åŠ æ¨¡å‹å®¹é‡ã€‚

**è¯¦ç»†è§£é‡Š**:
- **å·¥ä½œåŸç†**: è¾“å…¥é€šè¿‡é—¨æ§ç½‘ç»œé€‰æ‹©æ¿€æ´»å°‘æ•°å‡ ä¸ªä¸“å®¶
- **æ ¸å¿ƒä¼˜åŠ¿**: å‚æ•°é‡å¤§ä½†è®¡ç®—é‡å¯æ§ï¼Œå®ç°æ¡ä»¶è®¡ç®—
- **å®é™…åº”ç”¨**: Googleçš„Switch Transformerã€GLaMã€PaLMç­‰å¤§æ¨¡å‹

### Q2: MOEçš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ

**æ ¸å¿ƒæŒ‘æˆ˜**:

1. **è´Ÿè½½å‡è¡¡**: é˜²æ­¢æ‰€æœ‰è¾“å…¥éƒ½è·¯ç”±åˆ°å°‘æ•°ä¸“å®¶
2. **é€šä¿¡å¼€é”€**: åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„All-to-Allé€šä¿¡æˆæœ¬é«˜
3. **è®­ç»ƒä¸ç¨³å®š**: è·¯ç”±ç½‘ç»œçš„è®­ç»ƒå¯èƒ½ä¸æ”¶æ•›
4. **æ¨ç†å¤æ‚åº¦**: åŠ¨æ€è·¯ç”±å¢åŠ æ¨ç†æ—¶çš„è°ƒåº¦å¤æ‚æ€§

### Q3: å¦‚ä½•è§£å†³MOEçš„è´Ÿè½½å‡è¡¡é—®é¢˜ï¼Ÿ

**ä¸»è¦æ–¹æ³•**:

1. **è¾…åŠ©æŸå¤±**: æ·»åŠ é¼“åŠ±å‡åŒ€åˆ†å¸ƒçš„æ­£åˆ™åŒ–é¡¹
2. **ä¸“å®¶å®¹é‡é™åˆ¶**: é™åˆ¶æ¯ä¸ªä¸“å®¶å¤„ç†çš„tokenæ•°é‡
3. **Expert-Choiceè·¯ç”±**: è®©ä¸“å®¶ä¸»åŠ¨é€‰æ‹©è¦å¤„ç†çš„token
4. **å™ªå£°æ³¨å…¥**: åœ¨è·¯ç”±å†³ç­–ä¸­åŠ å…¥éšæœºæ€§

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] ç†è§£MOEçš„åŸºæœ¬æ¶æ„å’Œå·¥ä½œåŸç†
- [ ] æŒæ¡ä¸åŒè·¯ç”±ç­–ç•¥çš„ä¼˜åŠ£
- [ ] äº†è§£è´Ÿè½½å‡è¡¡çš„é‡è¦æ€§å’Œè§£å†³æ–¹æ¡ˆ
- [ ] ç†è§£MOEåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æŒ‘æˆ˜

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸‹ä¸€èŠ‚ï¼šåˆ†å¸ƒå¼è®­ç»ƒ](distributed.md)
- [ä¸‹ä¸€ç« ï¼šDeepSeekä¼˜åŒ–æŠ€æœ¯](../deepseek-innovations/index.md)
- [è¿”å›ï¼šLLMå‡çº§æŠ€æœ¯æ¦‚è§ˆ](index.md)