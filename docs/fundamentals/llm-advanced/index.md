# 第3节：LLM升级技术

## 🎯 学习目标

了解大语言模型的前沿优化技术，掌握MOE架构和分布式训练的基本概念。

**重点面试问题预览：**
- MOE是什么，有什么好处？
- 分布式训练的基本策略
- 大模型训练的工程挑战

## 📅 学习计划

**建议学习时间：1.5天**

- **Day 1**: MOE架构原理深入理解
- **半天**: 分布式训练基础概念

## 📚 学习路径

### 1. [MOE架构](moe.md)
- 专家混合模型原理
- 稀疏激活的优势
- 工程实现挑战

### 2. [分布式训练](distributed.md)
- 数据并行 vs 模型并行
- 流水线并行
- 通信优化策略

## ✅ 学习检验标准

完成以下两项才算掌握本节：

1. **问题解答**: 能解释MOE的工作原理和优势
2. **概念理解**: 理解大模型训练的分布式策略

## 💡 学习建议

这一节内容相对基础，主要以概念理解为主：
- **MOE部分**: 重点理解稀疏激活的优势和挑战
- **分布式训练**: 了解基本概念即可，无需深入工程细节
- **学习重点**: 为后续DeepSeek MoE等高级技术打下基础

## 🚀 开始学习

选择模块开始学习，重点掌握核心概念和原理。