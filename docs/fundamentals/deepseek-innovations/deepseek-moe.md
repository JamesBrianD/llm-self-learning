# DeepSeek MoEåˆ›æ–°

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æ·±å…¥ç†è§£DeepSeekåœ¨ä¸“å®¶æ··åˆæ¨¡å‹ä¸Šçš„åˆ›æ–°è®¾è®¡ï¼ŒæŒæ¡ç»†ç²’åº¦ä¸“å®¶å’Œå…±äº«ä¸“å®¶çš„æ ¸å¿ƒç†å¿µã€‚

## ğŸ“ æŠ€æœ¯åˆ›æ–°è§£æ

### DeepSeek MoEæ¼”è¿›å†ç¨‹

#### ç‰ˆæœ¬æ¼”è¿›
```
DeepSeek MoEæŠ€æœ¯æ¼”è¿›
â”œâ”€â”€ DeepSeek V1 (2023)
â”‚   â”œâ”€â”€ åŸºç¡€MoEæ¶æ„
â”‚   â””â”€â”€ æ ‡å‡†Token-Choiceè·¯ç”±
â”œâ”€â”€ DeepSeek V2 (2024)
â”‚   â”œâ”€â”€ ç»†ç²’åº¦ä¸“å®¶è®¾è®¡
â”‚   â”œâ”€â”€ å…±äº«ä¸“å®¶æœºåˆ¶
â”‚   â””â”€â”€ å¤šçº§è´Ÿè½½å‡è¡¡
â””â”€â”€ DeepSeek V3 (2024)
    â”œâ”€â”€ ä¼˜åŒ–è·¯ç”±ç­–ç•¥
    â”œâ”€â”€ åŠ¨æ€ä¸“å®¶å®¹é‡
    â””â”€â”€ æ›´ç²¾ç»†çš„è´Ÿè½½æ§åˆ¶
```

### æ ¸å¿ƒåˆ›æ–°æŠ€æœ¯

#### 1. ç»†ç²’åº¦ä¸“å®¶è®¾è®¡

**ä¼ ç»Ÿé—®é¢˜**: ç²—ç²’åº¦ä¸“å®¶å¯¼è‡´çš„ä¸“ä¸šåŒ–ä¸è¶³

**DeepSeekè§£å†³æ–¹æ¡ˆ**: ç»†ç²’åº¦ä¸“å®¶åˆ†å·¥

```python
# ä¼ ç»Ÿç²—ç²’åº¦ä¸“å®¶
class CoarseGrainedExpert(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.ffn = FeedForward(d_model, d_ff)  # å®Œæ•´FFN
    
    def forward(self, x):
        return self.ffn(x)

# DeepSeekç»†ç²’åº¦ä¸“å®¶
class FineGrainedExpert(nn.Module):
    def __init__(self, d_model, d_ff, expert_type='gate', shared_gate=None):
        super().__init__()
        self.expert_type = expert_type
        
        if expert_type == 'gate':
            # é—¨æ§ä¸“å®¶ï¼šåªè´Ÿè´£é—¨æ§æ¿€æ´»
            self.gate_proj = nn.Linear(d_model, d_ff, bias=False)
        elif expert_type == 'up':
            # ä¸ŠæŠ•å½±ä¸“å®¶ï¼šè´Ÿè´£ç‰¹å¾æå–
            self.up_proj = nn.Linear(d_model, d_ff, bias=False)
        elif expert_type == 'down':
            # ä¸‹æŠ•å½±ä¸“å®¶ï¼šè´Ÿè´£è¾“å‡ºæŠ•å½±
            self.down_proj = nn.Linear(d_ff, d_model, bias=False)
        
        self.shared_gate = shared_gate
    
    def forward(self, x):
        if self.expert_type == 'gate':
            return self.gate_proj(x)
        elif self.expert_type == 'up':
            return self.up_proj(x)
        elif self.expert_type == 'down':
            return self.down_proj(x)
```

**ç»†ç²’åº¦ä¸“å®¶çš„ä¼˜åŠ¿**:
- **æ›´ç²¾ç»†çš„ä¸“ä¸šåŒ–**: æ¯ç§æ“ä½œç±»å‹éƒ½æœ‰ä¸“é—¨çš„ä¸“å®¶
- **æ›´å¥½çš„å‚æ•°åˆ©ç”¨**: é¿å…äº†ä¸“å®¶å†…éƒ¨çš„å†—ä½™
- **çµæ´»çš„ç»„åˆ**: å¯ä»¥åŠ¨æ€ç»„åˆä¸åŒç±»å‹çš„ä¸“å®¶

#### 2. å…±äº«ä¸“å®¶æœºåˆ¶

**è®¾è®¡ç†å¿µ**: éƒ¨åˆ†çŸ¥è¯†å¯¹æ‰€æœ‰è¾“å…¥éƒ½æœ‰ç”¨ï¼Œåº”è¯¥è¢«å…±äº«

```python
class DeepSeekMoELayer(nn.Module):
    """DeepSeek MoEå±‚å®ç°"""
    
    def __init__(self, d_model, num_experts, num_shared_experts, 
                 expert_capacity, d_ff):
        super().__init__()
        
        # å…±äº«ä¸“å®¶ï¼šå§‹ç»ˆæ¿€æ´»
        self.shared_experts = nn.ModuleList([
            FeedForward(d_model, d_ff) 
            for _ in range(num_shared_experts)
        ])
        
        # è·¯ç”±ä¸“å®¶ï¼šåŠ¨æ€é€‰æ‹©
        self.routed_experts = nn.ModuleList([
            FeedForward(d_model, d_ff)
            for _ in range(num_experts)
        ])
        
        # è·¯ç”±ç½‘ç»œ
        self.router = Router(d_model, num_experts)
        
        # ä¸“å®¶é…ç½®
        self.num_experts = num_experts
        self.num_shared_experts = num_shared_experts
        self.expert_capacity = expert_capacity
    
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        
        # 1. å…±äº«ä¸“å®¶å¤„ç†ï¼ˆå§‹ç»ˆæ¿€æ´»ï¼‰
        shared_output = torch.zeros_like(x)
        for shared_expert in self.shared_experts:
            shared_output += shared_expert(x) / self.num_shared_experts
        
        # 2. è·¯ç”±ä¸“å®¶å¤„ç†ï¼ˆåŠ¨æ€é€‰æ‹©ï¼‰
        router_probs, expert_indices = self.router(x)
        routed_output = self.route_to_experts(x, router_probs, expert_indices)
        
        # 3. ç»„åˆå…±äº«å’Œè·¯ç”±è¾“å‡º
        final_output = shared_output + routed_output
        
        return final_output
    
    def route_to_experts(self, x, probs, indices):
        """è·¯ç”±åˆ°ä¸“å®¶çš„è¯¦ç»†å®ç°"""
        output = torch.zeros_like(x)
        
        # Token-choiceè·¯ç”±ç­–ç•¥
        for i in range(2):  # Top-2è·¯ç”±
            expert_idx = indices[:, :, i]
            expert_prob = probs[:, :, i]
            
            # ä¸“å®¶å®¹é‡é™åˆ¶
            expert_tokens = self.apply_capacity_limit(x, expert_idx, expert_prob)
            
            # ä¸“å®¶å¤„ç†
            for expert_id in range(self.num_experts):
                mask = (expert_idx == expert_id)
                if mask.any():
                    expert_input = x[mask]
                    expert_output = self.routed_experts[expert_id](expert_input)
                    output[mask] += expert_prob[mask].unsqueeze(-1) * expert_output
        
        return output
```

**å…±äº«ä¸“å®¶çš„ä½œç”¨**:
- **é€šç”¨çŸ¥è¯†**: å­˜å‚¨å¯¹æ‰€æœ‰è¾“å…¥éƒ½æœ‰ç”¨çš„åŸºç¡€çŸ¥è¯†
- **ç¨³å®šåŸºçº¿**: ä¸ºæ¨¡å‹æä¾›ç¨³å®šçš„åŸºç¡€è¾“å‡º
- **è´Ÿè½½åˆ†æ‹…**: å‡è½»è·¯ç”±ä¸“å®¶çš„è´Ÿæ‹…

#### 3. å¤šçº§è´Ÿè½½å‡è¡¡

**æŒ‘æˆ˜**: ä¸“å®¶è´Ÿè½½ä¸å‡è¡¡å¯¼è‡´çš„è®­ç»ƒä¸ç¨³å®š

**DeepSeekçš„å¤šçº§è§£å†³æ–¹æ¡ˆ**:

##### è®¾å¤‡çº§è´Ÿè½½å‡è¡¡
```python
def device_level_load_balancing(expert_assignments, world_size):
    """è®¾å¤‡çº§åˆ«çš„è´Ÿè½½å‡è¡¡"""
    
    # ç»Ÿè®¡æ¯ä¸ªè®¾å¤‡ä¸Šçš„tokenæ•°é‡
    device_loads = torch.zeros(world_size)
    
    for expert_id, tokens in expert_assignments.items():
        device_id = expert_id % world_size
        device_loads[device_id] += len(tokens)
    
    # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
    ideal_load = device_loads.sum() / world_size
    load_variance = torch.var(device_loads)
    
    device_balance_loss = load_variance / (ideal_load ** 2)
    
    return device_balance_loss

def expert_level_load_balancing(router_probs, expert_indices):
    """ä¸“å®¶çº§åˆ«çš„è´Ÿè½½å‡è¡¡"""
    
    num_experts = router_probs.size(-1)
    
    # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„é€‰æ‹©é¢‘ç‡
    expert_frequencies = torch.zeros(num_experts)
    for expert_id in range(num_experts):
        expert_frequencies[expert_id] = (expert_indices == expert_id).float().sum()
    
    # ç†æƒ³é¢‘ç‡
    total_selections = expert_indices.numel()
    ideal_frequency = total_selections / num_experts
    
    # è®¡ç®—å‡è¡¡æŸå¤±
    expert_balance_loss = torch.var(expert_frequencies) / (ideal_frequency ** 2)
    
    return expert_balance_loss
```

##### åŠ¨æ€å®¹é‡è°ƒæ•´
```python
class DynamicCapacityRouter(nn.Module):
    """åŠ¨æ€å®¹é‡è°ƒæ•´è·¯ç”±å™¨"""
    
    def __init__(self, d_model, num_experts, base_capacity_factor=1.25):
        super().__init__()
        self.gate = nn.Linear(d_model, num_experts)
        self.base_capacity_factor = base_capacity_factor
        self.expert_utilization = torch.ones(num_experts)  # ä¸“å®¶åˆ©ç”¨ç‡è·Ÿè¸ª
    
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        num_tokens = batch_size * seq_len
        
        # è®¡ç®—åŸºç¡€å®¹é‡
        base_capacity = int(self.base_capacity_factor * num_tokens / self.num_experts)
        
        # æ ¹æ®å†å²åˆ©ç”¨ç‡åŠ¨æ€è°ƒæ•´å®¹é‡
        adjusted_capacities = []
        for expert_id in range(self.num_experts):
            utilization = self.expert_utilization[expert_id]
            
            if utilization < 0.5:  # åˆ©ç”¨ç‡ä½ï¼Œå‡å°‘å®¹é‡
                adjusted_capacity = int(base_capacity * 0.8)
            elif utilization > 1.5:  # åˆ©ç”¨ç‡é«˜ï¼Œå¢åŠ å®¹é‡
                adjusted_capacity = int(base_capacity * 1.2)
            else:
                adjusted_capacity = base_capacity
            
            adjusted_capacities.append(adjusted_capacity)
        
        # è·¯ç”±è®¡ç®—
        logits = self.gate(x)
        return self.route_with_dynamic_capacity(x, logits, adjusted_capacities)
    
    def update_utilization(self, expert_assignments):
        """æ›´æ–°ä¸“å®¶åˆ©ç”¨ç‡ç»Ÿè®¡"""
        current_utilization = torch.zeros(self.num_experts)
        
        for expert_id, tokens in expert_assignments.items():
            current_utilization[expert_id] = len(tokens)
        
        # æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°
        alpha = 0.1
        self.expert_utilization = (1 - alpha) * self.expert_utilization + alpha * current_utilization
```

#### 4. è·¯ç”±ç­–ç•¥ä¼˜åŒ–

##### Expert-Choice vs Token-Choiceæ··åˆè·¯ç”±
```python
class HybridRouter(nn.Module):
    """æ··åˆè·¯ç”±ç­–ç•¥"""
    
    def __init__(self, d_model, num_experts):
        super().__init__()
        self.gate = nn.Linear(d_model, num_experts)
        self.routing_strategy = 'adaptive'  # adaptive, token_choice, expert_choice
    
    def forward(self, x):
        logits = self.gate(x)
        
        if self.routing_strategy == 'token_choice':
            return self.token_choice_routing(x, logits)
        elif self.routing_strategy == 'expert_choice':
            return self.expert_choice_routing(x, logits)
        else:  # adaptive
            return self.adaptive_routing(x, logits)
    
    def adaptive_routing(self, x, logits):
        """è‡ªé€‚åº”è·¯ç”±ç­–ç•¥"""
        batch_size, seq_len, _ = x.shape
        
        # æ ¹æ®è´Ÿè½½æƒ…å†µåŠ¨æ€é€‰æ‹©è·¯ç”±ç­–ç•¥
        current_load = self.estimate_current_load()
        
        if current_load > 0.8:  # é«˜è´Ÿè½½æ—¶ä½¿ç”¨expert-choice
            return self.expert_choice_routing(x, logits)
        else:  # ä½è´Ÿè½½æ—¶ä½¿ç”¨token-choice
            return self.token_choice_routing(x, logits)
    
    def estimate_current_load(self):
        """ä¼°è®¡å½“å‰ç³»ç»Ÿè´Ÿè½½"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºå†å²ç»Ÿè®¡
        return 0.6  # å ä½ç¬¦
```

### DeepSeek MoEæ¶æ„å›¾

```
                 è¾“å…¥Token
                     â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                 â”‚
       å…±äº«ä¸“å®¶           è·¯ç”±ç½‘ç»œ
      (å§‹ç»ˆæ¿€æ´»)           â”‚
            â”‚               â”‚
            â”‚        â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
            â”‚        â”‚           â”‚
            â”‚     Top-K        å®¹é‡
            â”‚     é€‰æ‹©         é™åˆ¶
            â”‚        â”‚           â”‚
            â”‚     è·¯ç”±ä¸“å®¶     è´Ÿè½½
            â”‚    (åŠ¨æ€é€‰æ‹©)    å‡è¡¡
            â”‚        â”‚           â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                  æœ€ç»ˆè¾“å‡º
```

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: DeepSeek MoEç›¸æ¯”ä¼ ç»ŸMoEæœ‰ä»€ä¹ˆåˆ›æ–°ï¼Ÿ

**æ ¸å¿ƒåˆ›æ–°**:

1. **ç»†ç²’åº¦ä¸“å®¶è®¾è®¡**: å°†FFNåˆ†è§£ä¸ºæ›´ä¸“ä¸šåŒ–çš„ç»„ä»¶
2. **å…±äº«ä¸“å®¶æœºåˆ¶**: éƒ¨åˆ†ä¸“å®¶å§‹ç»ˆæ¿€æ´»ï¼Œæä¾›ç¨³å®šåŸºçº¿
3. **å¤šçº§è´Ÿè½½å‡è¡¡**: è®¾å¤‡çº§å’Œä¸“å®¶çº§çš„åŒé‡å‡è¡¡ç­–ç•¥
4. **åŠ¨æ€å®¹é‡è°ƒæ•´**: æ ¹æ®ä¸“å®¶åˆ©ç”¨ç‡åŠ¨æ€è°ƒæ•´å®¹é‡

### Q2: å…±äº«ä¸“å®¶æœºåˆ¶æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ

**ä¸»è¦ä¼˜åŠ¿**:
- **çŸ¥è¯†å…±äº«**: é€šç”¨çŸ¥è¯†ä¸éœ€è¦åœ¨æ¯ä¸ªä¸“å®¶ä¸­é‡å¤
- **è®­ç»ƒç¨³å®š**: æä¾›ç¨³å®šçš„æ¢¯åº¦ä¿¡å·
- **è´Ÿè½½åˆ†æ‹…**: å‡å°‘è·¯ç”±ä¸“å®¶çš„å‹åŠ›
- **æ€§èƒ½ä¿è¯**: å³ä½¿è·¯ç”±å¤±è´¥ä¹Ÿæœ‰åŸºç¡€è¾“å‡º

### Q3: å¦‚ä½•è§£å†³MoEçš„è´Ÿè½½å‡è¡¡é—®é¢˜ï¼Ÿ

**DeepSeekçš„å¤šå±‚æ¬¡æ–¹æ¡ˆ**:

1. **è¾…åŠ©æŸå¤±å‡½æ•°**: 
   ```python
   balance_loss = device_balance_loss + expert_balance_loss
   total_loss = task_loss + Î» * balance_loss
   ```

2. **åŠ¨æ€å®¹é‡è°ƒæ•´**: æ ¹æ®ä¸“å®¶å†å²åˆ©ç”¨ç‡è°ƒæ•´å®¹é‡

3. **æ··åˆè·¯ç”±ç­–ç•¥**: åœ¨token-choiceå’Œexpert-choiceé—´è‡ªé€‚åº”åˆ‡æ¢

4. **ä¸“å®¶åˆ†ç»„**: é€šè¿‡å±‚æ¬¡åŒ–ç»“æ„æé«˜è´Ÿè½½åˆ†å¸ƒ

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] ç†è§£ç»†ç²’åº¦ä¸“å®¶vsç²—ç²’åº¦ä¸“å®¶çš„åŒºåˆ«
- [ ] æŒæ¡å…±äº«ä¸“å®¶æœºåˆ¶çš„è®¾è®¡ç†å¿µ
- [ ] äº†è§£å¤šçº§è´Ÿè½½å‡è¡¡çš„å®ç°ç­–ç•¥
- [ ] èƒ½è§£é‡ŠDeepSeek MoEçš„åˆ›æ–°ä»·å€¼

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸Šä¸€èŠ‚ï¼šMLAæ ¸å¿ƒæŠ€æœ¯](mla.md)
- [ä¸‹ä¸€èŠ‚ï¼šMTPå¤štokené¢„æµ‹](mtp.md)
- [è¿”å›ï¼šDeepSeekä¼˜åŒ–æŠ€æœ¯æ¦‚è§ˆ](index.md)