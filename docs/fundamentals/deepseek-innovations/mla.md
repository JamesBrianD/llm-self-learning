# MLAæ ¸å¿ƒæŠ€æœ¯

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æŒæ¡Multi-head Latent Attention (MLA)çš„å®Œæ•´æŠ€æœ¯åŸç†ï¼Œç†è§£å…¶å¦‚ä½•å®ç°é©å‘½æ€§çš„å†…å­˜ä¼˜åŒ–ã€‚

## ğŸ“ æŠ€æœ¯æ·±åº¦è§£æ

### MLAè®¾è®¡èƒŒæ™¯

ä¼ ç»Ÿå¤šå¤´æ³¨æ„åŠ›(MHA)é¢ä¸´çš„æ ¸å¿ƒé—®é¢˜ï¼š

#### å†…å­˜çˆ†ç‚¸é—®é¢˜
```python
# ä¼ ç»ŸMHAçš„KV Cacheéœ€æ±‚
kv_cache_size = num_layers Ã— num_heads Ã— head_dim Ã— seq_len Ã— 2  # Kå’ŒV

# å…·ä½“ä¾‹å­ï¼šLLaMA-7Bæ¨¡å‹
# 32å±‚ Ã— 32å¤´ Ã— 128ç»´ Ã— 2048åºåˆ—é•¿åº¦ Ã— 2 = 1GB+ å†…å­˜
```

#### æ¨ç†ç“¶é¢ˆ
- é•¿åºåˆ—æ¨ç†æ—¶KV Cacheå ç”¨å¤§é‡æ˜¾å­˜
- é™åˆ¶äº†batch sizeå’Œåºåˆ—é•¿åº¦
- æ¨ç†æˆæœ¬å±…é«˜ä¸ä¸‹

### MLAæ ¸å¿ƒåˆ›æ–°

#### 1. ä½ç§©KVè”åˆå‹ç¼©

**æ ¸å¿ƒæ€æƒ³**: å°†é«˜ç»´çš„Keyå’ŒValueçŸ©é˜µè”åˆå‹ç¼©åˆ°ä½ç»´æ½œåœ¨ç©ºé—´

##### æ•°å­¦åŸç†
$$c_t^{KV} = x_t W^{DKV}$$

å…¶ä¸­ï¼š
- $x_t \in \mathbb{R}^{d}$: è¾“å…¥å‘é‡
- $W^{DKV} \in \mathbb{R}^{d \times d_c}$: ä¸‹æŠ•å½±çŸ©é˜µ
- $c_t^{KV} \in \mathbb{R}^{d_c}$: å‹ç¼©åçš„æ½œåœ¨å‘é‡
- $d_c \ll h \cdot d_h$ (å‹ç¼©ç»´åº¦è¿œå°äºåŸå§‹ç»´åº¦)

##### æ¢å¤è¿‡ç¨‹
```python
def kv_compression_recovery():
    # 1. å‹ç¼©ï¼šå°†è¾“å…¥å‹ç¼©åˆ°ä½ç»´ç©ºé—´
    c_kv = x @ W_down_kv  # [batch, seq, d_model] -> [batch, seq, d_c]
    
    # 2. æ¢å¤ï¼šä»ä½ç»´ç©ºé—´æ¢å¤é«˜ç»´K,V
    k_compressed = c_kv @ W_up_k  # [batch, seq, d_c] -> [batch, seq, d_k]
    v_compressed = c_kv @ W_up_v  # [batch, seq, d_c] -> [batch, seq, d_v]
    
    return k_compressed, v_compressed
```

##### å‹ç¼©æ•ˆæœå¯¹æ¯”
| æ¨¡å‹è§„æ¨¡ | åŸå§‹KV Cache | MLAå‹ç¼©å | å‹ç¼©æ¯” |
|---------|-------------|-----------|--------|
| **7Bæ¨¡å‹** | 8192ç»´/token | 640ç»´/token | 12.8Ã— |
| **67Bæ¨¡å‹** | 16384ç»´/token | 1024ç»´/token | 16Ã— |
| **236Bæ¨¡å‹** | 32768ç»´/token | 1536ç»´/token | 21.3Ã— |

#### 2. RoPEè§£è€¦æœºåˆ¶

**é—®é¢˜**: ä¼ ç»ŸRoPEåœ¨å‹ç¼©ç©ºé—´ä¸­æ— æ³•æ­£ç¡®å·¥ä½œ

**è§£å†³æ–¹æ¡ˆ**: å°†Queryå’ŒKeyåˆ†ä¸ºä¸¤ä¸ªç‹¬ç«‹éƒ¨åˆ†

##### åˆ†ç¦»ç­–ç•¥
```python
def rope_decoupling(x, position):
    # 1. ç”ŸæˆåŸå§‹Query
    q_full = x @ W_q  # [batch, seq, d_model]
    
    # 2. åˆ†ç¦»ä¸ºä¸¤éƒ¨åˆ†
    q_compressed = q_full[:, :, :d_c]      # è¯­ä¹‰éƒ¨åˆ†ï¼Œå¯å‹ç¼©
    q_rope = q_full[:, :, d_c:d_c+d_r]     # ä½ç½®éƒ¨åˆ†ï¼Œä¿æŒåŸç»´åº¦
    
    # 3. åˆ†åˆ«å¤„ç†
    # è¯­ä¹‰éƒ¨åˆ†ï¼šé€šè¿‡å‹ç¼©ç©ºé—´å¤„ç†
    c_kv = x @ W_down_kv
    k_compressed = c_kv @ W_up_k
    
    # ä½ç½®éƒ¨åˆ†ï¼šç›´æ¥ç”Ÿæˆå¹¶åº”ç”¨RoPE
    k_rope = x @ W_k_rope
    q_rope_rotated = apply_rope(q_rope, position)
    k_rope_rotated = apply_rope(k_rope, position)
    
    # 4. ç»„åˆæœ€ç»ˆç»“æœ
    q_final = torch.cat([q_compressed, q_rope_rotated], dim=-1)
    k_final = torch.cat([k_compressed, k_rope_rotated], dim=-1)
    
    return q_final, k_final
```

##### RoPEè§£è€¦çš„æ•°å­¦è¡¨ç¤º
$$q_t = [q_t^C; q_t^R], \quad k_s = [k_s^C; k_s^R]$$

å…¶ä¸­ï¼š
- $q_t^C, k_s^C$: è¯­ä¹‰ç»„ä»¶ï¼Œé€šè¿‡æ½œåœ¨ç©ºé—´ç”Ÿæˆ
- $q_t^R, k_s^R$: ä½ç½®ç»„ä»¶ï¼Œåº”ç”¨RoPEæ—‹è½¬ç¼–ç 

æ³¨æ„åŠ›è®¡ç®—ï¼š
$$\text{Attention} = \text{softmax}\left(\frac{q_t^C (k_s^C)^T + q_t^R (k_s^R)^T}{\sqrt{d_h}}\right)$$

#### 3. æƒé‡å¸æ”¶ä¼˜åŒ–

**ç›®æ ‡**: å‡å°‘æ¨ç†æ—¶çš„çŸ©é˜µä¹˜æ³•æ“ä½œ

##### ä¼ ç»Ÿè®¡ç®—è·¯å¾„
```python
# éœ€è¦ä¸¤æ¬¡çŸ©é˜µä¹˜æ³•
c_kv = x @ W_down_kv     # ç¬¬ä¸€æ¬¡ï¼šé™ç»´
k = c_kv @ W_up_k        # ç¬¬äºŒæ¬¡ï¼šå‡ç»´æ¢å¤
```

##### æƒé‡å¸æ”¶å
```python
# é¢„è®¡ç®—åˆå¹¶æƒé‡
W_combined_k = W_down_kv @ W_up_k  # ç¦»çº¿è®¡ç®—
W_combined_v = W_down_kv @ W_up_v

# æ¨ç†æ—¶åªéœ€ä¸€æ¬¡çŸ©é˜µä¹˜æ³•
k_absorbed = x @ W_combined_k      # ç›´æ¥å¾—åˆ°ç»“æœ
v_absorbed = x @ W_combined_v
```

##### è®¡ç®—å¤æ‚åº¦åˆ†æ
| æ“ä½œ | ä¼ ç»ŸMLA | æƒé‡å¸æ”¶MLA | å‡å°‘é‡ |
|------|---------|-------------|--------|
| **çŸ©é˜µä¹˜æ³•æ¬¡æ•°** | 2æ¬¡ | 1æ¬¡ | 50% |
| **å†…å­˜è®¿é—®** | é«˜ | ä½ | ~30% |
| **æ¨ç†å»¶è¿Ÿ** | åŸºå‡† | å‡å°‘15-20% | æ˜¾è‘— |

### å®Œæ•´MLAå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class MLAAttention(nn.Module):
    """Multi-head Latent Attentionå®Œæ•´å®ç°"""
    
    def __init__(self, d_model, num_heads, d_compressed=None, d_rope=None):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        # å‹ç¼©ç»´åº¦è®¾ç½®
        self.d_compressed = d_compressed or d_model // 8  # é»˜è®¤8å€å‹ç¼©
        self.d_rope = d_rope or self.head_dim // 2        # RoPEç»´åº¦
        
        # QueryæŠ•å½±
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        
        # KVè”åˆå‹ç¼©æŠ•å½±
        self.W_down_kv = nn.Linear(d_model, self.d_compressed, bias=False)
        self.W_up_k = nn.Linear(self.d_compressed, d_model - self.d_rope * num_heads, bias=False)
        self.W_up_v = nn.Linear(self.d_compressed, d_model, bias=False)
        
        # RoPEéƒ¨åˆ†çš„KeyæŠ•å½±
        self.W_k_rope = nn.Linear(d_model, self.d_rope * num_heads, bias=False)
        
        # è¾“å‡ºæŠ•å½±
        self.W_o = nn.Linear(d_model, d_model, bias=False)
        
        # RoPEåˆå§‹åŒ–
        self.rope = RoPEEmbedding(self.d_rope)
        
        # æƒé‡å¸æ”¶ä¼˜åŒ–(å¯é€‰)
        self.enable_weight_absorption = True
        if self.enable_weight_absorption:
            self._setup_absorbed_weights()
    
    def _setup_absorbed_weights(self):
        """è®¾ç½®æƒé‡å¸æ”¶çš„åˆå¹¶çŸ©é˜µ"""
        # é¢„è®¡ç®—åˆå¹¶æƒé‡çŸ©é˜µ
        with torch.no_grad():
            self.W_absorbed_k = nn.Parameter(
                self.W_down_kv.weight.T @ self.W_up_k.weight.T
            )
            self.W_absorbed_v = nn.Parameter(
                self.W_down_kv.weight.T @ self.W_up_v.weight.T
            )
    
    def forward(self, x, position_ids=None, kv_cache=None):
        batch_size, seq_len, d_model = x.shape
        
        # 1. è®¡ç®—Query
        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # 2. åˆ†ç¦»Queryçš„å‹ç¼©éƒ¨åˆ†å’ŒRoPEéƒ¨åˆ†
        q_compressed = q[:, :, :, :-self.d_rope]  # è¯­ä¹‰éƒ¨åˆ†
        q_rope = q[:, :, :, -self.d_rope:]        # ä½ç½®éƒ¨åˆ†
        
        # 3. è®¡ç®—å‹ç¼©çš„Keyå’ŒValue
        if self.enable_weight_absorption:
            # ä½¿ç”¨æƒé‡å¸æ”¶ä¼˜åŒ–
            k_compressed_flat = x @ self.W_absorbed_k
            v_flat = x @ self.W_absorbed_v
        else:
            # æ ‡å‡†ä¸¤æ­¥è®¡ç®—
            c_kv = x @ self.W_down_kv.weight.T
            k_compressed_flat = c_kv @ self.W_up_k.weight.T
            v_flat = c_kv @ self.W_up_v.weight.T
        
        # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼
        k_compressed = k_compressed_flat.view(
            batch_size, seq_len, self.num_heads, -1
        )
        v = v_flat.view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # 4. è®¡ç®—RoPEéƒ¨åˆ†çš„Key
        k_rope_flat = x @ self.W_k_rope.weight.T
        k_rope = k_rope_flat.view(batch_size, seq_len, self.num_heads, self.d_rope)
        
        # 5. åº”ç”¨RoPEæ—‹è½¬ç¼–ç 
        if position_ids is not None:
            q_rope = self.rope(q_rope, position_ids)
            k_rope = self.rope(k_rope, position_ids)
        
        # 6. ç»„åˆå®Œæ•´çš„Key
        k = torch.cat([k_compressed, k_rope], dim=-1)
        q = torch.cat([q_compressed, q_rope], dim=-1)
        
        # 7. KV Cacheå¤„ç†
        if kv_cache is not None:
            # æ›´æ–°ç¼“å­˜ - åªç¼“å­˜å‹ç¼©åçš„è¡¨ç¤º
            compressed_cache = torch.cat([k_compressed, k_rope], dim=-1)
            k, v = kv_cache.update(compressed_cache, v)
        
        # 8. è®¡ç®—æ³¨æ„åŠ›
        # è½¬ç½®ç»´åº¦: [batch, seq, heads, head_dim] -> [batch, heads, seq, head_dim]
        q = q.transpose(1, 2)
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        # ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)
        attn_weights = F.softmax(scores, dim=-1)
        attn_output = torch.matmul(attn_weights, v)
        
        # 9. é‡å¡‘å’Œè¾“å‡ºæŠ•å½±
        attn_output = attn_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, d_model
        )
        
        return self.W_o(attn_output)

class MLAKVCache:
    """MLAä¸“ç”¨çš„KV Cache"""
    
    def __init__(self, max_seq_len, num_heads, compressed_dim, rope_dim):
        self.max_seq_len = max_seq_len
        self.num_heads = num_heads
        self.compressed_dim = compressed_dim
        self.rope_dim = rope_dim
        
        # åªç¼“å­˜å‹ç¼©åçš„è¡¨ç¤º
        self.cache_dim = compressed_dim + rope_dim
        self.cache_k = torch.zeros(max_seq_len, num_heads, self.cache_dim)
        self.cache_v = torch.zeros(max_seq_len, num_heads, compressed_dim)
        self.current_len = 0
    
    def update(self, new_k, new_v):
        """æ›´æ–°ç¼“å­˜å¹¶è¿”å›å®Œæ•´çš„K,V"""
        seq_len = new_k.size(1)
        
        # æ›´æ–°ç¼“å­˜
        end_pos = self.current_len + seq_len
        self.cache_k[:, self.current_len:end_pos] = new_k[0].transpose(0, 1)
        self.cache_v[:, self.current_len:end_pos] = new_v[0].transpose(0, 1)
        
        self.current_len = end_pos
        
        # è¿”å›å®Œæ•´çš„K,V
        return (
            self.cache_k[:self.current_len].transpose(0, 1).unsqueeze(0),
            self.cache_v[:self.current_len].transpose(0, 1).unsqueeze(0)
        )
    
    def get_memory_usage(self):
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        total_elements = self.current_len * self.num_heads * (self.cache_dim + self.compressed_dim)
        memory_mb = total_elements * 4 / 1024 / 1024  # å‡è®¾float32
        return memory_mb

class RoPEEmbedding(nn.Module):
    """æ—‹è½¬ä½ç½®ç¼–ç """
    
    def __init__(self, dim, base=10000):
        super().__init__()
        self.dim = dim
        self.base = base
        
        # é¢„è®¡ç®—é¢‘ç‡
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
    
    def forward(self, x, position_ids):
        # x: [batch, seq, heads, dim]
        # position_ids: [batch, seq]
        
        seq_len = x.size(1)
        position = position_ids.float()
        
        # è®¡ç®—è§’åº¦
        freqs = torch.outer(position.flatten(), self.inv_freq)
        
        # ç”Ÿæˆcoså’Œsin
        cos = freqs.cos().view(*position.shape, -1)
        sin = freqs.sin().view(*position.shape, -1)
        
        # åº”ç”¨æ—‹è½¬
        x1, x2 = x[..., ::2], x[..., 1::2]
        
        # æ—‹è½¬å˜æ¢
        rotated_x1 = x1 * cos.unsqueeze(2) - x2 * sin.unsqueeze(2)
        rotated_x2 = x1 * sin.unsqueeze(2) + x2 * cos.unsqueeze(2)
        
        # é‡æ–°ç»„åˆ
        rotated_x = torch.stack([rotated_x1, rotated_x2], dim=-1).flatten(-2)
        
        return rotated_x

# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
def benchmark_mla_vs_mha():
    """å¯¹æ¯”MLAå’ŒMHAçš„æ€§èƒ½"""
    
    d_model, num_heads = 4096, 32
    seq_len, batch_size = 2048, 4
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, d_model)
    position_ids = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1)
    
    # MLAæ¨¡å‹
    mla = MLAAttention(d_model, num_heads, d_compressed=512)
    
    # ä¼ ç»ŸMHAæ¨¡å‹ (ç®€åŒ–ç‰ˆæœ¬)
    mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
    
    print("=== MLA vs MHA æ€§èƒ½å¯¹æ¯” ===")
    
    # å‚æ•°é‡å¯¹æ¯”
    mla_params = sum(p.numel() for p in mla.parameters())
    mha_params = sum(p.numel() for p in mha.parameters())
    
    print(f"MLAå‚æ•°é‡: {mla_params:,}")
    print(f"MHAå‚æ•°é‡: {mha_params:,}")
    print(f"å‚æ•°å‡å°‘: {(mha_params - mla_params) / mha_params * 100:.1f}%")
    
    # KV Cacheå†…å­˜å¯¹æ¯”
    traditional_kv_cache = num_heads * (d_model // num_heads) * seq_len * 2
    mla_kv_cache = (512 + 64) * seq_len  # å‹ç¼©ç»´åº¦ + RoPEç»´åº¦
    
    print(f"ä¼ ç»ŸKV Cache: {traditional_kv_cache:,} å…ƒç´ ")
    print(f"MLA KV Cache: {mla_kv_cache:,} å…ƒç´ ")
    print(f"å†…å­˜å‡å°‘: {traditional_kv_cache / mla_kv_cache:.1f}Ã—")
    
    # æ¨ç†é€Ÿåº¦æµ‹è¯•
    import time
    
    with torch.no_grad():
        # MLAæ¨ç†æ—¶é—´
        start = time.time()
        for _ in range(100):
            _ = mla(x, position_ids)
        mla_time = time.time() - start
        
        # MHAæ¨ç†æ—¶é—´
        start = time.time()
        for _ in range(100):
            _, _ = mha(x, x, x)
        mha_time = time.time() - start
    
    print(f"MLAæ¨ç†æ—¶é—´: {mla_time:.4f}ç§’")
    print(f"MHAæ¨ç†æ—¶é—´: {mha_time:.4f}ç§’")
    print(f"é€Ÿåº¦æå‡: {mha_time / mla_time:.2f}Ã—")

if __name__ == "__main__":
    benchmark_mla_vs_mha()
```

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: MLAå¦‚ä½•å®ç°10å€ä»¥ä¸Šçš„KV Cacheå‹ç¼©ï¼Ÿ

**æ ¸å¿ƒæœºåˆ¶**:

1. **ä½ç§©è”åˆå‹ç¼©**: å°†åŸæœ¬éœ€è¦å­˜å‚¨çš„ `num_heads Ã— head_dim Ã— 2` ç»´åº¦å‹ç¼©åˆ° `compressed_dim`
2. **RoPEè§£è€¦**: åªå¯¹å°‘é‡ç»´åº¦ä¿æŒåŸå§‹ç²¾åº¦ï¼Œå¤§éƒ¨åˆ†ç»´åº¦å¯ä»¥å‹ç¼©
3. **æ™ºèƒ½è®¾è®¡**: åŸºäºæ³¨æ„åŠ›æ¨¡å¼çš„ä½ç§©ç‰¹æ€§è¿›è¡Œæœ‰æŸä½†åˆç†çš„å‹ç¼©

**å…·ä½“æ•°å­—**:
```
ä¼ ç»ŸMHA: 32å¤´ Ã— 128ç»´ Ã— 2 = 8192ç»´/token
MLAå‹ç¼©: 512ç»´(å‹ç¼©) + 128ç»´(RoPE) = 640ç»´/token
å‹ç¼©æ¯”: 8192 Ã· 640 = 12.8Ã—
```

### Q2: RoPEè§£è€¦ä¸ºä»€ä¹ˆæ˜¯å¿…è¦çš„ï¼Ÿ

**æ ¸å¿ƒé—®é¢˜**: ä½ç½®ç¼–ç ä¸å‹ç¼©çš„å†²çª

**è¯¦ç»†è§£é‡Š**:
1. **RoPEä¾èµ–**: æ—‹è½¬ä½ç½®ç¼–ç éœ€è¦åœ¨ç‰¹å®šç»´åº¦ç©ºé—´ä¸­å·¥ä½œ
2. **å‹ç¼©ç ´å**: ä½ç§©å‹ç¼©ä¼šç ´åRoPEçš„æ•°å­¦ç»“æ„
3. **è§£è€¦æ–¹æ¡ˆ**: å°†å‘é‡åˆ†ä¸ºè¯­ä¹‰éƒ¨åˆ†(å¯å‹ç¼©)å’Œä½ç½®éƒ¨åˆ†(ä¸å‹ç¼©)
4. **æ•ˆæœä¿è¯**: æ—¢è·å¾—å‹ç¼©æ”¶ç›Šï¼Œåˆä¿æŒä½ç½®æ•æ„Ÿæ€§

### Q3: æƒé‡å¸æ”¶ä¼˜åŒ–çš„å®é™…æ•ˆæœå¦‚ä½•ï¼Ÿ

**æ€§èƒ½æå‡**:
- **è®¡ç®—æ¬¡æ•°**: ä»2æ¬¡çŸ©é˜µä¹˜æ³•å‡å°‘åˆ°1æ¬¡
- **å†…å­˜è®¿é—®**: å‡å°‘ä¸­é—´ç»“æœçš„å­˜å‚¨å’Œè¯»å–
- **æ¨ç†å»¶è¿Ÿ**: é€šå¸¸å‡å°‘15-20%

**å·¥ç¨‹è€ƒè™‘**:
- éœ€è¦é¢å¤–çš„å‚æ•°å­˜å‚¨ç©ºé—´
- é¢„è®¡ç®—å¼€é”€ï¼ˆä¸€æ¬¡æ€§ï¼‰
- æ•°å€¼ç²¾åº¦å¯èƒ½æœ‰å¾®å°æŸå¤±

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] ç†è§£MLAçš„ä¸‰å¤§æ ¸å¿ƒæŠ€æœ¯åŸç†
- [ ] èƒ½è®¡ç®—MLAç›¸æ¯”MHAçš„å†…å­˜å‹ç¼©æ¯”
- [ ] æŒæ¡RoPEè§£è€¦çš„å¿…è¦æ€§å’Œå®ç°æ–¹æ³•
- [ ] ç†è§£æƒé‡å¸æ”¶ä¼˜åŒ–çš„å·¥ç¨‹ä»·å€¼
- [ ] èƒ½å®ç°ç®€åŒ–ç‰ˆçš„MLAæ³¨æ„åŠ›æœºåˆ¶

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸‹ä¸€èŠ‚ï¼šDeepSeek MoEåˆ›æ–°](deepseek-moe.md)
- [ä¸Šä¸€ç« ï¼šå¤šå¤´æ³¨æ„åŠ›å˜ä½“](../attention-advanced/mha-variants.md)
- [è¿”å›ï¼šDeepSeekä¼˜åŒ–æŠ€æœ¯æ¦‚è§ˆ](index.md)