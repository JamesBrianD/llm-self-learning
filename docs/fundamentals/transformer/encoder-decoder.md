# ç¼–ç å™¨-è§£ç å™¨æ¶æ„

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æ·±å…¥ç†è§£åŸå§‹Transformerçš„ç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼ŒæŒæ¡ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶çš„åŒºåˆ«å’Œä½œç”¨ã€‚

## ğŸ“ çŸ¥è¯†æ€»ç»“

### æ•´ä½“æ¶æ„æ¦‚è§ˆ

åŸå§‹Transformeré‡‡ç”¨**ç¼–ç å™¨-è§£ç å™¨(Encoder-Decoder)**æ¶æ„ï¼Œä¸“é—¨ç”¨äºåºåˆ—åˆ°åºåˆ—çš„è½¬æ¢ä»»åŠ¡ã€‚

```
è¾“å…¥åºåˆ— â†’ ç¼–ç å™¨ â†’ ç¼–ç è¡¨ç¤º â†’ è§£ç å™¨ â†’ è¾“å‡ºåºåˆ—
```

#### æ ¸å¿ƒç»„ä»¶
1. **ç¼–ç å™¨(Encoder)**: å°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºé«˜å±‚è¯­ä¹‰è¡¨ç¤º
2. **è§£ç å™¨(Decoder)**: åŸºäºç¼–ç è¡¨ç¤ºè‡ªå›å½’ç”Ÿæˆè¾“å‡ºåºåˆ—
3. **ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶**: è‡ªæ³¨æ„åŠ›ã€æ©ç è‡ªæ³¨æ„åŠ›ã€äº¤å‰æ³¨æ„åŠ›

### ç¼–ç å™¨ (Encoder) è¯¦è§£

#### ç»“æ„ç»„æˆ
- **Nå±‚ç›¸åŒçš„å±‚**(åŸè®ºæ–‡N=6)
- æ¯å±‚åŒ…å«ä¸¤ä¸ªå­å±‚ï¼š
  1. å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
  2. å‰é¦ˆç¥ç»ç½‘ç»œ
- æ¯ä¸ªå­å±‚ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–

#### æ•°å­¦è¡¨ç¤º
```
# ç¼–ç å™¨å±‚çš„è®¡ç®—è¿‡ç¨‹
def encoder_layer(x):
    # å¤šå¤´è‡ªæ³¨æ„åŠ›
    attn_output = multi_head_attention(x, x, x)
    x = layer_norm(x + attn_output)  # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    
    # å‰é¦ˆç½‘ç»œ
    ffn_output = feed_forward(x)
    x = layer_norm(x + ffn_output)   # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
    
    return x
```

#### æ ¸å¿ƒç‰¹å¾

**1. å¹¶è¡Œå¤„ç†**
- å¯ä»¥åŒæ—¶å¤„ç†æ•´ä¸ªè¾“å…¥åºåˆ—
- æ¯ä¸ªä½ç½®éƒ½èƒ½çœ‹åˆ°æ‰€æœ‰å…¶ä»–ä½ç½®
- è®­ç»ƒæ•ˆç‡é«˜ï¼Œæ— åºåˆ—è®¡ç®—ä¾èµ–

**2. åŒå‘ä¸Šä¸‹æ–‡**
- è‡ªæ³¨æ„åŠ›æœºåˆ¶å…è®¸æ¯ä¸ªä½ç½®å…³æ³¨æ•´ä¸ªåºåˆ—
- èƒ½å¤Ÿæ•è·å…¨å±€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- é€‚åˆç†è§£ç±»ä»»åŠ¡

**3. è¯­ä¹‰æå‡**
- å°†ä½é˜¶è¯å‘é‡è½¬æ¢ä¸ºé«˜é˜¶è¯­ä¹‰è¡¨ç¤º
- å¤šå±‚å †å é€æ­¥æŠ½è±¡è¯­ä¹‰ä¿¡æ¯
- æœ€ç»ˆè¾“å‡ºåŒ…å«ä¸°å¯Œçš„ä¸Šä¸‹æ–‡ä¿¡æ¯

### è§£ç å™¨ (Decoder) è¯¦è§£

#### ç»“æ„ç»„æˆ
- **Nå±‚ç›¸åŒçš„å±‚**(åŸè®ºæ–‡N=6)
- æ¯å±‚åŒ…å«ä¸‰ä¸ªå­å±‚ï¼š
  1. æ©ç å¤šå¤´è‡ªæ³¨æ„åŠ›
  2. ç¼–ç å™¨-è§£ç å™¨äº¤å‰æ³¨æ„åŠ›
  3. å‰é¦ˆç¥ç»ç½‘ç»œ
- æ¯ä¸ªå­å±‚ä½¿ç”¨æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–

#### æ•°å­¦è¡¨ç¤º
```
# è§£ç å™¨å±‚çš„è®¡ç®—è¿‡ç¨‹
def decoder_layer(x, encoder_output):
    # 1. æ©ç è‡ªæ³¨æ„åŠ›
    masked_attn = masked_multi_head_attention(x, x, x)
    x = layer_norm(x + masked_attn)
    
    # 2. äº¤å‰æ³¨æ„åŠ›
    cross_attn = multi_head_attention(
        query=x, 
        key=encoder_output, 
        value=encoder_output
    )
    x = layer_norm(x + cross_attn)
    
    # 3. å‰é¦ˆç½‘ç»œ
    ffn_output = feed_forward(x)
    x = layer_norm(x + ffn_output)
    
    return x
```

#### æ ¸å¿ƒç‰¹å¾

**1. è‡ªå›å½’ç”Ÿæˆ**
- é€æ­¥ç”Ÿæˆè¾“å‡ºåºåˆ—
- å½“å‰ä½ç½®åªèƒ½çœ‹åˆ°ä¹‹å‰çš„ä½ç½®
- ä½¿ç”¨æ©ç æœºåˆ¶é˜²æ­¢ä¿¡æ¯æ³„éœ²

**2. åŒè¾“å…¥æœºåˆ¶**
- è¾“å…¥1ï¼šè§£ç å™¨ä¹‹å‰çš„è¾“å‡º(è‡ªå›å½’)
- è¾“å…¥2ï¼šç¼–ç å™¨çš„è¾“å‡ºè¡¨ç¤º(äº¤å‰æ³¨æ„åŠ›)
- ç»“åˆè‡ªèº«å†å²å’Œæºåºåˆ—ä¿¡æ¯

### ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£

#### 1. ç¼–ç å™¨è‡ªæ³¨æ„åŠ› (Encoder Self-Attention)

**ä½œç”¨**: è®©ç¼–ç å™¨çš„æ¯ä¸ªä½ç½®å…³æ³¨è¾“å…¥åºåˆ—çš„æ‰€æœ‰ä½ç½®

```python
# ç¼–ç å™¨è‡ªæ³¨æ„åŠ›
Q = K = V = encoder_input  # éƒ½æ¥è‡ªè¾“å…¥åºåˆ—
attention_output = Attention(Q, K, V)
```

**ç‰¹ç‚¹**:
- æ— æ©ç é™åˆ¶ï¼Œå¯ä»¥çœ‹åˆ°å…¨åºåˆ—
- å»ºç«‹è¾“å…¥åºåˆ—å†…éƒ¨çš„ä¾èµ–å…³ç³»
- æ•è·é•¿è·ç¦»ä¾èµ–

#### 2. è§£ç å™¨æ©ç è‡ªæ³¨æ„åŠ› (Masked Self-Attention)

**ä½œç”¨**: è®©è§£ç å™¨çš„æ¯ä¸ªä½ç½®åªå…³æ³¨ä¹‹å‰çš„ä½ç½®

```python
# æ©ç è‡ªæ³¨æ„åŠ›
Q = K = V = decoder_input  # éƒ½æ¥è‡ªè§£ç å™¨è¾“å…¥
mask = create_causal_mask(seq_len)  # ä¸‹ä¸‰è§’æ©ç 
attention_output = Attention(Q, K, V, mask=mask)
```

**æ©ç æœºåˆ¶**:
```
ä½ç½®:  0  1  2  3
æ©ç : [1  0  0  0]  # ä½ç½®0åªèƒ½çœ‹è‡ªå·±
      [1  1  0  0]  # ä½ç½®1èƒ½çœ‹0,1
      [1  1  1  0]  # ä½ç½®2èƒ½çœ‹0,1,2
      [1  1  1  1]  # ä½ç½®3èƒ½çœ‹0,1,2,3
```

#### 3. ç¼–ç å™¨-è§£ç å™¨äº¤å‰æ³¨æ„åŠ› (Cross-Attention)

**ä½œç”¨**: è®©è§£ç å™¨å…³æ³¨ç¼–ç å™¨çš„è¾“å‡ºï¼Œå®ç°åºåˆ—å¯¹é½

```python
# äº¤å‰æ³¨æ„åŠ›
Q = decoder_hidden        # æŸ¥è¯¢æ¥è‡ªè§£ç å™¨
K = V = encoder_output    # é”®å€¼æ¥è‡ªç¼–ç å™¨
attention_output = Attention(Q, K, V)
```

**å·¥ä½œåŸç†**:
- Queryï¼šè§£ç å™¨æƒ³è¦ä»€ä¹ˆä¿¡æ¯
- Keyï¼šç¼–ç å™¨æœ‰ä»€ä¹ˆä¿¡æ¯
- Valueï¼šç¼–ç å™¨æä¾›çš„å…·ä½“ä¿¡æ¯
- å®ç°æºåºåˆ—å’Œç›®æ ‡åºåˆ—çš„å¯¹é½

### æ¶æ„ä¼˜åŠ¿ä¸åº”ç”¨

#### ä¼˜åŠ¿ç‰¹ç‚¹

**1. å¹¶è¡Œè®­ç»ƒ**
- ç¼–ç å™¨å¯ä»¥å¹¶è¡Œå¤„ç†æ•´ä¸ªè¾“å…¥
- è§£ç å™¨åœ¨è®­ç»ƒæ—¶ä¹Ÿå¯ä»¥å¹¶è¡Œ(Teacher Forcing)
- ç›¸æ¯”RNNè®­ç»ƒé€Ÿåº¦å¤§å¹…æå‡

**2. é•¿è·ç¦»ä¾èµ–**
- æ³¨æ„åŠ›æœºåˆ¶ç›´æ¥è¿æ¥ä»»æ„ä¸¤ä¸ªä½ç½®
- é¿å…äº†RNNçš„æ¢¯åº¦ä¼ æ’­é—®é¢˜
- æ›´å¥½åœ°æ•è·é•¿è·ç¦»ä¾èµ–å…³ç³»

**3. å¯è§£é‡Šæ€§**
- æ³¨æ„åŠ›æƒé‡æä¾›æ¨¡å‹å†³ç­–çš„å¯è§†åŒ–
- å¯ä»¥çœ‹åˆ°æ¨¡å‹å…³æ³¨çš„è¾“å…¥éƒ¨åˆ†
- ä¾¿äºåˆ†æå’Œè°ƒè¯•

#### å…¸å‹åº”ç”¨

**1. æœºå™¨ç¿»è¯‘**
- åŸå§‹Transformerçš„è®¾è®¡ç›®æ ‡
- ç¼–ç å™¨ç†è§£æºè¯­è¨€ï¼Œè§£ç å™¨ç”Ÿæˆç›®æ ‡è¯­è¨€
- é€šè¿‡äº¤å‰æ³¨æ„åŠ›å®ç°è¯­è¨€å¯¹é½

**2. æ–‡æœ¬æ‘˜è¦**
- ç¼–ç å™¨ç†è§£åŸæ–‡ï¼Œè§£ç å™¨ç”Ÿæˆæ‘˜è¦
- äº¤å‰æ³¨æ„åŠ›é€‰æ‹©é‡è¦ä¿¡æ¯
- æ§åˆ¶æ‘˜è¦é•¿åº¦å’Œå†…å®¹

**3. å¯¹è¯ç³»ç»Ÿ**
- ç¼–ç å™¨ç†è§£ç”¨æˆ·è¾“å…¥
- è§£ç å™¨ç”Ÿæˆå›å¤
- ç»´æŒå¯¹è¯ä¸Šä¸‹æ–‡

### ç°ä»£å‘å±•è¶‹åŠ¿

#### æ¶æ„æ¼”è¿›

**Encoder-Only**:
- ä»£è¡¨: BERT, RoBERTa
- æ“…é•¿: ç†è§£ä»»åŠ¡(åˆ†ç±»ã€é˜…è¯»ç†è§£)
- ç‰¹ç‚¹: åŒå‘æ³¨æ„åŠ›ï¼Œå¹¶è¡Œè®­ç»ƒ

**Decoder-Only**:
- ä»£è¡¨: GPT, LLaMA, ChatGPT
- æ“…é•¿: ç”Ÿæˆä»»åŠ¡(å¯¹è¯ã€å†™ä½œ)
- ç‰¹ç‚¹: å› æœæ³¨æ„åŠ›ï¼Œç»Ÿä¸€èŒƒå¼

**ä¸ºä»€ä¹ˆDecoder-Onlyæˆä¸ºä¸»æµï¼Ÿ**
1. **ä»»åŠ¡ç»Ÿä¸€**: æ‰€æœ‰ä»»åŠ¡éƒ½å¯ä»¥è¡¨è¿°ä¸ºç”Ÿæˆé—®é¢˜
2. **æ‰©å±•æ€§å¥½**: æ›´å®¹æ˜“æ‰©å±•åˆ°å¤§è§„æ¨¡
3. **æ¶Œç°èƒ½åŠ›**: å¤§è§„æ¨¡åå±•ç°å¼ºå¤§çš„few-shotèƒ½åŠ›
4. **å·¥ç¨‹ç®€åŒ–**: æ¶æ„æ›´ç®€å•ï¼Œæ˜“äºä¼˜åŒ–

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: ç¼–ç å™¨å’Œè§£ç å™¨çš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

**æ ¸å¿ƒåŒºåˆ«**:

| ç»´åº¦ | ç¼–ç å™¨ | è§£ç å™¨ |
|------|--------|--------|
| **æ³¨æ„åŠ›ç±»å‹** | åŒå‘è‡ªæ³¨æ„åŠ› | å•å‘æ©ç è‡ªæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ› |
| **å¤„ç†æ–¹å¼** | å¹¶è¡Œå¤„ç† | è‡ªå›å½’ç”Ÿæˆ |
| **è¾“å…¥æ¥æº** | åŸå§‹è¾“å…¥åºåˆ— | å‰ä¸€æ­¥è¾“å‡º + ç¼–ç å™¨è¾“å‡º |
| **ä¸»è¦åŠŸèƒ½** | ç†è§£å’Œç¼–ç  | ç”Ÿæˆå’Œè§£ç  |

### Q2: äº¤å‰æ³¨æ„åŠ›çš„å·¥ä½œåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ

**å·¥ä½œæœºåˆ¶**:
1. **Queryæ¥è‡ªè§£ç å™¨**: è¡¨ç¤º"æˆ‘æƒ³è¦ä»€ä¹ˆä¿¡æ¯"
2. **Key/Valueæ¥è‡ªç¼–ç å™¨**: è¡¨ç¤º"å¯ä»¥æä¾›ä»€ä¹ˆä¿¡æ¯"
3. **æ³¨æ„åŠ›è®¡ç®—**: è®¡ç®—è§£ç å™¨å¯¹ç¼–ç å™¨æ¯ä¸ªä½ç½®çš„å…³æ³¨åº¦
4. **ä¿¡æ¯èåˆ**: æ ¹æ®æ³¨æ„åŠ›æƒé‡èšåˆç¼–ç å™¨ä¿¡æ¯

**æ•°å­¦è¿‡ç¨‹**:
$$CrossAttention = Attention(Q_{decoder}, K_{encoder}, V_{encoder})$$

### Q3: ä¸ºä»€ä¹ˆè§£ç å™¨éœ€è¦æ©ç è‡ªæ³¨æ„åŠ›ï¼Ÿ

**æ ¸å¿ƒåŸå› **: é˜²æ­¢ä¿¡æ¯æ³„éœ²

**è¯¦ç»†è§£é‡Š**:
1. **è®­ç»ƒæ—¶**: ä½¿ç”¨Teacher Forcingï¼Œæ¨¡å‹èƒ½çœ‹åˆ°å®Œæ•´ç›®æ ‡åºåˆ—
2. **æ¨ç†æ—¶**: åªèƒ½çœ‹åˆ°å·²ç”Ÿæˆçš„éƒ¨åˆ†
3. **ä¸€è‡´æ€§è¦æ±‚**: è®­ç»ƒå’Œæ¨ç†çš„å¯è§ä¿¡æ¯å¿…é¡»ä¸€è‡´
4. **æ©ç ä½œç”¨**: åœ¨è®­ç»ƒæ—¶äººä¸ºé™åˆ¶å¯è§èŒƒå›´

**ä»£ç ç¤ºä¾‹**:
```python
def create_causal_mask(seq_len):
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask.masked_fill(mask == 0, float('-inf'))
```

### Q4: ä¸ºä»€ä¹ˆç°åœ¨æ›´æµè¡ŒDecoder-Onlyæ¶æ„ï¼Ÿ

**ä¸»è¦åŸå› **:

1. **ç»Ÿä¸€æ€§**: 
   - æ‰€æœ‰ä»»åŠ¡éƒ½å¯ä»¥è½¬åŒ–ä¸ºç”Ÿæˆä»»åŠ¡
   - åˆ†ç±» â†’ ç”Ÿæˆç±»åˆ«æ ‡ç­¾
   - é—®ç­” â†’ ç”Ÿæˆç­”æ¡ˆ

2. **æ‰©å±•æ€§**:
   - æ¶æ„ç®€å•ï¼Œæ˜“äºæ‰©å¤§è§„æ¨¡
   - è®­ç»ƒæ›´ç¨³å®šï¼Œå‚æ•°åˆ©ç”¨ç‡é«˜

3. **æ¶Œç°èƒ½åŠ›**:
   - å¤§è§„æ¨¡è®­ç»ƒåå±•ç°å¼ºå¤§çš„zero/few-shotèƒ½åŠ›
   - æŒ‡ä»¤è·Ÿéšã€ä¸Šä¸‹æ–‡å­¦ä¹ ç­‰èƒ½åŠ›

4. **å·¥ç¨‹ä¼˜åŠ¿**:
   - å®ç°ç®€å•ï¼Œä¼˜åŒ–æˆç†Ÿ
   - æ¨ç†æ•ˆç‡é«˜(KV Cacheç­‰æŠ€æœ¯)

## ğŸ’» ä»£ç å®ç°

### å®Œæ•´Encoder-Decoderå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class EncoderDecoderTransformer(nn.Module):
    """å®Œæ•´çš„Encoder-Decoder Transformerå®ç°"""
    
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, 
                 num_heads=8, num_layers=6, d_ff=2048, max_seq_len=1000):
        super().__init__()
        
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        
        # è¯åµŒå…¥å±‚
        self.src_embedding = nn.Embedding(src_vocab_size, d_model)
        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        
        # ç¼–ç å™¨å’Œè§£ç å™¨
        self.encoder = Encoder(d_model, num_heads, num_layers, d_ff)
        self.decoder = Decoder(d_model, num_heads, num_layers, d_ff)
        
        # è¾“å‡ºæŠ•å½±å±‚
        self.output_projection = nn.Linear(d_model, tgt_vocab_size)
        
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, src, tgt, src_mask=None, tgt_mask=None):
        # æºåºåˆ—ç¼–ç 
        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)
        src_emb = self.pos_encoding(src_emb)
        src_emb = self.dropout(src_emb)
        
        # ç›®æ ‡åºåˆ—ç¼–ç 
        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)
        tgt_emb = self.pos_encoding(tgt_emb)
        tgt_emb = self.dropout(tgt_emb)
        
        # ç¼–ç å™¨
        encoder_output = self.encoder(src_emb, src_mask)
        
        # è§£ç å™¨
        decoder_output = self.decoder(tgt_emb, encoder_output, tgt_mask, src_mask)
        
        # è¾“å‡ºæŠ•å½±
        output = self.output_projection(decoder_output)
        
        return output

class Encoder(nn.Module):
    """Transformerç¼–ç å™¨"""
    
    def __init__(self, d_model, num_heads, num_layers, d_ff):
        super().__init__()
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ])
    
    def forward(self, x, mask=None):
        for layer in self.layers:
            x = layer(x, mask)
        return x

class EncoderLayer(nn.Module):
    """ç¼–ç å™¨å±‚"""
    
    def __init__(self, d_model, num_heads, d_ff):
        super().__init__()
        self.self_attention = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model, d_ff)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ›å­å±‚
        attn_output = self.self_attention(x, x, x, mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # å‰é¦ˆç½‘ç»œå­å±‚
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))
        
        return x

class Decoder(nn.Module):
    """Transformerè§£ç å™¨"""
    
    def __init__(self, d_model, num_heads, num_layers, d_ff):
        super().__init__()
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ])
    
    def forward(self, x, encoder_output, tgt_mask=None, src_mask=None):
        for layer in self.layers:
            x = layer(x, encoder_output, tgt_mask, src_mask)
        return x

class DecoderLayer(nn.Module):
    """è§£ç å™¨å±‚"""
    
    def __init__(self, d_model, num_heads, d_ff):
        super().__init__()
        self.masked_self_attention = MultiHeadAttention(d_model, num_heads)
        self.cross_attention = MultiHeadAttention(d_model, num_heads)
        self.feed_forward = FeedForward(d_model, d_ff)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x, encoder_output, tgt_mask=None, src_mask=None):
        # æ©ç è‡ªæ³¨æ„åŠ›å­å±‚
        masked_attn = self.masked_self_attention(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(masked_attn))
        
        # äº¤å‰æ³¨æ„åŠ›å­å±‚
        cross_attn = self.cross_attention(x, encoder_output, encoder_output, src_mask)
        x = self.norm2(x + self.dropout(cross_attn))
        
        # å‰é¦ˆç½‘ç»œå­å±‚
        ff_output = self.feed_forward(x)
        x = self.norm3(x + self.dropout(ff_output))
        
        return x

def create_causal_mask(seq_len):
    """åˆ›å»ºå› æœæ©ç (ä¸‹ä¸‰è§’çŸ©é˜µ)"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask.masked_fill(mask == 0, float('-inf'))

def create_padding_mask(seq, pad_token=0):
    """åˆ›å»ºå¡«å……æ©ç """
    return (seq != pad_token).unsqueeze(1).unsqueeze(2)

# ä½¿ç”¨ç¤ºä¾‹
def demo_encoder_decoder():
    """æ¼”ç¤ºç¼–ç å™¨-è§£ç å™¨çš„ä½¿ç”¨"""
    
    # æ¨¡å‹å‚æ•°
    src_vocab_size = 1000
    tgt_vocab_size = 1000
    d_model = 512
    seq_len = 20
    batch_size = 2
    
    # åˆ›å»ºæ¨¡å‹
    model = EncoderDecoderTransformer(
        src_vocab_size, tgt_vocab_size, d_model
    )
    
    # æ¨¡æ‹Ÿæ•°æ®
    src = torch.randint(1, src_vocab_size, (batch_size, seq_len))
    tgt = torch.randint(1, tgt_vocab_size, (batch_size, seq_len))
    
    # åˆ›å»ºæ©ç 
    tgt_mask = create_causal_mask(seq_len)
    src_mask = create_padding_mask(src)
    
    # å‰å‘ä¼ æ’­
    output = model(src, tgt, src_mask, tgt_mask)
    
    print(f"è¾“å…¥å½¢çŠ¶: {src.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

if __name__ == "__main__":
    demo_encoder_decoder()
```

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] ç†è§£ç¼–ç å™¨-è§£ç å™¨æ¶æ„çš„æ•´ä½“è®¾è®¡
- [ ] æŒæ¡ä¸‰ç§æ³¨æ„åŠ›æœºåˆ¶çš„åŒºåˆ«å’Œä½œç”¨
- [ ] ç†è§£æ©ç æœºåˆ¶çš„å¿…è¦æ€§å’Œå®ç°
- [ ] èƒ½å¤Ÿå®ç°å®Œæ•´çš„Encoder-Decoderæ¨¡å‹
- [ ] ç†è§£ä¸ºä»€ä¹ˆç°ä»£æ¨¡å‹åå‘Decoder-Only

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸Šä¸€èŠ‚ï¼šå‰é¦ˆç¥ç»ç½‘ç»œ](ffn.md)
- [ä¸‹ä¸€èŠ‚ï¼šAttentionå‡çº§æŠ€æœ¯](../attention-advanced/index.md)
- [è¿”å›ï¼šTransformeråŸºç¡€æ¦‚è§ˆ](index.md)