# TokenizeræŠ€æœ¯

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æ·±å…¥ç†è§£Tokenizerçš„å·¥ä½œåŸç†ï¼ŒæŒæ¡ä¸åŒtokenizationç®—æ³•çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ï¼Œä¸ºå¤§æ¨¡å‹çš„æ–‡æœ¬å¤„ç†å¥ å®šåŸºç¡€ã€‚

## ğŸ“ æŠ€æœ¯åŸç†è§£æ

### TokenizeråŸºæœ¬æ¦‚å¿µ

**Tokenizer**æ˜¯å°†åŸå§‹æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯å¤„ç†çš„æ•°å€¼è¡¨ç¤ºçš„å…³é”®ç»„ä»¶ï¼Œæ˜¯è¿æ¥äººç±»è¯­è¨€å’Œæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¡¥æ¢ã€‚

#### æ ¸å¿ƒåŠŸèƒ½
```python
# Tokenizerçš„åŸºæœ¬æµç¨‹
text = "Hello, world! This is a test."
     â†“ 1. æ–‡æœ¬è§„èŒƒåŒ–(Normalization)
normalized = "hello world this is a test"
     â†“ 2. é¢„åˆ†è¯(Pre-tokenization)  
pre_tokens = ["hello", "world", "this", "is", "a", "test"]
     â†“ 3. æ¨¡å‹å¤„ç†(Model Processing)
tokens = ["hello", "wor", "##ld", "this", "is", "a", "test"]
     â†“ 4. åå¤„ç†(Post-processing)
final_tokens = ["[CLS]", "hello", "wor", "##ld", "this", "is", "a", "test", "[SEP]"]
     â†“ 5. æ•°å€¼æ˜ å°„
token_ids = [101, 7592, 24829, 2094, 2023, 2003, 1037, 3231, 102]
```

### ä¸‰å¤§TokenizationèŒƒå¼

#### 1. è¯çº§åˆ«åˆ†è¯ (Word-level)

**åŸç†**: å°†æ–‡æœ¬æŒ‰å®Œæ•´å•è¯åˆ†å‰²

```python
class WordLevelTokenizer:
    def __init__(self, vocab_path):
        self.word_to_id = self.load_vocab(vocab_path)
        self.id_to_word = {v: k for k, v in self.word_to_id.items()}
        self.unk_token = "[UNK]"
    
    def tokenize(self, text):
        words = text.lower().split()
        tokens = []
        for word in words:
            if word in self.word_to_id:
                tokens.append(word)
            else:
                tokens.append(self.unk_token)  # æœªçŸ¥è¯å¤„ç†
        return tokens
    
    def encode(self, text):
        tokens = self.tokenize(text)
        return [self.word_to_id.get(token, self.word_to_id[self.unk_token]) 
                for token in tokens]
```

**ä¼˜åŠ¿**:
- ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
- ç¬¦åˆäººç±»è®¤çŸ¥ä¹ æƒ¯
- å®ç°ç®€å•ç›´è§‚

**åŠ£åŠ¿**:
- è¯æ±‡è¡¨åºå¤§(é€šå¸¸>100k)
- æœªç™»å½•è¯(OOV)é—®é¢˜ä¸¥é‡
- æ— æ³•å¤„ç†å½¢æ€å˜åŒ–ä¸°å¯Œçš„è¯­è¨€

#### 2. å­—ç¬¦çº§åˆ«åˆ†è¯ (Character-level)

**åŸç†**: å°†æ–‡æœ¬åˆ†è§£ä¸ºå•ä¸ªå­—ç¬¦

```python
class CharacterLevelTokenizer:
    def __init__(self):
        # åŸºæœ¬å­—ç¬¦é›†
        self.chars = list("abcdefghijklmnopqrstuvwxyz0123456789 .,!?'")
        self.char_to_id = {char: i for i, char in enumerate(self.chars)}
        self.id_to_char = {i: char for i, char in enumerate(self.chars)}
    
    def tokenize(self, text):
        return list(text.lower())
    
    def encode(self, text):
        chars = self.tokenize(text)
        return [self.char_to_id.get(char, 0) for char in chars]  # 0 for unknown
    
    def decode(self, token_ids):
        chars = [self.id_to_char.get(id, "") for id in token_ids]
        return "".join(chars)
```

**ä¼˜åŠ¿**:
- è¯æ±‡è¡¨æå°(é€šå¸¸<100)
- æ— OOVé—®é¢˜
- å¯¹æ‹¼å†™é”™è¯¯é²æ£’

**åŠ£åŠ¿**:
- åºåˆ—é•¿åº¦å¤§å¹…å¢åŠ 
- ä¸¢å¤±è¯è¾¹ç•Œä¿¡æ¯
- è¯­ä¹‰ç†è§£å›°éš¾

#### 3. å­è¯çº§åˆ«åˆ†è¯ (Subword-level)

**æ ¸å¿ƒæ€æƒ³**: åœ¨è¯æ±‡è¡¨å¤§å°å’Œè¯­ä¹‰ä¿æŒä¹‹é—´æ‰¾åˆ°å¹³è¡¡

## ğŸ”¬ ä¸»æµSubwordç®—æ³•è¯¦è§£

### 1. BPE (Byte Pair Encoding)

**ç®—æ³•åŸç†**: è¿­ä»£åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹

#### è®­ç»ƒè¿‡ç¨‹
```python
class BPETokenizer:
    def __init__(self):
        self.vocab = {}
        self.merges = []
    
    def train(self, corpus, vocab_size):
        # 1. åˆå§‹åŒ–ï¼šå°†æ‰€æœ‰å­—ç¬¦ä½œä¸ºåŸºç¡€è¯æ±‡
        word_freqs = self.get_word_frequencies(corpus)
        
        # å°†æ¯ä¸ªè¯åˆ†è§£ä¸ºå­—ç¬¦
        vocab = set()
        for word in word_freqs:
            for char in word:
                vocab.add(char)
        
        vocab = list(vocab)
        
        # 2. è¿­ä»£åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹
        while len(vocab) < vocab_size:
            # ç»Ÿè®¡æ‰€æœ‰å­—ç¬¦å¯¹çš„é¢‘ç‡
            pairs = self.get_all_pairs(word_freqs)
            
            if not pairs:
                break
            
            # æ‰¾åˆ°æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹
            best_pair = max(pairs, key=pairs.get)
            
            # åˆå¹¶è¿™ä¸ªå­—ç¬¦å¯¹
            vocab.append(''.join(best_pair))
            self.merges.append(best_pair)
            
            # æ›´æ–°è¯é¢‘å­—å…¸
            word_freqs = self.merge_vocab(best_pair, word_freqs)
        
        self.vocab = {token: i for i, token in enumerate(vocab)}
    
    def get_all_pairs(self, word_freqs):
        """è·å–æ‰€æœ‰ç›¸é‚»å­—ç¬¦å¯¹åŠå…¶é¢‘ç‡"""
        pairs = {}
        for word, freq in word_freqs.items():
            symbols = word.split()
            for i in range(len(symbols) - 1):
                pair = (symbols[i], symbols[i + 1])
                pairs[pair] = pairs.get(pair, 0) + freq
        return pairs
    
    def merge_vocab(self, pair, word_freqs):
        """åˆå¹¶æŒ‡å®šå­—ç¬¦å¯¹"""
        new_word_freqs = {}
        bigram = ' '.join(pair)
        replacement = ''.join(pair)
        
        for word in word_freqs:
            new_word = word.replace(bigram, replacement)
            new_word_freqs[new_word] = word_freqs[word]
        
        return new_word_freqs
    
    def tokenize(self, text):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„BPEè¿›è¡Œåˆ†è¯"""
        words = text.split()
        result = []
        
        for word in words:
            # å°†è¯åˆ†è§£ä¸ºå­—ç¬¦
            word_tokens = list(word)
            
            # åº”ç”¨æ‰€æœ‰å­¦åˆ°çš„åˆå¹¶è§„åˆ™
            for merge in self.merges:
                word_tokens = self.apply_merge(word_tokens, merge)
            
            result.extend(word_tokens)
        
        return result
    
    def apply_merge(self, tokens, merge):
        """åº”ç”¨å•ä¸ªåˆå¹¶è§„åˆ™"""
        new_tokens = []
        i = 0
        while i < len(tokens):
            if (i < len(tokens) - 1 and 
                tokens[i] == merge[0] and 
                tokens[i + 1] == merge[1]):
                # æ‰¾åˆ°åŒ¹é…çš„å¯¹ï¼Œåˆå¹¶
                new_tokens.append(''.join(merge))
                i += 2
            else:
                new_tokens.append(tokens[i])
                i += 1
        return new_tokens

# ä½¿ç”¨ç¤ºä¾‹
def demo_bpe():
    corpus = ["low lower newest widest", "low lower newest widest"] * 1000
    
    bpe = BPETokenizer()
    bpe.train(corpus, vocab_size=1000)
    
    # æµ‹è¯•åˆ†è¯
    text = "lowest"
    tokens = bpe.tokenize(text)
    print(f"'{text}' -> {tokens}")
    # å¯èƒ½è¾“å‡º: ['low', 'est'] æˆ–ç±»ä¼¼çš„å­è¯ç»„åˆ

demo_bpe()
```

**BPEç‰¹ç‚¹**:
- æ•°æ®é©±åŠ¨ï¼Œæ— éœ€è¯­è¨€å­¦çŸ¥è¯†
- èƒ½å¤„ç†æœªè§è¿‡çš„è¯
- GPTç³»åˆ—æ¨¡å‹çš„æ ‡å‡†é€‰æ‹©

### 2. WordPiece

**æ ¸å¿ƒæ”¹è¿›**: åŸºäºè¯­è¨€æ¨¡å‹ä¼¼ç„¶åº¦é€‰æ‹©åˆå¹¶

```python
class WordPieceTokenizer:
    def __init__(self):
        self.vocab = {}
        self.unk_token = "[UNK]"
    
    def train(self, corpus, vocab_size):
        # 1. åˆå§‹åŒ–åŸºç¡€è¯æ±‡(å­—ç¬¦ + ç‰¹æ®Štoken)
        base_vocab = set()
        for text in corpus:
            for char in text:
                base_vocab.add(char)
        
        base_vocab.update(["[UNK]", "[CLS]", "[SEP]", "[MASK]"])
        vocab = list(base_vocab)
        
        # 2. è¿­ä»£æ·»åŠ æœ€ä¼˜å­è¯
        while len(vocab) < vocab_size:
            best_subword = self.find_best_subword(corpus, vocab)
            if best_subword is None:
                break
            vocab.append(best_subword)
        
        self.vocab = {token: i for i, token in enumerate(vocab)}
    
    def find_best_subword(self, corpus, current_vocab):
        """æ‰¾åˆ°èƒ½æœ€å¤§åŒ–è¯­è¨€æ¨¡å‹ä¼¼ç„¶åº¦çš„å­è¯"""
        candidates = self.generate_candidates(corpus, current_vocab)
        
        best_score = float('-inf')
        best_subword = None
        
        for candidate in candidates:
            # è®¡ç®—æ·»åŠ è¿™ä¸ªå­è¯åçš„è¯­è¨€æ¨¡å‹å¾—åˆ†
            score = self.calculate_lm_score(corpus, current_vocab + [candidate])
            
            if score > best_score:
                best_score = score
                best_subword = candidate
        
        return best_subword
    
    def generate_candidates(self, corpus, vocab):
        """ç”Ÿæˆå€™é€‰å­è¯"""
        candidates = set()
        
        # åŸºäºç°æœ‰è¯æ±‡ç”Ÿæˆå€™é€‰
        for text in corpus:
            tokens = self.basic_tokenize(text)
            for token in tokens:
                for i in range(len(token)):
                    for j in range(i + 1, len(token) + 1):
                        subword = token[i:j]
                        if len(subword) > 1 and subword not in vocab:
                            candidates.add(subword)
        
        return list(candidates)
    
    def tokenize(self, text):
        """WordPieceåˆ†è¯ç®—æ³•"""
        tokens = []
        
        for word in text.split():
            # è´ªå¿ƒæœ€é•¿åŒ¹é…
            start = 0
            sub_tokens = []
            
            while start < len(word):
                end = len(word)
                cur_substr = None
                
                # ä»æœ€é•¿å­ä¸²å¼€å§‹å°è¯•
                while start < end:
                    substr = word[start:end]
                    if start > 0:
                        substr = "##" + substr  # WordPieceå‰ç¼€
                    
                    if substr in self.vocab:
                        cur_substr = substr
                        break
                    end -= 1
                
                if cur_substr is None:
                    # æ— æ³•åˆ†è¯ï¼Œæ ‡è®°ä¸ºæœªçŸ¥
                    sub_tokens.append(self.unk_token)
                    break
                
                sub_tokens.append(cur_substr)
                start = end
            
            tokens.extend(sub_tokens)
        
        return tokens

# BERTä½¿ç”¨çš„WordPieceç¤ºä¾‹
def demo_wordpiece():
    text = "unaffable"
    # WordPieceå¯èƒ½åˆ†è¯ä¸º: ["un", "##aff", "##able"]
    
    wp = WordPieceTokenizer()
    # å‡è®¾å·²è®­ç»ƒ
    wp.vocab = {"un": 1, "##aff": 2, "##able": 3, "[UNK]": 0}
    
    tokens = wp.tokenize(text)
    print(f"'{text}' -> {tokens}")

demo_wordpiece()
```

**WordPieceä¼˜åŠ¿**:
- åŸºäºè¯­è¨€æ¨¡å‹ä¼˜åŒ–ï¼Œç†è®ºæ›´ä¸¥æ ¼
- BERTç­‰æ¨¡å‹çš„æ ‡å‡†é€‰æ‹©
- èƒ½æ›´å¥½åœ°ä¿æŒè¯­ä¹‰ç›¸å…³æ€§

### 3. Unigram Language Model

**æ ¸å¿ƒæ€æƒ³**: ä»å¤§è¯æ±‡è¡¨å¼€å§‹ï¼Œé€æ­¥ç§»é™¤ä¸é‡è¦çš„token

```python
class UnigramTokenizer:
    def __init__(self):
        self.vocab = {}
        self.token_probs = {}
    
    def train(self, corpus, vocab_size):
        # 1. åˆå§‹åŒ–å¤§è¯æ±‡è¡¨ï¼ˆåŒ…å«æ‰€æœ‰å¯èƒ½çš„å­ä¸²ï¼‰
        initial_vocab = self.build_initial_vocab(corpus)
        
        # 2. åˆå§‹åŒ–tokenæ¦‚ç‡
        self.token_probs = self.initialize_probabilities(initial_vocab, corpus)
        
        # 3. è¿­ä»£å‡å°‘è¯æ±‡è¡¨
        current_vocab = list(initial_vocab)
        
        while len(current_vocab) > vocab_size:
            # è®¡ç®—ç§»é™¤æ¯ä¸ªtokençš„æŸå¤±
            losses = {}
            for token in current_vocab:
                if self.is_removable(token):  # ä¿æŠ¤ç‰¹æ®Štoken
                    losses[token] = self.calculate_removal_loss(token, corpus)
            
            # ç§»é™¤æŸå¤±æœ€å°çš„token
            if losses:
                token_to_remove = min(losses, key=losses.get)
                current_vocab.remove(token_to_remove)
                del self.token_probs[token_to_remove]
                
                # é‡æ–°è®¡ç®—æ¦‚ç‡
                self.update_probabilities(current_vocab, corpus)
        
        self.vocab = {token: i for i, token in enumerate(current_vocab)}
    
    def calculate_removal_loss(self, token, corpus):
        """è®¡ç®—ç§»é™¤tokençš„ä¼¼ç„¶åº¦æŸå¤±"""
        total_loss = 0
        
        for text in corpus:
            # è®¡ç®—æœ‰tokenæ—¶çš„æœ€ä¼˜åˆ†è¯ä¼¼ç„¶åº¦
            with_token = self.get_best_segmentation(text, include_token=token)
            
            # è®¡ç®—æ— tokenæ—¶çš„æœ€ä¼˜åˆ†è¯ä¼¼ç„¶åº¦  
            without_token = self.get_best_segmentation(text, exclude_token=token)
            
            # æŸå¤± = æ— tokenä¼¼ç„¶åº¦ - æœ‰tokenä¼¼ç„¶åº¦
            loss = without_token['log_prob'] - with_token['log_prob']
            total_loss += loss
        
        return total_loss
    
    def get_best_segmentation(self, text, include_token=None, exclude_token=None):
        """ä½¿ç”¨åŠ¨æ€è§„åˆ’æ‰¾æœ€ä¼˜åˆ†è¯"""
        n = len(text)
        
        # dp[i] = (æœ€ä¼˜å¯¹æ•°ä¼¼ç„¶åº¦, åˆ†è¯æ–¹æ¡ˆ)
        dp = [(-float('inf'), [])] * (n + 1)
        dp[0] = (0.0, [])
        
        for i in range(n + 1):
            if dp[i][0] == -float('inf'):
                continue
                
            for j in range(i + 1, n + 1):
                token = text[i:j]
                
                # æ£€æŸ¥tokenæ˜¯å¦å¯ç”¨
                if exclude_token and token == exclude_token:
                    continue
                if include_token and token not in self.token_probs and token != include_token:
                    continue
                if token not in self.token_probs:
                    continue
                
                # è®¡ç®—æ–°çš„ä¼¼ç„¶åº¦
                token_prob = self.token_probs.get(token, 1e-10)
                new_prob = dp[i][0] + math.log(token_prob)
                
                if new_prob > dp[j][0]:
                    dp[j] = (new_prob, dp[i][1] + [token])
        
        return {'log_prob': dp[n][0], 'tokens': dp[n][1]}
    
    def tokenize(self, text):
        """ä½¿ç”¨è®­ç»ƒå¥½çš„Unigramæ¨¡å‹åˆ†è¯"""
        result = self.get_best_segmentation(text)
        return result['tokens']

# SentencePieceä½¿ç”¨çš„Unigramç¤ºä¾‹
def demo_unigram():
    corpus = ["hello world", "hello universe", "hi world"]
    
    unigram = UnigramTokenizer()
    unigram.train(corpus, vocab_size=20)
    
    text = "hello world"
    tokens = unigram.tokenize(text)
    print(f"'{text}' -> {tokens}")

demo_unigram()
```

### 4. å­—èŠ‚çº§BPE (Byte-level BPE)

**åˆ›æ–°ç‚¹**: åœ¨UTF-8å­—èŠ‚çº§åˆ«è¿›è¡ŒBPE

```python
class ByteLevelBPE:
    def __init__(self):
        # UTF-8å­—èŠ‚åˆ°å¯æ‰“å°å­—ç¬¦çš„æ˜ å°„
        self.byte_encoder = self.bytes_to_unicode()
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
    
    def bytes_to_unicode(self):
        """åˆ›å»ºå­—èŠ‚åˆ°Unicodeå­—ç¬¦çš„æ˜ å°„"""
        bs = list(range(ord("!"), ord("~")+1))+list(range(ord("Â¡"), ord("Â¬")+1))+list(range(ord("Â®"), ord("Ã¿")+1))
        cs = bs[:]
        n = 0
        for b in range(2**8):
            if b not in bs:
                bs.append(b)
                cs.append(2**8+n)
                n += 1
        cs = [chr(n) for n in cs]
        return dict(zip(bs, cs))
    
    def encode_text_to_bytes(self, text):
        """å°†æ–‡æœ¬ç¼–ç ä¸ºå­—èŠ‚çº§è¡¨ç¤º"""
        byte_encoded = []
        for char in text:
            # è·å–UTF-8å­—èŠ‚
            utf8_bytes = char.encode('utf-8')
            for byte in utf8_bytes:
                byte_encoded.append(self.byte_encoder[byte])
        return ''.join(byte_encoded)
    
    def decode_bytes_to_text(self, byte_string):
        """å°†å­—èŠ‚çº§è¡¨ç¤ºè§£ç å›æ–‡æœ¬"""
        bytes_list = []
        for char in byte_string:
            bytes_list.append(self.byte_decoder[char])
        
        byte_array = bytes(bytes_list)
        return byte_array.decode('utf-8', errors='replace')
    
    def train(self, corpus, vocab_size):
        """åœ¨å­—èŠ‚çº§åˆ«è®­ç»ƒBPE"""
        # 1. å°†æ‰€æœ‰æ–‡æœ¬è½¬æ¢ä¸ºå­—èŠ‚çº§è¡¨ç¤º
        byte_corpus = []
        for text in corpus:
            byte_text = self.encode_text_to_bytes(text)
            byte_corpus.append(byte_text)
        
        # 2. åœ¨å­—èŠ‚çº§åˆ«åº”ç”¨æ ‡å‡†BPE
        self.bpe = BPETokenizer()
        self.bpe.train(byte_corpus, vocab_size)
    
    def tokenize(self, text):
        """å­—èŠ‚çº§BPEåˆ†è¯"""
        # è½¬æ¢ä¸ºå­—èŠ‚çº§è¡¨ç¤º
        byte_text = self.encode_text_to_bytes(text)
        
        # åº”ç”¨BPE
        byte_tokens = self.bpe.tokenize(byte_text)
        
        return byte_tokens

# GPT-2ä½¿ç”¨çš„Byte-level BPE
def demo_byte_bpe():
    text = "Hello, ä¸–ç•Œ!"  # åŒ…å«ä¸­æ–‡
    
    bbpe = ByteLevelBPE()
    byte_text = bbpe.encode_text_to_bytes(text)
    print(f"å­—èŠ‚çº§ç¼–ç : {byte_text}")
    
    decoded = bbpe.decode_bytes_to_text(byte_text)
    print(f"è§£ç ç»“æœ: {decoded}")

demo_byte_bpe()
```

**å­—èŠ‚çº§BPEä¼˜åŠ¿**:
- é€šç”¨æ€§å¼ºï¼Œæ”¯æŒæ‰€æœ‰è¯­è¨€
- è¯æ±‡è¡¨ç´§å‡‘
- GPT-2/3/4çš„æ ‡å‡†é€‰æ‹©

## ğŸ’¬ ç°ä»£Tokenizerå‘å±•è¶‹åŠ¿

### 1. å¤§æ¦‚å¿µæ¨¡å‹ (Large Concept Models)

**çªç ´æ€§æ€è·¯**: è¶…è¶Štokençº§åˆ«ï¼Œç›´æ¥å¤„ç†æ¦‚å¿µçº§åˆ«

```python
class ConceptLevelTokenizer:
    """æ¦‚å¿µçº§åˆ«çš„tokenizer (ç†è®ºæ¨¡å‹)"""
    
    def __init__(self, concept_encoder):
        self.concept_encoder = concept_encoder  # é¢„è®­ç»ƒçš„æ¦‚å¿µç¼–ç å™¨
    
    def encode_to_concepts(self, text):
        """å°†æ–‡æœ¬ç›´æ¥ç¼–ç ä¸ºæ¦‚å¿µå‘é‡"""
        # ä½¿ç”¨å¥å­çº§åˆ«çš„ç¼–ç å™¨
        sentences = self.split_to_sentences(text)
        
        concept_vectors = []
        for sentence in sentences:
            # å°†å¥å­ç¼–ç ä¸ºæ¦‚å¿µå‘é‡è€Œétokenåºåˆ—
            concept_vec = self.concept_encoder.encode(sentence)
            concept_vectors.append(concept_vec)
        
        return concept_vectors
    
    def decode_from_concepts(self, concept_vectors):
        """ä»æ¦‚å¿µå‘é‡è§£ç å›æ–‡æœ¬"""
        sentences = []
        for concept_vec in concept_vectors:
            sentence = self.concept_encoder.decode(concept_vec)
            sentences.append(sentence)
        
        return ' '.join(sentences)
```

### 2. åŠ¨æ€ä¸Šä¸‹æ–‡åˆ†è¯

**æ ¸å¿ƒæ€æƒ³**: æ ¹æ®ä¸Šä¸‹æ–‡åŠ¨æ€è°ƒæ•´åˆ†è¯ç­–ç•¥

```python
class ContextAwareTokenizer:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åŠ¨æ€åˆ†è¯å™¨"""
    
    def __init__(self, base_tokenizer, context_model):
        self.base_tokenizer = base_tokenizer
        self.context_model = context_model
    
    def tokenize_with_context(self, text, context=""):
        """åŸºäºä¸Šä¸‹æ–‡çš„åŠ¨æ€åˆ†è¯"""
        
        # 1. åˆ†æä¸Šä¸‹æ–‡ç¡®å®šåˆ†è¯ç­–ç•¥
        context_features = self.analyze_context(context)
        
        # 2. æ ¹æ®ä¸Šä¸‹æ–‡é€‰æ‹©åˆ†è¯ç²’åº¦
        if context_features['domain'] == 'technical':
            # æŠ€æœ¯æ–‡æœ¬ï¼šæ›´ç»†ç²’åº¦åˆ†è¯
            granularity = 'fine'
        elif context_features['domain'] == 'casual':
            # æ—¥å¸¸å¯¹è¯ï¼šè¾ƒç²—ç²’åº¦åˆ†è¯
            granularity = 'coarse'
        else:
            granularity = 'medium'
        
        # 3. åº”ç”¨åŠ¨æ€åˆ†è¯
        tokens = self.adaptive_tokenize(text, granularity)
        
        return tokens
    
    def adaptive_tokenize(self, text, granularity):
        """è‡ªé€‚åº”åˆ†è¯"""
        if granularity == 'fine':
            # æ›´å¤šå­è¯åˆ†å‰²
            return self.base_tokenizer.tokenize(text, merge_threshold=0.3)
        elif granularity == 'coarse':
            # æ›´å°‘å­è¯åˆ†å‰²
            return self.base_tokenizer.tokenize(text, merge_threshold=0.8)
        else:
            # æ ‡å‡†åˆ†è¯
            return self.base_tokenizer.tokenize(text)
```

### 3. å¤šæ¨¡æ€Tokenizer

**æ‰©å±•æ€è·¯**: ç»Ÿä¸€å¤„ç†æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ç­‰å¤šç§æ¨¡æ€

```python
class MultimodalTokenizer:
    """å¤šæ¨¡æ€ç»Ÿä¸€tokenizer"""
    
    def __init__(self):
        self.text_tokenizer = BPETokenizer()
        self.image_tokenizer = ImagePatchTokenizer()
        self.audio_tokenizer = AudioSegmentTokenizer()
        
        # ç»Ÿä¸€è¯æ±‡è¡¨
        self.unified_vocab = self.build_unified_vocab()
    
    def tokenize_multimodal(self, inputs):
        """å¤šæ¨¡æ€è¾“å…¥çš„ç»Ÿä¸€åˆ†è¯"""
        unified_tokens = []
        
        for modality, data in inputs.items():
            if modality == 'text':
                tokens = self.text_tokenizer.tokenize(data)
                # æ·»åŠ æ¨¡æ€æ ‡è¯†
                unified_tokens.extend([f"<text>{token}" for token in tokens])
                
            elif modality == 'image':
                patches = self.image_tokenizer.tokenize(data)
                unified_tokens.extend([f"<image>{patch}" for patch in patches])
                
            elif modality == 'audio':
                segments = self.audio_tokenizer.tokenize(data)
                unified_tokens.extend([f"<audio>{segment}" for segment in segments])
        
        return unified_tokens
```

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: BPEã€WordPieceã€Unigramçš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

**ç®€æ´å¯¹æ¯”**:

| ç»´åº¦ | BPE | WordPiece | Unigram |
|------|-----|-----------|---------|
| **æ ¸å¿ƒç­–ç•¥** | é¢‘ç‡é©±åŠ¨åˆå¹¶ | ä¼¼ç„¶åº¦é©±åŠ¨åˆå¹¶ | æ¦‚ç‡é©±åŠ¨å‰ªæ |
| **è®­ç»ƒæ–¹å‘** | ä»å­—ç¬¦å‘ä¸Šæ„å»º | ä»å­—ç¬¦å‘ä¸Šæ„å»º | ä»å®Œæ•´è¯æ±‡å‘ä¸‹å‰ªæ |
| **é€‰æ‹©æ ‡å‡†** | å­—ç¬¦å¯¹é¢‘ç‡ | è¯­è¨€æ¨¡å‹ä¼¼ç„¶åº¦ | Tokenç§»é™¤æŸå¤± |
| **ä»£è¡¨æ¨¡å‹** | GPTç³»åˆ— | BERTç³»åˆ— | T5ã€mT5 |

### Q2: ä¸ºä»€ä¹ˆç°ä»£å¤§æ¨¡å‹å¤šé‡‡ç”¨Byte-level BPEï¼Ÿ

**æ ¸å¿ƒä¼˜åŠ¿**:

1. **é€šç”¨æ€§**: æ”¯æŒæ‰€æœ‰è¯­è¨€å’Œå­—ç¬¦ï¼Œæ— éœ€ç‰¹æ®Šé¢„å¤„ç†
2. **ç´§å‡‘æ€§**: åŸºç¡€è¯æ±‡è¡¨åªéœ€256ä¸ªå­—èŠ‚
3. **é²æ£’æ€§**: èƒ½å¤„ç†ä»»ä½•UTF-8ç¼–ç çš„æ–‡æœ¬
4. **ä¸€è‡´æ€§**: è®­ç»ƒå’Œæ¨ç†æ—¶çš„å¤„ç†å®Œå…¨ä¸€è‡´

**æŠ€æœ¯ç»†èŠ‚**:
```python
# ä¼ ç»ŸBPEå¯èƒ½é‡åˆ°çš„é—®é¢˜
text = "cafÃ©"  # åŒ…å«é‡éŸ³å­—ç¬¦
# å¯èƒ½å¯¼è‡´ç¼–ç ä¸ä¸€è‡´æˆ–OOVé—®é¢˜

# Byte-level BPEçš„è§£å†³æ–¹æ¡ˆ
bytes_representation = text.encode('utf-8')  # [99, 97, 102, 195, 169]
# æ¯ä¸ªå­—èŠ‚éƒ½æœ‰å›ºå®šçš„æ˜ å°„ï¼Œä¿è¯ä¸€è‡´æ€§
```

### Q3: Tokenizerçš„è¯æ±‡è¡¨å¤§å°å¦‚ä½•é€‰æ‹©ï¼Ÿ

**æƒè¡¡è€ƒè™‘**:

**è¯æ±‡è¡¨è¿‡å°**:
- åºåˆ—å˜é•¿ï¼Œè®¡ç®—æˆæœ¬å¢åŠ 
- è¯­ä¹‰ä¿¡æ¯ä¸¢å¤±
- é•¿è·ç¦»ä¾èµ–å»ºæ¨¡å›°éš¾

**è¯æ±‡è¡¨è¿‡å¤§**:
- åµŒå…¥å±‚å‚æ•°æ¿€å¢
- ç¨€æœ‰tokenè®­ç»ƒä¸å……åˆ†
- æ¨ç†æ—¶å†…å­˜å ç”¨å¤§

**ç»éªŒæ³•åˆ™**:
```python
# å¸¸è§é…ç½®
model_configs = {
    'GPT-2': {'vocab_size': 50257, 'algorithm': 'Byte-level BPE'},
    'BERT': {'vocab_size': 30522, 'algorithm': 'WordPiece'},
    'T5': {'vocab_size': 32128, 'algorithm': 'SentencePiece Unigram'},
    'LLaMA': {'vocab_size': 32000, 'algorithm': 'SentencePiece BPE'}
}

# é€‰æ‹©åŸåˆ™ï¼š
# 1. è‹±æ–‡ï¼š20k-50kè¾ƒä¸ºåˆé€‚
# 2. å¤šè¯­è¨€ï¼š50k-100k
# 3. ä»£ç ï¼šç‰¹æ®Šè€ƒè™‘ï¼Œå¯èƒ½éœ€è¦æ›´å¤§è¯æ±‡è¡¨
```

### Q4: å¦‚ä½•è¯„ä¼°Tokenizerçš„å¥½åï¼Ÿ

**è¯„ä¼°ç»´åº¦**:

1. **å‹ç¼©æ•ˆç‡**: `compression_ratio = original_chars / num_tokens`
2. **è¯æ±‡è¦†ç›–ç‡**: æµ‹è¯•é›†ä¸­UNK tokençš„æ¯”ä¾‹
3. **è¯­ä¹‰ä¿æŒåº¦**: é‡è¦è¯æ±‡æ˜¯å¦è¢«åˆç†åˆ†å‰²
4. **fertility**: å¹³å‡æ¯ä¸ªè¯è¢«åˆ†æˆå¤šå°‘ä¸ªtoken

```python
def evaluate_tokenizer(tokenizer, test_corpus):
    """tokenizerè¯„ä¼°å‡½æ•°"""
    
    total_chars = sum(len(text) for text in test_corpus)
    total_tokens = sum(len(tokenizer.tokenize(text)) for text in test_corpus)
    
    # å‹ç¼©æ¯”
    compression_ratio = total_chars / total_tokens
    
    # UNKæ¯”ä¾‹
    unk_count = sum(text.count('[UNK]') for text in 
                   [' '.join(tokenizer.tokenize(text)) for text in test_corpus])
    unk_ratio = unk_count / total_tokens
    
    # Fertility (è¯æ±‡åˆ†å‰²åº¦)
    word_tokens = []
    for text in test_corpus:
        words = text.split()
        for word in words:
            tokens = tokenizer.tokenize(word)
            word_tokens.append(len(tokens))
    
    fertility = sum(word_tokens) / len(word_tokens)
    
    return {
        'compression_ratio': compression_ratio,
        'unk_ratio': unk_ratio,
        'fertility': fertility
    }
```

## ğŸ’» å®Œæ•´å®ç°ç¤ºä¾‹

```python
# ç°ä»£Tokenizerçš„å®Œæ•´å®ç°ç¤ºä¾‹
from transformers import AutoTokenizer

class ModernTokenizerExample:
    """ç°ä»£tokenizerä½¿ç”¨ç¤ºä¾‹"""
    
    def __init__(self, model_name="gpt2"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
    def demonstrate_tokenization(self):
        """æ¼”ç¤ºå„ç§tokenizationåœºæ™¯"""
        
        test_cases = [
            "Hello, world!",
            "artificial intelligence",
            "æœªæ¥çš„äººå·¥æ™ºèƒ½",
            "cafÃ© rÃ©sumÃ© naÃ¯ve",
            "COVID-19 vaccination",
            "user@example.com",
            "print('Hello, World!')"
        ]
        
        print("=== Tokenizationæ¼”ç¤º ===")
        for text in test_cases:
            tokens = self.tokenizer.tokenize(text)
            token_ids = self.tokenizer.encode(text)
            decoded = self.tokenizer.decode(token_ids)
            
            print(f"\nåŸæ–‡: {text}")
            print(f"Tokens: {tokens}")
            print(f"Token IDs: {token_ids}")
            print(f"è§£ç : {decoded}")
            print(f"å‹ç¼©æ¯”: {len(text)/len(tokens):.2f}")
    
    def analyze_special_tokens(self):
        """åˆ†æç‰¹æ®Štokençš„å¤„ç†"""
        
        special_cases = [
            "Mr. Smith went to the U.S.A.",  # ç¼©å†™
            "She said, \"Hello!\"",          # å¼•å·
            "Visit https://example.com",      # URL
            "Temperature: 25.6Â°C",           # ç¬¦å·å’Œæ•°å­—
            "   extra    spaces   ",         # å¤šä½™ç©ºæ ¼
        ]
        
        print("\n=== ç‰¹æ®Šæƒ…å†µå¤„ç† ===")
        for text in special_cases:
            tokens = self.tokenizer.tokenize(text)
            print(f"'{text}' -> {tokens}")
    
    def compare_tokenizers(self):
        """å¯¹æ¯”ä¸åŒtokenizer"""
        
        models = ['bert-base-uncased', 'gpt2', 'facebook/bart-base']
        test_text = "The quick brown fox jumps over the lazy dog."
        
        print("\n=== Tokenizerå¯¹æ¯” ===")
        for model_name in models:
            try:
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                tokens = tokenizer.tokenize(test_text)
                
                print(f"\n{model_name}:")
                print(f"Vocab size: {tokenizer.vocab_size}")
                print(f"Tokens: {tokens}")
                print(f"Token count: {len(tokens)}")
                
            except Exception as e:
                print(f"Error loading {model_name}: {e}")

# è¿è¡Œæ¼”ç¤º
if __name__ == "__main__":
    demo = ModernTokenizerExample()
    demo.demonstrate_tokenization()
    demo.analyze_special_tokens()
    demo.compare_tokenizers()
```

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] ç†è§£Tokenizerçš„åŸºæœ¬å·¥ä½œæµç¨‹
- [ ] æŒæ¡BPEã€WordPieceã€Unigramçš„æ ¸å¿ƒç®—æ³•
- [ ] äº†è§£Byte-level BPEçš„ä¼˜åŠ¿
- [ ] èƒ½åˆ†æä¸åŒtokenizerçš„é€‚ç”¨åœºæ™¯
- [ ] ç†è§£tokenizerå¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸Šä¸€èŠ‚ï¼šè¯­è¨€æ¨¡å‹æ¶æ„](language-models.md)
- [ä¸‹ä¸€èŠ‚ï¼šAttentionå‡çº§æŠ€æœ¯](../attention-advanced/index.md)
- [è¿”å›ï¼šTransformeråŸºç¡€æ¦‚è§ˆ](index.md)