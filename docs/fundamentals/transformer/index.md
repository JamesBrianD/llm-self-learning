# 第1节：Transformer基础

## 🎯 学习目标

掌握Transformer架构的核心概念，理解Attention机制原理，区分不同的语言模型架构，为面试做好基础准备。

**重点面试问题预览：**
- Attention计算公式和原理
- Encoder-Only vs Decoder-Only架构差异
- 为什么主流大模型选择Decoder-Only
- GPT和BERT的架构区别

## 📅 学习计划

**建议学习时间：2天**

- **Day 1**: Attention机制深入理解
- **Day 2**: 语言模型架构对比分析

## 📚 学习路径

### 1. [Attention机制](attention.md)
- 自注意力计算公式推导
- Softmax和缩放因子作用
- 多头注意力机制
- 代码实现练习

### 2. [前馈神经网络](ffn.md)
- FFN结构和功能原理
- 激活函数演进(ReLU→GELU→SwiGLU)
- 知识存储机制
- 与注意力的互补关系

### 3. [编码器-解码器架构](encoder-decoder.md)
- 原始Transformer完整架构
- 三种注意力机制详解
- 掩码机制和交叉注意力
- 现代架构演进趋势

### 4. [语言模型架构](language-models.md)  
- Encoder-Only vs Decoder-Only
- GPT vs BERT架构对比
- 主流模型选择分析
- 模型架构图解

## ✅ 学习检验标准

完成以下两项才算掌握本节：

1. **问题解答**: 能用自己的话回答所有面试问题
2. **代码实现**: 完成Self-Attention和Multi-Head Attention的编程练习

## 🚀 开始学习

选择一个子模块开始你的学习之旅！建议按顺序学习，每个模块都包含精选的阅读材料、视频资源和实战练习。