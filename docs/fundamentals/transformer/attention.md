# Attentionæœºåˆ¶

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æ·±å…¥ç†è§£Self-Attentionæœºåˆ¶çš„æ•°å­¦åŸç†å’Œè®¡ç®—è¿‡ç¨‹ï¼ŒæŒæ¡é¢è¯•ä¸­çš„é«˜é¢‘é—®é¢˜ã€‚

## ğŸ“– é˜…è¯»ææ–™

### å¿…è¯»æ–‡ç« 
1. [ã€ŠAttention is All You Needã€‹æµ…è¯»ï¼ˆç®€ä»‹+ä»£ç ï¼‰](https://kexue.fm/archives/4765/comment-page-1#comments) - ç§‘å­¦ç©ºé—´
2. [GPTä¸BERTå·®åˆ«æ·±å…¥è§£æ](https://www.zhihu.com/tardis/zm/art/607605399?source_id=1003) - çŸ¥ä¹
3. [æ·±å…¥æµ…å‡ºç†è§£transformer](https://www.zhihu.com/question/471328838/answer/51545078376) - çŸ¥ä¹  
4. [Transformeræ¨¡å‹è¯¦è§£ï¼ˆå›¾è§£æœ€å®Œæ•´ç‰ˆï¼‰](https://zhuanlan.zhihu.com/p/338817680) - çŸ¥ä¹
5. [Transformerä½ç½®ç¼–ç ï¼ˆåŸºç¡€ï¼‰](https://zhuanlan.zhihu.com/p/631363482) - çŸ¥ä¹

### åŸè®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - ç»å…¸å¿…è¯»

## ğŸ¬ è§†é¢‘ææ–™

> **å­¦ä¹ å»ºè®®ï¼š** å€é€Ÿè§‚çœ‹ï¼Œé‡ç‚¹ç†è§£æ¦‚å¿µè€Œéç»†èŠ‚

1. [GPTï¼ŒGPT-2ï¼ŒGPT-3 è®ºæ–‡ç²¾è¯»](https://www.bilibili.com/video/BV1AF411b7xQ?spm_id_from=333.788.videopod.sections&vd_source=a5dd5ad68f590473458dad8b99349b27) - å“”å“©å“”å“©
2. [Llama 3.1è®ºæ–‡ç²¾è¯»](https://www.bilibili.com/video/BV1Q4421Z7Tj/?spm_id_from=333.1387.search.video_card.click&vd_source=a5dd5ad68f590473458dad8b99349b27) - å“”å“©å“”å“©

## ğŸ“ çŸ¥è¯†æ€»ç»“

### Self-Attentionè®¡ç®—å…¬å¼

```math
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
```

**æ ¸å¿ƒç»„ä»¶:**
- **Query (Q)**: æŸ¥è¯¢å‘é‡ï¼Œå†³å®šå½“å‰ä½ç½®å…³æ³¨ä»€ä¹ˆ
- **Key (K)**: é”®å‘é‡ï¼Œè¢«æŸ¥è¯¢çš„å†…å®¹
- **Value (V)**: å€¼å‘é‡ï¼Œå®é™…ä¼ é€’çš„ä¿¡æ¯
- **ç¼©æ”¾å› å­**: $\sqrt{d_k}$ï¼Œé˜²æ­¢softmaxæ¢¯åº¦æ¶ˆå¤±

### Multi-Head Attention

å°†è¾“å…¥æŠ•å½±åˆ°å¤šä¸ªä¸åŒçš„å­ç©ºé—´ï¼Œå¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›å¤´ï¼š

```math
MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O
```

å…¶ä¸­ $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: Attentionè®¡ç®—å…¬å¼æ˜¯ä»€ä¹ˆï¼Ÿ

**æ ‡å‡†å›ç­”ï¼š**
Self-Attentionçš„æ ¸å¿ƒå…¬å¼æ˜¯ `Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V`ã€‚

**è¯¦ç»†è§£é‡Šï¼š**
1. å…ˆè®¡ç®—Queryå’ŒKeyçš„ç‚¹ç§¯å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°
2. é™¤ä»¥âˆšd_kè¿›è¡Œç¼©æ”¾
3. é€šè¿‡softmaxå½’ä¸€åŒ–å¾—åˆ°æ³¨æ„åŠ›æƒé‡
4. æœ€åä¸Valueç›¸ä¹˜å¾—åˆ°è¾“å‡º

### Q1.1: ä¸ºä»€ä¹ˆè¦é™¤ä»¥æ ¹å·d_kï¼Ÿ

**æ ¸å¿ƒåŸå› ï¼š** é˜²æ­¢softmaxå‡½æ•°è¿›å…¥é¥±å’ŒåŒºå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ã€‚

**æŠ€æœ¯è§£é‡Šï¼š**
- å½“d_kè¾ƒå¤§æ—¶ï¼ŒQK^Tçš„æ–¹å·®ä¼šå¾ˆå¤§
- å¤§çš„æ•°å€¼ç»è¿‡softmaxåæ¢¯åº¦æ¥è¿‘0
- é™¤ä»¥âˆšd_kå¯ä»¥æ§åˆ¶æ–¹å·®ä¸º1ï¼Œä¿æŒæ¢¯åº¦ç¨³å®š

### Q1.2: Softmaxçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

**ä¸»è¦ä½œç”¨ï¼š**
1. **å½’ä¸€åŒ–**: ç¡®ä¿æ³¨æ„åŠ›æƒé‡ä¹‹å’Œä¸º1
2. **çªå‡ºé‡ç‚¹**: é€šè¿‡æŒ‡æ•°å‡½æ•°æ”¾å¤§é‡è¦ç‰¹å¾
3. **å¯å¾®åˆ†**: ä¿è¯åå‘ä¼ æ’­å¯ä»¥æ­£å¸¸è¿›è¡Œ

## ğŸ’» ä»£ç å®ç°

### ç»ƒä¹ 1: å®ç°Self-Attentionæœºåˆ¶
**å¹³å°**: [Deep-ML Self-Attention](https://www.deep-ml.com/problems/53)

### ç»ƒä¹ 2: å®ç°Multi-Head Attention  
**å¹³å°**: [Deep-ML Multi-Head Attention](https://www.deep-ml.com/problems/94)

### ä»£ç æ¨¡æ¿

```python
import torch
import torch.nn as nn
import math

class SelfAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        
        self.W_q = nn.Linear(d_model, d_model)
        self.W_k = nn.Linear(d_model, d_model)
        self.W_v = nn.Linear(d_model, d_model)
        self.W_o = nn.Linear(d_model, d_model)
        
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        # TODO: å®ç°æ³¨æ„åŠ›è®¡ç®—
        pass
        
    def forward(self, x, mask=None):
        # TODO: å®ç°å®Œæ•´çš„å‰å‘ä¼ æ’­
        pass
```

## âœ… å­¦ä¹ æ£€éªŒ

å®Œæˆä»¥ä¸‹æ£€éªŒæ‰ç®—æŒæ¡æœ¬èŠ‚ï¼š

- [ ] èƒ½ç”»å‡ºSelf-Attentionçš„è®¡ç®—æµç¨‹å›¾
- [ ] å¯ä»¥æ‰‹ç®—ç®€å•çš„Attentionæƒé‡
- [ ] å®ŒæˆDeep-MLå¹³å°çš„ä¸¤ä¸ªç¼–ç¨‹ç»ƒä¹ 
- [ ] é¢è¯•é—®é¢˜èƒ½ç”¨è‡ªå·±çš„è¯æµåˆ©å›ç­”

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸‹ä¸€èŠ‚ï¼šè¯­è¨€æ¨¡å‹æ¶æ„](language-models.md)
- [è¿”å›ï¼šTransformeråŸºç¡€æ¦‚è§ˆ](index.md)