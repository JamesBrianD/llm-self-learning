# å‰é¦ˆç¥ç»ç½‘ç»œ (FFN)

## ğŸ¯ æœ¬èŠ‚ç›®æ ‡

æ·±å…¥ç†è§£Transformerä¸­å‰é¦ˆç¥ç»ç½‘ç»œçš„ä½œç”¨æœºåˆ¶ï¼ŒæŒæ¡ä¸åŒæ¿€æ´»å‡½æ•°çš„æ¼”è¿›å’ŒçŸ¥è¯†å­˜å‚¨åŸç†ã€‚

## ğŸ“ çŸ¥è¯†æ€»ç»“

### FFNçš„åŸºæœ¬ç»“æ„

**å‰é¦ˆç¥ç»ç½‘ç»œ(Feed-Forward Network)**æ˜¯Transformerä¸­é™¤æ³¨æ„åŠ›æœºåˆ¶å¤–çš„å¦ä¸€ä¸ªæ ¸å¿ƒç»„ä»¶ï¼Œä½äºæ¯ä¸ªTransformerå±‚ä¸­ã€‚

#### æ•°å­¦è¡¨ç¤º
$$FFN(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

å…¶ä¸­ï¼š
- $W_1$: ç¬¬ä¸€å±‚æƒé‡çŸ©é˜µ (å‡ç»´)
- $W_2$: ç¬¬äºŒå±‚æƒé‡çŸ©é˜µ (é™ç»´)  
- $b_1, b_2$: åç½®å‘é‡
- $\max(0, Â·)$: ReLUæ¿€æ´»å‡½æ•°

#### ç»´åº¦å˜åŒ–
```
è¾“å…¥: [batch_size, seq_len, d_model]
  â†“ W1 (çº¿æ€§å±‚1)
ä¸­é—´: [batch_size, seq_len, d_ff]    # d_ff = 4 * d_model
  â†“ æ¿€æ´»å‡½æ•°
ä¸­é—´: [batch_size, seq_len, d_ff]    
  â†“ W2 (çº¿æ€§å±‚2)  
è¾“å‡º: [batch_size, seq_len, d_model]
```

### FFNçš„æ ¸å¿ƒåŠŸèƒ½

#### 1. è¯­ä¹‰ä¿¡æ¯æå–
- **é€ä½ç½®å¤„ç†**: å¯¹åºåˆ—ä¸­æ¯ä¸ªä½ç½®ç‹¬ç«‹è¿›è¡Œéçº¿æ€§å˜æ¢
- **ç‰¹å¾æ˜ å°„**: å°†æ³¨æ„åŠ›è¾“å‡ºæ˜ å°„åˆ°æ›´é«˜ç»´çš„ç‰¹å¾ç©ºé—´
- **æ¨¡å¼è¯†åˆ«**: æ•è·å¤æ‚çš„è¯­ä¹‰æ¨¡å¼å’Œç‰¹å¾ç»„åˆ

#### 2. çŸ¥è¯†å­˜å‚¨æœºåˆ¶
FFNè¢«è®¤ä¸ºæ˜¯Transformerçš„"è®°å¿†åº“"ï¼š

**åˆ†å¸ƒå¼å­˜å‚¨**:
- ä¸åŒçš„ç¥ç»å…ƒä¸“é—¨å­˜å‚¨ä¸åŒç±»å‹çš„çŸ¥è¯†
- é€šè¿‡æƒé‡çŸ©é˜µç¼–ç è¯­è¨€æ¨¡å¼å’Œä¸–ç•ŒçŸ¥è¯†
- ç±»ä¼¼äºé”®å€¼å­˜å‚¨ï¼Œè¾“å…¥ä½œä¸º"é”®"ï¼Œæ¿€æ´»æ¨¡å¼ä½œä¸º"å€¼"

**çŸ¥è¯†ç”µè·¯**:
- FFNä¸­çš„ç‰¹å®šç¥ç»å…ƒæ¿€æ´»è·¯å¾„å½¢æˆ"çŸ¥è¯†ç”µè·¯"
- è¿™äº›ç”µè·¯ç¼–ç ç‰¹å®šçš„è¯­ä¹‰å…³ç³»å’Œäº‹å®çŸ¥è¯†
- å¤šå±‚FFNååŒå·¥ä½œï¼Œæ„å»ºå¤æ‚çš„çŸ¥è¯†è¡¨ç¤º

#### 3. è¡¨è¾¾èƒ½åŠ›å¢å¼º
- **éçº¿æ€§å˜æ¢**: æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›
- **ç»´åº¦æ‰©å±•**: ä¸­é—´å±‚çš„é«˜ç»´åº¦æä¾›æ›´ä¸°å¯Œçš„è¡¨ç¤ºç©ºé—´
- **ç‰¹å¾äº¤äº’**: ä¿ƒè¿›ä¸åŒç‰¹å¾ç»´åº¦ä¹‹é—´çš„äº¤äº’

### æ¿€æ´»å‡½æ•°çš„æ¼”è¿›

#### 1. ReLU (æ—©æœŸTransformer)
$$\text{ReLU}(x) = \max(0, x)$$

**ç‰¹ç‚¹**:
- ç®€å•é«˜æ•ˆï¼Œè®¡ç®—é‡å°
- è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- ä½†å­˜åœ¨"æ­»ç¥ç»å…ƒ"é—®é¢˜

#### 2. GELU (GPTç­‰æ¨¡å‹)
$$\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}[1 + \text{erf}(\frac{x}{\sqrt{2}})]$$

**ç‰¹ç‚¹**:
- æ›´å¹³æ»‘çš„æ¿€æ´»å‡½æ•°
- åœ¨è´Ÿå€¼åŒºåŸŸæœ‰éé›¶æ¢¯åº¦
- æ€§èƒ½é€šå¸¸ä¼˜äºReLU

#### 3. SwiGLU (ç°ä»£å¤§æ¨¡å‹)
$$\text{SwiGLU}(x) = \text{Swish}(W_1 x) \odot (W_2 x)$$
$$\text{Swish}(x) = x \cdot \sigma(x)$$

**ç‰¹ç‚¹**:
- é—¨æ§æœºåˆ¶ï¼Œæ›´å¥½çš„ç‰¹å¾é€‰æ‹©
- éœ€è¦é¢å¤–å‚æ•°ä½†æ€§èƒ½æå‡æ˜æ˜¾
- LLaMAã€PaLMç­‰ç°ä»£æ¨¡å‹çš„æ ‡å‡†é€‰æ‹©

### FFNçš„ç‹¬ç‰¹ç‰¹æ€§

#### 1. ä½ç½®æ— å…³å¤„ç†
```python
# FFNå¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹å¤„ç†
for position in sequence:
    hidden = ffn_layer1(input[position])
    hidden = activation(hidden)
    output[position] = ffn_layer2(hidden)
```

#### 2. ä¸æ³¨æ„åŠ›æœºåˆ¶çš„äº’è¡¥
| æœºåˆ¶ | æ³¨æ„åŠ› | FFN |
|------|--------|-----|
| **åŠŸèƒ½** | åºåˆ—å†…ä¿¡æ¯æ•´åˆ | ä½ç½®å†…ç‰¹å¾æå– |
| **ä¾èµ–** | å…¨åºåˆ— | å•ä¸ªä½ç½® |
| **ä½œç”¨** | å»ºæ¨¡å…³ç³» | å­˜å‚¨çŸ¥è¯† |
| **è®¡ç®—** | åºåˆ—é•¿åº¦ç›¸å…³ | åºåˆ—é•¿åº¦æ— å…³ |

### ç°ä»£FFNä¼˜åŒ–æŠ€æœ¯

#### 1. Mixture of Experts (MoE)
- å°†FFNæ›¿æ¢ä¸ºå¤šä¸ªä¸“å®¶ç½‘ç»œ
- é€šè¿‡è·¯ç”±æœºåˆ¶åŠ¨æ€é€‰æ‹©ä¸“å®¶
- åœ¨ä¿æŒè®¡ç®—é‡çš„åŒæ—¶å¤§å¹…å¢åŠ å‚æ•°

#### 2. Memory Layers
- å¼•å…¥å¤–éƒ¨è®°å¿†æœºåˆ¶
- ç¼“å­˜å’Œæ£€ç´¢ç›¸å…³çŸ¥è¯†
- æé«˜é•¿åºåˆ—å¤„ç†èƒ½åŠ›

#### 3. KAN (Kolmogorov-Arnold Networks)
- æ›¿ä»£ä¼ ç»Ÿçš„çº¿æ€§å±‚ç»“æ„
- ä½¿ç”¨å¯å­¦ä¹ çš„æ¿€æ´»å‡½æ•°
- ç†è®ºä¸Šæ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›

## ğŸ’¬ é¢è¯•é—®é¢˜è§£ç­”

### Q1: FFNåœ¨Transformerä¸­èµ·ä»€ä¹ˆä½œç”¨ï¼Ÿ

**æ ¸å¿ƒä½œç”¨**:
1. **çŸ¥è¯†å­˜å‚¨**: ä½œä¸ºæ¨¡å‹çš„"è®°å¿†åº“"ï¼Œå­˜å‚¨è¯­è¨€æ¨¡å¼å’Œä¸–ç•ŒçŸ¥è¯†
2. **ç‰¹å¾æå–**: å¯¹æ¯ä¸ªä½ç½®è¿›è¡Œéçº¿æ€§ç‰¹å¾å˜æ¢
3. **è¡¨è¾¾å¢å¼º**: é€šè¿‡é«˜ç»´æ˜ å°„å¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›
4. **ä¸æ³¨æ„åŠ›äº’è¡¥**: æä¾›ä½ç½®å†…çš„æ·±åº¦å¤„ç†

**æŠ€æœ¯ç»†èŠ‚**:
- é€ä½ç½®ç‹¬ç«‹å¤„ç†ï¼Œä¸æ³¨æ„åŠ›çš„åºåˆ—å»ºæ¨¡å½¢æˆäº’è¡¥
- é€šè¿‡å‡ç»´-æ¿€æ´»-é™ç»´çš„è¿‡ç¨‹å¢å¼ºç‰¹å¾è¡¨ç¤º
- å‚æ•°é‡é€šå¸¸å Transformeræ¨¡å‹æ€»å‚æ•°çš„2/3

### Q2: ä¸ºä»€ä¹ˆFFNè¦å…ˆå‡ç»´å†é™ç»´ï¼Ÿ

**è®¾è®¡åŸç†**:
1. **è¡¨ç¤ºç©ºé—´æ‰©å±•**: å‡ç»´æä¾›æ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤ºç©ºé—´
2. **éçº¿æ€§å»ºæ¨¡**: é«˜ç»´ç©ºé—´ä¸­æ›´å®¹æ˜“æ‹Ÿåˆå¤æ‚å‡½æ•°
3. **ç‰¹å¾äº¤äº’**: æ›´å¤šç»´åº¦å…è®¸æ›´å¤æ‚çš„ç‰¹å¾ç»„åˆ
4. **ä¿¡æ¯ç“¶é¢ˆ**: æœ€ç»ˆé™ç»´èµ·åˆ°ä¿¡æ¯ç­›é€‰çš„ä½œç”¨

**æ•°å­¦ç›´è§‰**:
```
d_model â†’ d_ff â†’ d_model
512 â†’ 2048 â†’ 512
```
ä¸­é—´çš„é«˜ç»´ç©ºé—´æä¾›äº†æ›´å¼ºçš„éçº¿æ€§å»ºæ¨¡èƒ½åŠ›ã€‚

### Q3: ä¸åŒæ¿€æ´»å‡½æ•°å¯¹æ¨¡å‹æ€§èƒ½æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

**æ€§èƒ½å¯¹æ¯”**:

| æ¿€æ´»å‡½æ•° | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|----------|------|------|----------|
| **ReLU** | è®¡ç®—ç®€å•ï¼Œè®­ç»ƒå¿« | æ­»ç¥ç»å…ƒé—®é¢˜ | æ—©æœŸæ¨¡å‹ |
| **GELU** | å¹³æ»‘ï¼Œæ€§èƒ½å¥½ | è®¡ç®—ç¨å¤æ‚ | ä¸­ç­‰è§„æ¨¡æ¨¡å‹ |
| **SwiGLU** | æ€§èƒ½æœ€ä½³ï¼Œé—¨æ§æœºåˆ¶ | å‚æ•°é‡å¢åŠ  | ç°ä»£å¤§æ¨¡å‹ |

**é€‰æ‹©ç­–ç•¥**:
- è®¡ç®—èµ„æºå……è¶³ï¼šé€‰æ‹©SwiGLU
- å¹³è¡¡æ€§èƒ½å’Œæ•ˆç‡ï¼šé€‰æ‹©GELU
- æåº¦å…³æ³¨é€Ÿåº¦ï¼šé€‰æ‹©ReLU

### Q4: FFNå¦‚ä½•å­˜å‚¨å’Œæ£€ç´¢çŸ¥è¯†ï¼Ÿ

**å­˜å‚¨æœºåˆ¶**:
1. **åˆ†å¸ƒå¼è¡¨ç¤º**: çŸ¥è¯†åˆ†å¸ƒåœ¨ä¸åŒç¥ç»å…ƒçš„æƒé‡ä¸­
2. **æ¿€æ´»æ¨¡å¼**: ç‰¹å®šè¾“å…¥è§¦å‘ç‰¹å®šçš„ç¥ç»å…ƒç»„åˆ
3. **å±‚æ¬¡ç»“æ„**: ä¸åŒå±‚çš„FFNå­˜å‚¨ä¸åŒæŠ½è±¡å±‚æ¬¡çš„çŸ¥è¯†

**æ£€ç´¢è¿‡ç¨‹**:
```python
# ç®€åŒ–çš„çŸ¥è¯†æ£€ç´¢è¿‡ç¨‹
input_features = attention_output  # æŸ¥è¯¢"é”®"
activated_neurons = ffn_layer1(input_features)  # æ¿€æ´»ç›¸å…³ç¥ç»å…ƒ
knowledge_pattern = activation_function(activated_neurons)  # çŸ¥è¯†æ¨¡å¼
output_knowledge = ffn_layer2(knowledge_pattern)  # æ£€ç´¢"å€¼"
```

## ğŸ’» ä»£ç å®ç°

### æ ‡å‡†FFNå®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class FeedForward(nn.Module):
    """æ ‡å‡†Transformer FFNå®ç°"""
    
    def __init__(self, d_model, d_ff, dropout=0.1, activation='relu'):
        super().__init__()
        self.d_model = d_model
        self.d_ff = d_ff
        
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
        # é€‰æ‹©æ¿€æ´»å‡½æ•°
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'gelu':
            self.activation = nn.GELU()
        else:
            raise ValueError(f"Unsupported activation: {activation}")
    
    def forward(self, x):
        # x shape: [batch_size, seq_len, d_model]
        
        # ç¬¬ä¸€å±‚ï¼šå‡ç»´ + æ¿€æ´»
        hidden = self.activation(self.linear1(x))
        hidden = self.dropout(hidden)
        
        # ç¬¬äºŒå±‚ï¼šé™ç»´
        output = self.linear2(hidden)
        
        return output

class SwiGLU(nn.Module):
    """SwiGLUæ¿€æ´»å‡½æ•°çš„FFNå®ç°"""
    
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.d_ff = d_ff
        
        # SwiGLUéœ€è¦ä¸¤ä¸ªçº¿æ€§å±‚ç”¨äºé—¨æ§
        self.w1 = nn.Linear(d_model, d_ff, bias=False)
        self.w2 = nn.Linear(d_model, d_ff, bias=False) 
        self.w3 = nn.Linear(d_ff, d_model, bias=False)
        
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        # SwiGLU: swish(W1*x) âŠ™ (W2*x)
        swish_gate = F.silu(self.w1(x))  # Swish activation
        linear_part = self.w2(x)
        
        # é—¨æ§æœºåˆ¶
        gated = swish_gate * linear_part
        gated = self.dropout(gated)
        
        # è¾“å‡ºæŠ•å½±
        output = self.w3(gated)
        
        return output

# æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹
def compare_ffn_activations():
    """å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°çš„FFNæ€§èƒ½"""
    
    d_model, d_ff = 512, 2048
    batch_size, seq_len = 32, 128
    
    # æµ‹è¯•æ•°æ®
    x = torch.randn(batch_size, seq_len, d_model)
    
    # ä¸åŒFFNå®ç°
    ffn_relu = FeedForward(d_model, d_ff, activation='relu')
    ffn_gelu = FeedForward(d_model, d_ff, activation='gelu')
    ffn_swiglu = SwiGLU(d_model, d_ff)
    
    print("=== FFNæ¿€æ´»å‡½æ•°å¯¹æ¯” ===")
    
    # å‚æ•°é‡å¯¹æ¯”
    relu_params = sum(p.numel() for p in ffn_relu.parameters())
    gelu_params = sum(p.numel() for p in ffn_gelu.parameters())
    swiglu_params = sum(p.numel() for p in ffn_swiglu.parameters())
    
    print(f"ReLU FFNå‚æ•°é‡: {relu_params:,}")
    print(f"GELU FFNå‚æ•°é‡: {gelu_params:,}")
    print(f"SwiGLU FFNå‚æ•°é‡: {swiglu_params:,}")
    
    # è®¡ç®—æ—¶é—´å¯¹æ¯”
    import time
    
    with torch.no_grad():
        # ReLU
        start = time.time()
        for _ in range(100):
            _ = ffn_relu(x)
        relu_time = time.time() - start
        
        # GELU
        start = time.time()
        for _ in range(100):
            _ = ffn_gelu(x)
        gelu_time = time.time() - start
        
        # SwiGLU
        start = time.time()
        for _ in range(100):
            _ = ffn_swiglu(x)
        swiglu_time = time.time() - start
    
    print(f"ReLUæ¨ç†æ—¶é—´: {relu_time:.4f}ç§’")
    print(f"GELUæ¨ç†æ—¶é—´: {gelu_time:.4f}ç§’")
    print(f"SwiGLUæ¨ç†æ—¶é—´: {swiglu_time:.4f}ç§’")

# çŸ¥è¯†å­˜å‚¨å¯è§†åŒ–
class KnowledgeAnalyzer:
    """åˆ†æFFNä¸­çš„çŸ¥è¯†å­˜å‚¨æ¨¡å¼"""
    
    def __init__(self, ffn_model):
        self.ffn = ffn_model
    
    def analyze_neuron_activation(self, inputs, texts):
        """åˆ†æä¸åŒè¾“å…¥å¯¹ç¥ç»å…ƒçš„æ¿€æ´»æ¨¡å¼"""
        
        activations = []
        with torch.no_grad():
            for input_tensor in inputs:
                # è·å–ç¬¬ä¸€å±‚çš„æ¿€æ´»
                hidden = torch.relu(self.ffn.linear1(input_tensor))
                activations.append(hidden.mean(dim=1))  # å¹³å‡æ± åŒ–
        
        # åˆ†ææ¿€æ´»æ¨¡å¼
        activations = torch.stack(activations)
        
        # æ‰¾å‡ºæœ€æ´»è·ƒçš„ç¥ç»å…ƒ
        neuron_activity = activations.mean(dim=0)
        top_neurons = torch.topk(neuron_activity, k=10).indices
        
        print("æœ€æ´»è·ƒçš„ç¥ç»å…ƒç´¢å¼•:", top_neurons.tolist())
        
        # åˆ†æä¸åŒè¾“å…¥çš„æ¿€æ´»ç›¸ä¼¼æ€§
        similarity_matrix = torch.cosine_similarity(
            activations.unsqueeze(1), 
            activations.unsqueeze(0), 
            dim=2
        )
        
        return {
            'activations': activations,
            'top_neurons': top_neurons,
            'similarity_matrix': similarity_matrix
        }

if __name__ == "__main__":
    compare_ffn_activations()
```

## âœ… å­¦ä¹ æ£€éªŒ

- [ ] ç†è§£FFNçš„åŸºæœ¬ç»“æ„å’Œæ•°å­¦åŸç†
- [ ] æŒæ¡ä¸åŒæ¿€æ´»å‡½æ•°çš„ç‰¹ç‚¹å’Œé€‚ç”¨åœºæ™¯
- [ ] ç†è§£FFNçš„çŸ¥è¯†å­˜å‚¨æœºåˆ¶
- [ ] èƒ½å®ç°å’Œå¯¹æ¯”ä¸åŒçš„FFNå˜ä½“
- [ ] ç†è§£FFNä¸æ³¨æ„åŠ›æœºåˆ¶çš„äº’è¡¥å…³ç³»

## ğŸ”— ç›¸å…³é“¾æ¥

- [ä¸‹ä¸€èŠ‚ï¼šç¼–ç å™¨-è§£ç å™¨æ¶æ„](encoder-decoder.md)
- [è¿”å›ï¼šTransformeråŸºç¡€æ¦‚è§ˆ](index.md)