# ç›‘ç£å¾®è°ƒæŠ€æœ¯

## ğŸ¯ å­¦ä¹ ç›®æ ‡

å…¨é¢æŒæ¡ç›‘ç£å¾®è°ƒ(Supervised Fine-Tuning, SFT)æŠ€æœ¯ï¼Œç†è§£æŒ‡ä»¤å¾®è°ƒçš„æ ¸å¿ƒåŸç†ï¼Œå­¦ä¼šæ„å»ºé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†å’Œå®ç°æœ‰æ•ˆçš„å¾®è°ƒæµç¨‹ã€‚

**é‡ç‚¹é¢è¯•é—®é¢˜é¢„è§ˆï¼š**
- SFTåœ¨æ•´ä¸ªè®­ç»ƒæµç¨‹ä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
- æŒ‡ä»¤å¾®è°ƒä¸ä¼ ç»Ÿå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
- å¦‚ä½•æ„å»ºé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†ï¼Ÿ
- SFTè®­ç»ƒä¸­å¸¸è§çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Ÿ

## ğŸ—ï¸ SFTæ ¸å¿ƒæ¦‚å¿µ

### å®šä¹‰ä¸ä½œç”¨

ç›‘ç£å¾®è°ƒ(SFT)æ˜¯ä½¿ç”¨**æŒ‡ä»¤-å›ç­”å¯¹**æ•°æ®è®­ç»ƒæ¨¡å‹éµå¾ªæŒ‡ä»¤å’Œäº§ç”ŸæœŸæœ›è¾“å‡ºçš„è¿‡ç¨‹ï¼Œæ˜¯è¿æ¥é¢„è®­ç»ƒæ¨¡å‹ä¸å®é™…åº”ç”¨çš„å…³é”®æ¡¥æ¢ã€‚

```
SFTåœ¨è®­ç»ƒæµç¨‹ä¸­çš„ä½ç½®
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å®Œæ•´LLMè®­ç»ƒæµç¨‹                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  é¢„è®­ç»ƒ â†’ åé¢„è®­ç»ƒ â†’ ç›‘ç£å¾®è°ƒ(SFT) â†’ å¼ºåŒ–å­¦ä¹ å¯¹é½                 â”‚
â”‚                        â–²                                        â”‚
â”‚                   å…³é”®è½¬æ¢ç‚¹                                      â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   è¯­è¨€å»ºæ¨¡       â”‚â”€â”€â–¶â”‚   æŒ‡ä»¤éµå¾ª       â”‚â”€â”€â–¶â”‚   äººç±»åå¥½       â”‚  â”‚
â”‚  â”‚ ä¸‹ä¸€è¯é¢„æµ‹       â”‚   â”‚ ä»»åŠ¡å®Œæˆ        â”‚   â”‚ ä»·å€¼å¯¹é½        â”‚  â”‚
â”‚  â”‚ æ— ç›‘ç£å­¦ä¹        â”‚   â”‚ ç›‘ç£å­¦ä¹         â”‚   â”‚ å¼ºåŒ–å­¦ä¹         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â–²                      â–²                      â–²          â”‚
â”‚    é€šç”¨è¯­è¨€èƒ½åŠ›              æŒ‡ä»¤éµå¾ªèƒ½åŠ›              å¯¹é½èƒ½åŠ›     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒä»·å€¼

1. **èƒ½åŠ›è½¬æ¢**: ä»è¯­è¨€å»ºæ¨¡èƒ½åŠ›è½¬æ¢ä¸ºä»»åŠ¡æ‰§è¡Œèƒ½åŠ›
2. **æ ¼å¼è§„èŒƒ**: æ•™ä¼šæ¨¡å‹æ ‡å‡†çš„è¾“å…¥è¾“å‡ºæ ¼å¼
3. **æŒ‡ä»¤éµå¾ª**: åŸ¹å…»éµå¾ªç”¨æˆ·æŒ‡ä»¤çš„åŸºç¡€èƒ½åŠ›
4. **å¤šä»»åŠ¡ç»Ÿä¸€**: ç”¨ç»Ÿä¸€æ ¼å¼å¤„ç†å¤šç§ä¸åŒä»»åŠ¡

## ğŸ“Š æŒ‡ä»¤æ•°æ®æ„å»º

### é«˜è´¨é‡æŒ‡ä»¤æ•°æ®çš„ç‰¹å¾

```python
class InstructionDataBuilder:
    """æŒ‡ä»¤æ•°æ®æ„å»ºå™¨"""
    
    def __init__(self):
        self.quality_criteria = {
            "å¤šæ ·æ€§": "æ¶µç›–ä¸åŒé¢†åŸŸã€ä»»åŠ¡ç±»å‹ã€éš¾åº¦å±‚çº§",
            "å‡†ç¡®æ€§": "å›ç­”å‡†ç¡®ã€äº‹å®æ­£ç¡®ã€é€»è¾‘æ¸…æ™°",
            "å®Œæ•´æ€§": "å›ç­”å®Œæ•´ã€ç»“æ„åŒ–ã€æ»¡è¶³æŒ‡ä»¤è¦æ±‚",
            "ä¸€è‡´æ€§": "æ ¼å¼ç»Ÿä¸€ã€é£æ ¼ä¸€è‡´ã€æ ‡å‡†è§„èŒƒ"
        }
    
    def instruction_template_design(self):
        """æŒ‡ä»¤æ¨¡æ¿è®¾è®¡"""
        
        templates = {
            "åŸºç¡€é—®ç­”æ¨¡æ¿": {
                "æ ¼å¼": "### Instruction:\n{instruction}\n\n### Response:\n{response}",
                "é€‚ç”¨": "ç®€å•é—®ç­”ã€çŸ¥è¯†æŸ¥è¯¢ç±»ä»»åŠ¡",
                "ç¤ºä¾‹": {
                    "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
                    "response": "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ è§„å¾‹..."
                }
            },
            
            "å¤šè½®å¯¹è¯æ¨¡æ¿": {
                "æ ¼å¼": "### Conversation:\n{conversation_history}\n\n### Human:\n{human_input}\n\n### Assistant:\n{assistant_response}",
                "é€‚ç”¨": "å¯¹è¯ç³»ç»Ÿã€èŠå¤©æœºå™¨äºº",
                "ç¤ºä¾‹": {
                    "conversation_history": "ä¹‹å‰çš„å¯¹è¯å†å²",
                    "human_input": "ç”¨æˆ·å½“å‰è¾“å…¥",
                    "assistant_response": "åŠ©æ‰‹å›åº”"
                }
            },
            
            "ä»»åŠ¡å¯¼å‘æ¨¡æ¿": {
                "æ ¼å¼": "### Task:\n{task_description}\n\n### Input:\n{input_data}\n\n### Output:\n{expected_output}",
                "é€‚ç”¨": "ç»“æ„åŒ–ä»»åŠ¡ã€æ•°æ®å¤„ç†",
                "ç¤ºä¾‹": {
                    "task_description": "å°†ä»¥ä¸‹æ–‡æœ¬ç¿»è¯‘æˆè‹±æ–‡",
                    "input_data": "ä½ å¥½ï¼Œä¸–ç•Œ",
                    "expected_output": "Hello, world"
                }
            },
            
            "æ€ç»´é“¾æ¨¡æ¿": {
                "æ ¼å¼": "### Problem:\n{problem}\n\n### Solution:\n{reasoning_steps}\n\n### Answer:\n{final_answer}",
                "é€‚ç”¨": "æ¨ç†ä»»åŠ¡ã€æ•°å­¦é—®é¢˜ã€é€»è¾‘åˆ†æ",
                "ç¤ºä¾‹": {
                    "problem": "è®¡ç®— (3+5) Ã— 2 = ?",
                    "reasoning_steps": "é¦–å…ˆè®¡ç®—æ‹¬å·å†…: 3+5=8\nç„¶åä¹˜ä»¥2: 8Ã—2=16",
                    "final_answer": "16"
                }
            }
        }
        
        return templates
    
    def data_diversity_strategy(self):
        """æ•°æ®å¤šæ ·æ€§ç­–ç•¥"""
        
        diversity_dimensions = {
            "ä»»åŠ¡ç±»å‹å¤šæ ·æ€§": {
                "æ–‡æœ¬ç”Ÿæˆ": ["åˆ›æ„å†™ä½œ", "æ‘˜è¦ç”Ÿæˆ", "ç»­å†™è¡¥å…¨"],
                "ä¿¡æ¯æŠ½å–": ["å®ä½“è¯†åˆ«", "å…³ç³»æŠ½å–", "å…³é”®è¯æå–"],
                "æ–‡æœ¬åˆ†æ": ["æƒ…æ„Ÿåˆ†æ", "ä¸»é¢˜åˆ†ç±»", "æ„å›¾ç†è§£"],
                "æ¨ç†é—®ç­”": ["å¸¸è¯†æ¨ç†", "æ•°å­¦è®¡ç®—", "é€»è¾‘æ¨ç†"],
                "ä»£ç ç›¸å…³": ["ä»£ç ç”Ÿæˆ", "ä»£ç è§£é‡Š", "Bugä¿®å¤"],
                "åˆ›æ„ä»»åŠ¡": ["å¤´è„‘é£æš´", "æ•…äº‹åˆ›ä½œ", "è¯—æ­Œåˆ›ä½œ"]
            },
            
            "é¢†åŸŸè¦†ç›–å¤šæ ·æ€§": {
                "ç§‘æŠ€é¢†åŸŸ": "è®¡ç®—æœºã€äººå·¥æ™ºèƒ½ã€ç”Ÿç‰©æŠ€æœ¯",
                "å•†ä¸šé¢†åŸŸ": "é‡‘èã€ç®¡ç†ã€è¥é”€",
                "æ•™è‚²é¢†åŸŸ": "æ•°å­¦ã€ç‰©ç†ã€åŒ–å­¦ã€å†å²",
                "ç”Ÿæ´»é¢†åŸŸ": "å¥åº·ã€ç¾é£Ÿã€æ—…æ¸¸",
                "æ–‡åŒ–é¢†åŸŸ": "æ–‡å­¦ã€è‰ºæœ¯ã€å“²å­¦"
            },
            
            "éš¾åº¦å±‚çº§å¤šæ ·æ€§": {
                "ç®€å•": "åŸºç¡€æ¦‚å¿µã€ç®€å•é—®ç­”ã€ç›´æ¥æŸ¥è¯¢",
                "ä¸­ç­‰": "åˆ†æè§£é‡Šã€å¤šæ­¥æ¨ç†ã€ç»¼åˆåº”ç”¨",
                "å›°éš¾": "å¤æ‚æ¨ç†ã€åˆ›æ–°æ€è€ƒã€ä¸“ä¸šæ·±åº¦"
            },
            
            "è¯­è¨€é£æ ¼å¤šæ ·æ€§": {
                "æ­£å¼é£æ ¼": "å­¦æœ¯è®ºæ–‡ã€å•†åŠ¡æŠ¥å‘Šã€å®˜æ–¹æ–‡æ¡£",
                "éæ­£å¼é£æ ¼": "æ—¥å¸¸å¯¹è¯ã€ç¤¾äº¤åª’ä½“ã€ä¸ªäººåšå®¢",
                "ä¸“ä¸šé£æ ¼": "æŠ€æœ¯æ–‡æ¡£ã€åŒ»å­¦æŠ¥å‘Šã€æ³•å¾‹æ¡æ–‡",
                "åˆ›æ„é£æ ¼": "æ–‡å­¦åˆ›ä½œã€å¹¿å‘Šæ–‡æ¡ˆã€è‰ºæœ¯è¯„è®º"
            }
        }
        
        return diversity_dimensions
    
    def generate_instruction_data(self, seed_topics, num_per_topic=50):
        """ç”ŸæˆæŒ‡ä»¤æ•°æ®"""
        
        instruction_data = []
        templates = self.instruction_template_design()
        
        # æŒ‡ä»¤ç”Ÿæˆæç¤ºæ¨¡æ¿
        generation_prompts = {
            "é—®ç­”ç±»": "åŸºäºä¸»é¢˜'{topic}'ï¼Œç”Ÿæˆä¸€ä¸ªéœ€è¦è¯¦ç»†è§£é‡Šçš„é—®é¢˜ï¼š",
            "ä»»åŠ¡ç±»": "åŸºäºä¸»é¢˜'{topic}'ï¼Œè®¾è®¡ä¸€ä¸ªå…·ä½“çš„ä»»åŠ¡æŒ‡ä»¤ï¼š",
            "æ¨ç†ç±»": "åŸºäºä¸»é¢˜'{topic}'ï¼Œåˆ›å»ºä¸€ä¸ªéœ€è¦é€»è¾‘æ¨ç†çš„é—®é¢˜ï¼š",
            "åˆ›æ„ç±»": "åŸºäºä¸»é¢˜'{topic}'ï¼Œè®¾è®¡ä¸€ä¸ªåˆ›æ„ç”Ÿæˆä»»åŠ¡ï¼š"
        }
        
        for topic in seed_topics:
            for prompt_type, prompt_template in generation_prompts.items():
                for i in range(num_per_topic // len(generation_prompts)):
                    
                    # ç”ŸæˆæŒ‡ä»¤
                    instruction_prompt = prompt_template.format(topic=topic)
                    instruction = self.generate_with_llm(instruction_prompt)
                    
                    # ç”Ÿæˆå›ç­”
                    response_prompt = f"è¯·è¯¦ç»†å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š\n{instruction}"
                    response = self.generate_with_llm(response_prompt)
                    
                    # è´¨é‡æ£€æŸ¥
                    if self.quality_check(instruction, response):
                        instruction_data.append({
                            "instruction": instruction,
                            "response": response,
                            "topic": topic,
                            "type": prompt_type,
                            "quality_score": self.calculate_quality_score(instruction, response)
                        })
        
        return instruction_data
    
    def quality_check(self, instruction, response):
        """è´¨é‡æ£€æŸ¥"""
        
        # åŸºç¡€é•¿åº¦æ£€æŸ¥
        if len(instruction.split()) < 5 or len(response.split()) < 10:
            return False
        
        # ç›¸å…³æ€§æ£€æŸ¥
        relevance_score = self.calculate_relevance(instruction, response)
        if relevance_score < 0.7:
            return False
        
        # å®Œæ•´æ€§æ£€æŸ¥
        completeness_score = self.calculate_completeness(response)
        if completeness_score < 0.6:
            return False
        
        # å®‰å…¨æ€§æ£€æŸ¥
        if self.contains_harmful_content(instruction + " " + response):
            return False
        
        return True
    
    def create_balanced_dataset(self, instruction_data, target_size=10000):
        """åˆ›å»ºå¹³è¡¡æ•°æ®é›†"""
        
        # æŒ‰ç±»å‹å’Œä¸»é¢˜åˆ†ç»„
        grouped_data = {}
        for item in instruction_data:
            key = f"{item['type']}_{item['topic']}"
            if key not in grouped_data:
                grouped_data[key] = []
            grouped_data[key].append(item)
        
        # è®¡ç®—æ¯ç»„ç›®æ ‡æ•°é‡
        num_groups = len(grouped_data)
        per_group_target = target_size // num_groups
        
        # ä»æ¯ç»„é‡‡æ ·
        balanced_data = []
        for group, items in grouped_data.items():
            # æŒ‰è´¨é‡åˆ†æ•°æ’åºï¼Œé€‰æ‹©é«˜è´¨é‡æ ·æœ¬
            sorted_items = sorted(items, key=lambda x: x['quality_score'], reverse=True)
            selected = sorted_items[:min(per_group_target, len(sorted_items))]
            balanced_data.extend(selected)
        
        return balanced_data

# ä½¿ç”¨ç¤ºä¾‹
builder = InstructionDataBuilder()
templates = builder.instruction_template_design()
diversity = builder.data_diversity_strategy()

print("æŒ‡ä»¤æ¨¡æ¿:", templates)
print("å¤šæ ·æ€§ç­–ç•¥:", diversity)
```

### æ•°æ®å¢å¼ºæŠ€æœ¯

```python
class InstructionDataAugmentation:
    """æŒ‡ä»¤æ•°æ®å¢å¼ºæŠ€æœ¯"""
    
    def __init__(self, base_llm):
        self.base_llm = base_llm
        
    def paraphrase_augmentation(self, instruction, response):
        """æ”¹å†™å¢å¼º"""
        
        # æŒ‡ä»¤æ”¹å†™
        paraphrase_prompts = [
            f"è¯·ç”¨ä¸åŒçš„æ–¹å¼è¡¨è¾¾ä»¥ä¸‹æŒ‡ä»¤ï¼Œä¿æŒå«ä¹‰ä¸å˜ï¼š\n{instruction}",
            f"å°†ä»¥ä¸‹æŒ‡ä»¤æ”¹å†™å¾—æ›´åŠ æ­£å¼ï¼š\n{instruction}",
            f"å°†ä»¥ä¸‹æŒ‡ä»¤æ”¹å†™å¾—æ›´åŠ ç®€æ´ï¼š\n{instruction}"
        ]
        
        augmented_pairs = []
        
        for prompt in paraphrase_prompts:
            new_instruction = self.base_llm.generate(prompt)
            
            # æ£€æŸ¥æ”¹å†™è´¨é‡
            if self.is_valid_paraphrase(instruction, new_instruction):
                # ä¸ºæ–°æŒ‡ä»¤ç”Ÿæˆå¯¹åº”å›ç­”
                new_response = self.base_llm.generate(
                    f"è¯·å›ç­”ï¼š{new_instruction}"
                )
                
                augmented_pairs.append({
                    "instruction": new_instruction,
                    "response": new_response,
                    "augmentation_type": "paraphrase",
                    "original_instruction": instruction
                })
        
        return augmented_pairs
    
    def difficulty_augmentation(self, instruction, response):
        """éš¾åº¦è°ƒèŠ‚å¢å¼º"""
        
        # ç®€åŒ–ç‰ˆæœ¬
        simplify_prompt = f"""
        å°†ä»¥ä¸‹æŒ‡ä»¤ç®€åŒ–ï¼Œä½¿å…¶æ›´å®¹æ˜“ç†è§£ï¼š
        
        åŸæŒ‡ä»¤ï¼š{instruction}
        
        ç®€åŒ–æŒ‡ä»¤ï¼š
        """
        
        simple_instruction = self.base_llm.generate(simplify_prompt)
        simple_response = self.base_llm.generate(f"è¯·ç®€å•å›ç­”ï¼š{simple_instruction}")
        
        # å¤æ‚ç‰ˆæœ¬  
        complicate_prompt = f"""
        å°†ä»¥ä¸‹æŒ‡ä»¤æ‰©å±•å¾—æ›´åŠ å…·ä½“å’Œå¤æ‚ï¼š
        
        åŸæŒ‡ä»¤ï¼š{instruction}
        
        æ‰©å±•æŒ‡ä»¤ï¼š
        """
        
        complex_instruction = self.base_llm.generate(complicate_prompt)
        complex_response = self.base_llm.generate(f"è¯·è¯¦ç»†å›ç­”ï¼š{complex_instruction}")
        
        return [
            {
                "instruction": simple_instruction,
                "response": simple_response,
                "augmentation_type": "simplification",
                "difficulty_level": "easy"
            },
            {
                "instruction": complex_instruction, 
                "response": complex_response,
                "augmentation_type": "complication",
                "difficulty_level": "hard"
            }
        ]
    
    def format_augmentation(self, instruction, response):
        """æ ¼å¼å˜æ¢å¢å¼º"""
        
        format_variations = []
        
        # é—®ç­”æ ¼å¼
        qa_format = {
            "instruction": f"é—®é¢˜ï¼š{instruction}",
            "response": f"å›ç­”ï¼š{response}",
            "format_type": "qa_format"
        }
        
        # å¯¹è¯æ ¼å¼
        dialog_format = {
            "instruction": f"ç”¨æˆ·ï¼š{instruction}",
            "response": f"åŠ©æ‰‹ï¼š{response}",
            "format_type": "dialog_format"
        }
        
        # ç»“æ„åŒ–æ ¼å¼
        structured_format = {
            "instruction": f"ä»»åŠ¡ï¼š{instruction}\nè¦æ±‚ï¼šè¯·æä¾›è¯¦ç»†å›ç­”",
            "response": f"è§£ç­”ï¼š\n{response}",
            "format_type": "structured_format"
        }
        
        format_variations.extend([qa_format, dialog_format, structured_format])
        
        return format_variations
    
    def context_augmentation(self, instruction, response):
        """ä¸Šä¸‹æ–‡å¢å¼º"""
        
        # æ·»åŠ èƒŒæ™¯ä¿¡æ¯
        context_enhanced = []
        
        contexts = [
            "åœ¨å­¦æœ¯ç ”ç©¶ç¯å¢ƒä¸­ï¼Œ",
            "åœ¨æ—¥å¸¸ç”Ÿæ´»åœºæ™¯ä¸‹ï¼Œ", 
            "åœ¨å•†ä¸šåº”ç”¨ä¸­ï¼Œ",
            "åœ¨æ•™è‚²æ•™å­¦ä¸­ï¼Œ"
        ]
        
        for context in contexts:
            contextualized_instruction = context + instruction.lower()
            
            # ç”Ÿæˆé€‚åº”ä¸Šä¸‹æ–‡çš„å›ç­”
            context_response = self.base_llm.generate(
                f"åœ¨{context[:-1]}çš„èƒŒæ™¯ä¸‹ï¼Œè¯·å›ç­”ï¼š{instruction}"
            )
            
            context_enhanced.append({
                "instruction": contextualized_instruction,
                "response": context_response,
                "augmentation_type": "context_enhancement",
                "context": context.strip("ï¼Œ")
            })
        
        return context_enhanced

# æ•°æ®å¢å¼ºä½¿ç”¨ç¤ºä¾‹
def augment_instruction_dataset(original_data, target_multiplier=3):
    """æ•°æ®å¢å¼ºæµç¨‹"""
    
    augmenter = InstructionDataAugmentation(base_llm)
    augmented_data = []
    
    for item in original_data:
        instruction = item["instruction"]
        response = item["response"]
        
        # åŸå§‹æ•°æ®
        augmented_data.append(item)
        
        # æ”¹å†™å¢å¼º
        paraphrases = augmenter.paraphrase_augmentation(instruction, response)
        augmented_data.extend(paraphrases)
        
        # éš¾åº¦å¢å¼º
        difficulty_variants = augmenter.difficulty_augmentation(instruction, response)
        augmented_data.extend(difficulty_variants)
        
        # æ ¼å¼å¢å¼º
        format_variants = augmenter.format_augmentation(instruction, response)
        augmented_data.extend(format_variants)
        
        # ä¸Šä¸‹æ–‡å¢å¼º
        context_variants = augmenter.context_augmentation(instruction, response)
        augmented_data.extend(context_variants)
        
        # æ§åˆ¶å¢å¼ºå€æ•°
        if len(augmented_data) >= len(original_data) * target_multiplier:
            break
    
    return augmented_data[:len(original_data) * target_multiplier]
```

## ğŸš€ SFTè®­ç»ƒå®ç°

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import torch

class SupervisedFineTuner:
    """ç›‘ç£å¾®è°ƒè®­ç»ƒå™¨"""
    
    def __init__(self, model_name, max_seq_length=2048):
        self.model_name = model_name
        self.max_seq_length = max_seq_length
        
        # åˆå§‹åŒ–tokenizerå’Œmodel
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # è®¾ç½®pad token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            load_in_8bit=True,  # 8bité‡åŒ–èŠ‚çœæ˜¾å­˜
            trust_remote_code=True
        )
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœæ˜¾å­˜
        self.model.gradient_checkpointing_enable()
    
    def format_instruction_data(self, instruction_data, template_type="alpaca"):
        """æ ¼å¼åŒ–æŒ‡ä»¤æ•°æ®"""
        
        formatted_data = []
        
        templates = {
            "alpaca": "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n### Instruction:\n{instruction}\n\n### Response:\n{response}",
            
            "vicuna": "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n\nUSER: {instruction}\nASSISTANT: {response}",
            
            "chinese": "ä»¥ä¸‹æ˜¯ä¸€ä¸ªæè¿°ä»»åŠ¡çš„æŒ‡ä»¤ã€‚è¯·ç¼–å†™ä¸€ä¸ªé€‚å½“å®Œæˆè¯·æ±‚çš„å›ç­”ã€‚\n\n### æŒ‡ä»¤ï¼š\n{instruction}\n\n### å›ç­”ï¼š\n{response}",
            
            "simple": "### Question:\n{instruction}\n\n### Answer:\n{response}"
        }
        
        template = templates.get(template_type, templates["alpaca"])
        
        for item in instruction_data:
            formatted_text = template.format(
                instruction=item["instruction"],
                response=item["response"]
            )
            
            formatted_data.append({
                "text": formatted_text,
                "instruction": item["instruction"],
                "response": item["response"]
            })
        
        return formatted_data
    
    def tokenize_function(self, examples):
        """Tokenizationå‡½æ•°"""
        
        # Tokenizeå®Œæ•´æ–‡æœ¬
        tokenized = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=self.max_seq_length,
            return_tensors=None
        )
        
        # ä¸ºæŒ‡ä»¤å¾®è°ƒè®¾ç½®labels
        # åªå¯¹responseéƒ¨åˆ†è®¡ç®—lossï¼Œinstructionéƒ¨åˆ†maskæ‰
        input_ids = tokenized["input_ids"]
        labels = []
        
        for i, text in enumerate(examples["text"]):
            # æ‰¾åˆ°responseå¼€å§‹ä½ç½®
            if "### Response:" in text:
                response_start = text.find("### Response:") + len("### Response:")
            elif "ASSISTANT:" in text:
                response_start = text.find("ASSISTANT:") + len("ASSISTANT:")
            elif "### å›ç­”ï¼š" in text:
                response_start = text.find("### å›ç­”ï¼š") + len("### å›ç­”ï¼š")
            else:
                response_start = len(text) // 2  # é»˜è®¤ä»ä¸­é—´å¼€å§‹
            
            # Tokenizeåˆ°responseå¼€å§‹ä½ç½®çš„æ–‡æœ¬
            prefix_tokens = self.tokenizer(
                text[:response_start],
                truncation=True,
                padding=False,
                max_length=self.max_seq_length,
                return_tensors=None
            )["input_ids"]
            
            # åˆ›å»ºlabelsï¼šinstructionéƒ¨åˆ†ä¸º-100ï¼Œresponseéƒ¨åˆ†ä¸ºæ­£å¸¸token
            label = [-100] * len(prefix_tokens) + input_ids[i][len(prefix_tokens):]
            
            # ç¡®ä¿é•¿åº¦ä¸€è‡´
            if len(label) > len(input_ids[i]):
                label = label[:len(input_ids[i])]
            elif len(label) < len(input_ids[i]):
                label.extend(input_ids[i][len(label):])
            
            labels.append(label)
        
        tokenized["labels"] = labels
        return tokenized
    
    def create_data_collator(self):
        """åˆ›å»ºæ•°æ®æ•´ç†å™¨"""
        
        return DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # ä¸ä½¿ç”¨æ©ç è¯­è¨€æ¨¡å‹
            pad_to_multiple_of=8,  # ä¸ºäº†tensor coreä¼˜åŒ–
        )
    
    def train(self, train_data, eval_data=None, output_dir="./sft_output", 
              epochs=3, batch_size=4, learning_rate=2e-4):
        """æ‰§è¡ŒSFTè®­ç»ƒ"""
        
        # æ ¼å¼åŒ–æ•°æ®
        formatted_train = self.format_instruction_data(train_data)
        train_dataset = Dataset.from_list(formatted_train)
        
        # Tokenizeæ•°æ®
        tokenized_train = train_dataset.map(
            self.tokenize_function,
            batched=True,
            remove_columns=train_dataset.column_names
        )
        
        # å¤„ç†éªŒè¯æ•°æ®
        eval_dataset = None
        if eval_data:
            formatted_eval = self.format_instruction_data(eval_data)
            eval_dataset = Dataset.from_list(formatted_eval)
            tokenized_eval = eval_dataset.map(
                self.tokenize_function,
                batched=True,
                remove_columns=eval_dataset.column_names
            )
            eval_dataset = tokenized_eval
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=output_dir,
            
            # åŸºç¡€è®­ç»ƒè®¾ç½®
            num_train_epochs=epochs,
            per_device_train_batch_size=batch_size,
            per_device_eval_batch_size=batch_size,
            gradient_accumulation_steps=8,  # æœ‰æ•ˆbatch size = batch_size * 8
            
            # ä¼˜åŒ–å™¨è®¾ç½®
            learning_rate=learning_rate,
            weight_decay=0.01,
            warmup_ratio=0.1,
            lr_scheduler_type="cosine",
            
            # ç²¾åº¦å’Œå†…å­˜ä¼˜åŒ–
            bf16=True,
            gradient_checkpointing=True,
            max_grad_norm=1.0,
            
            # è¯„ä¼°å’Œä¿å­˜
            evaluation_strategy="steps" if eval_dataset else "no",
            eval_steps=500,
            save_strategy="steps",
            save_steps=1000,
            save_total_limit=3,
            load_best_model_at_end=True if eval_dataset else False,
            metric_for_best_model="eval_loss" if eval_dataset else None,
            
            # æ—¥å¿—è®¾ç½®
            logging_dir=f"{output_dir}/logs",
            logging_steps=100,
            report_to="tensorboard",
            
            # å…¶ä»–ä¼˜åŒ–
            dataloader_pin_memory=True,
            dataloader_num_workers=4,
            remove_unused_columns=False,
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_train,
            eval_dataset=eval_dataset,
            data_collator=self.create_data_collator(),
            tokenizer=self.tokenizer,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("å¼€å§‹SFTè®­ç»ƒ...")
        train_result = trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        trainer.save_state()
        
        print(f"è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
        print(f"è®­ç»ƒç»Ÿè®¡: {train_result.metrics}")
        
        return trainer, train_result

# å¤šä»»åŠ¡SFTè®­ç»ƒ
class MultiTaskSFTTrainer(SupervisedFineTuner):
    """å¤šä»»åŠ¡SFTè®­ç»ƒå™¨"""
    
    def __init__(self, model_name, task_weights=None):
        super().__init__(model_name)
        self.task_weights = task_weights or {}
    
    def prepare_multitask_data(self, task_datasets):
        """å‡†å¤‡å¤šä»»åŠ¡æ•°æ®"""
        
        all_data = []
        task_info = {}
        
        for task_name, task_data in task_datasets.items():
            # è®¡ç®—ä»»åŠ¡æƒé‡
            weight = self.task_weights.get(task_name, 1.0)
            
            # æ ¹æ®æƒé‡é‡‡æ ·æ•°æ®
            sample_size = int(len(task_data) * weight)
            sampled_data = np.random.choice(task_data, sample_size, replace=False)
            
            # æ·»åŠ ä»»åŠ¡æ ‡è¯†
            for item in sampled_data:
                item_with_task = item.copy()
                item_with_task["task"] = task_name
                all_data.append(item_with_task)
            
            task_info[task_name] = {
                "original_size": len(task_data),
                "sampled_size": sample_size,
                "weight": weight
            }
        
        # éšæœºæ‰“ä¹±
        np.random.shuffle(all_data)
        
        print("å¤šä»»åŠ¡æ•°æ®ç»Ÿè®¡:")
        for task, info in task_info.items():
            print(f"  {task}: {info['sampled_size']} æ ·æœ¬ (æƒé‡: {info['weight']})")
        
        return all_data, task_info
    
    def task_aware_formatting(self, instruction_data):
        """ä»»åŠ¡æ„ŸçŸ¥çš„æ ¼å¼åŒ–"""
        
        task_templates = {
            "qa": "Question: {instruction}\nAnswer: {response}",
            "summarization": "Summarize the following text:\n{instruction}\n\nSummary: {response}",
            "translation": "Translate to English:\n{instruction}\n\nTranslation: {response}",
            "classification": "Classify the following text:\n{instruction}\n\nCategory: {response}",
            "generation": "Complete the following:\n{instruction}\n\nCompletion: {response}"
        }
        
        formatted_data = []
        
        for item in instruction_data:
            task = item.get("task", "general")
            template = task_templates.get(task, task_templates["qa"])
            
            formatted_text = template.format(
                instruction=item["instruction"],
                response=item["response"]
            )
            
            formatted_data.append({
                "text": formatted_text,
                "task": task,
                "instruction": item["instruction"],
                "response": item["response"]
            })
        
        return formatted_data

# ä½¿ç”¨ç¤ºä¾‹
def run_sft_training():
    """è¿è¡ŒSFTè®­ç»ƒç¤ºä¾‹"""
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    train_data = [
        {
            "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ",
            "response": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚æ¨¡å¼..."
        },
        {
            "instruction": "å†™ä¸€ä¸ªPythonå‡½æ•°è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
            "response": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)"
        },
        # ... æ›´å¤šè®­ç»ƒæ•°æ®
    ]
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SupervisedFineTuner("microsoft/DialoGPT-medium")
    
    # æ‰§è¡Œè®­ç»ƒ
    trained_model, results = trainer.train(
        train_data=train_data,
        eval_data=train_data[:100],  # ä½¿ç”¨éƒ¨åˆ†æ•°æ®ä½œä¸ºéªŒè¯é›†
        output_dir="./my_sft_model",
        epochs=3,
        batch_size=2,
        learning_rate=2e-4
    )
    
    return trained_model, results

# è¿è¡Œè®­ç»ƒ
# model, results = run_sft_training()
```

## ğŸ“Š SFTæ•ˆæœè¯„ä¼°

### å¤šç»´åº¦è¯„ä¼°æ¡†æ¶

```python
class SFTEvaluator:
    """SFTæ•ˆæœè¯„ä¼°å™¨"""
    
    def __init__(self, base_model, sft_model, tokenizer):
        self.base_model = base_model
        self.sft_model = sft_model
        self.tokenizer = tokenizer
    
    def instruction_following_evaluation(self, test_instructions):
        """æŒ‡ä»¤éµå¾ªèƒ½åŠ›è¯„ä¼°"""
        
        results = {
            "base_model": [],
            "sft_model": [],
            "improvement_scores": []
        }
        
        for instruction in test_instructions:
            # åŸºç¡€æ¨¡å‹å“åº”
            base_response = self.generate_response(self.base_model, instruction)
            base_score = self.evaluate_instruction_following(instruction, base_response)
            
            # SFTæ¨¡å‹å“åº”
            sft_response = self.generate_response(self.sft_model, instruction)
            sft_score = self.evaluate_instruction_following(instruction, sft_response)
            
            # è®°å½•ç»“æœ
            results["base_model"].append({
                "instruction": instruction,
                "response": base_response,
                "score": base_score
            })
            
            results["sft_model"].append({
                "instruction": instruction,
                "response": sft_response,
                "score": sft_score
            })
            
            results["improvement_scores"].append(sft_score - base_score)
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        avg_improvement = np.mean(results["improvement_scores"])
        improvement_rate = np.mean([s > 0 for s in results["improvement_scores"]])
        
        return {
            "average_improvement": avg_improvement,
            "improvement_rate": improvement_rate,
            "detailed_results": results
        }
    
    def evaluate_instruction_following(self, instruction, response):
        """è¯„ä¼°æŒ‡ä»¤éµå¾ªè´¨é‡"""
        
        score = 0.0
        
        # 1. æ ¼å¼æ£€æŸ¥ (0-0.3åˆ†)
        format_score = self.check_response_format(response)
        score += format_score * 0.3
        
        # 2. ç›¸å…³æ€§æ£€æŸ¥ (0-0.4åˆ†)
        relevance_score = self.check_relevance(instruction, response)
        score += relevance_score * 0.4
        
        # 3. å®Œæ•´æ€§æ£€æŸ¥ (0-0.2åˆ†)
        completeness_score = self.check_completeness(instruction, response)
        score += completeness_score * 0.2
        
        # 4. è´¨é‡æ£€æŸ¥ (0-0.1åˆ†)
        quality_score = self.check_response_quality(response)
        score += quality_score * 0.1
        
        return score
    
    def task_specific_evaluation(self, task_test_suites):
        """ä»»åŠ¡ç‰¹å®šè¯„ä¼°"""
        
        task_results = {}
        
        for task_name, test_suite in task_test_suites.items():
            print(f"è¯„ä¼°ä»»åŠ¡: {task_name}")
            
            task_scores = {
                "base_model": [],
                "sft_model": []
            }
            
            for test_case in test_suite:
                instruction = test_case["instruction"]
                expected = test_case.get("expected", "")
                
                # ç”Ÿæˆå›ç­”
                base_response = self.generate_response(self.base_model, instruction)
                sft_response = self.generate_response(self.sft_model, instruction)
                
                # ä»»åŠ¡ç‰¹å®šè¯„ä¼°
                base_score = self.task_specific_score(
                    task_name, instruction, base_response, expected
                )
                sft_score = self.task_specific_score(
                    task_name, instruction, sft_response, expected
                )
                
                task_scores["base_model"].append(base_score)
                task_scores["sft_model"].append(sft_score)
            
            # è®¡ç®—ä»»åŠ¡å¹³å‡åˆ†
            task_results[task_name] = {
                "base_avg": np.mean(task_scores["base_model"]),
                "sft_avg": np.mean(task_scores["sft_model"]),
                "improvement": np.mean(task_scores["sft_model"]) - np.mean(task_scores["base_model"])
            }
        
        return task_results
    
    def output_format_analysis(self, test_instructions):
        """è¾“å‡ºæ ¼å¼åˆ†æ"""
        
        format_metrics = {
            "ç»“æ„åŒ–ç¨‹åº¦": [],
            "é•¿åº¦é€‚ä¸­æ€§": [],
            "è¯­è¨€æµç•…æ€§": [],
            "ä¸“ä¸šæ€§": []
        }
        
        for instruction in test_instructions:
            sft_response = self.generate_response(self.sft_model, instruction)
            
            # åˆ†æå„ä¸ªç»´åº¦
            format_metrics["ç»“æ„åŒ–ç¨‹åº¦"].append(
                self.analyze_structure(sft_response)
            )
            format_metrics["é•¿åº¦é€‚ä¸­æ€§"].append(
                self.analyze_length_appropriateness(instruction, sft_response)
            )
            format_metrics["è¯­è¨€æµç•…æ€§"].append(
                self.analyze_fluency(sft_response)
            )
            format_metrics["ä¸“ä¸šæ€§"].append(
                self.analyze_professionalism(sft_response)
            )
        
        # è®¡ç®—å¹³å‡åˆ†
        avg_metrics = {
            metric: np.mean(scores) 
            for metric, scores in format_metrics.items()
        }
        
        return avg_metrics
    
    def generate_evaluation_report(self, instruction_eval, task_eval, format_eval):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        
        report = {
            "SFTè®­ç»ƒæ•ˆæœæ€»ç»“": {
                "æŒ‡ä»¤éµå¾ªæ”¹å–„": f"{instruction_eval['average_improvement']:.3f}åˆ† ({instruction_eval['improvement_rate']*100:.1f}%æ ·æœ¬æ”¹å–„)",
                "ä»»åŠ¡èƒ½åŠ›æå‡": {
                    task: f"{results['improvement']:.3f}åˆ†" 
                    for task, results in task_eval.items()
                },
                "è¾“å‡ºæ ¼å¼è´¨é‡": {
                    metric: f"{score:.3f}åˆ†" 
                    for metric, score in format_eval.items()
                }
            },
            
            "å…³é”®å‘ç°": {
                "æœ€ä½³æ”¹å–„ä»»åŠ¡": max(task_eval.items(), key=lambda x: x[1]['improvement'])[0],
                "éœ€è¦å…³æ³¨çš„ä»»åŠ¡": [
                    task for task, results in task_eval.items() 
                    if results['improvement'] < 0.1
                ],
                "æ ¼å¼åŒ–è´¨é‡": "è‰¯å¥½" if np.mean(list(format_eval.values())) > 0.7 else "éœ€è¦æ”¹è¿›"
            },
            
            "æ”¹è¿›å»ºè®®": self.generate_improvement_suggestions(
                instruction_eval, task_eval, format_eval
            )
        }
        
        return report

# è¯„ä¼°ä½¿ç”¨ç¤ºä¾‹
def evaluate_sft_model(base_model_path, sft_model_path, tokenizer):
    """è¯„ä¼°SFTæ¨¡å‹æ•ˆæœ"""
    
    # åŠ è½½æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(base_model_path)
    sft_model = AutoModelForCausalLM.from_pretrained(sft_model_path)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = SFTEvaluator(base_model, sft_model, tokenizer)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_instructions = [
        "è§£é‡Šé‡å­è®¡ç®—çš„åŸºæœ¬åŸç†",
        "å†™ä¸€ä¸ªæ’åºç®—æ³•çš„Pythonå®ç°",
        "åˆ†æè¿™æ®µä»£ç çš„æ—¶é—´å¤æ‚åº¦",
        # ... æ›´å¤šæµ‹è¯•æŒ‡ä»¤
    ]
    
    task_test_suites = {
        "é—®ç­”": [
            {"instruction": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", "expected": "è¯¦ç»†è§£é‡Š"},
            {"instruction": "äººå·¥æ™ºèƒ½çš„å‘å±•å†å²", "expected": "å†å²ä»‹ç»"}
        ],
        "ä»£ç ç”Ÿæˆ": [
            {"instruction": "å†™ä¸€ä¸ªå¿«é€Ÿæ’åºå‡½æ•°", "expected": "Pythonä»£ç "},
            {"instruction": "å®ç°äºŒå‰æ ‘éå†", "expected": "ç®—æ³•å®ç°"}
        ]
    }
    
    # æ‰§è¡Œè¯„ä¼°
    instruction_results = evaluator.instruction_following_evaluation(test_instructions)
    task_results = evaluator.task_specific_evaluation(task_test_suites)
    format_results = evaluator.output_format_analysis(test_instructions)
    
    # ç”ŸæˆæŠ¥å‘Š
    evaluation_report = evaluator.generate_evaluation_report(
        instruction_results, task_results, format_results
    )
    
    print("SFTè¯„ä¼°æŠ¥å‘Š:")
    print(json.dumps(evaluation_report, indent=2, ensure_ascii=False))
    
    return evaluation_report
```

## ğŸ¯ é¢è¯•é—®ç­”æ€»ç»“

### Q1: SFTåœ¨æ•´ä¸ªè®­ç»ƒæµç¨‹ä¸­çš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
**A**: SFTæ˜¯å…³é”®çš„èƒ½åŠ›è½¬æ¢é˜¶æ®µï¼š
- **åŠŸèƒ½è½¬æ¢**: ä»è¯­è¨€å»ºæ¨¡è½¬ä¸ºä»»åŠ¡æ‰§è¡Œ
- **æ ¼å¼è§„èŒƒ**: æ•™ä¼šæ¨¡å‹æ ‡å‡†è¾“å…¥è¾“å‡ºæ ¼å¼
- **æŒ‡ä»¤éµå¾ª**: å»ºç«‹åŸºç¡€çš„æŒ‡ä»¤ç†è§£å’Œæ‰§è¡Œèƒ½åŠ›
- **åç»­åŸºç¡€**: ä¸ºRLHFç­‰åç»­å¯¹é½è®­ç»ƒæä¾›åŸºç¡€

### Q2: æŒ‡ä»¤å¾®è°ƒä¸ä¼ ç»Ÿå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
**A**:
- **æ•°æ®æ ¼å¼**: æŒ‡ä»¤å¾®è°ƒä½¿ç”¨æŒ‡ä»¤-å›ç­”å¯¹ï¼Œä¼ ç»Ÿå¾®è°ƒä½¿ç”¨æ ‡æ³¨æ•°æ®
- **è®­ç»ƒç›®æ ‡**: æŒ‡ä»¤å¾®è°ƒè®­ç»ƒé€šç”¨æŒ‡ä»¤éµå¾ªï¼Œä¼ ç»Ÿå¾®è°ƒè®­ç»ƒç‰¹å®šä»»åŠ¡
- **æ³›åŒ–èƒ½åŠ›**: æŒ‡ä»¤å¾®è°ƒå…·å¤‡é›¶æ ·æœ¬æ³›åŒ–ï¼Œä¼ ç»Ÿå¾®è°ƒå±€é™äºè®­ç»ƒä»»åŠ¡
- **åº”ç”¨èŒƒå›´**: æŒ‡ä»¤å¾®è°ƒé€‚ç”¨äºå¯¹è¯åŠ©æ‰‹ï¼Œä¼ ç»Ÿå¾®è°ƒé€‚ç”¨äºä¸“é—¨ä»»åŠ¡

### Q3: å¦‚ä½•æ„å»ºé«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®é›†ï¼Ÿ
**A**: å››ä¸ªå…³é”®ç»´åº¦ï¼š
- **å¤šæ ·æ€§**: ä»»åŠ¡ç±»å‹ã€é¢†åŸŸã€éš¾åº¦ã€é£æ ¼çš„å…¨é¢è¦†ç›–
- **å‡†ç¡®æ€§**: å›ç­”å‡†ç¡®ã€äº‹å®æ­£ç¡®ã€é€»è¾‘æ¸…æ™°
- **å®Œæ•´æ€§**: å›ç­”å®Œæ•´ã€ç»“æ„åŒ–ã€æ»¡è¶³æŒ‡ä»¤è¦æ±‚
- **ä¸€è‡´æ€§**: æ ¼å¼ç»Ÿä¸€ã€é£æ ¼ä¸€è‡´ã€æ ‡å‡†è§„èŒƒ

### Q4: SFTè®­ç»ƒä¸­å¸¸è§çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼Ÿ
**A**:
- **è¿‡æ‹Ÿåˆ**: ä½¿ç”¨éªŒè¯é›†æ—©åœã€å¢åŠ æ•°æ®å¤šæ ·æ€§ã€é€‚å½“æ­£åˆ™åŒ–
- **æ ¼å¼ä¸è§„èŒƒ**: ç»Ÿä¸€æ•°æ®æ¨¡æ¿ã€æ ‡å‡†åŒ–é¢„å¤„ç†ã€è´¨é‡æ£€æŸ¥
- **èƒ½åŠ›ä¸å‡è¡¡**: å¹³è¡¡é‡‡æ ·ã€ä»»åŠ¡æƒé‡ã€å¤šè½®è®­ç»ƒ
- **è®¡ç®—èµ„æºä¸è¶³**: é‡åŒ–è®­ç»ƒã€æ¢¯åº¦ç´¯ç§¯ã€æ¨¡å‹å¹¶è¡Œ

## ğŸš€ å­¦ä¹ å»ºè®®

1. **ç†è§£æ ¸å¿ƒ**: æ·±å…¥ç†è§£SFTåœ¨æ•´ä¸ªè®­ç»ƒæµç¨‹ä¸­çš„å…³é”®ä½œç”¨
2. **æ•°æ®ä¸ºç‹**: é‡ç‚¹æŒæ¡é«˜è´¨é‡æŒ‡ä»¤æ•°æ®çš„æ„å»ºæ–¹æ³•
3. **å®è·µéªŒè¯**: åœ¨å®é™…æ•°æ®ä¸Šå®Œæ•´è·‘é€šSFTè®­ç»ƒæµç¨‹
4. **æ•ˆæœè¯„ä¼°**: å»ºç«‹å¤šç»´åº¦çš„SFTæ•ˆæœè¯„ä¼°ä½“ç³»

SFTæ˜¯è¿æ¥é¢„è®­ç»ƒä¸åº”ç”¨çš„å…³é”®æ¡¥æ¢ï¼Œæ˜¯ç°ä»£LLMä¸å¯æˆ–ç¼ºçš„æ ¸å¿ƒæŠ€æœ¯ï¼