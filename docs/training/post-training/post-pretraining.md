# åé¢„è®­ç»ƒæŠ€æœ¯

## ğŸ¯ å­¦ä¹ ç›®æ ‡

æ·±å…¥ç†è§£åé¢„è®­ç»ƒ(Post-Pretraining)æŠ€æœ¯ï¼ŒæŒæ¡é¢†åŸŸé€‚åº”å’ŒçŸ¥è¯†æ³¨å…¥çš„æ ¸å¿ƒæ–¹æ³•ï¼Œå­¦ä¼šåœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šè¿›è¡Œæœ‰æ•ˆçš„é¢†åŸŸå¢å¼ºã€‚

**é‡ç‚¹é¢è¯•é—®é¢˜é¢„è§ˆï¼š**
- ä»€ä¹ˆæ˜¯åé¢„è®­ç»ƒï¼Ÿä¸é¢„è®­ç»ƒå’Œå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
- åé¢„è®­ç»ƒçš„æ•°æ®é…æ¯”ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ
- å¦‚ä½•é˜²æ­¢åé¢„è®­ç»ƒä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼Ÿ
- åé¢„è®­ç»ƒåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹æœ€æœ‰ä»·å€¼ï¼Ÿ

## ğŸ—ï¸ åé¢„è®­ç»ƒæ ¸å¿ƒæ¦‚å¿µ

### å®šä¹‰ä¸å®šä½

åé¢„è®­ç»ƒ(Post-Pretraining)æ˜¯æŒ‡åœ¨é€šç”¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šï¼Œä½¿ç”¨**ç‰¹å®šé¢†åŸŸæ•°æ®**è¿›è¡Œ**ç»§ç»­é¢„è®­ç»ƒ**çš„è¿‡ç¨‹ï¼Œç›®æ ‡æ˜¯åœ¨ä¿æŒé€šç”¨èƒ½åŠ›çš„åŒæ—¶å¢å¼ºç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†å’Œèƒ½åŠ›ã€‚

```
è®­ç»ƒé˜¶æ®µå®šä½å›¾
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å¤§æ¨¡å‹å®Œæ•´è®­ç»ƒæµç¨‹                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  é¢„è®­ç»ƒ        åé¢„è®­ç»ƒ       ç›‘ç£å¾®è°ƒ      å¼ºåŒ–å­¦ä¹ å¯¹é½          â”‚
â”‚ Pretraining â†’ Post-Pretraining â†’ SFT â†’ RLHF                    â”‚
â”‚                                                                 â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚ â”‚é€šç”¨è¯­æ–™  â”‚   â”‚ é¢†åŸŸ+é€šç”¨    â”‚   â”‚æŒ‡ä»¤å¯¹  â”‚   â”‚ åå¥½æ•°æ®     â”‚      â”‚
â”‚ â”‚CommonCrawlâ”‚  â”‚ åŒ»ç–—+Common â”‚   â”‚Instructâ”‚   â”‚ Human Pref  â”‚      â”‚
â”‚ â”‚C4, ä¹¦ç±ç­‰ â”‚   â”‚ æ³•å¾‹+Books  â”‚   â”‚Dataset â”‚   â”‚ RLHF Data   â”‚      â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚      â–²              â–²             â–²             â–²               â”‚
â”‚  åŸºç¡€èƒ½åŠ›        é¢†åŸŸå¢å¼º       ä»»åŠ¡é€‚åº”       ä»·å€¼å¯¹é½            â”‚
â”‚  è¯­è¨€ç†è§£        ä¸“ä¸šçŸ¥è¯†       æŒ‡ä»¤éµå¾ª       äººç±»åå¥½            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒä»·å€¼ä¸»å¼ 

1. **çŸ¥è¯†æ³¨å…¥**: å‘æ¨¡å‹æ³¨å…¥ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†
2. **èƒ½åŠ›ä¿æŒ**: ç»´æŒåŸæœ‰çš„é€šç”¨è¯­è¨€èƒ½åŠ›
3. **æˆæœ¬æ•ˆç›Š**: ç›¸æ¯”ä»å¤´è®­ç»ƒï¼Œå¤§å¹…é™ä½è®¡ç®—æˆæœ¬
4. **çµæ´»é€‚åº”**: å¯é’ˆå¯¹ä¸åŒé¢†åŸŸå¿«é€Ÿå®šåˆ¶

## ğŸ“Š æŠ€æœ¯åŸç†ä¸ç­–ç•¥

### æ•°æ®é…æ¯”é»„é‡‘æ³•åˆ™

åŸºäºå¤§é‡å®éªŒéªŒè¯çš„æœ€ä½³å®è·µï¼š

```python
class PostPretrainingDataStrategy:
    """åé¢„è®­ç»ƒæ•°æ®ç­–ç•¥"""
    
    def __init__(self, domain_data_size, general_data_size):
        self.domain_data = domain_data_size
        self.general_data = general_data_size
        
    def optimal_mixing_ratio(self):
        """æœ€ä¼˜æ•°æ®æ··åˆæ¯”ä¾‹"""
        
        # é»„é‡‘æ¯”ä¾‹: é¢†åŸŸæ•°æ®:é€šç”¨æ•°æ® = 1:5
        recommended_ratio = {
            "é¢†åŸŸç‰¹å®šæ•°æ®": "16.7%",
            "é€šç”¨æ•°æ®": "83.3%", 
            "æ··åˆç­–ç•¥": "æ¯ä¸ªbatchå†…éšæœºæ··åˆ",
            "ç†è®ºä¾æ®": "å¹³è¡¡ä¸“ä¸šæ€§ä¸é€šç”¨æ€§ï¼Œé˜²æ­¢ç¾éš¾æ€§é—å¿˜"
        }
        
        return recommended_ratio
    
    def data_preprocessing_pipeline(self, domain_corpus, general_corpus):
        """æ•°æ®é¢„å¤„ç†æµç¨‹"""
        
        # 1. é¢†åŸŸæ•°æ®å¤„ç†
        domain_processed = self.process_domain_data(domain_corpus)
        
        # 2. é€šç”¨æ•°æ®é‡‡æ ·
        general_sampled = self.sample_general_data(general_corpus, 
                                                  target_size=len(domain_processed) * 5)
        
        # 3. æ•°æ®å»é‡
        dedup_domain = self.deduplicate_data(domain_processed)
        dedup_general = self.deduplicate_data(general_sampled)
        
        # 4. è´¨é‡è¿‡æ»¤
        high_quality_domain = self.quality_filter(dedup_domain, threshold=0.8)
        high_quality_general = self.quality_filter(dedup_general, threshold=0.7)
        
        # 5. æ··åˆç­–ç•¥
        mixed_dataset = self.create_mixed_dataset(high_quality_domain, high_quality_general)
        
        return mixed_dataset
    
    def create_mixed_dataset(self, domain_data, general_data):
        """åˆ›å»ºæ··åˆæ•°æ®é›†"""
        import random
        
        mixed_data = []
        
        # ç¡®ä¿æ¯ä¸ªbatchéƒ½æœ‰é€‚å½“æ¯”ä¾‹çš„æ··åˆ
        batch_size = 1000
        domain_per_batch = batch_size // 6  # ~16.7%
        general_per_batch = batch_size - domain_per_batch  # ~83.3%
        
        domain_idx, general_idx = 0, 0
        
        while domain_idx < len(domain_data) and general_idx < len(general_data):
            batch_data = []
            
            # æ·»åŠ é¢†åŸŸæ•°æ®
            batch_data.extend(domain_data[domain_idx:domain_idx + domain_per_batch])
            domain_idx += domain_per_batch
            
            # æ·»åŠ é€šç”¨æ•°æ®
            batch_data.extend(general_data[general_idx:general_idx + general_per_batch])
            general_idx += general_per_batch
            
            # éšæœºæ‰“ä¹±batchå†…æ•°æ®
            random.shuffle(batch_data)
            mixed_data.extend(batch_data)
        
        return mixed_data

# å®é™…åº”ç”¨ç¤ºä¾‹
strategy = PostPretrainingDataStrategy(
    domain_data_size=1000000,  # 100ä¸‡æ¡åŒ»ç–—æ•°æ®
    general_data_size=5000000  # 500ä¸‡æ¡é€šç”¨æ•°æ®
)

optimal_ratio = strategy.optimal_mixing_ratio()
print("æœ€ä¼˜æ··åˆæ¯”ä¾‹:", optimal_ratio)
```

### è®­ç»ƒè¶…å‚æ•°é…ç½®

```python
class PostPretrainingConfig:
    """åé¢„è®­ç»ƒé…ç½®ç®¡ç†"""
    
    def __init__(self, model_size="7B"):
        self.model_size = model_size
        
    def get_training_config(self):
        """è·å–è®­ç»ƒé…ç½®"""
        
        base_config = {
            # å­¦ä¹ ç‡ç­–ç•¥
            "learning_rate": 1e-5,  # æ¯”é¢„è®­ç»ƒä½1-2ä¸ªæ•°é‡çº§
            "min_learning_rate": 1e-6,
            "warmup_ratio": 0.03,   # 3%çš„warmup
            "lr_scheduler": "cosine",
            
            # è®­ç»ƒç­–ç•¥
            "epochs": 1,            # å•è½®è®­ç»ƒé¿å…è¿‡æ‹Ÿåˆ
            "batch_size": self._get_batch_size(),
            "gradient_accumulation": self._get_grad_accumulation(),
            "max_grad_norm": 1.0,
            
            # æ•°æ®ç­–ç•¥
            "max_seq_length": 2048,
            "data_mixture_ratio": {"domain": 0.167, "general": 0.833},
            
            # æ­£åˆ™åŒ–
            "weight_decay": 0.1,
            "dropout": 0.1,
            
            # ç›‘æ§æŒ‡æ ‡
            "eval_steps": 500,
            "save_steps": 1000,
            "logging_steps": 100
        }
        
        return base_config
    
    def _get_batch_size(self):
        """æ ¹æ®æ¨¡å‹å¤§å°è°ƒæ•´batch size"""
        size_mapping = {
            "1B": 64,
            "3B": 32, 
            "7B": 16,
            "13B": 8,
            "30B": 4,
            "70B": 2
        }
        return size_mapping.get(self.model_size, 16)
    
    def _get_grad_accumulation(self):
        """æ¢¯åº¦ç´¯ç§¯æ­¥æ•°"""
        # ç¡®ä¿æœ‰æ•ˆbatch sizeè¶³å¤Ÿå¤§
        target_effective_batch = 256
        actual_batch = self._get_batch_size()
        return max(1, target_effective_batch // actual_batch)
    
    def catastrophic_forgetting_prevention(self):
        """ç¾éš¾æ€§é—å¿˜é˜²æ­¢ç­–ç•¥"""
        
        strategies = {
            "æ•°æ®å±‚é¢": {
                "é€šç”¨æ•°æ®æ··åˆ": "ä¿æŒ83.3%é€šç”¨æ•°æ®æ¯”ä¾‹",
                "æ•°æ®è´¨é‡": "ç¡®ä¿é¢†åŸŸæ•°æ®è´¨é‡ï¼Œé¿å…å™ªå£°æ•°æ®",
                "åºåˆ—é•¿åº¦": "ä½¿ç”¨å®Œæ•´åºåˆ—ï¼Œé¿å…æˆªæ–­ä¸¢å¤±ä¸Šä¸‹æ–‡"
            },
            
            "è®­ç»ƒå±‚é¢": {
                "å­¦ä¹ ç‡": "ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡(1e-5)ï¼Œæ¸©å’Œæ›´æ–°",
                "è®­ç»ƒè½®æ¬¡": "å•è½®è®­ç»ƒï¼Œé¿å…è¿‡åº¦æ‹Ÿåˆé¢†åŸŸæ•°æ®",
                "æ¢¯åº¦è£å‰ª": "é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å¯¼è‡´çš„çªå˜"
            },
            
            "è¯„ä¼°å±‚é¢": {
                "é€šç”¨èƒ½åŠ›ç›‘æ§": "å®šæœŸåœ¨é€šç”¨åŸºå‡†ä¸Šè¯„ä¼°",
                "é¢†åŸŸèƒ½åŠ›è¯„ä¼°": "éªŒè¯é¢†åŸŸå¢å¼ºæ•ˆæœ",
                "æ—©åœæœºåˆ¶": "å‘ç°é—å¿˜æ—¶åŠæ—¶åœæ­¢"
            }
        }
        
        return strategies

# é…ç½®ç¤ºä¾‹
config = PostPretrainingConfig("7B")
training_config = config.get_training_config()
forgetting_prevention = config.catastrophic_forgetting_prevention()

print("è®­ç»ƒé…ç½®:", training_config)
print("é—å¿˜é˜²æ­¢ç­–ç•¥:", forgetting_prevention)
```

## ğŸ’» å®é™…å®ç°æ¡ˆä¾‹

### åŒ»ç–—é¢†åŸŸåé¢„è®­ç»ƒç¤ºä¾‹

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from datasets import Dataset, concatenate_datasets
import numpy as np

class MedicalPostPretraining:
    """åŒ»ç–—é¢†åŸŸåé¢„è®­ç»ƒå®ç°"""
    
    def __init__(self, base_model_name="meta-llama/Llama-2-7b-hf"):
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            load_in_8bit=True  # å†…å­˜ä¼˜åŒ–
        )
        
        # æ·»åŠ padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
    
    def prepare_medical_data(self, medical_texts, general_texts):
        """å‡†å¤‡åŒ»ç–—é¢†åŸŸæ··åˆæ•°æ®"""
        
        # 1. å¤„ç†åŒ»ç–—æ–‡æœ¬
        medical_processed = []
        for text in medical_texts:
            if self._is_high_quality_medical(text):
                medical_processed.append({"text": text, "source": "medical"})
        
        # 2. é‡‡æ ·é€šç”¨æ–‡æœ¬ (5å€åŒ»ç–—æ•°æ®é‡)
        target_general_size = len(medical_processed) * 5
        general_sampled = np.random.choice(
            general_texts, 
            size=min(target_general_size, len(general_texts)),
            replace=False
        )
        
        general_processed = [
            {"text": text, "source": "general"} 
            for text in general_sampled
        ]
        
        # 3. åˆ›å»ºæ··åˆæ•°æ®é›†
        all_data = medical_processed + general_processed
        np.random.shuffle(all_data)  # éšæœºæ‰“ä¹±
        
        return Dataset.from_list(all_data)
    
    def _is_high_quality_medical(self, text):
        """åŒ»ç–—æ–‡æœ¬è´¨é‡æ£€æŸ¥"""
        
        # åŸºæœ¬é•¿åº¦æ£€æŸ¥
        if len(text.split()) < 50 or len(text.split()) > 2000:
            return False
        
        # åŒ»ç–—æœ¯è¯­å¯†åº¦æ£€æŸ¥
        medical_terms = [
            "patient", "diagnosis", "treatment", "symptom", "disease",
            "medication", "therapy", "clinical", "medical", "health",
            "ç—…äºº", "è¯Šæ–­", "æ²»ç–—", "ç—‡çŠ¶", "ç–¾ç—…", "è¯ç‰©", "ä¸´åºŠ", "åŒ»ç–—"
        ]
        
        term_count = sum(1 for term in medical_terms if term.lower() in text.lower())
        term_density = term_count / len(text.split())
        
        # è‡³å°‘åŒ…å«ä¸€å®šå¯†åº¦çš„åŒ»ç–—æœ¯è¯­
        return term_density > 0.02
    
    def tokenize_function(self, examples):
        """æ•°æ®tokenization"""
        
        # Tokenizeæ–‡æœ¬
        tokenized = self.tokenizer(
            examples["text"],
            truncation=True,
            padding=False,
            max_length=2048,
            return_tensors=None
        )
        
        # å¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼Œlabelså°±æ˜¯input_ids
        tokenized["labels"] = tokenized["input_ids"].copy()
        
        return tokenized
    
    def create_data_collator(self):
        """åˆ›å»ºæ•°æ®æ•´ç†å™¨"""
        from transformers import DataCollatorForLanguageModeling
        
        return DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # ä¸ä½¿ç”¨masked language modeling
            pad_to_multiple_of=8  # ä¼˜åŒ–GPUå†…å­˜å¯¹é½
        )
    
    def train(self, train_dataset, eval_dataset, output_dir):
        """æ‰§è¡Œåé¢„è®­ç»ƒ"""
        
        # è®­ç»ƒé…ç½®
        training_args = TrainingArguments(
            output_dir=output_dir,
            
            # åŸºæœ¬è®¾ç½®
            num_train_epochs=1,
            per_device_train_batch_size=4,
            per_device_eval_batch_size=4,
            gradient_accumulation_steps=16,  # æœ‰æ•ˆbatch size = 64
            
            # å­¦ä¹ ç‡è®¾ç½®
            learning_rate=1e-5,
            warmup_ratio=0.03,
            lr_scheduler_type="cosine",
            
            # ä¼˜åŒ–è®¾ç½®
            bf16=True,  # ä½¿ç”¨bfloat16
            gradient_checkpointing=True,  # èŠ‚çœæ˜¾å­˜
            max_grad_norm=1.0,
            weight_decay=0.1,
            
            # è¯„ä¼°å’Œä¿å­˜
            evaluation_strategy="steps",
            eval_steps=500,
            save_steps=1000,
            save_total_limit=3,
            
            # æ—¥å¿—è®¾ç½®
            logging_steps=100,
            logging_dir=f"{output_dir}/logs",
            report_to="tensorboard",
            
            # å…¶ä»–è®¾ç½®
            dataloader_pin_memory=True,
            remove_unused_columns=False,
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=self.create_data_collator(),
            tokenizer=self.tokenizer,
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        
        return trainer

# ä½¿ç”¨ç¤ºä¾‹
def run_medical_post_pretraining():
    """è¿è¡ŒåŒ»ç–—é¢†åŸŸåé¢„è®­ç»ƒ"""
    
    # 1. å‡†å¤‡æ•°æ® (ç¤ºä¾‹æ•°æ®)
    medical_texts = [
        "æ‚£è€…ä¸»è¯‰èƒ¸ç—›3å¤©ï¼Œä¼´æœ‰å‘¼å¸å›°éš¾ã€‚ä½“æ ¼æ£€æŸ¥å‘ç°å¿ƒç‡å¢å¿«...",
        "The patient presents with acute myocardial infarction...",
        # ... æ›´å¤šåŒ»ç–—æ–‡æœ¬
    ]
    
    general_texts = [
        "Today is a beautiful day for outdoor activities...",
        "Machine learning is transforming various industries...",
        # ... æ›´å¤šé€šç”¨æ–‡æœ¬
    ]
    
    # 2. åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = MedicalPostPretraining()
    
    # 3. å‡†å¤‡æ•°æ®
    train_data = trainer.prepare_medical_data(medical_texts, general_texts)
    tokenized_train = train_data.map(trainer.tokenize_function, batched=True)
    
    # åˆ’åˆ†è®­ç»ƒå’ŒéªŒè¯é›†
    train_test_split = tokenized_train.train_test_split(test_size=0.1)
    train_dataset = train_test_split["train"]
    eval_dataset = train_test_split["test"]
    
    # 4. æ‰§è¡Œè®­ç»ƒ
    trained_model = trainer.train(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        output_dir="./medical_llama_7b"
    )
    
    return trained_model

# æ‰§è¡Œè®­ç»ƒ
# trained_model = run_medical_post_pretraining()
```

### æ•ˆæœè¯„ä¼°ä¸ç›‘æ§

```python
class PostPretrainingEvaluator:
    """åé¢„è®­ç»ƒæ•ˆæœè¯„ä¼°å™¨"""
    
    def __init__(self, base_model, finetuned_model, tokenizer):
        self.base_model = base_model
        self.finetuned_model = finetuned_model
        self.tokenizer = tokenizer
    
    def domain_knowledge_evaluation(self, domain_questions):
        """é¢†åŸŸçŸ¥è¯†è¯„ä¼°"""
        
        results = {
            "base_model_scores": [],
            "finetuned_model_scores": [],
            "improvements": []
        }
        
        for question in domain_questions:
            # åŸºç¡€æ¨¡å‹å›ç­”
            base_answer = self.generate_answer(self.base_model, question)
            base_score = self.evaluate_domain_answer(question, base_answer)
            
            # å¾®è°ƒæ¨¡å‹å›ç­”
            ft_answer = self.generate_answer(self.finetuned_model, question)
            ft_score = self.evaluate_domain_answer(question, ft_answer)
            
            # è®°å½•ç»“æœ
            results["base_model_scores"].append(base_score)
            results["finetuned_model_scores"].append(ft_score)
            results["improvements"].append(ft_score - base_score)
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        avg_improvement = np.mean(results["improvements"])
        improvement_std = np.std(results["improvements"])
        
        return {
            "average_improvement": avg_improvement,
            "improvement_std": improvement_std,
            "improvement_rate": np.mean(np.array(results["improvements"]) > 0),
            "detailed_results": results
        }
    
    def general_capability_retention(self, general_benchmarks):
        """é€šç”¨èƒ½åŠ›ä¿æŒè¯„ä¼°"""
        
        retention_scores = {}
        
        for benchmark_name, benchmark_data in general_benchmarks.items():
            base_score = self.run_benchmark(self.base_model, benchmark_data)
            ft_score = self.run_benchmark(self.finetuned_model, benchmark_data)
            
            retention_rate = ft_score / base_score if base_score > 0 else 0
            
            retention_scores[benchmark_name] = {
                "base_score": base_score,
                "finetuned_score": ft_score,
                "retention_rate": retention_rate,
                "acceptable": retention_rate > 0.95  # ä¿æŒ95%ä»¥ä¸Šä¸ºacceptable
            }
        
        return retention_scores
    
    def catastrophic_forgetting_detection(self, test_samples):
        """ç¾éš¾æ€§é—å¿˜æ£€æµ‹"""
        
        forgetting_indicators = {
            "è¯­æ³•èƒ½åŠ›ä¸‹é™": self.test_grammar_capability(test_samples["grammar"]),
            "å¸¸è¯†æ¨ç†é€€åŒ–": self.test_commonsense_reasoning(test_samples["commonsense"]),
            "åŸºç¡€çŸ¥è¯†ä¸¢å¤±": self.test_factual_knowledge(test_samples["facts"]),
            "é€»è¾‘æ¨ç†èƒ½åŠ›": self.test_logical_reasoning(test_samples["logic"])
        }
        
        # è®¡ç®—æ€»ä½“é—å¿˜é£é™©
        forgetting_risk = self.calculate_forgetting_risk(forgetting_indicators)
        
        return {
            "individual_indicators": forgetting_indicators,
            "overall_forgetting_risk": forgetting_risk,
            "risk_level": self.categorize_risk_level(forgetting_risk)
        }
    
    def calculate_forgetting_risk(self, indicators):
        """è®¡ç®—é—å¿˜é£é™©åˆ†æ•°"""
        
        risk_weights = {
            "è¯­æ³•èƒ½åŠ›ä¸‹é™": 0.3,
            "å¸¸è¯†æ¨ç†é€€åŒ–": 0.3, 
            "åŸºç¡€çŸ¥è¯†ä¸¢å¤±": 0.2,
            "é€»è¾‘æ¨ç†èƒ½åŠ›": 0.2
        }
        
        total_risk = 0
        for indicator, score in indicators.items():
            # scoreè¶Šä½é£é™©è¶Šé«˜
            risk = (1 - score) * risk_weights[indicator]
            total_risk += risk
        
        return total_risk
    
    def generate_evaluation_report(self, domain_eval, retention_eval, forgetting_eval):
        """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š"""
        
        report = {
            "åé¢„è®­ç»ƒæ•ˆæœæ€»ç»“": {
                "é¢†åŸŸèƒ½åŠ›æå‡": f"{domain_eval['average_improvement']:.3f} (+{domain_eval['improvement_rate']*100:.1f}% cases improved)",
                "é€šç”¨èƒ½åŠ›ä¿æŒ": f"å¹³å‡ä¿æŒç‡: {np.mean([r['retention_rate'] for r in retention_eval.values()]):.3f}",
                "é—å¿˜é£é™©è¯„ä¼°": forgetting_eval['risk_level']
            },
            
            "è¯¦ç»†åˆ†æ": {
                "æœ€ä½³æå‡é¢†åŸŸ": self.find_best_improvements(domain_eval),
                "éœ€è¦å…³æ³¨çš„é€šç”¨èƒ½åŠ›": self.find_concerning_retentions(retention_eval),
                "é—å¿˜é£é™©ç‚¹": self.find_forgetting_risks(forgetting_eval)
            },
            
            "æ”¹è¿›å»ºè®®": self.generate_improvement_suggestions(
                domain_eval, retention_eval, forgetting_eval
            )
        }
        
        return report

# è¯„ä¼°ä½¿ç”¨ç¤ºä¾‹
def evaluate_medical_post_pretraining(base_model, finetuned_model, tokenizer):
    """åŒ»ç–—é¢†åŸŸåé¢„è®­ç»ƒè¯„ä¼°"""
    
    evaluator = PostPretrainingEvaluator(base_model, finetuned_model, tokenizer)
    
    # å‡†å¤‡è¯„ä¼°æ•°æ®
    medical_questions = [
        "What are the symptoms of myocardial infarction?",
        "Explain the mechanism of action of ACE inhibitors",
        # ... æ›´å¤šåŒ»ç–—é—®é¢˜
    ]
    
    general_benchmarks = {
        "common_sense": load_commonsense_qa(),
        "reading_comprehension": load_reading_comprehension(),
        "arithmetic": load_arithmetic_tasks()
    }
    
    forgetting_test_samples = {
        "grammar": load_grammar_tests(),
        "commonsense": load_commonsense_tests(),
        "facts": load_factual_tests(),
        "logic": load_logic_tests()
    }
    
    # æ‰§è¡Œè¯„ä¼°
    domain_results = evaluator.domain_knowledge_evaluation(medical_questions)
    retention_results = evaluator.general_capability_retention(general_benchmarks)
    forgetting_results = evaluator.catastrophic_forgetting_detection(forgetting_test_samples)
    
    # ç”ŸæˆæŠ¥å‘Š
    evaluation_report = evaluator.generate_evaluation_report(
        domain_results, retention_results, forgetting_results
    )
    
    print("åé¢„è®­ç»ƒè¯„ä¼°æŠ¥å‘Š:")
    print(json.dumps(evaluation_report, indent=2, ensure_ascii=False))
    
    return evaluation_report

# è¿è¡Œè¯„ä¼°
# evaluation_report = evaluate_medical_post_pretraining(base_model, finetuned_model, tokenizer)
```

## ğŸ“‹ æœ€ä½³å®è·µæ€»ç»“

### æˆåŠŸè¦ç´ æ¸…å•

```python
def post_pretraining_checklist():
    """åé¢„è®­ç»ƒæˆåŠŸè¦ç´ æ¸…å•"""
    
    return {
        "æ•°æ®å‡†å¤‡": {
            "âœ“ é¢†åŸŸæ•°æ®è´¨é‡": "ç¡®ä¿æ•°æ®æ¥æºæƒå¨ã€å†…å®¹å‡†ç¡®",
            "âœ“ æ•°æ®è§„æ¨¡å……è¶³": "è‡³å°‘10äº¿tokençš„é¢†åŸŸæ•°æ®",
            "âœ“ é€šç”¨æ•°æ®æ··åˆ": "ä¿æŒ1:5çš„é¢†åŸŸ:é€šç”¨æ¯”ä¾‹",
            "âœ“ æ•°æ®å»é‡å¤„ç†": "é¿å…é‡å¤æ•°æ®å¯¼è‡´è¿‡æ‹Ÿåˆ",
            "âœ“ æ ¼å¼ç»Ÿä¸€åŒ–": "ä¿æŒä¸é¢„è®­ç»ƒé˜¶æ®µä¸€è‡´çš„æ ¼å¼"
        },
        
        "è®­ç»ƒé…ç½®": {
            "âœ“ å­¦ä¹ ç‡è®¾ç½®": "ä½¿ç”¨1e-5å·¦å³çš„è¾ƒä½å­¦ä¹ ç‡",
            "âœ“ å•è½®è®­ç»ƒ": "é¿å…å¤šè½®è®­ç»ƒå¯¼è‡´çš„è¿‡æ‹Ÿåˆ",
            "âœ“ æ‰¹é‡å¤§å°": "ç¡®ä¿æœ‰æ•ˆæ‰¹é‡å¤§å°è¶³å¤Ÿå¤§",
            "âœ“ åºåˆ—é•¿åº¦": "ä½¿ç”¨æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦",
            "âœ“ æ­£åˆ™åŒ–ç­–ç•¥": "é€‚å½“çš„dropoutå’Œweight decay"
        },
        
        "ç›‘æ§è¯„ä¼°": {
            "âœ“ é¢†åŸŸèƒ½åŠ›è·Ÿè¸ª": "å®šæœŸè¯„ä¼°é¢†åŸŸç‰¹å®šä»»åŠ¡è¡¨ç°",
            "âœ“ é€šç”¨èƒ½åŠ›ç›‘æ§": "ç¡®ä¿é€šç”¨benchmarkä¸é€€åŒ–",
            "âœ“ é—å¿˜é£é™©æ£€æµ‹": "ç›‘æ§ç¾éš¾æ€§é—å¿˜æŒ‡æ ‡",
            "âœ“ æ—©åœæœºåˆ¶": "è®¾ç½®åˆç†çš„æ—©åœæ¡ä»¶",
            "âœ“ å®šæœŸcheckpointing": "ä¿å­˜è®­ç»ƒè¿‡ç¨‹ä¸­çš„å…³é”®èŠ‚ç‚¹"
        }
    }

# å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ
def common_issues_solutions():
    """å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ"""
    
    return {
        "é—®é¢˜1: ç¾éš¾æ€§é—å¿˜": {
            "ç—‡çŠ¶": "é€šç”¨èƒ½åŠ›æ˜¾è‘—ä¸‹é™ï¼ŒåŸºç¡€ä»»åŠ¡è¡¨ç°å˜å·®",
            "åŸå› ": "é¢†åŸŸæ•°æ®æ¯”ä¾‹è¿‡é«˜ï¼Œå­¦ä¹ ç‡è¿‡å¤§ï¼Œè®­ç»ƒè¿‡ä¹…",
            "è§£å†³": "é™ä½é¢†åŸŸæ•°æ®æ¯”ä¾‹ï¼Œå‡å°å­¦ä¹ ç‡ï¼Œå•è½®è®­ç»ƒ"
        },
        
        "é—®é¢˜2: é¢†åŸŸèƒ½åŠ›æå‡ä¸æ˜æ˜¾": {
            "ç—‡çŠ¶": "åœ¨é¢†åŸŸä»»åŠ¡ä¸Šè¡¨ç°æ²¡æœ‰æ˜æ˜¾æ”¹å–„",
            "åŸå› ": "é¢†åŸŸæ•°æ®è´¨é‡å·®ï¼Œæ•°æ®é‡ä¸è¶³ï¼Œè®­ç»ƒä¸å……åˆ†",
            "è§£å†³": "æé«˜æ•°æ®è´¨é‡ï¼Œå¢åŠ æ•°æ®é‡ï¼Œé€‚å½“å¢åŠ è®­ç»ƒæ­¥æ•°"
        },
        
        "é—®é¢˜3: è®­ç»ƒä¸ç¨³å®š": {
            "ç—‡çŠ¶": "æŸå¤±éœ‡è¡ï¼Œè®­ç»ƒå‘æ•£ï¼Œæ€§èƒ½å¿½é«˜å¿½ä½",
            "åŸå› ": "å­¦ä¹ ç‡è¿‡é«˜ï¼Œæ‰¹é‡å¤§å°ä¸åˆé€‚ï¼Œæ•°æ®è´¨é‡é—®é¢˜",
            "è§£å†³": "é™ä½å­¦ä¹ ç‡ï¼Œè°ƒæ•´æ‰¹é‡å¤§å°ï¼Œæ¸…æ´—æ•°æ®"
        },
        
        "é—®é¢˜4: èµ„æºä¸è¶³": {
            "ç—‡çŠ¶": "æ˜¾å­˜ä¸å¤Ÿï¼Œè®­ç»ƒæ—¶é—´è¿‡é•¿",
            "åŸå› ": "æ¨¡å‹å¤ªå¤§ï¼Œæ‰¹é‡å¤§å°è¿‡å¤§ï¼Œåºåˆ—é•¿åº¦è¿‡é•¿",
            "è§£å†³": "ä½¿ç”¨é‡åŒ–ï¼Œå‡å°æ‰¹é‡ï¼Œæ¢¯åº¦ç´¯ç§¯ï¼Œæ¨¡å‹å¹¶è¡Œ"
        }
    }
```

## ğŸ¯ é¢è¯•é—®ç­”æ€»ç»“

### Q1: ä»€ä¹ˆæ˜¯åé¢„è®­ç»ƒï¼Ÿä¸é¢„è®­ç»ƒå’Œå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
**A**: åé¢„è®­ç»ƒæ˜¯åœ¨é€šç”¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šï¼Œä½¿ç”¨é¢†åŸŸç‰¹å®šæ•°æ®è¿›è¡Œç»§ç»­é¢„è®­ç»ƒçš„è¿‡ç¨‹ï¼š
- **ä¸é¢„è®­ç»ƒåŒºåˆ«**: é¢„è®­ç»ƒä½¿ç”¨é€šç”¨è¯­æ–™ï¼Œåé¢„è®­ç»ƒä½¿ç”¨é¢†åŸŸ+é€šç”¨æ··åˆæ•°æ®
- **ä¸å¾®è°ƒåŒºåˆ«**: å¾®è°ƒä½¿ç”¨ä»»åŠ¡æ ‡æ³¨æ•°æ®ï¼Œåé¢„è®­ç»ƒä»ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ 
- **ç›®æ ‡**: åœ¨ä¿æŒé€šç”¨èƒ½åŠ›åŸºç¡€ä¸Šæ³¨å…¥é¢†åŸŸçŸ¥è¯†

### Q2: åé¢„è®­ç»ƒçš„æ•°æ®é…æ¯”ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ
**A**: ç»éªŒéªŒè¯çš„é»„é‡‘æ¯”ä¾‹æ˜¯**é¢†åŸŸ:é€šç”¨ = 1:5**
- **ç†è®ºä¾æ®**: å¹³è¡¡é¢†åŸŸå¢å¼ºä¸èƒ½åŠ›ä¿æŒ
- **å®æ–½æ–¹æ³•**: æ¯ä¸ªbatchå†…éšæœºæ··åˆï¼Œç¡®ä¿æ¯”ä¾‹ç¨³å®š
- **è°ƒä¼˜ç©ºé—´**: å¯æ ¹æ®å…·ä½“éœ€æ±‚åœ¨1:3åˆ°1:8ä¹‹é—´è°ƒæ•´

### Q3: å¦‚ä½•é˜²æ­¢åé¢„è®­ç»ƒä¸­çš„ç¾éš¾æ€§é—å¿˜ï¼Ÿ
**A**: å¤šå±‚é¢é˜²æŠ¤ç­–ç•¥ï¼š
- **æ•°æ®å±‚é¢**: ä¿æŒè¶³å¤Ÿæ¯”ä¾‹çš„é€šç”¨æ•°æ®æ··åˆ
- **è®­ç»ƒå±‚é¢**: ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼Œå•è½®è®­ç»ƒï¼Œæ¢¯åº¦è£å‰ª
- **ç›‘æ§å±‚é¢**: å®šæœŸè¯„ä¼°é€šç”¨åŸºå‡†ï¼Œè®¾ç½®æ—©åœæœºåˆ¶

### Q4: åé¢„è®­ç»ƒåœ¨ä»€ä¹ˆåœºæ™¯ä¸‹æœ€æœ‰ä»·å€¼ï¼Ÿ
**A**: æœ€é€‚ç”¨çš„ä¸‰ä¸ªåœºæ™¯ï¼š
1. **æœ‰å¤§é‡é¢†åŸŸè¯­æ–™** (>10äº¿token)ä¸”è´¨é‡é«˜
2. **é¢†åŸŸçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡** (å¦‚åŒ»ç–—ã€æ³•å¾‹ã€é‡‘è)
3. **éœ€è¦ä¿æŒé€šç”¨èƒ½åŠ›** çš„ä¸“ä¸šåº”ç”¨

## ğŸš€ å­¦ä¹ å»ºè®®

1. **ç†è®ºç†è§£**: æ·±å…¥ç†è§£åé¢„è®­ç»ƒçš„å®šä½å’Œä»·å€¼
2. **å®è·µéªŒè¯**: åœ¨å°è§„æ¨¡æ•°æ®ä¸ŠéªŒè¯é…æ¯”å’Œè¶…å‚æ•°
3. **ç›‘æ§ä½“ç³»**: å»ºç«‹å®Œæ•´çš„èƒ½åŠ›è¯„ä¼°å’Œé—å¿˜æ£€æµ‹æœºåˆ¶
4. **é¢†åŸŸåº”ç”¨**: é€‰æ‹©å…·ä½“é¢†åŸŸæ·±å…¥å®è·µ

åé¢„è®­ç»ƒæ˜¯è¿æ¥é€šç”¨æ¨¡å‹ä¸ä¸“ä¸šåº”ç”¨çš„é‡è¦æ¡¥æ¢ï¼Œæ˜¯ç°ä»£LLMè½åœ°çš„å…³é”®æŠ€æœ¯ï¼