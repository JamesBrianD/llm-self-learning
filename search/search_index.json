{"config":{"lang":["zh","en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"LLM\u81ea\u5b66\u6307\u5357","text":"<p>\ud83c\udfaf 15\u5929\u638c\u63e1\u5927\u8bed\u8a00\u6a21\u578b\u6838\u5fc3\u6280\u672f\uff0c\u52a9\u529b\u6280\u672f\u9762\u8bd5\u6210\u529f\uff01</p> <p>\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u6280\u672f\u9762\u8bd5\u8bbe\u8ba1\u7684LLM\u5b66\u4e60\u8d44\u6e90\u5e93\uff0c\u6db5\u76d6\u4ece\u57fa\u7840\u539f\u7406\u5230\u524d\u6cbf\u6280\u672f\u7684\u5b8c\u6574\u77e5\u8bc6\u4f53\u7cfb\u3002</p>"},{"location":"#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\ud83d\udcdd \u9762\u8bd5\u5bfc\u5411: \u91cd\u70b9\u8986\u76d6\u6280\u672f\u9762\u8bd5\u9ad8\u9891\u8003\u70b9</li> <li>\ud83d\udd2c \u6280\u672f\u6df1\u5ea6: \u65e2\u6709\u539f\u7406\u7406\u89e3\u53c8\u6709\u4ee3\u7801\u5b9e\u8df5</li> <li>\u26a1 \u9ad8\u6548\u5b66\u4e60: 15\u5929\u7cfb\u7edf\u5316\u5b66\u4e60\u8def\u5f84</li> <li>\ud83c\udf1f \u524d\u6cbf\u6280\u672f: \u5305\u542bDeepSeek\u7b49\u6700\u65b0\u6280\u672f\u521b\u65b0</li> <li>\ud83d\udcaa \u5b9e\u6218\u80fd\u529b: \u5b8c\u6574\u7684\u4ee3\u7801\u5b9e\u73b0\u548c\u9879\u76ee\u7ec3\u4e60</li> </ul>"},{"location":"#15","title":"\ud83d\udcc5 15\u5929\u5b66\u4e60\u8ba1\u5212","text":"\u9636\u6bb5 \u7ae0\u8282 \u4e3b\u8981\u5185\u5bb9 \u5b66\u4e60\u65f6\u95f4 \u91cd\u70b9\u6280\u80fd \u57fa\u7840\u7406\u8bba \u7b2c1\u8282 Transformer\u57fa\u7840 Attention + FFN + \u7f16\u7801\u5668-\u89e3\u7801\u5668 + Tokenizer 3\u5929 \u67b6\u6784\u7406\u89e3 \u6280\u672f\u5347\u7ea7 \u7b2c2\u8282 Attention\u5347\u7ea7 MHA\u2192MQA\u2192GQA\u2192MLA + KV Cache + RoPE 3\u5929 \u4f18\u5316\u6280\u672f \u73b0\u4ee3\u67b6\u6784 \u7b2c3\u8282 LLM\u5347\u7ea7\u6280\u672f MOE\u67b6\u6784 + \u5206\u5e03\u5f0f\u8bad\u7ec3\u57fa\u7840 1.5\u5929 \u5de5\u7a0b\u6982\u5ff5 \u524d\u6cbf\u521b\u65b0 \u7b2c4\u8282 DeepSeek\u6280\u672f MLA + DeepSeek MoE + MTP 3\u5929 \u524d\u6cbf\u6280\u672f \u5e94\u7528\u5b9e\u6218 \u7b2c5\u8282 Context Engineering \u4e0a\u4e0b\u6587\u5de5\u7a0b + \u63d0\u793a\u8bcd\u8bbe\u8ba1 1.5\u5929 \u5e94\u7528\u6280\u80fd \u5e94\u7528\u5b9e\u6218 \u7b2c6\u8282 RAG\u4e0eAgent RAG\u6280\u672f + AI Agent\u6846\u67b6 1.5\u5929 \u7cfb\u7edf\u67b6\u6784 \u5e94\u7528\u5b9e\u6218 \u7b2c7\u8282 CoT\u4e0e\u8bc4\u6d4b \u601d\u7ef4\u94fe + LangChain + \u6a21\u578b\u8bc4\u6d4b 1.5\u5929 \u8bc4\u6d4b\u65b9\u6cd5 <p>\u603b\u8ba1\uff1a15\u5929 | \u6838\u5fc3\u6280\u80fd\uff1a\u7406\u8bba+\u5b9e\u8df5+\u9762\u8bd5</p>"},{"location":"#_2","title":"\ud83d\ude80 \u5feb\u901f\u5f00\u59cb","text":""},{"location":"#_3","title":"\u63a8\u8350\u5b66\u4e60\u8def\u5f84","text":"<ol> <li>\u6309\u987a\u5e8f\u5b66\u4e60: \u6bcf\u4e2a\u7ae0\u8282\u90fd\u6709\u524d\u7f6e\u4f9d\u8d56\u5173\u7cfb</li> <li>\u7406\u8bba+\u5b9e\u8df5: \u6bcf\u8282\u90fd\u5305\u542b\u4ee3\u7801\u5b9e\u73b0\u548c\u9762\u8bd5\u95ee\u7b54</li> <li>\u91cd\u70b9\u5173\u6ce8: \u6807\u8bb0\u4e3a\ud83c\udf1f\u7684DeepSeek\u521b\u65b0\u6280\u672f\u662f\u9762\u8bd5\u70ed\u70b9</li> </ol>"},{"location":"#_4","title":"\u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<ul> <li>\u2705 \u7406\u8bba\u7406\u89e3: \u80fd\u6e05\u6670\u89e3\u91ca\u6838\u5fc3\u539f\u7406</li> <li>\u2705 \u4ee3\u7801\u5b9e\u73b0: \u80fd\u5199\u51fa\u5173\u952e\u7b97\u6cd5\u4ee3\u7801</li> <li>\u2705 \u9762\u8bd5\u51c6\u5907: \u80fd\u56de\u7b54\u6280\u672f\u9762\u8bd5\u95ee\u9898</li> </ul>"},{"location":"#_5","title":"\ud83d\udd25 \u6838\u5fc3\u7279\u8272","text":""},{"location":"#_6","title":"\ud83d\udcca \u6280\u672f\u8986\u76d6","text":"<pre><code>\u6a21\u578b\u57fa\u7840 (10.5\u5929)\n\u251c\u2500\u2500 Transformer\u57fa\u7840 (3\u5929)\n\u251c\u2500\u2500 Attention\u5347\u7ea7 (3\u5929)\n\u251c\u2500\u2500 \u73b0\u4ee3\u67b6\u6784 (1.5\u5929)\n\u2514\u2500\u2500 DeepSeek\u524d\u6cbf (3\u5929)\n\n\u5e94\u7528\u5b9e\u6218 (4.5\u5929)\n\u251c\u2500\u2500 Context Engineering (1.5\u5929)\n\u251c\u2500\u2500 RAG\u4e0eAgent (1.5\u5929)\n\u2514\u2500\u2500 CoT\u4e0e\u8bc4\u6d4b (1.5\u5929)\n</code></pre>"},{"location":"#_7","title":"\ud83c\udf96\ufe0f \u6280\u672f\u6df1\u5ea6","text":"<ul> <li>\u57fa\u7840\u624e\u5b9e: \u4eceAttention\u5230Transformer\u5b8c\u6574\u7406\u89e3</li> <li>\u6280\u672f\u524d\u6cbf: DeepSeek MLA/MoE\u7b49\u6700\u65b0\u521b\u65b0</li> <li>\u5b9e\u6218\u5bfc\u5411: \u5b8c\u6574\u7684\u5de5\u7a0b\u5b9e\u73b0\u548c\u4f18\u5316\u7b56\u7565</li> </ul> <p>\u5f00\u59cb\u4f60\u7684LLM\u5b66\u4e60\u4e4b\u65c5\u5427\uff01\u70b9\u51fb\u5de6\u4fa7\u5bfc\u822a\u680f\u9009\u62e9\u611f\u5174\u8da3\u7684\u7ae0\u8282\u3002</p>"},{"location":"applications/context-engineering/","title":"\u7b2c4\u8282\uff1aContext Engineering","text":""},{"location":"applications/context-engineering/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6838\u5fc3\u6280\u5de7\uff0c\u5b66\u4f1a\u8bbe\u8ba1\u9ad8\u6548\u7684\u63d0\u793a\u8bcd\u6765\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - \u4ec0\u4e48\u662f\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff1f - \u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\uff1f - \u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u7684\u5e94\u5bf9\u7b56\u7565</p>"},{"location":"applications/context-engineering/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a1.5\u5929</p> <ul> <li>Day 1: \u4e0a\u4e0b\u6587\u5de5\u7a0b\u57fa\u7840\u7406\u8bba\u548c\u8bbe\u8ba1\u539f\u5219</li> <li>\u534a\u5929: \u5b9e\u8df5\u6280\u5de7\u548c\u9ad8\u7ea7\u6848\u4f8b\u5206\u6790</li> </ul>"},{"location":"applications/context-engineering/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"applications/context-engineering/#1","title":"1. \u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5","text":"<ul> <li>\u63d0\u793a\u8bcd\u8bbe\u8ba1\u539f\u5219</li> <li>Few-shot\u5b66\u4e60\u6280\u5de7</li> <li>\u4e0a\u4e0b\u6587\u4f18\u5316\u7b56\u7565</li> </ul>"},{"location":"applications/context-engineering/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u9879\u76ee\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u7406\u8bba\u638c\u63e1: \u7406\u89e3\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6838\u5fc3\u539f\u7406</li> <li>\u5b9e\u8df5\u5e94\u7528: \u80fd\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\u6a21\u677f</li> </ol>"},{"location":"applications/context-engineering/#_5","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u8fd9\u662f\u5927\u6a21\u578b\u5e94\u7528\u7684\u6838\u5fc3\u6280\u80fd\uff0c\u5b9e\u7528\u6027\u5f88\u5f3a\uff01</p>"},{"location":"applications/context-engineering/practices/","title":"\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5","text":""},{"location":"applications/context-engineering/practices/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u638c\u63e1\u5b9e\u7528\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6280\u5de7\u548c\u6700\u4f73\u5b9e\u8df5\u3002</p>"},{"location":"applications/context-engineering/practices/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"applications/context-engineering/practices/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Manus \u5185\u90e8\u7684 Context \u5de5\u7a0b\u7ecf\u9a8c - \u77e5\u4e4e</li> <li>Context Engineering GitHub\u9879\u76ee - \u5b9e\u8df5\u6848\u4f8b</li> </ol>"},{"location":"applications/context-engineering/practices/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/context-engineering/practices/#_6","title":"\u63d0\u793a\u8bcd\u8bbe\u8ba1\u539f\u5219","text":"<ol> <li>\u660e\u786e\u6027: \u6e05\u6670\u8868\u8fbe\u4efb\u52a1\u8981\u6c42</li> <li>\u7ed3\u6784\u5316: \u4f7f\u7528\u4e00\u81f4\u7684\u683c\u5f0f</li> <li>\u793a\u4f8b\u5f15\u5bfc: \u63d0\u4f9bFew-shot\u793a\u4f8b</li> <li>\u89d2\u8272\u8bbe\u5b9a: \u660e\u786eAI\u7684\u89d2\u8272\u5b9a\u4f4d</li> </ol>"},{"location":"applications/context-engineering/practices/#_7","title":"\u5e38\u7528\u6280\u5de7","text":"<ul> <li>\u94fe\u5f0f\u601d\u7ef4(CoT): \u5f15\u5bfc\u9010\u6b65\u63a8\u7406</li> <li>\u6a21\u677f\u5316: \u6807\u51c6\u5316\u63d0\u793a\u8bcd\u7ed3\u6784</li> <li>\u4e0a\u4e0b\u6587\u538b\u7f29: \u4f18\u5316\u4fe1\u606f\u5bc6\u5ea6</li> </ul>"},{"location":"applications/context-engineering/practices/#_8","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/context-engineering/practices/#q1","title":"Q1: \u4ec0\u4e48\u662f\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff1f","text":"<p>\u6838\u5fc3\u5b9a\u4e49: \u4e0a\u4e0b\u6587\u5de5\u7a0b\u662f\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u8f93\u5165\u63d0\u793a\u8bcd\u6765\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u671f\u671b\u8f93\u51fa\u7684\u6280\u672f\u3002</p> <p>\u5173\u952e\u8981\u7d20: - \u4efb\u52a1\u63cf\u8ff0\u6e05\u6670 - \u63d0\u4f9b\u76f8\u5173\u793a\u4f8b - \u8bbe\u7f6e\u5408\u9002\u7684\u7ea6\u675f\u6761\u4ef6</p>"},{"location":"applications/context-engineering/practices/#_9","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u63d0\u793a\u8bcd\u8bbe\u8ba1\u7684\u6838\u5fc3\u539f\u5219</li> <li>[ ] \u80fd\u8bbe\u8ba1\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u63d0\u793a\u8bcd\u6a21\u677f</li> </ul>"},{"location":"applications/context-engineering/practices/#_10","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aRAG\u4e0eAgent</li> <li>\u8fd4\u56de\uff1aContext Engineering\u6982\u89c8</li> </ul>"},{"location":"applications/cot-evaluation/","title":"\u7b2c7\u8282\uff1aCoT\u4e0e\u8bc4\u6d4b","text":""},{"location":"applications/cot-evaluation/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u601d\u7ef4\u94fe(Chain of Thought)\u6280\u672f\u548c\u5927\u6a21\u578b\u8bc4\u6d4b\u65b9\u6cd5\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - CoT\u7684\u5de5\u4f5c\u539f\u7406\u548c\u5e94\u7528\u573a\u666f - LangChain\u7684\u6838\u5fc3\u7ec4\u4ef6 - \u5927\u6a21\u578b\u8bc4\u6d4b\u7684\u4e3b\u8981\u6307\u6807</p>"},{"location":"applications/cot-evaluation/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a1.5\u5929</p> <ul> <li>Day 1: CoT\u6280\u672f\u548cLangChain\u6846\u67b6</li> <li>\u534a\u5929: \u6a21\u578b\u8bc4\u6d4b\u65b9\u6cd5\u548c\u57fa\u51c6</li> </ul>"},{"location":"applications/cot-evaluation/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"applications/cot-evaluation/#1","title":"1. \u601d\u7ef4\u94fe\u6280\u672f","text":"<ul> <li>CoT\u7684\u539f\u7406\u548c\u5e94\u7528</li> <li>\u63d0\u5347\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5</li> </ul>"},{"location":"applications/cot-evaluation/#2-langchain","title":"2. LangChain\u6846\u67b6","text":"<ul> <li>LangChain\u7684\u6838\u5fc3\u6982\u5ff5</li> <li>\u94fe\u5f0f\u8c03\u7528\u548c\u5de5\u5177\u96c6\u6210</li> </ul>"},{"location":"applications/cot-evaluation/#3","title":"3. \u6a21\u578b\u8bc4\u6d4b","text":"<ul> <li>\u8bc4\u6d4b\u6307\u6807\u548c\u65b9\u6cd5</li> <li>\u57fa\u51c6\u6d4b\u8bd5\u4ecb\u7ecd</li> </ul>"},{"location":"applications/cot-evaluation/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<ul> <li>[ ] \u7406\u89e3CoT\u7684\u5de5\u4f5c\u539f\u7406</li> <li>[ ] \u4e86\u89e3\u6a21\u578b\u8bc4\u6d4b\u7684\u57fa\u672c\u65b9\u6cd5</li> </ul>"},{"location":"applications/cot-evaluation/cot/","title":"\u601d\u7ef4\u94fe\u6280\u672f","text":""},{"location":"applications/cot-evaluation/cot/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u601d\u7ef4\u94fe(Chain of Thought)\u6280\u672f\u5982\u4f55\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002</p>"},{"location":"applications/cot-evaluation/cot/#_3","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/cot-evaluation/cot/#cot","title":"CoT\u662f\u4ec0\u4e48\uff1f","text":"<p>Chain of Thought (CoT) \u662f\u4e00\u79cd\u63d0\u793a\u6280\u672f\uff0c\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u5c55\u793a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u6765\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002</p>"},{"location":"applications/cot-evaluation/cot/#_4","title":"\u6838\u5fc3\u539f\u7406","text":"<ol> <li>\u5206\u6b65\u63a8\u7406: \u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u6b65\u9aa4</li> <li>\u663e\u5f0f\u601d\u8003: \u8ba9\u6a21\u578b\u5c55\u793a\u601d\u8003\u8fc7\u7a0b</li> <li>\u9010\u6b65\u5f15\u5bfc: \u901a\u8fc7\u793a\u4f8b\u6559\u4f1a\u6a21\u578b\u63a8\u7406\u6a21\u5f0f</li> </ol>"},{"location":"applications/cot-evaluation/cot/#_5","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/cot-evaluation/cot/#q1-cot","title":"Q1: \u4ec0\u4e48\u662f\u601d\u7ef4\u94fe(CoT)\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54: CoT\u662f\u901a\u8fc7\"\u8ba9\u6211\u60f3\u60f3...\"\u7684\u65b9\u5f0f\u5f15\u5bfc\u5927\u6a21\u578b\u8fdb\u884c\u5206\u6b65\u63a8\u7406\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u8868\u73b0\u7684\u6280\u672f\u3002</p> <p>\u6838\u5fc3\u673a\u5236: - \u4e0d\u76f4\u63a5\u7ed9\u7b54\u6848\uff0c\u800c\u662f\u5c55\u793a\u63a8\u7406\u8fc7\u7a0b - \u901a\u8fc7\u4e2d\u95f4\u6b65\u9aa4\u63d0\u5347\u6700\u7ec8\u7b54\u6848\u8d28\u91cf - \u5bf9\u6570\u5b66\u3001\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u7279\u522b\u6709\u6548</p>"},{"location":"applications/cot-evaluation/cot/#_6","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3CoT\u7684\u57fa\u672c\u539f\u7406</li> <li>[ ] \u77e5\u9053CoT\u9002\u7528\u7684\u573a\u666f</li> </ul>"},{"location":"applications/cot-evaluation/cot/#_7","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aLangChain\u6846\u67b6</li> <li>\u8fd4\u56de\uff1aCoT\u4e0e\u8bc4\u6d4b\u6982\u89c8</li> </ul>"},{"location":"applications/cot-evaluation/evaluation/","title":"\u6a21\u578b\u8bc4\u6d4b","text":""},{"location":"applications/cot-evaluation/evaluation/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u4e86\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u6d4b\u65b9\u6cd5\u548c\u4e3b\u8981\u57fa\u51c6\u3002</p>"},{"location":"applications/cot-evaluation/evaluation/#_3","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/cot-evaluation/evaluation/#_4","title":"\u8bc4\u6d4b\u7684\u91cd\u8981\u6027","text":"<p>\u6a21\u578b\u8bc4\u6d4b\u662f\u8861\u91cfLLM\u6027\u80fd\u3001\u6307\u5bfc\u6a21\u578b\u6539\u8fdb\u7684\u91cd\u8981\u624b\u6bb5\u3002</p>"},{"location":"applications/cot-evaluation/evaluation/#_5","title":"\u4e3b\u8981\u8bc4\u6d4b\u7ef4\u5ea6","text":"<ol> <li>\u80fd\u529b\u8bc4\u6d4b: \u63a8\u7406\u3001\u77e5\u8bc6\u3001\u8bed\u8a00\u7406\u89e3</li> <li>\u5b89\u5168\u8bc4\u6d4b: \u6709\u5bb3\u8f93\u51fa\u3001\u504f\u89c1\u68c0\u6d4b</li> <li>\u6548\u7387\u8bc4\u6d4b: \u901f\u5ea6\u3001\u8d44\u6e90\u6d88\u8017</li> <li>\u53ef\u9760\u6027: \u4e00\u81f4\u6027\u3001\u7a33\u5b9a\u6027</li> </ol>"},{"location":"applications/cot-evaluation/evaluation/#_6","title":"\u5e38\u89c1\u57fa\u51c6","text":"<ul> <li>MMLU: \u591a\u5b66\u79d1\u77e5\u8bc6\u7406\u89e3</li> <li>HellaSwag: \u5e38\u8bc6\u63a8\u7406</li> <li>HumanEval: \u4ee3\u7801\u751f\u6210\u80fd\u529b</li> <li>GSM8K: \u6570\u5b66\u63a8\u7406</li> </ul>"},{"location":"applications/cot-evaluation/evaluation/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/cot-evaluation/evaluation/#q1","title":"Q1: \u5927\u6a21\u578b\u8bc4\u6d4b\u6709\u54ea\u4e9b\u4e3b\u8981\u65b9\u6cd5\uff1f","text":"<p>\u4e3b\u8981\u65b9\u6cd5: - \u81ea\u52a8\u8bc4\u6d4b: \u57fa\u4e8e\u6807\u51c6\u7b54\u6848\u7684\u5ba2\u89c2\u6307\u6807 - \u4eba\u5de5\u8bc4\u6d4b: \u4e3b\u89c2\u8d28\u91cf\u8bc4\u4f30 - \u5bf9\u6bd4\u8bc4\u6d4b: \u6a21\u578b\u95f4\u76f8\u5bf9\u8868\u73b0 - \u5728\u7ebf\u8bc4\u6d4b: \u5b9e\u9645\u5e94\u7528\u573a\u666f\u6d4b\u8bd5</p>"},{"location":"applications/cot-evaluation/evaluation/#_8","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u4e86\u89e3\u6a21\u578b\u8bc4\u6d4b\u7684\u57fa\u672c\u6982\u5ff5</li> <li>[ ] \u77e5\u9053\u4e3b\u8981\u7684\u8bc4\u6d4b\u57fa\u51c6</li> </ul>"},{"location":"applications/cot-evaluation/evaluation/#_9","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aLangChain\u6846\u67b6</li> <li>\u9762\u8bd5\u9898\u5e93</li> <li>\u8fd4\u56de\uff1aCoT\u4e0e\u8bc4\u6d4b\u6982\u89c8</li> </ul>"},{"location":"applications/cot-evaluation/langchain/","title":"LangChain\u6846\u67b6","text":""},{"location":"applications/cot-evaluation/langchain/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u4e86\u89e3LangChain\u6846\u67b6\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u5e94\u7528\u573a\u666f\u3002</p>"},{"location":"applications/cot-evaluation/langchain/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/cot-evaluation/langchain/#langchain_1","title":"LangChain\u662f\u4ec0\u4e48\uff1f","text":"<p>LangChain \u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u94fe\u5f0f\u8c03\u7528\u3001\u5de5\u5177\u96c6\u6210\u7b49\u529f\u80fd\u3002</p>"},{"location":"applications/cot-evaluation/langchain/#_3","title":"\u6838\u5fc3\u7ec4\u4ef6","text":"<ol> <li>Chains: \u94fe\u5f0f\u8c03\u7528\u591a\u4e2a\u7ec4\u4ef6</li> <li>Agents: \u667a\u80fd\u4f53\u548c\u5de5\u5177\u4f7f\u7528</li> <li>Memory: \u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406</li> <li>Tools: \u5916\u90e8\u5de5\u5177\u96c6\u6210</li> </ol>"},{"location":"applications/cot-evaluation/langchain/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/cot-evaluation/langchain/#q1-langchain","title":"Q1: LangChain\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u4f5c\u7528: - \u7b80\u5316LLM\u5e94\u7528\u5f00\u53d1 - \u63d0\u4f9b\u6807\u51c6\u5316\u7684\u7ec4\u4ef6\u548c\u63a5\u53e3 - \u652f\u6301\u590d\u6742\u7684\u5de5\u4f5c\u6d41\u7f16\u6392</p>"},{"location":"applications/cot-evaluation/langchain/#_5","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u4e86\u89e3LangChain\u7684\u57fa\u672c\u6982\u5ff5</li> <li>[ ] \u7406\u89e3\u94fe\u5f0f\u8c03\u7528\u7684\u4f18\u52bf</li> </ul>"},{"location":"applications/cot-evaluation/langchain/#_6","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u601d\u7ef4\u94fe\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1a\u6a21\u578b\u8bc4\u6d4b</li> <li>\u8fd4\u56de\uff1aCoT\u4e0e\u8bc4\u6d4b\u6982\u89c8</li> </ul>"},{"location":"applications/rag-agent/","title":"\u7b2c6\u8282\uff1aRAG\u4e0eAgent","text":""},{"location":"applications/rag-agent/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u7406\u89e3\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548cAI Agent\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u638c\u63e1\u8fd9\u4e24\u4e2a\u91cd\u8981\u7684LLM\u5e94\u7528\u8303\u5f0f\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - RAG\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u6838\u5fc3\u7ec4\u4ef6 - AI Agent\u4e0e\u4f20\u7edf\u7a0b\u5e8f\u7684\u533a\u522b - \u5411\u91cf\u6570\u636e\u5e93\u7684\u9009\u62e9\u548c\u4f18\u5316</p>"},{"location":"applications/rag-agent/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a1.5\u5929</p> <ul> <li>Day 1: RAG\u6280\u672f\u539f\u7406\u548c\u5b9e\u73b0</li> <li>\u534a\u5929: AI Agent\u6982\u5ff5\u548c\u6846\u67b6</li> </ul>"},{"location":"applications/rag-agent/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"applications/rag-agent/#1-rag","title":"1. RAG\u6280\u672f","text":"<ul> <li>\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u539f\u7406</li> <li>RAG\u7cfb\u7edf\u67b6\u6784</li> <li>\u5411\u91cf\u6570\u636e\u5e93\u5e94\u7528</li> </ul>"},{"location":"applications/rag-agent/#2-ai-agent","title":"2. AI Agent","text":"<ul> <li>Agent\u7684\u5b9a\u4e49\u548c\u7279\u70b9</li> <li>\u591a\u6b65\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528</li> <li>Agent\u6846\u67b6\u548c\u5b9e\u73b0</li> </ul>"},{"location":"applications/rag-agent/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<ul> <li>[ ] \u7406\u89e3RAG\u7684\u5de5\u4f5c\u6d41\u7a0b</li> <li>[ ] \u638c\u63e1Agent\u7684\u57fa\u672c\u6982\u5ff5</li> </ul>"},{"location":"applications/rag-agent/agent/","title":"AI Agent","text":""},{"location":"applications/rag-agent/agent/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3AI Agent\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u5b9e\u73b0\u65b9\u5f0f\u3002</p>"},{"location":"applications/rag-agent/agent/#_2","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"applications/rag-agent/agent/#_3","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>AI-Agent\u7cfb\u5217(\u4e00)\uff1a\u667a\u80fd\u4f53\u8d77\u6e90\u63a2\u7a76 - \u7406\u8bba\u57fa\u7840</li> <li>\u5927\u6a21\u578b-Agent \u9762\u8bd5\u516b\u80a1\u6587 - \u77e5\u4e4e</li> </ol>"},{"location":"applications/rag-agent/agent/#_4","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/rag-agent/agent/#agent","title":"Agent\u662f\u4ec0\u4e48\uff1f","text":"<p>AI Agent \u662f\u80fd\u591f\u611f\u77e5\u73af\u5883\u3001\u505a\u51fa\u51b3\u7b56\u5e76\u6267\u884c\u884c\u52a8\u4ee5\u5b9e\u73b0\u76ee\u6807\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002</p>"},{"location":"applications/rag-agent/agent/#_5","title":"\u6838\u5fc3\u7279\u5f81","text":"<ol> <li>\u81ea\u4e3b\u6027: \u80fd\u591f\u72ec\u7acb\u505a\u51b3\u7b56</li> <li>\u53cd\u5e94\u6027: \u5bf9\u73af\u5883\u53d8\u5316\u505a\u51fa\u54cd\u5e94  </li> <li>\u4e3b\u52a8\u6027: \u4e3b\u52a8\u91c7\u53d6\u884c\u52a8\u5b9e\u73b0\u76ee\u6807</li> <li>\u5b66\u4e60\u6027: \u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u6539\u8fdb</li> </ol>"},{"location":"applications/rag-agent/agent/#_6","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/rag-agent/agent/#q1-ai-agent","title":"Q1: AI Agent\u6709\u4ec0\u4e48\u7279\u70b9\uff1f","text":"<p>\u6838\u5fc3\u7279\u70b9: - \u591a\u6b65\u63a8\u7406: \u80fd\u591f\u5206\u89e3\u590d\u6742\u4efb\u52a1 - \u5de5\u5177\u4f7f\u7528: \u53ef\u4ee5\u8c03\u7528\u5916\u90e8\u5de5\u5177\u548cAPI - \u8bb0\u5fc6\u673a\u5236: \u7ef4\u62a4\u5bf9\u8bdd\u5386\u53f2\u548c\u72b6\u6001 - \u76ee\u6807\u5bfc\u5411: \u671d\u7740\u7279\u5b9a\u76ee\u6807\u6267\u884c\u8ba1\u5212</p>"},{"location":"applications/rag-agent/agent/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3Agent\u7684\u57fa\u672c\u6982\u5ff5\u548c\u7279\u5f81</li> <li>[ ] \u4e86\u89e3Agent\u4e0e\u666e\u901aLLM\u7684\u533a\u522b</li> </ul>"},{"location":"applications/rag-agent/agent/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aRAG\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1aCoT\u4e0e\u8bc4\u6d4b</li> <li>\u8fd4\u56de\uff1aRAG\u4e0eAgent\u6982\u89c8</li> </ul>"},{"location":"applications/rag-agent/rag/","title":"RAG\u6280\u672f","text":""},{"location":"applications/rag-agent/rag/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u5de5\u4f5c\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\u3002</p>"},{"location":"applications/rag-agent/rag/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/rag-agent/rag/#rag_1","title":"RAG\u662f\u4ec0\u4e48\uff1f","text":"<p>Retrieval-Augmented Generation (RAG) \u662f\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u548c\u751f\u6210\u7684\u6280\u672f\uff0c\u901a\u8fc7\u68c0\u7d22\u5916\u90e8\u77e5\u8bc6\u5e93\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56de\u7b54\u80fd\u529b\u3002</p>"},{"location":"applications/rag-agent/rag/#_3","title":"\u6838\u5fc3\u4f18\u52bf","text":"<ol> <li>\u77e5\u8bc6\u66f4\u65b0: \u53ef\u4ee5\u8bbf\u95ee\u6700\u65b0\u4fe1\u606f</li> <li>\u51cf\u5c11\u5e7b\u89c9: \u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u56de\u7b54</li> <li>\u53ef\u8ffd\u6eaf\u6027: \u53ef\u4ee5\u63d0\u4f9b\u4fe1\u606f\u6765\u6e90</li> </ol>"},{"location":"applications/rag-agent/rag/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/rag-agent/rag/#q1-rag","title":"Q1: RAG\u7684\u5de5\u4f5c\u6d41\u7a0b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u6d41\u7a0b: 1. \u7d22\u5f15\u6784\u5efa: \u5c06\u6587\u6863\u5411\u91cf\u5316\u5b58\u50a8 2. \u67e5\u8be2\u68c0\u7d22: \u6839\u636e\u95ee\u9898\u68c0\u7d22\u76f8\u5173\u6587\u6863 3. \u589e\u5f3a\u751f\u6210: \u7ed3\u5408\u68c0\u7d22\u7ed3\u679c\u751f\u6210\u7b54\u6848</p>"},{"location":"applications/rag-agent/rag/#_5","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3RAG\u7684\u57fa\u672c\u5de5\u4f5c\u6d41\u7a0b</li> <li>[ ] \u77e5\u9053RAG\u76f8\u6bd4\u7eafLLM\u7684\u4f18\u52bf</li> </ul>"},{"location":"applications/rag-agent/rag/#_6","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aAI Agent</li> <li>\u8fd4\u56de\uff1aRAG\u4e0eAgent\u6982\u89c8</li> </ul>"},{"location":"code-examples/","title":"\u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"code-examples/#_2","title":"\ud83c\udfaf \u76ee\u6807","text":"<p>\u63d0\u4f9bLLM\u76f8\u5173\u6280\u672f\u7684\u53ef\u6267\u884c\u4ee3\u7801\u793a\u4f8b\uff0c\u5e2e\u52a9\u52a0\u6df1\u7406\u89e3\u3002</p>"},{"location":"code-examples/#_3","title":"\ud83d\udcbb \u4ee3\u7801\u5206\u7c7b","text":""},{"location":"code-examples/#_4","title":"\u57fa\u7840\u5b9e\u73b0","text":"<ul> <li>Self-Attention\u673a\u5236</li> <li>Multi-Head Attention</li> <li>Softmax\u51fd\u6570</li> </ul>"},{"location":"code-examples/#_5","title":"\u4f18\u5316\u6280\u672f","text":"<ul> <li>GQA (Grouped-Query Attention) \u5b9e\u73b0</li> <li>KV Cache\u6f14\u793a\u4ee3\u7801</li> <li>RoPE\u4f4d\u7f6e\u7f16\u7801\u5b8c\u6574\u5b9e\u73b0</li> <li>\u4e09\u79cd\u5f52\u4e00\u5316\u6280\u672f\u5bf9\u6bd4</li> </ul>"},{"location":"code-examples/#_6","title":"\u5e94\u7528\u793a\u4f8b","text":"<ul> <li>RAG\u7cfb\u7edf\u7b80\u5316\u7248</li> <li>\u7b80\u5355Agent\u6846\u67b6</li> <li>\u63d0\u793a\u8bcd\u5de5\u7a0b\u6a21\u677f</li> </ul>"},{"location":"code-examples/#_7","title":"\ud83d\udee0\ufe0f \u5b9e\u8df5\u5efa\u8bae","text":"<ol> <li>\u52a8\u624b\u7f16\u5199: \u4e0d\u8981\u53ea\u770b\u4ee3\u7801\uff0c\u8981\u81ea\u5df1\u5b9e\u73b0</li> <li>\u7406\u89e3\u539f\u7406: \u6bcf\u884c\u4ee3\u7801\u90fd\u8981\u77e5\u9053\u4e3a\u4ec0\u4e48\u8fd9\u6837\u5199</li> <li>\u8c03\u8bd5\u8fd0\u884c: \u786e\u4fdd\u4ee3\u7801\u80fd\u6b63\u786e\u6267\u884c</li> <li>\u6027\u80fd\u5bf9\u6bd4: \u6d4b\u8bd5\u4e0d\u540c\u5b9e\u73b0\u7684\u6548\u679c\u5dee\u5f02</li> </ol>"},{"location":"code-examples/#_8","title":"\ud83d\udccb \u68c0\u9a8c\u6e05\u5355","text":"<ul> <li>[ ] \u5b8c\u6210Self-Attention\u7f16\u7a0b\u7ec3\u4e60</li> <li>[ ] \u5b9e\u73b0KV Cache\u6f14\u793a</li> <li>[ ] \u7f16\u5199RoPE\u4f4d\u7f6e\u7f16\u7801</li> <li>[ ] \u5bf9\u6bd4\u4e0d\u540c\u5f52\u4e00\u5316\u6280\u672f</li> <li>[ ] \u8bbe\u8ba1\u63d0\u793a\u8bcd\u6a21\u677f</li> </ul> <p>\u6240\u6709\u4ee3\u7801\u90fd\u5728\u5bf9\u5e94\u6280\u672f\u7ae0\u8282\u4e2d\u63d0\u4f9b\uff0c\u8fd9\u91cc\u4f5c\u4e3a\u603b\u5165\u53e3\uff01</p>"},{"location":"fundamentals/attention-advanced/","title":"\u7b2c2\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f","text":""},{"location":"fundamentals/attention-advanced/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u73b0\u4ee3\u5927\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u6280\u672f\uff0c\u7406\u89e3\u63a8\u7406\u52a0\u901f\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - MHA/MQA/GQA/MLA\u7684\u533a\u522b\u548c\u4f18\u52bf - KV Cache\u7684\u5de5\u4f5c\u539f\u7406\u548c\u52a0\u901f\u6548\u679c - LayerNorm vs RMSNorm\u7684\u9009\u62e9 - RoPE\u4f4d\u7f6e\u7f16\u7801\u7684\u6570\u5b66\u539f\u7406</p>"},{"location":"fundamentals/attention-advanced/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a3\u5929</p> <ul> <li>Day 1: \u6ce8\u610f\u529b\u53d8\u4f53\u6df1\u5ea6\u5b66\u4e60 (MHA\u2192MQA\u2192GQA\u2192MLA)</li> <li>Day 2: KV Cache\u6280\u672f + \u5f52\u4e00\u5316\u6280\u672f\u8be6\u89e3</li> <li>Day 3: \u4f4d\u7f6e\u7f16\u7801\u5347\u7ea7 + \u7efc\u5408\u6280\u672f\u5bf9\u6bd4\u5206\u6790</li> </ul>"},{"location":"fundamentals/attention-advanced/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/attention-advanced/#1","title":"1. \u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53","text":"<ul> <li>MHA \u2192 MQA \u2192 GQA \u2192 MLA\u6f14\u8fdb</li> <li>\u6ce8\u610f\u529b\u5934\u6570\u4f18\u5316\u7b56\u7565</li> <li>\u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790</li> </ul>"},{"location":"fundamentals/attention-advanced/#2-kv-cache","title":"2. KV Cache\u6280\u672f","text":"<ul> <li>\u63a8\u7406\u52a0\u901f\u539f\u7406</li> <li>\u5185\u5b58\u4f18\u5316\u7b56\u7565</li> <li>\u5b9e\u73b0\u7ec6\u8282\u548c\u4ee3\u7801\u793a\u4f8b</li> </ul>"},{"location":"fundamentals/attention-advanced/#3","title":"3. \u5f52\u4e00\u5316\u6280\u672f","text":"<ul> <li>BatchNorm vs LayerNorm vs RMSNorm</li> <li>Pre-Norm vs Post-Norm</li> <li>\u8bad\u7ec3\u7a33\u5b9a\u6027\u5206\u6790</li> </ul>"},{"location":"fundamentals/attention-advanced/#4","title":"4. \u4f4d\u7f6e\u7f16\u7801","text":"<ul> <li>\u7edd\u5bf9\u4f4d\u7f6e vs \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801</li> <li>RoPE\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u63a8\u5bfc</li> <li>\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b</li> </ul>"},{"location":"fundamentals/attention-advanced/#_4","title":"\ud83d\udcd6 \u6838\u5fc3\u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/#_5","title":"\u5fc5\u8bfb\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Transformer\u7684Attention\u53ca\u5176\u5404\u79cd\u53d8\u4f53 - \u51b7\u7738\u535a\u5ba2</li> <li>\u7f13\u5b58\u4e0e\u6548\u679c\u7684\u6781\u9650\u62c9\u626f\uff1a\u4eceMHA\u3001MQA\u3001GQA\u5230MLA - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u5927\u6a21\u578b\u4e2d\u5e38\u89c1\u76843\u79cdNorm - \u77e5\u4e4e</li> <li>\u4e3a\u4ec0\u4e48\u5f53\u524d\u4e3b\u6d41\u7684\u5927\u6a21\u578b\u90fd\u4f7f\u7528RMS-Norm\uff1f - \u77e5\u4e4e</li> <li>\u4e3a\u4ec0\u4e48Pre Norm\u7684\u6548\u679c\u4e0d\u5982Post Norm\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> <li>Sinusoidal\u4f4d\u7f6e\u7f16\u7801\u8ffd\u6839\u6eaf\u6e90 - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u535a\u91c7\u4f17\u957f\u7684\u65cb\u8f6c\u5f0f\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4</li> </ol>"},{"location":"fundamentals/attention-advanced/#_6","title":"\u9009\u8bfb\u6df1\u5165\u6750\u6599","text":"<ul> <li>BN\u7a76\u7adf\u8d77\u4e86\u4ec0\u4e48\u4f5c\u7528\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> </ul>"},{"location":"fundamentals/attention-advanced/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u9879\u76ee\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u6280\u672f\u5bf9\u6bd4: \u6e05\u6670\u533a\u5206MHA/MQA/GQA/MLA\u7684\u4f18\u7f3a\u70b9\u548c\u9002\u7528\u573a\u666f</li> <li>\u4ee3\u7801\u5b9e\u73b0: \u5b8c\u6210GQA\u9002\u914d\u548cKV Cache\u6f14\u793a\u4ee3\u7801</li> <li>\u539f\u7406\u7406\u89e3: \u80fd\u4ece\u6570\u5b66\u89d2\u5ea6\u89e3\u91caRoPE\u548cRMSNorm\u7684\u5de5\u4f5c\u539f\u7406</li> <li>\u9762\u8bd5\u51c6\u5907: \u80fd\u89e3\u91ca\u6bcf\u79cd\u6280\u672f\u9009\u62e9\u80cc\u540e\u7684\u5de5\u7a0btrade-off</li> </ol>"},{"location":"fundamentals/attention-advanced/#_8","title":"\ud83d\udca1 \u5b66\u4e60\u63d0\u793a","text":"<p>\u8fd9\u4e00\u8282\u6280\u672f\u542b\u91cf\u5f88\u9ad8\uff0c\u662f\u73b0\u4ee3\u5927\u6a21\u578b\u7684\u6838\u5fc3\u4f18\u5316\u6280\u672f\uff0c\u5efa\u8bae\uff1a - \u5faa\u5e8f\u6e10\u8fdb: \u5148\u7406\u89e3\u57fa\u7840\u6982\u5ff5\uff0c\u518d\u6df1\u5165\u6570\u5b66\u63a8\u5bfc - \u91cd\u70b9\u5173\u6ce8: \u6bcf\u79cd\u6280\u672f\u7684motivation\u548c\u89e3\u51b3\u7684\u5177\u4f53\u95ee\u9898 - \u5bf9\u6bd4\u5b66\u4e60: \u901a\u8fc7\u6280\u672f\u5bf9\u6bd4\u52a0\u6df1\u7406\u89e3\u5404\u81ea\u7684trade-off - \u5b9e\u8df5\u9a8c\u8bc1: \u901a\u8fc7\u4ee3\u7801\u5b9e\u73b0\u52a0\u6df1\u5bf9\u539f\u7406\u7684\u7406\u89e3 - \u9762\u8bd5\u5bfc\u5411: \u91cd\u70b9\u638c\u63e1\u6280\u672f\u9009\u62e9\u7684\u5de5\u7a0b\u8003\u91cf</p>"},{"location":"fundamentals/attention-advanced/#_9","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u611f\u5174\u8da3\u7684\u6280\u672f\u6a21\u5757\u6df1\u5165\u5b66\u4e60\uff0c\u6bcf\u4e2a\u90fd\u662f\u73b0\u4ee3\u5927\u6a21\u578b\u7684\u6838\u5fc3\u6280\u672f\uff01</p>"},{"location":"fundamentals/attention-advanced/kv-cache/","title":"KV Cache\u6280\u672f","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3KV Cache\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u638c\u63e1\u8fd9\u4e2a\u8ba9\u5927\u6a21\u578b\u63a8\u7406\u63d0\u901f\u6570\u500d\u7684\u5173\u952e\u6280\u672f\u3002</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_1","title":"KV Cache\u662f\u4ec0\u4e48\uff1f","text":"<p>\u5b9a\u4e49: KV Cache\u662f\u4e00\u79cd\u63a8\u7406\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u7f13\u5b58\u4e4b\u524d\u8ba1\u7b97\u8fc7\u7684Key\u548cValue\u77e9\u9635\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97\uff0c\u5927\u5e45\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_2","title":"\u4e3a\u4ec0\u4e48\u9700\u8981KV Cache\uff1f","text":"<p>\u95ee\u9898\u80cc\u666f: \u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u91cd\u590d\u8ba1\u7b97</p> <pre><code># \u751f\u6210\"\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\"\u7684\u8fc7\u7a0b\nStep 1: \u8f93\u5165[\"\u6211\"]        \u2192 \u9884\u6d4b\"\u7231\" \nStep 2: \u8f93\u5165[\"\u6211\",\"\u7231\"]    \u2192 \u9884\u6d4b\"\u5317\"\nStep 3: \u8f93\u5165[\"\u6211\",\"\u7231\",\"\u5317\"] \u2192 \u9884\u6d4b\"\u4eac\"\n...\n</code></pre> <p>\u91cd\u590d\u8ba1\u7b97\u95ee\u9898: - \u6bcf\u4e00\u6b65\u90fd\u8981\u91cd\u65b0\u8ba1\u7b97\u6240\u6709previous tokens\u7684K,V\u77e9\u9635 - \u8ba1\u7b97\u590d\u6742\u5ea6: O(n\u00b2)\uff0c\u5176\u4e2dn\u662f\u5e8f\u5217\u957f\u5ea6 - \u5927\u91cf\u91cd\u590d\u8ba1\u7b97\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_3","title":"KV Cache\u5de5\u4f5c\u539f\u7406","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#1","title":"1. \u4f20\u7edf\u65b9\u5f0f (\u65e0\u7f13\u5b58)","text":"<pre><code># \u6bcf\u6b21\u90fd\u91cd\u65b0\u8ba1\u7b97\u5168\u90e8K,V\ndef generate_token_naive(tokens):\n    # \u91cd\u65b0\u8ba1\u7b97\u6240\u6709token\u7684K,V - \u975e\u5e38\u4f4e\u6548\uff01\n    K = compute_K(tokens)  # \u5305\u542b\u6240\u6709\u5386\u53f2token\n    V = compute_V(tokens)  # \u5305\u542b\u6240\u6709\u5386\u53f2token\n    Q = compute_Q(tokens[-1])  # \u53ea\u9700\u8981\u6700\u540e\u4e00\u4e2atoken\u7684Q\n\n    attention_output = attention(Q, K, V)\n    return next_token\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#2-kv-cache","title":"2. KV Cache\u4f18\u5316\u65b9\u5f0f","text":"<pre><code># \u53ea\u8ba1\u7b97\u65b0token\u7684K,V\uff0c\u590d\u7528\u5386\u53f2\u7f13\u5b58\ndef generate_token_with_cache(new_token, kv_cache):\n    # \u53ea\u8ba1\u7b97\u65b0token\u7684K,V\n    new_K = compute_K(new_token)  \n    new_V = compute_V(new_token)\n\n    # \u66f4\u65b0\u7f13\u5b58\n    kv_cache.append(new_K, new_V)\n\n    # \u4f7f\u7528\u5b8c\u6574\u7684K,V (\u5386\u53f2+\u65b0\u589e)\n    Q = compute_Q(new_token)\n    attention_output = attention(Q, kv_cache.K, kv_cache.V)\n\n    return next_token\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#_3","title":"\u52a0\u901f\u6548\u679c\u5206\u6790","text":"<p>\u65f6\u95f4\u590d\u6742\u5ea6\u5bf9\u6bd4:</p> \u751f\u6210\u6b65\u9aa4 \u65e0Cache \u6709Cache \u52a0\u901f\u6bd4 \u7b2c1\u6b65 O(1) O(1) 1x \u7b2c2\u6b65 O(4) O(1) 4x \u7b2c3\u6b65 O(9) O(1) 9x \u7b2cn\u6b65 O(n\u00b2) O(1) n\u00b2x <p>\u5185\u5b58\u4f7f\u7528: - \u7a7a\u95f4\u6362\u65f6\u95f4\u7684\u7b56\u7565 - \u9700\u8981\u5b58\u50a8: <code>seq_len \u00d7 num_heads \u00d7 head_dim \u00d7 2</code> (K\u548cV) - \u957f\u5e8f\u5217\u65f6\u5185\u5b58\u9700\u6c42\u663e\u8457\u589e\u52a0</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#q1-kv-cachekv-cache","title":"Q1: KV Cache\u662f\u4ec0\u4e48\uff0c\u4e3a\u4ec0\u4e48KV Cache\u80fd\u52a0\u901f\u6a21\u578b\u63a8\u7406\uff1f","text":"<p>\u6838\u5fc3\u7b54\u6848:  KV Cache\u662f\u7f13\u5b58\u6ce8\u610f\u529b\u673a\u5236\u4e2dKey\u548cValue\u77e9\u9635\u7684\u6280\u672f\uff0c\u901a\u8fc7\u907f\u514d\u91cd\u590d\u8ba1\u7b97\u5386\u53f2token\u7684K,V\u6765\u52a0\u901f\u63a8\u7406\u3002</p> <p>\u8be6\u7ec6\u89e3\u91ca:</p> <ol> <li>\u95ee\u9898\u6839\u6e90: </li> <li>\u81ea\u56de\u5f52\u751f\u6210\u6bcf\u6b65\u90fd\u9700\u8981\u5b8c\u6574\u7684attention\u8ba1\u7b97</li> <li>\u5386\u53f2token\u7684K,V\u77e9\u9635\u5728\u6bcf\u6b65\u4e2d\u4fdd\u6301\u4e0d\u53d8</li> <li> <p>\u91cd\u590d\u8ba1\u7b97\u9020\u6210O(n\u00b2)\u7684\u65f6\u95f4\u590d\u6742\u5ea6</p> </li> <li> <p>\u89e3\u51b3\u65b9\u6848:</p> </li> <li>\u7f13\u5b58\u5df2\u8ba1\u7b97\u7684K,V\u77e9\u9635</li> <li>\u65b0token\u53ea\u9700\u8ba1\u7b97\u81ea\u5df1\u7684K,V\u5e76\u8ffd\u52a0\u5230\u7f13\u5b58</li> <li> <p>\u5c06\u65f6\u95f4\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u4f4e\u5230O(1)</p> </li> <li> <p>\u52a0\u901f\u539f\u7406:    <pre><code>\u4f20\u7edf\u65b9\u5f0f: \u6bcf\u6b65\u8ba1\u7b97\u5b8c\u6574\u5e8f\u5217\u7684K,V\n\u7f13\u5b58\u65b9\u5f0f: \u53ea\u8ba1\u7b97\u65b0\u589etoken\u7684K,V\n</code></pre></p> </li> </ol>"},{"location":"fundamentals/attention-advanced/kv-cache/#q2-kv-cache","title":"Q2: KV Cache\u7684\u5185\u5b58\u5f00\u9500\u5982\u4f55\uff1f","text":"<p>\u5185\u5b58\u9700\u6c42\u8ba1\u7b97: <pre><code>memory_per_token = num_layers \u00d7 num_heads \u00d7 head_dim \u00d7 2 \u00d7 dtype_size\ntotal_memory = memory_per_token \u00d7 max_seq_length\n</code></pre></p> <p>\u5177\u4f53\u4f8b\u5b50 (LLaMA-7B): <pre><code>\u53c2\u6570: 32\u5c42, 32\u5934, 128\u7ef4\u5ea6, FP16\n\u6bcf\u4e2atoken: 32 \u00d7 32 \u00d7 128 \u00d7 2 \u00d7 2 bytes = 524KB\n2048\u957f\u5ea6: 524KB \u00d7 2048 \u2248 1GB\n</code></pre></p> <p>\u5185\u5b58\u4f18\u5316\u7b56\u7565: - \u4f7f\u7528\u66f4\u4f4e\u7cbe\u5ea6(FP16/INT8) - \u5206\u5c42\u7f13\u5b58\uff0c\u53ea\u4fdd\u7559\u6700\u8fd1\u7684token - \u6ed1\u52a8\u7a97\u53e3\uff0c\u4e22\u5f03\u8fc7\u65e7\u7684\u7f13\u5b58</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#q3-kv-cache","title":"Q3: KV Cache\u5728\u4e0d\u540c\u6ce8\u610f\u529b\u53d8\u4f53\u4e2d\u7684\u8868\u73b0\uff1f","text":"<p>\u5404\u53d8\u4f53\u7684KV Cache\u9700\u6c42:</p> \u6ce8\u610f\u529b\u7c7b\u578b KV Cache\u5927\u5c0f \u8bf4\u660e MHA <code>h \u00d7 d \u00d7 L</code> \u6bcf\u4e2a\u5934\u72ec\u7acb\u5b58\u50a8K,V MQA <code>d \u00d7 L</code> \u6240\u6709\u5934\u5171\u4eabK,V\uff0c\u5185\u5b58\u51cf\u5c11h\u500d GQA <code>g \u00d7 d \u00d7 L</code> \u5206\u7ec4\u5171\u4eab\uff0c\u5185\u5b58\u9700\u6c42\u5728MHA\u548cMQA\u4e4b\u95f4 MLA \u6700\u5c0f \u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u8fdb\u4e00\u6b65\u538b\u7f29 <p>h=\u5934\u6570, d=\u7ef4\u5ea6, L=\u5e8f\u5217\u957f\u5ea6, g=\u7ec4\u6570</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#_5","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_4","title":"\u5b8c\u6574KV Cache\u6f14\u793a","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass KVCache:\n    \"\"\"\u7b80\u5316\u7684KV Cache\u5b9e\u73b0\"\"\"\n\n    def __init__(self, max_seq_len, num_heads, head_dim, device='cpu'):\n        self.max_seq_len = max_seq_len\n        self.num_heads = num_heads\n        self.head_dim = head_dim\n        self.device = device\n\n        # \u9884\u5206\u914d\u7f13\u5b58\u7a7a\u95f4\n        self.cache_k = torch.zeros(\n            max_seq_len, num_heads, head_dim, \n            device=device, dtype=torch.float16\n        )\n        self.cache_v = torch.zeros(\n            max_seq_len, num_heads, head_dim,\n            device=device, dtype=torch.float16\n        )\n\n        self.current_length = 0\n\n    def update_cache(self, new_k, new_v):\n        \"\"\"\u66f4\u65b0\u7f13\u5b58\u5e76\u8fd4\u56de\u5b8c\u6574\u7684K,V\"\"\"\n        batch_size, seq_len, num_heads, head_dim = new_k.shape\n\n        # \u68c0\u67e5\u662f\u5426\u8d85\u51fa\u7f13\u5b58\u5bb9\u91cf\n        if self.current_length + seq_len &gt; self.max_seq_len:\n            raise ValueError(\"Sequence length exceeds cache capacity\")\n\n        # \u66f4\u65b0\u7f13\u5b58\n        end_pos = self.current_length + seq_len\n        self.cache_k[self.current_length:end_pos] = new_k[0]  # \u5047\u8bbebatch_size=1\n        self.cache_v[self.current_length:end_pos] = new_v[0]\n\n        self.current_length = end_pos\n\n        # \u8fd4\u56de\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u5b8c\u6574K,V\n        return (\n            self.cache_k[:self.current_length].unsqueeze(0),  # \u6dfb\u52a0batch\u7ef4\u5ea6\n            self.cache_v[:self.current_length].unsqueeze(0)\n        )\n\n    def clear(self):\n        \"\"\"\u6e05\u7a7a\u7f13\u5b58\"\"\"\n        self.current_length = 0\n\n    def get_cache_info(self):\n        \"\"\"\u83b7\u53d6\u7f13\u5b58\u72b6\u6001\u4fe1\u606f\"\"\"\n        return {\n            'current_length': self.current_length,\n            'capacity': self.max_seq_len,\n            'usage_ratio': self.current_length / self.max_seq_len,\n            'memory_mb': self.cache_k.numel() * 2 * 2 / 1024 / 1024  # FP16\n        }\n\n\nclass AttentionWithKVCache(nn.Module):\n    \"\"\"\u5e26KV Cache\u7684\u6ce8\u610f\u529b\u5c42\"\"\"\n\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.kv_cache = None\n\n    def setup_cache(self, max_seq_len, device):\n        \"\"\"\u521d\u59cb\u5316KV Cache\"\"\"\n        self.kv_cache = KVCache(max_seq_len, self.num_heads, self.head_dim, device)\n\n    def forward(self, x, use_cache=False):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u8ba1\u7b97Q, K, V\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        if use_cache and self.kv_cache is not None:\n            # \u4f7f\u7528\u7f13\u5b58\u6a21\u5f0f\uff1a\u66f4\u65b0\u7f13\u5b58\u5e76\u83b7\u53d6\u5b8c\u6574\u7684K,V\n            K, V = self.kv_cache.update_cache(K, V)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attn_weights = torch.softmax(scores, dim=-1)\n        out = torch.matmul(attn_weights, V)\n\n        # \u5408\u5e76\u591a\u5934\u8f93\u51fa\n        out = out.view(batch_size, seq_len, d_model)\n        return self.W_o(out)\n\n\n# \u4f7f\u7528\u793a\u4f8b\ndef demo_kv_cache():\n    \"\"\"KV Cache\u4f7f\u7528\u6f14\u793a\"\"\"\n\n    # \u521d\u59cb\u5316\u6a21\u578b\n    attention = AttentionWithKVCache(d_model=512, num_heads=8)\n    attention.setup_cache(max_seq_len=1024, device='cpu')\n\n    print(\"=== KV Cache\u6f14\u793a ===\")\n\n    # \u6a21\u62df\u751f\u6210\u8fc7\u7a0b\n    vocab_size = 1000\n    sequence = []\n\n    for step in range(5):\n        if step == 0:\n            # \u7b2c\u4e00\u6b65\uff1a\u8f93\u5165\u5b8c\u6574\u7684prompt\n            current_input = torch.randint(0, vocab_size, (1, 3, 512))  # 3\u4e2atoken\u7684prompt\n            print(f\"Step {step}: \u8f93\u5165prompt (3 tokens)\")\n        else:\n            # \u540e\u7eed\u6b65\u9aa4\uff1a\u53ea\u8f93\u5165\u65b0\u751f\u6210\u7684token\n            current_input = torch.randint(0, vocab_size, (1, 1, 512))  # 1\u4e2a\u65b0token\n            print(f\"Step {step}: \u8f93\u5165\u65b0token (1 token)\")\n\n        # \u524d\u5411\u4f20\u64ad\uff08\u4f7f\u7528\u7f13\u5b58\uff09\n        output = attention(current_input, use_cache=True)\n\n        # \u663e\u793a\u7f13\u5b58\u72b6\u6001\n        cache_info = attention.kv_cache.get_cache_info()\n        print(f\"  \u7f13\u5b58\u957f\u5ea6: {cache_info['current_length']}\")\n        print(f\"  \u5185\u5b58\u4f7f\u7528: {cache_info['memory_mb']:.2f} MB\")\n        print()\n\nif __name__ == \"__main__\":\n    demo_kv_cache()\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#_6","title":"\u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5","text":"<pre><code>import time\n\ndef benchmark_with_without_cache():\n    \"\"\"\u5bf9\u6bd4\u6709\u65e0KV Cache\u7684\u6027\u80fd\"\"\"\n\n    d_model, num_heads = 768, 12\n    max_seq_len = 512\n\n    # \u521d\u59cb\u5316\u6a21\u578b\n    attention_with_cache = AttentionWithKVCache(d_model, num_heads)\n    attention_with_cache.setup_cache(max_seq_len, 'cpu')\n\n    attention_without_cache = AttentionWithKVCache(d_model, num_heads)\n\n    # \u6a21\u62df\u5e8f\u5217\u751f\u6210\n    prompt_len = 50\n    generate_len = 100\n\n    print(\"=== \u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5 ===\")\n\n    # \u6d4b\u8bd5\u65e0\u7f13\u5b58\u7248\u672c\n    start_time = time.time()\n    sequence_input = torch.randn(1, prompt_len, d_model)\n\n    for i in range(generate_len):\n        # \u6bcf\u6b21\u90fd\u8f93\u5165\u5b8c\u6574\u5e8f\u5217\uff08\u65e0\u7f13\u5b58\uff09\n        full_input = torch.randn(1, prompt_len + i + 1, d_model)\n        _ = attention_without_cache(full_input, use_cache=False)\n\n    no_cache_time = time.time() - start_time\n    print(f\"\u65e0\u7f13\u5b58\u751f\u6210\u65f6\u95f4: {no_cache_time:.3f}\u79d2\")\n\n    # \u6d4b\u8bd5\u6709\u7f13\u5b58\u7248\u672c  \n    start_time = time.time()\n\n    # \u5904\u7406prompt\n    _ = attention_with_cache(sequence_input, use_cache=True)\n\n    # \u9010\u6b65\u751f\u6210\n    for i in range(generate_len):\n        # \u6bcf\u6b21\u53ea\u8f93\u5165\u65b0token\uff08\u6709\u7f13\u5b58\uff09\n        new_token = torch.randn(1, 1, d_model)\n        _ = attention_with_cache(new_token, use_cache=True)\n\n    with_cache_time = time.time() - start_time\n    print(f\"\u6709\u7f13\u5b58\u751f\u6210\u65f6\u95f4: {with_cache_time:.3f}\u79d2\")\n\n    speedup = no_cache_time / with_cache_time\n    print(f\"\u52a0\u901f\u500d\u6570: {speedup:.1f}x\")\n\nif __name__ == \"__main__\":\n    benchmark_with_without_cache()\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3KV Cache\u7684\u5de5\u4f5c\u539f\u7406\u548c\u52a0\u901f\u673a\u5236</li> <li>[ ] \u80fd\u8ba1\u7b97KV Cache\u7684\u5185\u5b58\u9700\u6c42</li> <li>[ ] \u5b8c\u6210KV Cache\u6f14\u793a\u4ee3\u7801\u7684\u7f16\u5199\u548c\u6d4b\u8bd5</li> <li>[ ] \u7406\u89e3\u4e0d\u540c\u6ce8\u610f\u529b\u53d8\u4f53\u5bf9KV Cache\u7684\u5f71\u54cd</li> </ul>"},{"location":"fundamentals/attention-advanced/kv-cache/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53</li> <li>\u4e0b\u4e00\u8282\uff1a\u5f52\u4e00\u5316\u6280\u672f</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/","title":"\u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4eceMHA\u5230MLA\u7684\u6280\u672f\u6f14\u8fdb\uff0c\u638c\u63e1\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u5316\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\u3002</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Transformer\u7684Attention\u53ca\u5176\u5404\u79cd\u53d8\u4f53 - \u8be6\u7ec6\u5bf9\u6bd4\u5206\u6790</li> <li>\u7f13\u5b58\u4e0e\u6548\u679c\u7684\u6781\u9650\u62c9\u626f\uff1a\u4eceMHA\u3001MQA\u3001GQA\u5230MLA - \u79d1\u5b66\u7a7a\u95f4\u6df1\u5ea6\u89e3\u6790</li> </ol>"},{"location":"fundamentals/attention-advanced/mha-variants/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#_6","title":"\u6280\u672f\u6f14\u8fdb\u8def\u5f84","text":"<pre><code>MHA (\u6807\u51c6\u591a\u5934) \u2192 MQA (\u5171\u4eabKV) \u2192 GQA (\u5206\u7ec4\u5171\u4eab) \u2192 MLA (\u6f5c\u5728\u7a7a\u95f4)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_7","title":"\u5404\u53d8\u4f53\u8be6\u7ec6\u5bf9\u6bd4","text":"\u53d8\u4f53 KV Cache\u9700\u6c42 \u8ba1\u7b97\u590d\u6742\u5ea6 \u6027\u80fd\u8868\u73b0 \u4e3b\u8981\u5e94\u7528 MHA O(h\u00d7d\u00d7L) O(H x N^2 x D) \u57fa\u51c6\u6027\u80fd \u6807\u51c6Transformer MQA O(d\u00d7L) O(N^2\u00d7D) \u8f7b\u5fae\u4e0b\u964d \u8d44\u6e90\u53d7\u9650\u573a\u666f GQA O(g\u00d7d\u00d7L) O(G\u00d7N^2\u00d7D) \u5e73\u8861\u4f18\u79c0 \u4e3b\u6d41\u5927\u6a21\u578b MLA \u6700\u4f18\u5316 O(R\u00d7N^2\u00d7D) \u63a5\u8fd1MHA \u957f\u4e0a\u4e0b\u6587 <p>\u5176\u4e2d\uff1ah=\u5934\u6570\uff0cd=\u7ef4\u5ea6\uff0cL=\u5e8f\u5217\u957f\u5ea6\uff0cg=\u7ec4\u6570</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_8","title":"\u6838\u5fc3\u6280\u672f\u7ec6\u8282","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#1-mha-multi-head-attention","title":"1. MHA (Multi-Head Attention)","text":"<pre><code># \u6bcf\u4e2a\u5934\u90fd\u6709\u72ec\u7acb\u7684Q\u3001K\u3001V\nfor i in range(num_heads):\n    Q_i = input @ W_Q_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u67e5\u8be2\u77e9\u9635\n    K_i = input @ W_K_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u952e\u77e9\u9635  \n    V_i = input @ W_V_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u503c\u77e9\u9635\n    head_i = attention(Q_i, K_i, V_i)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#2-mqa-multi-query-attention","title":"2. MQA (Multi-Query Attention)","text":"<pre><code># \u6240\u6709\u5934\u5171\u4eabK\u3001V\uff0c\u53ea\u6709Q\u72ec\u7acb\nK_shared = input @ W_K  # \u5171\u4eab\u7684\u952e\u77e9\u9635\nV_shared = input @ W_V  # \u5171\u4eab\u7684\u503c\u77e9\u9635\n\nfor i in range(num_heads):\n    Q_i = input @ W_Q_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u67e5\u8be2\u77e9\u9635\n    head_i = attention(Q_i, K_shared, V_shared)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3-gqa-grouped-query-attention","title":"3. GQA (Grouped-Query Attention)","text":"<pre><code># \u5206\u7ec4\u5171\u4eab\uff1a\u6bcf\u7ec4\u5185\u5171\u4eabK\u3001V\nnum_groups = num_heads // group_size\n\nfor g in range(num_groups):\n    K_g = input @ W_K_g  # \u7ec4\u5171\u4eab\u7684\u952e\u77e9\u9635\n    V_g = input @ W_V_g  # \u7ec4\u5171\u4eab\u7684\u503c\u77e9\u9635\n\n    for i in range(group_size):\n        Q_i = input @ W_Q_i\n        head_i = attention(Q_i, K_g, V_g)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_9","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#q1-mhamqagqamla","title":"Q1: MHA\u3001MQA\u3001GQA\u3001MLA\u90fd\u662f\u4ec0\u4e48\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54\uff1a \u8fd9\u662fTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u56db\u4e2a\u6f14\u8fdb\u9636\u6bb5\uff0c\u4e3b\u8981\u4f18\u5316KV Cache\u7684\u5b58\u50a8\u9700\u6c42\uff1a</p> <ul> <li>MHA: \u6807\u51c6\u591a\u5934\u6ce8\u610f\u529b\uff0c\u6bcf\u4e2a\u5934\u72ec\u7acbQKV</li> <li>MQA: \u591a\u67e5\u8be2\u6ce8\u610f\u529b\uff0c\u6240\u6709\u5934\u5171\u4eabKV</li> <li>GQA: \u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\uff0c\u5206\u7ec4\u5185\u5171\u4eabKV  </li> <li>MLA: \u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u4f18\u5316</li> </ul> <p>\u6280\u672f\u7ec6\u8282\uff1a</p> <p>MHA\u95ee\u9898: KV Cache\u968f\u5934\u6570\u7ebf\u6027\u589e\u957f\uff0c\u5185\u5b58\u5f00\u9500\u5927 <pre><code>\u5185\u5b58\u9700\u6c42 = \u5934\u6570 \u00d7 \u7ef4\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6\n</code></pre></p> <p>MQA\u89e3\u51b3\u65b9\u6848: \u5171\u4eabKV\u77e9\u9635\uff0c\u5185\u5b58\u9700\u6c42\u964d\u4f4eh\u500d <pre><code># \u4ece h\u00d7(d_k + d_v) \u964d\u4f4e\u5230 (d_k + d_v)\n</code></pre></p> <p>GQA\u5e73\u8861\u65b9\u6848: \u5206\u7ec4\u5171\u4eab\uff0c\u517c\u987e\u6027\u80fd\u548c\u6548\u7387 <pre><code># \u5185\u5b58\u9700\u6c42 = \u7ec4\u6570 \u00d7 \u7ef4\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6  \n# \u5176\u4e2d\uff1a\u7ec4\u6570 = \u5934\u6570 / \u6bcf\u7ec4\u5934\u6570\n</code></pre></p> <p>MLA\u7ec8\u6781\u4f18\u5316: \u6f5c\u5728\u7a7a\u95f4\u6295\u5f71\uff0c\u6700\u5c0f\u5316KV Cache</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#mla","title":"\ud83d\udd2c MLA\u6280\u672f\u6df1\u5ea6\u89e3\u6790","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#mla_1","title":"MLA\u6838\u5fc3\u521b\u65b0","text":"<p>Multi-head Latent Attention (MLA) \u662fDeepSeek\u56e2\u961f\u63d0\u51fa\u7684\u9769\u547d\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u4e09\u5927\u521b\u65b0\u663e\u8457\u964d\u4f4eKV Cache\u5185\u5b58\u9700\u6c42\uff1a</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#1-kv","title":"1. \u4f4e\u79e9KV\u8054\u5408\u538b\u7f29","text":"<p>\u6838\u5fc3\u601d\u60f3: \u5c06\u9ad8\u7ef4\u7684Key\u548cValue\u77e9\u9635\u8054\u5408\u538b\u7f29\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4</p> <pre><code># \u4f20\u7edf\u65b9\u5f0f\uff1a\u6bcf\u4e2a\u5934\u72ec\u7acb\u5b58\u50a8KV\ntraditional_kv_cache = num_heads \u00d7 head_dim \u00d7 seq_len \u00d7 2  # K\u548cV\n\n# MLA\u65b9\u5f0f\uff1a\u538b\u7f29\u540e\u7684\u6f5c\u5728\u5411\u91cf\nmla_kv_cache = compressed_dim \u00d7 seq_len + rope_dim \u00d7 seq_len\n</code></pre> <p>\u538b\u7f29\u8fc7\u7a0b: \\(\\(c_t^{KV} = x_t W^{DKV}\\)\\)</p> <p>\u5176\u4e2d \\(W^{DKV} \\in \\mathbb{R}^{d \\times d_c}\\)\uff0c\\(d_c \\ll h \\cdot d_h\\)</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#2-rope","title":"2. RoPE\u89e3\u8026\u673a\u5236","text":"<p>\u95ee\u9898: \u4f4d\u7f6e\u7f16\u7801\u4e0e\u538b\u7f29\u673a\u5236\u7684\u51b2\u7a81 - \u4f20\u7edfRoPE\u9700\u8981\u5728\u539f\u59cbQK\u7a7a\u95f4\u4e2d\u5e94\u7528 - \u538b\u7f29\u7834\u574f\u4e86\u4f4d\u7f6e\u4fe1\u606f\u7684\u6b63\u786e\u4f20\u9012</p> <p>\u89e3\u51b3\u65b9\u6848: \u5c06Query\u548cKey\u5206\u4e3a\u4e24\u90e8\u5206 - \u8bed\u4e49\u90e8\u5206 (\\(q^C, k^C\\)): \u643a\u5e26\u4e3b\u8981\u8bed\u4e49\u4fe1\u606f\uff0c\u53ef\u4ee5\u538b\u7f29 - \u4f4d\u7f6e\u90e8\u5206 (\\(q^R, k^R\\)): \u643a\u5e26\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4fdd\u6301\u539f\u7ef4\u5ea6</p> <pre><code>def mla_with_rope_decoupling(x, position):\n    # 1. \u751f\u6210\u6f5c\u5728\u5411\u91cf\n    c_kv = x @ W_down_kv  # \u538b\u7f29\n\n    # 2. Query\u5206\u79bb\n    q = x @ W_q\n    q_c, q_r = q[:, :d_c], q[:, d_c:]  # \u8bed\u4e49 + \u4f4d\u7f6e\n\n    # 3. Key\u5206\u79bb\u548c\u6062\u590d\n    k_c = c_kv @ W_up_k   # \u4ece\u6f5c\u5728\u7a7a\u95f4\u6062\u590d\u8bed\u4e49Key\n    k_r = x @ W_k_r       # \u76f4\u63a5\u751f\u6210\u4f4d\u7f6eKey\n\n    # 4. \u5206\u522b\u5e94\u7528RoPE\n    q_r = apply_rope(q_r, position)\n    k_r = apply_rope(k_r, position)\n\n    # 5. \u7ec4\u5408\u8ba1\u7b97\n    q_combined = concat([q_c, q_r])\n    k_combined = concat([k_c, k_r])\n    v = c_kv @ W_up_v\n\n    return attention(q_combined, k_combined, v)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3","title":"3. \u6743\u91cd\u5438\u6536\u4f18\u5316","text":"<p>\u76ee\u6807: \u51cf\u5c11\u63a8\u7406\u65f6\u7684\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c</p> <p>\u6280\u672f: \u5229\u7528\u77e9\u9635\u4e58\u6cd5\u7ed3\u5408\u5f8b\uff0c\u9884\u5148\u5408\u5e76\u6743\u91cd\u77e9\u9635</p> <pre><code># \u539f\u59cb\u8ba1\u7b97\uff1a\u4e24\u6b21\u77e9\u9635\u4e58\u6cd5\nc_kv = x @ W_down_kv\nk = c_kv @ W_up_k\n\n# \u6743\u91cd\u5438\u6536\uff1a\u5408\u5e76\u4e3a\u4e00\u6b21\u4e58\u6cd5\nW_combined = W_down_kv @ W_up_k\nk = x @ W_combined\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_10","title":"\u5185\u5b58\u6548\u7387\u5bf9\u6bd4","text":"\u65b9\u6cd5 KV Cache\u5927\u5c0f \u538b\u7f29\u6bd4 MHA \\(2 h \\cdot d_h \\cdot L\\) 1.0\u00d7 (\u57fa\u51c6) MQA \\(2 d_h \\cdot L\\) \\(h\\)\u00d7 GQA \\(2 g \\cdot d_h \\cdot L\\) \\(h/g\\)\u00d7 MLA \\((d_c + d_h^R) \\cdot L\\) ~10-20\u00d7 <p>\u5177\u4f53\u4f8b\u5b50 (LLaMA-7B\u89c4\u6a21): - \u539f\u59cbMHA: 32\u5934 \u00d7 128\u7ef4 \u00d7 2 = 8192\u7ef4/token - MLA\u538b\u7f29: 512\u7ef4 + 128\u7ef4 = 640\u7ef4/token - \u538b\u7f29\u6bd4: 12.8\u00d7</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_11","title":"\u6027\u80fd\u4fdd\u6301\u673a\u5236","text":"<p>\u5c3d\u7ba1\u5927\u5e45\u538b\u7f29\uff0cMLA\u901a\u8fc7\u5de7\u5999\u8bbe\u8ba1\u4fdd\u6301\u4e86\u63a5\u8fd1MHA\u7684\u6027\u80fd\uff1a</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#1","title":"1. \u8868\u8fbe\u80fd\u529b\u4fdd\u6301","text":"<ul> <li>\u4f4e\u79e9\u5047\u8bbe\uff1a\u5927\u90e8\u5206\u6ce8\u610f\u529b\u6a21\u5f0f\u53ef\u4ee5\u7528\u4f4e\u79e9\u77e9\u9635\u8fd1\u4f3c</li> <li>\u5173\u952e\u4fe1\u606f\u4fdd\u7559\uff1a\u4f4d\u7f6e\u4fe1\u606f\u901a\u8fc7\u89e3\u8026\u673a\u5236\u5b8c\u6574\u4fdd\u7559</li> <li>\u6e10\u8fdb\u6062\u590d\uff1a\u591a\u5c42\u5806\u53e0\u9010\u6b65\u6062\u590d\u5b8c\u6574\u4fe1\u606f</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/#2","title":"2. \u8bad\u7ec3\u7a33\u5b9a\u6027","text":"<pre><code># \u6b8b\u5dee\u8fde\u63a5\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\ndef mla_block(x):\n    # MLA\u6ce8\u610f\u529b\n    attn_out = mla_attention(x)\n    x = x + attn_out  # \u6b8b\u5dee\u8fde\u63a5\n\n    # FFN\n    ffn_out = feed_forward(x)\n    x = x + ffn_out   # \u6b8b\u5dee\u8fde\u63a5\n\n    return x\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3_1","title":"3. \u4f4d\u7f6e\u654f\u611f\u6027","text":"<ul> <li>RoPE\u89e3\u8026\u786e\u4fdd\u4f4d\u7f6e\u4fe1\u606f\u4e0d\u4e22\u5931</li> <li>\u4f4d\u7f6e\u7f16\u7801\u7ef4\u5ea6\u53ef\u4ee5\u6839\u636e\u4efb\u52a1\u9700\u6c42\u8c03\u6574</li> <li>\u957f\u5e8f\u5217\u5916\u63a8\u80fd\u529b\u5f97\u5230\u4fdd\u6301</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/#q2","title":"Q2: \u4e3a\u4ec0\u4e48\u9700\u8981\u8fd9\u4e9b\u4f18\u5316\uff1f","text":"<p>\u6838\u5fc3\u52a8\u673a\uff1a</p> <ol> <li>\u5185\u5b58\u74f6\u9888</li> <li>\u957f\u5e8f\u5217\u63a8\u7406\u65f6KV Cache\u5360\u7528\u5927\u91cf\u663e\u5b58</li> <li> <p>\u9650\u5236\u4e86\u6a21\u578b\u7684\u90e8\u7f72\u548c\u6269\u5c55\u80fd\u529b</p> </li> <li> <p>\u63a8\u7406\u901f\u5ea6</p> </li> <li>\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387</li> <li> <p>\u652f\u6301\u66f4\u5927\u7684batch size</p> </li> <li> <p>\u6210\u672c\u8003\u8651</p> </li> <li>\u964d\u4f4e\u786c\u4ef6\u8981\u6c42</li> <li>\u63d0\u9ad8\u670d\u52a1\u5e76\u53d1\u80fd\u529b</li> </ol>"},{"location":"fundamentals/attention-advanced/mha-variants/#q3","title":"Q3: \u5404\u53d8\u4f53\u7684\u4f18\u7f3a\u70b9\u5bf9\u6bd4\uff1f","text":"\u7ef4\u5ea6 MHA MQA GQA MLA \u6027\u80fd \ud83d\udfe2 \u57fa\u51c6\u6700\u597d \ud83d\udfe1 \u8f7b\u5fae\u4e0b\u964d \ud83d\udfe2 \u63a5\u8fd1MHA \ud83d\udfe2 \u8d85\u8d8aMHA \u5185\u5b58 \ud83d\udd34 \u9700\u6c42\u6700\u9ad8 \ud83d\udfe2 \u663e\u8457\u964d\u4f4e \ud83d\udfe1 \u9002\u4e2d \ud83d\udfe2 \u6700\u4f18 \u901f\u5ea6 \ud83d\udfe1 \u6807\u51c6 \ud83d\udfe2 \u6700\u5feb \ud83d\udfe2 \u8f83\u5feb \ud83d\udfe2 \u4f18\u79c0 \u5b9e\u73b0 \ud83d\udfe2 \u7b80\u5355 \ud83d\udfe2 \u7b80\u5355 \ud83d\udfe1 \u4e2d\u7b49 \ud83d\udd34 \u590d\u6742"},{"location":"fundamentals/attention-advanced/mha-variants/#q4","title":"Q4: \u5982\u4f55\u9009\u62e9\u5408\u9002\u7684\u6ce8\u610f\u529b\u673a\u5236\uff1f","text":"<p>\u9009\u62e9\u7b56\u7565\uff1a</p> <pre><code>if \u8d44\u6e90\u5145\u8db3 and \u8ffd\u6c42\u6700\u4f73\u6027\u80fd:\n    \u9009\u62e9 MHA\nelif \u8d44\u6e90\u4e25\u91cd\u53d7\u9650 and \u53ef\u63a5\u53d7\u6027\u80fd\u635f\u5931:\n    \u9009\u62e9 MQA  \nelif \u9700\u8981\u5e73\u8861\u6027\u80fd\u548c\u6548\u7387:\n    \u9009\u62e9 GQA  # \u4e3b\u6d41\u9009\u62e9\nelif \u957f\u4e0a\u4e0b\u6587 and \u5185\u5b58\u654f\u611f:\n    \u9009\u62e9 MLA\n</code></pre> <p>\u5b9e\u9645\u8003\u8651\u56e0\u7d20\uff1a - \u786c\u4ef6\u5185\u5b58\u9650\u5236 - \u5e8f\u5217\u957f\u5ea6\u9700\u6c42 - \u5ef6\u8fdf\u8981\u6c42 - \u5f00\u53d1\u590d\u6742\u5ea6</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_12","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#1-softmax","title":"\u7ec3\u4e601: \u5b9e\u73b0Softmax\u51fd\u6570","text":"<p>\u5e73\u53f0: Deep-ML Softmax</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#2-mhagqa","title":"\u7ec3\u4e602: MHA\u5230GQA\u7684\u9002\u914d","text":"<pre><code>class GroupedQueryAttention(nn.Module):\n    def __init__(self, d_model, num_heads, num_groups):\n        super().__init__()\n        assert num_heads % num_groups == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.num_groups = num_groups\n        self.heads_per_group = num_heads // num_groups\n        self.d_k = d_model // num_heads\n\n        # Q\u77e9\u9635\uff1a\u6bcf\u4e2a\u5934\u72ec\u7acb\n        self.W_q = nn.Linear(d_model, d_model)\n\n        # K,V\u77e9\u9635\uff1a\u6309\u7ec4\u5171\u4eab\n        self.W_k = nn.Linear(d_model, num_groups * self.d_k)\n        self.W_v = nn.Linear(d_model, num_groups * self.d_k)\n\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u751f\u6210Q\uff1a\u6bcf\u4e2a\u5934\u72ec\u7acb\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k)\n\n        # \u751f\u6210K,V\uff1a\u6309\u7ec4\u5171\u4eab\n        K = self.W_k(x).view(batch_size, seq_len, self.num_groups, self.d_k)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_groups, self.d_k)\n\n        # \u91cd\u590dK,V\u4ee5\u5339\u914dQ\u7684\u5934\u6570\n        K = K.repeat_interleave(self.heads_per_group, dim=2)\n        V = V.repeat_interleave(self.heads_per_group, dim=2)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        attn_weights = F.softmax(scores, dim=-1)\n        out = torch.matmul(attn_weights, V)\n\n        # \u5408\u5e76\u591a\u5934\u8f93\u51fa\n        out = out.view(batch_size, seq_len, d_model)\n        return self.W_o(out)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3-kv-cache","title":"\u7ec3\u4e603: KV Cache\u5b9e\u73b0\u9884\u89c8","text":"<pre><code>class KVCache:\n    def __init__(self, max_seq_len, num_heads, d_k):\n        self.max_seq_len = max_seq_len\n        self.cache_k = torch.zeros(max_seq_len, num_heads, d_k)\n        self.cache_v = torch.zeros(max_seq_len, num_heads, d_k) \n        self.current_len = 0\n\n    def update(self, new_k, new_v):\n        \"\"\"\u66f4\u65b0\u7f13\u5b58\u5e76\u8fd4\u56de\u5b8c\u6574\u7684K,V\"\"\"\n        seq_len = new_k.size(0)\n\n        # \u5b58\u50a8\u65b0\u7684K,V\n        self.cache_k[self.current_len:self.current_len+seq_len] = new_k\n        self.cache_v[self.current_len:self.current_len+seq_len] = new_v\n\n        self.current_len += seq_len\n\n        # \u8fd4\u56de\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u5b8c\u6574K,V\n        return (self.cache_k[:self.current_len], \n                self.cache_v[:self.current_len])\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_13","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u80fd\u89e3\u91ca\u5404\u53d8\u4f53\u7684\u6838\u5fc3\u533a\u522b</li> <li>[ ] \u7406\u89e3KV Cache\u4f18\u5316\u7684\u539f\u7406</li> <li>[ ] \u5b8c\u6210GQA\u4ee3\u7801\u5b9e\u73b0</li> <li>[ ] \u80fd\u6839\u636e\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u6ce8\u610f\u529b\u673a\u5236</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/#_14","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aKV Cache\u6280\u672f</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/attention-advanced/normalization/","title":"\u5f52\u4e00\u5316\u6280\u672f","text":""},{"location":"fundamentals/attention-advanced/normalization/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u638c\u63e1\u6df1\u5ea6\u5b66\u4e60\u4e2d\u4e09\u5927\u5f52\u4e00\u5316\u6280\u672f\u7684\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\uff0c\u7406\u89e3Pre-Norm\u4e0ePost-Norm\u5728Transformer\u4e2d\u7684\u5f71\u54cd\u3002</p>"},{"location":"fundamentals/attention-advanced/normalization/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/normalization/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>\u5927\u6a21\u578b\u4e2d\u5e38\u89c1\u76843\u79cdNorm - \u77e5\u4e4e</li> <li>\u4e3a\u4ec0\u4e48\u5f53\u524d\u4e3b\u6d41\u7684\u5927\u6a21\u578b\u90fd\u4f7f\u7528RMS-Norm\uff1f - \u77e5\u4e4e  </li> <li>\u4e3a\u4ec0\u4e48Pre Norm\u7684\u6548\u679c\u4e0d\u5982Post Norm\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> </ol>"},{"location":"fundamentals/attention-advanced/normalization/#_5","title":"\u9009\u8bfb\u6df1\u5165\u6750\u6599","text":"<ul> <li>BN\u7a76\u7adf\u8d77\u4e86\u4ec0\u4e48\u4f5c\u7528\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> </ul>"},{"location":"fundamentals/attention-advanced/normalization/#_6","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/normalization/#_7","title":"\u4e09\u5927\u5f52\u4e00\u5316\u6280\u672f\u5bf9\u6bd4","text":"\u6280\u672f \u5f52\u4e00\u5316\u7ef4\u5ea6 \u9002\u7528\u573a\u666f \u4e3b\u8981\u4f18\u52bf \u8ba1\u7b97\u6210\u672c BatchNorm \u8de8\u6837\u672c\u7279\u5f81\u7ef4\u5ea6 CNN\u3001\u5927batch \u8bad\u7ec3\u52a0\u901f\u3001\u9632\u8fc7\u62df\u5408 \u4e2d\u7b49 LayerNorm \u5355\u6837\u672c\u6240\u6709\u7279\u5f81 RNN\u3001Transformer \u4e0d\u4f9d\u8d56batch\u5927\u5c0f \u8f83\u9ad8 RMSNorm \u5355\u6837\u672cRMS\u5f52\u4e00\u5316 \u5927\u578b\u8bed\u8a00\u6a21\u578b \u8ba1\u7b97\u9ad8\u6548\u3001\u6548\u679c\u76f8\u5f53 \u6700\u4f4e"},{"location":"fundamentals/attention-advanced/normalization/#_8","title":"\u6570\u5b66\u516c\u5f0f\u8be6\u89e3","text":""},{"location":"fundamentals/attention-advanced/normalization/#1-batch-normalization","title":"1. Batch Normalization","text":"\\[BN(x) = \u03b3 \u00d7 \\frac{x - \u03bc_B}{\\sqrt{\u03c3_B^2 + \u03b5}} + \u03b2\\] <p>\u6838\u5fc3\u7279\u70b9: - \u03bc_B, \u03c3_B: \u5728batch\u7ef4\u5ea6\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee - \u8bad\u7ec3\u65f6\u4f7f\u7528\u5f53\u524dbatch\u7edf\u8ba1\u91cf\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u79fb\u52a8\u5e73\u5747 - \u9700\u8981\u03b3(\u7f29\u653e)\u548c\u03b2(\u504f\u79fb)\u53ef\u5b66\u4e60\u53c2\u6570</p> <p>\u95ee\u9898: - \u4f9d\u8d56batch\u5927\u5c0f\uff0c\u5c0fbatch\u6548\u679c\u5dee - \u8bad\u7ec3\u548c\u63a8\u7406\u4e0d\u4e00\u81f4 - \u5728\u5e8f\u5217\u6a21\u578b\u4e2d\u6548\u679c\u4e0d\u4f73</p>"},{"location":"fundamentals/attention-advanced/normalization/#2-layer-normalization","title":"2. Layer Normalization","text":"\\[LN(x) = \u03b3 \u00d7 \\frac{x - \u03bc_L}{\\sqrt{\u03c3_L^2 + \u03b5}} + \u03b2\\] <p>\u6838\u5fc3\u7279\u70b9: - \u03bc_L, \u03c3_L: \u5728\u7279\u5f81\u7ef4\u5ea6\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee - \u6bcf\u4e2a\u6837\u672c\u72ec\u7acb\u5f52\u4e00\u5316\uff0c\u4e0d\u4f9d\u8d56\u5176\u4ed6\u6837\u672c - \u8bad\u7ec3\u548c\u63a8\u7406\u4e00\u81f4</p> <p>\u4f18\u52bf: - \u9002\u5408\u53d8\u957f\u5e8f\u5217 - \u4e0d\u53d7batch\u5927\u5c0f\u5f71\u54cd - Transformer\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/attention-advanced/normalization/#3-rms-normalization","title":"3. RMS Normalization","text":"\\[RMSNorm(x) = \u03b3 \u00d7 \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2 + \u03b5}}\\] <p>\u6838\u5fc3\u7279\u70b9: - \u53ea\u8ba1\u7b97RMS\uff0c\u4e0d\u51cf\u53bb\u5747\u503c - \u7b80\u5316\u4e86LayerNorm\u7684\u8ba1\u7b97 - \u53ea\u9700\u8981\u03b3\u53c2\u6570\uff0c\u65e0\u9700\u03b2</p> <p>\u4f18\u52bf: - \u8ba1\u7b97\u6210\u672c\u66f4\u4f4e - \u5728\u5927\u6a21\u578b\u4e2d\u6548\u679c\u4e0d\u8f93LayerNorm - \u5185\u5b58\u53cb\u597d</p>"},{"location":"fundamentals/attention-advanced/normalization/#pre-norm-vs-post-norm","title":"Pre-Norm vs Post-Norm","text":""},{"location":"fundamentals/attention-advanced/normalization/#post-norm-transformer","title":"Post-Norm (\u539f\u59cbTransformer)","text":"<pre><code>Input \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 LayerNorm \u2192 Output\n</code></pre> <p>\u7279\u70b9: - \u5f52\u4e00\u5316\u5728\u6b8b\u5dee\u8fde\u63a5\u4e4b\u540e - \u9700\u8981\u5b66\u4e60\u7387warmup\u624d\u80fd\u7a33\u5b9a\u8bad\u7ec3 - \u6d45\u5c42\u6a21\u578b(\u22646\u5c42)\u6548\u679c\u66f4\u597d - \u68af\u5ea6\u4f20\u64ad\u53ef\u80fd\u4e0d\u7a33\u5b9a</p>"},{"location":"fundamentals/attention-advanced/normalization/#pre-norm","title":"Pre-Norm (\u73b0\u4ee3\u4e3b\u6d41)","text":"<pre><code>Input \u2192 LayerNorm \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 Output\n</code></pre> <p>\u7279\u70b9: - \u5f52\u4e00\u5316\u5728\u6b8b\u5dee\u8fde\u63a5\u4e4b\u524d - \u8bad\u7ec3\u66f4\u7a33\u5b9a\uff0c\u65e0\u9700warmup - \u6df1\u5c42\u6a21\u578b\u8bad\u7ec3\u66f4\u5bb9\u6613 - \u73b0\u4ee3\u5927\u6a21\u578b\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/attention-advanced/normalization/#_9","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/normalization/#q1-batch-normlayer-norm","title":"Q1: Batch Norm\u548cLayer Norm\u7684\u533a\u522b\uff1f","text":"<p>\u6838\u5fc3\u533a\u522b:</p> <ol> <li>\u5f52\u4e00\u5316\u7ef4\u5ea6\u4e0d\u540c:</li> <li>BatchNorm: \u5728batch\u7ef4\u5ea6\u5f52\u4e00\u5316\uff0c\u6bcf\u4e2a\u7279\u5f81\u72ec\u7acb</li> <li> <p>LayerNorm: \u5728\u7279\u5f81\u7ef4\u5ea6\u5f52\u4e00\u5316\uff0c\u6bcf\u4e2a\u6837\u672c\u72ec\u7acb</p> </li> <li> <p>\u5e94\u7528\u573a\u666f:</p> </li> <li>BatchNorm: CNN\u3001\u89c6\u89c9\u4efb\u52a1\u3001\u5927batch\u8bad\u7ec3</li> <li> <p>LayerNorm: NLP\u3001\u5e8f\u5217\u6a21\u578b\u3001\u5c0fbatch\u6216\u53d8\u957f\u5e8f\u5217</p> </li> <li> <p>\u4f9d\u8d56\u6027:</p> </li> <li>BatchNorm: \u4f9d\u8d56batch\u5927\u5c0f\u548c\u5176\u4ed6\u6837\u672c</li> <li>LayerNorm: \u53ea\u4f9d\u8d56\u5f53\u524d\u6837\u672c\uff0c\u66f4\u7a33\u5b9a</li> </ol> <p>\u6280\u672f\u7ec6\u8282: <pre><code># BatchNorm: \u5728batch\u7ef4\u5ea6\u8ba1\u7b97\u7edf\u8ba1\u91cf\nbatch_mean = x.mean(dim=0)  # [features]\nbatch_var = x.var(dim=0)    # [features]\n\n# LayerNorm: \u5728\u7279\u5f81\u7ef4\u5ea6\u8ba1\u7b97\u7edf\u8ba1\u91cf  \nlayer_mean = x.mean(dim=-1, keepdim=True)  # [batch, 1]\nlayer_var = x.var(dim=-1, keepdim=True)    # [batch, 1]\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/normalization/#q2-rmsnorm","title":"Q2: \u4e3a\u4ec0\u4e48\u73b0\u5728\u7528RMSNorm\uff1f","text":"<p>\u4e3b\u8981\u539f\u56e0:</p> <ol> <li>\u8ba1\u7b97\u6548\u7387:</li> <li>\u7701\u7565\u4e86\u5747\u503c\u8ba1\u7b97\uff0c\u51cf\u5c11\u4e86\u7ea615%\u7684\u8ba1\u7b97\u91cf</li> <li> <p>\u5185\u5b58\u8bbf\u95ee\u66f4\u5c11\uff0c\u5bf9GPU\u66f4\u53cb\u597d</p> </li> <li> <p>\u6548\u679c\u76f8\u5f53:</p> </li> <li>\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eRMSNorm\u6548\u679c\u4e0d\u8f93LayerNorm</li> <li> <p>\u5728\u5927\u6a21\u578b\u4e2d\u8868\u73b0\u751a\u81f3\u66f4\u597d</p> </li> <li> <p>\u7b80\u5316\u5b9e\u73b0:</p> </li> <li>\u4e0d\u9700\u8981\u03b2\u53c2\u6570\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u91cf</li> <li>\u6570\u503c\u7a33\u5b9a\u6027\u66f4\u597d</li> </ol> <p>\u6280\u672f\u539f\u7406: <pre><code># LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\nmean = x.mean(dim=-1, keepdim=True)\nvar = x.var(dim=-1, keepdim=True)  \nln_out = gamma * (x - mean) / sqrt(var + eps) + beta\n\n# RMSNorm\u53ea\u9700\u8981\u8ba1\u7b97RMS\nrms = sqrt(x.pow(2).mean(dim=-1, keepdim=True) + eps)\nrms_out = gamma * x / rms\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/normalization/#q3-pre-normpost-norm","title":"Q3: Pre-Norm\u548cPost-Norm\u7684\u4f4d\u7f6e\u533a\u522b\uff1f","text":"<p>\u67b6\u6784\u5bf9\u6bd4:</p> <p>Post-Norm (\u539f\u59cb): <pre><code>x \u2192 Attention \u2192 (+) \u2192 LayerNorm \u2192 FFN \u2192 (+) \u2192 LayerNorm\n    \u2191_______________|              \u2191_______|\n</code></pre></p> <p>Pre-Norm (\u73b0\u4ee3): <pre><code>x \u2192 LayerNorm \u2192 Attention \u2192 (+) \u2192 LayerNorm \u2192 FFN \u2192 (+)\n    \u2191________________________|              \u2191______|\n</code></pre></p> <p>\u8bad\u7ec3\u7a33\u5b9a\u6027\u5dee\u5f02:</p> \u65b9\u9762 Post-Norm Pre-Norm \u5b66\u4e60\u7387warmup \u5fc5\u9700 \u53ef\u9009 \u6df1\u5c42\u8bad\u7ec3 \u5bb9\u6613\u5931\u8d25 \u7a33\u5b9a \u68af\u5ea6\u4f20\u64ad \u53ef\u80fd\u4e0d\u7a33\u5b9a \u66f4\u5e73\u6ed1 \u6536\u655b\u901f\u5ea6 \u8f83\u6162 \u8f83\u5feb \u6700\u7ec8\u6027\u80fd \u6d45\u5c42\u66f4\u597d \u6df1\u5c42\u66f4\u4f18"},{"location":"fundamentals/attention-advanced/normalization/#q4-pre-norm","title":"Q4: \u4e3a\u4ec0\u4e48Pre-Norm\u8bad\u7ec3\u66f4\u7a33\u5b9a\uff1f","text":"<p>\u68af\u5ea6\u4f20\u64ad\u5206\u6790:</p> <ol> <li>Post-Norm\u95ee\u9898:</li> <li>\u68af\u5ea6\u9700\u8981\u7ecf\u8fc7LayerNorm\u7684\u53cd\u5411\u4f20\u64ad</li> <li>LayerNorm\u7684\u5bfc\u6570\u53ef\u80fd\u653e\u5927\u6216\u7f29\u5c0f\u68af\u5ea6</li> <li> <p>\u6df1\u5c42\u7f51\u7edc\u5bb9\u6613\u51fa\u73b0\u68af\u5ea6\u7206\u70b8/\u6d88\u5931</p> </li> <li> <p>Pre-Norm\u4f18\u52bf:</p> </li> <li>\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u6052\u7b49\u8def\u5f84(identity path)</li> <li>\u68af\u5ea6\u53ef\u4ee5\u66f4\u76f4\u63a5\u5730\u53cd\u5411\u4f20\u64ad</li> <li>\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u68af\u5ea6\u7684\u6570\u91cf\u7ea7\u4e3a\u221aL (L\u4e3a\u5c42\u6570)</li> </ol> <p>\u6570\u5b66\u76f4\u89c9: <pre><code>Post-Norm: \u68af\u5ea6\u9700\u8981\u7a7f\u8fc7LayerNorm\n\u2207L/\u2202x = \u2207L/\u2202norm \u00d7 \u2202norm/\u2202x  (\u4e0d\u7a33\u5b9a)\n\nPre-Norm: \u6052\u7b49\u8def\u5f84\u66f4\u5f3a  \n\u2207L/\u2202x = \u2207L/\u2202residual + \u2207L/\u2202processed  (\u66f4\u7a33\u5b9a)\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/normalization/#_10","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/normalization/#normpytorch","title":"\u4e09\u79cdNorm\u7684PyTorch\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass BatchNorm1d(nn.Module):\n    \"\"\"\u81ea\u5b9e\u73b0BatchNorm\"\"\"\n    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n\n        # \u53ef\u5b66\u4e60\u53c2\u6570\n        self.gamma = nn.Parameter(torch.ones(num_features))\n        self.beta = nn.Parameter(torch.zeros(num_features))\n\n        # \u79fb\u52a8\u5e73\u5747\u7edf\u8ba1\u91cf\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n\n    def forward(self, x):\n        # x shape: [batch_size, num_features]\n        if self.training:\n            # \u8ba1\u7b97\u5f53\u524dbatch\u7684\u7edf\u8ba1\u91cf\n            batch_mean = x.mean(dim=0)\n            batch_var = x.var(dim=0, unbiased=False)\n\n            # \u66f4\u65b0\u79fb\u52a8\u5e73\u5747\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n\n            # \u4f7f\u7528\u5f53\u524dbatch\u7edf\u8ba1\u91cf\u5f52\u4e00\u5316\n            mean, var = batch_mean, batch_var\n        else:\n            # \u63a8\u7406\u65f6\u4f7f\u7528\u79fb\u52a8\u5e73\u5747\n            mean, var = self.running_mean, self.running_var\n\n        # \u5f52\u4e00\u5316\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        return self.gamma * x_norm + self.beta\n\nclass LayerNorm(nn.Module):\n    \"\"\"\u81ea\u5b9e\u73b0LayerNorm\"\"\"\n    def __init__(self, normalized_shape, eps=1e-5):\n        super().__init__()\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n\n    def forward(self, x):\n        # \u5728\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8ba1\u7b97\u7edf\u8ba1\u91cf\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n\n        # \u5f52\u4e00\u5316\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        return self.gamma * x_norm + self.beta\n\nclass RMSNorm(nn.Module):\n    \"\"\"\u81ea\u5b9e\u73b0RMSNorm\"\"\"\n    def __init__(self, normalized_shape, eps=1e-5):\n        super().__init__()\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n\n    def forward(self, x):\n        # \u8ba1\u7b97RMS\n        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n\n        # RMS\u5f52\u4e00\u5316\n        x_norm = x / rms\n        return self.gamma * x_norm\n\n# \u4f7f\u7528\u793a\u4f8b\u548c\u6027\u80fd\u5bf9\u6bd4\ndef compare_normalization():\n    \"\"\"\u5bf9\u6bd4\u4e09\u79cd\u5f52\u4e00\u5316\u7684\u8ba1\u7b97\u6210\u672c\"\"\"\n\n    batch_size, seq_len, d_model = 32, 512, 768\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # \u521d\u59cb\u5316\u4e09\u79cdnorm\n    bn = BatchNorm1d(d_model)\n    ln = LayerNorm(d_model)\n    rms = RMSNorm(d_model)\n\n    print(\"=== \u5f52\u4e00\u5316\u6280\u672f\u5bf9\u6bd4 ===\")\n\n    # \u6d4b\u8bd5LayerNorm\n    import time\n    start_time = time.time()\n    for _ in range(1000):\n        _ = ln(x)\n    ln_time = time.time() - start_time\n    print(f\"LayerNorm\u8017\u65f6: {ln_time:.4f}\u79d2\")\n\n    # \u6d4b\u8bd5RMSNorm\n    start_time = time.time()\n    for _ in range(1000):\n        _ = rms(x)\n    rms_time = time.time() - start_time\n    print(f\"RMSNorm\u8017\u65f6: {rms_time:.4f}\u79d2\")\n\n    speedup = ln_time / rms_time\n    print(f\"RMSNorm\u52a0\u901f\u500d\u6570: {speedup:.2f}x\")\n\n    # \u53c2\u6570\u91cf\u5bf9\u6bd4\n    ln_params = sum(p.numel() for p in ln.parameters())\n    rms_params = sum(p.numel() for p in rms.parameters())\n    print(f\"LayerNorm\u53c2\u6570\u91cf: {ln_params}\")\n    print(f\"RMSNorm\u53c2\u6570\u91cf: {rms_params}\")\n    print(f\"\u53c2\u6570\u51cf\u5c11: {(ln_params - rms_params) / ln_params * 100:.1f}%\")\n\n# Pre-Norm vs Post-Norm\u5b9e\u73b0\nclass PostNormBlock(nn.Module):\n    \"\"\"Post-Norm Transformer Block\"\"\"\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model)\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        # Post-Norm: Attention \u2192 Add \u2192 Norm\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)\n\n        # FFN \u2192 Add \u2192 Norm\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)\n        return x\n\nclass PreNormBlock(nn.Module):\n    \"\"\"Pre-Norm Transformer Block\"\"\"\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model)\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        # Pre-Norm: Norm \u2192 Attention \u2192 Add\n        norm_x = self.norm1(x)\n        attn_out, _ = self.attention(norm_x, norm_x, norm_x)\n        x = x + attn_out\n\n        # Norm \u2192 FFN \u2192 Add\n        norm_x = self.norm2(x)\n        ffn_out = self.ffn(norm_x)\n        x = x + ffn_out\n        return x\n\nif __name__ == \"__main__\":\n    compare_normalization()\n</code></pre>"},{"location":"fundamentals/attention-advanced/normalization/#_11","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u4e09\u79cd\u5f52\u4e00\u5316\u7684\u6570\u5b66\u539f\u7406\u548c\u8ba1\u7b97\u65b9\u5f0f</li> <li>[ ] \u80fd\u89e3\u91ca\u4e3a\u4ec0\u4e48Transformer\u9009\u62e9LayerNorm\u800c\u975eBatchNorm</li> <li>[ ] \u638c\u63e1Pre-Norm\u76f8\u6bd4Post-Norm\u7684\u8bad\u7ec3\u4f18\u52bf</li> <li>[ ] \u5b8c\u6210\u5f52\u4e00\u5316\u6280\u672f\u7684\u4ee3\u7801\u5b9e\u73b0\u548c\u6027\u80fd\u5bf9\u6bd4</li> </ul>"},{"location":"fundamentals/attention-advanced/normalization/#_12","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aKV Cache\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1a\u4f4d\u7f6e\u7f16\u7801</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/attention-advanced/positional-encoding/","title":"\u4f4d\u7f6e\u7f16\u7801","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4f4d\u7f6e\u7f16\u7801\u5728Transformer\u4e2d\u7684\u4f5c\u7528\uff0c\u638c\u63e1\u4ece\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u5230RoPE\u7684\u6280\u672f\u6f14\u8fdb\uff0c\u80fd\u591f\u63a8\u5bfcRoPE\u7684\u6570\u5b66\u539f\u7406\u3002</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Sinusoidal\u4f4d\u7f6e\u7f16\u7801\u8ffd\u6839\u6eaf\u6e90 - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u535a\u91c7\u4f17\u957f\u7684\u65cb\u8f6c\u5f0f\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u8ba9\u7814\u7a76\u4eba\u5458\u7ede\u5c3d\u8111\u6c41\u7684Transformer\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4</li> </ol>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#_6","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u4f4d\u7f6e\u7f16\u7801\uff1f","text":"<p>\u6838\u5fc3\u95ee\u9898: Transformer\u7684Self-Attention\u673a\u5236\u662f\u7f6e\u6362\u4e0d\u53d8\u7684\uff08permutation invariant\uff09\uff0c\u65e0\u6cd5\u533a\u5206token\u7684\u987a\u5e8f\u3002</p> <pre><code># \u6ca1\u6709\u4f4d\u7f6e\u4fe1\u606f\u65f6\uff0c\u8fd9\u4e24\u4e2a\u5e8f\u5217\u662f\u7b49\u4ef7\u7684\nsequence1 = [\"\u6211\", \"\u7231\", \"\u5317\u4eac\"]\nsequence2 = [\"\u7231\", \"\u5317\u4eac\", \"\u6211\"]\n# Self-Attention\u4f1a\u7ed9\u51fa\u76f8\u540c\u7684\u7ed3\u679c\uff01\n</code></pre> <p>\u89e3\u51b3\u65b9\u6848: \u5728\u8f93\u5165\u4e2d\u6ce8\u5165\u4f4d\u7f6e\u4fe1\u606f\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u7406\u89e3token\u4e4b\u95f4\u7684\u76f8\u5bf9\u6216\u7edd\u5bf9\u4f4d\u7f6e\u5173\u7cfb\u3002</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_7","title":"\u4f4d\u7f6e\u7f16\u7801\u5206\u7c7b","text":"<pre><code>\u4f4d\u7f6e\u7f16\u7801\n\u251c\u2500\u2500 \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 (APE)\n\u2502   \u251c\u2500\u2500 \u53ef\u8bad\u7ec3\u4f4d\u7f6e\u7f16\u7801 (Learned PE)\n\u2502   \u2514\u2500\u2500 \u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801 (Sinusoidal PE)\n\u2514\u2500\u2500 \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 (RPE)\n    \u251c\u2500\u2500 \u7ecf\u5178\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\n    \u251c\u2500\u2500 \u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 (RoPE)\n    \u2514\u2500\u2500 \u5176\u4ed6\u53d8\u4f53 (ALiBi\u7b49)\n</code></pre>"},{"location":"fundamentals/attention-advanced/positional-encoding/#vs","title":"\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 vs \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801","text":"\u7ef4\u5ea6 \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 \u7f16\u7801\u5bf9\u8c61 token\u7684\u7edd\u5bf9\u4f4d\u7f6e token\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb \u64cd\u4f5c\u4f4d\u7f6e \u8f93\u5165\u5c42\u6dfb\u52a0\u4f4d\u7f6e\u5411\u91cf \u6ce8\u610f\u529b\u5c42\u4fee\u6539\u8ba1\u7b97\u65b9\u5f0f \u5b9e\u73b0\u590d\u6742\u5ea6 \u7b80\u5355 \u76f8\u5bf9\u590d\u6742 \u957f\u5ea6\u5916\u63a8 \u8f83\u5dee \u8f83\u597d \u6027\u80fd\u8868\u73b0 \u77ed\u5e8f\u5217\u8db3\u591f \u957f\u5e8f\u5217\u66f4\u4f18"},{"location":"fundamentals/attention-advanced/positional-encoding/#_8","title":"\u6838\u5fc3\u6280\u672f\u8be6\u89e3","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#1-sinusoidal-transformer","title":"1. Sinusoidal\u4f4d\u7f6e\u7f16\u7801 (\u539f\u59cbTransformer)","text":"<p>\u6570\u5b66\u516c\u5f0f: \\(\\(PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}})\\)\\) \\(\\(PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}})\\)\\)</p> <p>\u6838\u5fc3\u7279\u70b9: - \u4f7f\u7528\u6b63\u5f26\u4f59\u5f26\u51fd\u6570\u751f\u6210\u4f4d\u7f6e\u7f16\u7801 - \u4e0d\u540c\u7ef4\u5ea6\u4f7f\u7528\u4e0d\u540c\u7684\u9891\u7387 - \u56fa\u5b9a\u7f16\u7801\uff0c\u4e0d\u9700\u8981\u8bad\u7ec3 - \u7406\u8bba\u4e0a\u652f\u6301\u4efb\u610f\u957f\u5ea6\u5e8f\u5217</p> <p>\u4f18\u52bf: - \u8ba1\u7b97\u7b80\u5355\uff0c\u4e0d\u5360\u7528\u53c2\u6570 - \u5177\u6709\u4e00\u5b9a\u7684\u5916\u63a8\u80fd\u529b - \u76f8\u5bf9\u4f4d\u7f6e\u6709\u4e00\u5b9a\u7684\u89c4\u5f8b\u6027</p> <p>\u7f3a\u70b9: - \u4f4d\u7f6e\u4fe1\u606f\u5728\u6df1\u5c42\u53ef\u80fd\u8870\u51cf - \u5bf9\u76f8\u5bf9\u4f4d\u7f6e\u7684\u5efa\u6a21\u4e0d\u591f\u76f4\u63a5</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#2","title":"2. \u53ef\u8bad\u7ec3\u4f4d\u7f6e\u7f16\u7801","text":"<p>\u5b9e\u73b0\u65b9\u5f0f: <pre><code># \u4e3a\u6bcf\u4e2a\u4f4d\u7f6e\u5b66\u4e60\u4e00\u4e2a\u5411\u91cf\nposition_embeddings = nn.Embedding(max_seq_len, d_model)\npos_emb = position_embeddings(position_ids)\ninput_emb = token_emb + pos_emb\n</code></pre></p> <p>\u7279\u70b9: - \u6bcf\u4e2a\u4f4d\u7f6e\u5bf9\u5e94\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u5411\u91cf - \u901a\u8fc7\u8bad\u7ec3\u4f18\u5316\u4f4d\u7f6e\u8868\u793a - \u5728\u8bad\u7ec3\u957f\u5ea6\u8303\u56f4\u5185\u6548\u679c\u901a\u5e38\u66f4\u597d</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#3-rope","title":"3. RoPE (\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801)","text":"<p>\u6838\u5fc3\u601d\u60f3: \u901a\u8fc7\u65cb\u8f6c\u53d8\u6362\u5c06\u4f4d\u7f6e\u4fe1\u606f\u7f16\u7801\u5230\u67e5\u8be2\u548c\u952e\u5411\u91cf\u4e2d\uff0c\u4f7f\u5f97\u6ce8\u610f\u529b\u5206\u6570\u81ea\u7136\u5730\u4f9d\u8d56\u4e8e\u76f8\u5bf9\u4f4d\u7f6e\u3002</p> <p>\u6570\u5b66\u63a8\u5bfc:</p> <p>\u6b65\u9aa41: \u5c06\u7279\u5f81\u5206\u4e3apairs\uff0c\u6bcf\u5bf9\u7279\u5f81\u770b\u4f5c2D\u5e73\u9762\u7684\u5750\u6807 \\(\\(x = [x_1, x_2, x_3, x_4, ...] \u2192 [(x_1, x_2), (x_3, x_4), ...]\\)\\)</p> <p>\u6b65\u9aa42: \u5bf9\u6bcf\u4e00\u5bf9\u7279\u5f81\u5e94\u7528\u65cb\u8f6c\u77e9\u9635 \\(\\(\\begin{pmatrix} x_{m}^{(1)} \\\\ x_{m}^{(2)} \\end{pmatrix} \u2192 \\begin{pmatrix} \\cos(m\\theta) &amp; -\\sin(m\\theta) \\\\ \\sin(m\\theta) &amp; \\cos(m\\theta) \\end{pmatrix} \\begin{pmatrix} x_{m}^{(1)} \\\\ x_{m}^{(2)} \\end{pmatrix}\\)\\)</p> <p>\u6b65\u9aa43: \u65cb\u8f6c\u540e\u7684\u5411\u91cf \\(\\(\\begin{pmatrix} x_{m}^{(1)} \\cos(m\\theta) - x_{m}^{(2)} \\sin(m\\theta) \\\\ x_{m}^{(2)} \\cos(m\\theta) + x_{m}^{(1)} \\sin(m\\theta) \\end{pmatrix}\\)\\)</p> <p>\u6838\u5fc3\u6027\u8d28: \u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56 \\(\\(\\langle RoPE(q_m), RoPE(k_n) \\rangle = \\langle q_m, k_n \\rangle \\cos((m-n)\\theta) + \\text{\u5176\u4ed6\u9879}\\)\\)</p> <p>\u6ce8\u610f\u529b\u5206\u6570\u53ea\u4f9d\u8d56\u4e8e\u76f8\u5bf9\u8ddd\u79bb (m-n)\uff01</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_9","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#q1","title":"Q1: \u4ec0\u4e48\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff1f","text":"<p>\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 (APE): - \u5b9a\u4e49: \u4e3a\u6bcf\u4e2atoken\u7684\u7edd\u5bf9\u4f4d\u7f6e\u5206\u914d\u4e00\u4e2a\u4f4d\u7f6e\u5411\u91cf - \u5b9e\u73b0: \u5728\u8f93\u5165\u5c42\u5c06\u4f4d\u7f6e\u5411\u91cf\u52a0\u5230token embedding\u4e0a - \u7279\u70b9: \u7b80\u5355\u76f4\u63a5\uff0c\u6bcf\u4e2a\u4f4d\u7f6e\u6709\u56fa\u5b9a\u7684\u7f16\u7801</p> <p>\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 (RPE): - \u5b9a\u4e49: \u5728\u8ba1\u7b97\u6ce8\u610f\u529b\u65f6\u8003\u8651token\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb - \u5b9e\u73b0: \u4fee\u6539\u6ce8\u610f\u529b\u8ba1\u7b97\u516c\u5f0f\uff0c\u52a0\u5165\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e - \u7279\u70b9: \u66f4\u7b26\u5408\u76f4\u89c9\uff0c\u5916\u63a8\u80fd\u529b\u66f4\u5f3a</p> <p>\u6280\u672f\u7ec6\u8282\u5bf9\u6bd4: <pre><code># \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\ninput_emb = token_emb + position_emb[pos]\n\n# \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801  \nattention_score = QK^T + relative_position_bias[i-j]\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#q2-rope","title":"Q2: \u63a8\u5bfcRoPE\u7684\u6570\u5b66\u539f\u7406","text":"<p>\u63a8\u5bfc\u6b65\u9aa4:</p> <p>\u76ee\u6807: \u8bbe\u8ba1\u4e00\u4e2a\u51fd\u6570f\uff0c\u4f7f\u5f97\uff1a \\(\\(\\langle f(q, m), f(k, n) \\rangle = g(q, k, m-n)\\)\\) \u5373\u6ce8\u610f\u529b\u5206\u6570\u53ea\u4f9d\u8d56\u76f8\u5bf9\u4f4d\u7f6e m-n\u3002</p> <p>\u89e3\u51b3\u65b9\u6848: \u590d\u6570\u57df\u7684\u65cb\u8f6c\u53d8\u6362</p> <p>\u6b65\u9aa41: \u5c06\u5b9e\u6570\u5411\u91cf\u6620\u5c04\u5230\u590d\u6570 \\(\\(q_{1} + i q_{2} \u2192 q_{complex}\\)\\)</p> <p>\u6b65\u9aa42: \u5e94\u7528\u590d\u6570\u65cb\u8f6c \\(\\(f(q, m) = q_{complex} \\cdot e^{im\\theta} = q_{complex} \\cdot (\\cos(m\\theta) + i\\sin(m\\theta))\\)\\)</p> <p>\u6b65\u9aa43: \u9a8c\u8bc1\u76f8\u5bf9\u4f4d\u7f6e\u6027\u8d28 \\(\\(\\langle f(q,m), f(k,n) \\rangle^* = \\langle q \\cdot e^{im\\theta}, k \\cdot e^{in\\theta} \\rangle = \\langle q, k \\rangle \\cdot e^{i(m-n)\\theta}\\)\\)</p> <p>\u53ea\u4f9d\u8d56\u4e8e (m-n)\uff01</p> <p>\u6b65\u9aa44: \u8f6c\u6362\u56de\u5b9e\u6570\u57df \\(\\(\\begin{pmatrix} q_1 \\cos(m\\theta) - q_2 \\sin(m\\theta) \\\\ q_1 \\sin(m\\theta) + q_2 \\cos(m\\theta) \\end{pmatrix}\\)\\)</p> <p>\u5173\u952e\u6d1e\u5bdf: \u901a\u8fc7\u65cb\u8f6c\u53d8\u6362\uff0c\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u81ea\u7136\u5730\u7f16\u7801\u5728\u4e86\u5411\u91cf\u7684\u51e0\u4f55\u5173\u7cfb\u4e2d\u3002</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#q3-rope","title":"Q3: RoPE\u76f8\u6bd4\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801\u7684\u4f18\u52bf\uff1f","text":"<p>\u6838\u5fc3\u4f18\u52bf:</p> <ol> <li>\u81ea\u7136\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56</li> <li>\u6ce8\u610f\u529b\u5206\u6570\u76f4\u63a5\u4f9d\u8d56\u76f8\u5bf9\u8ddd\u79bb</li> <li> <p>\u65e0\u9700\u989d\u5916\u7684\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e\u9879</p> </li> <li> <p>\u4f18\u79c0\u7684\u5916\u63a8\u80fd\u529b</p> </li> <li>\u8bad\u7ec3\u65f6\u7684\u76f8\u5bf9\u4f4d\u7f6e\u6a21\u5f0f\u53ef\u4ee5\u6cdb\u5316\u5230\u66f4\u957f\u5e8f\u5217</li> <li> <p>\u7406\u8bba\u4e0a\u652f\u6301\u65e0\u9650\u957f\u5ea6\u5916\u63a8</p> </li> <li> <p>\u8ba1\u7b97\u9ad8\u6548</p> </li> <li>\u65e0\u9700\u5b58\u50a8\u4f4d\u7f6e\u5d4c\u5165\u8868</li> <li> <p>\u65cb\u8f6c\u64cd\u4f5c\u53ef\u4ee5\u9ad8\u6548\u5b9e\u73b0</p> </li> <li> <p>\u7406\u8bba\u4f18\u96c5</p> </li> <li>\u6570\u5b66\u57fa\u7840\u624e\u5b9e</li> <li>\u57fa\u4e8e\u590d\u6570\u65cb\u8f6c\u7684\u51e0\u4f55\u76f4\u89c9</li> </ol> <p>\u5b9e\u9a8c\u9a8c\u8bc1: - \u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801 - \u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u8868\u73b0\u7279\u522b\u7a81\u51fa - \u5df2\u88ab\u591a\u4e2a\u5927\u6a21\u578b\u91c7\u7528(LLaMA\u3001PaLM\u7b49)</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#q4-rope","title":"Q4: RoPE\u5728\u5b9e\u9645\u5b9e\u73b0\u4e2d\u6709\u4ec0\u4e48\u6280\u5de7\uff1f","text":"<p>\u5b9e\u73b0\u4f18\u5316:</p> <ol> <li> <p>\u9891\u7387\u9009\u62e9 <pre><code># \u4e0d\u540c\u7ef4\u5ea6\u4f7f\u7528\u4e0d\u540c\u9891\u7387\ntheta = 10000 ** (-2 * torch.arange(0, dim, 2) / dim)\n</code></pre></p> </li> <li> <p>\u9884\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635 <pre><code># \u907f\u514d\u91cd\u590d\u8ba1\u7b97sin/cos\ncos_cached = torch.cos(position * theta)\nsin_cached = torch.sin(position * theta)\n</code></pre></p> </li> <li> <p>\u5411\u91cf\u5316\u5b9e\u73b0 <pre><code># \u540c\u65f6\u5904\u7406\u6240\u6709\u4f4d\u7f6e\u548c\u7ef4\u5ea6\nq_rot = q * cos_cached - q_shifted * sin_cached\n</code></pre></p> </li> </ol>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_10","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#rope","title":"RoPE\u5b8c\u6574\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass RotaryPositionalEmbedding(nn.Module):\n    \"\"\"RoPE (Rotary Position Embedding) \u5b9e\u73b0\"\"\"\n\n    def __init__(self, dim, max_seq_len=2048, base=10000):\n        super().__init__()\n        self.dim = dim\n        self.max_seq_len = max_seq_len\n        self.base = base\n\n        # \u8ba1\u7b97\u65cb\u8f6c\u9891\u7387\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n        # \u9884\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801\n        self._build_cache(max_seq_len)\n\n    def _build_cache(self, seq_len):\n        \"\"\"\u9884\u8ba1\u7b97\u5e76\u7f13\u5b58\u65cb\u8f6c\u77e9\u9635\"\"\"\n        # \u751f\u6210\u4f4d\u7f6e\u5e8f\u5217\n        position = torch.arange(seq_len).float()\n\n        # \u8ba1\u7b97\u89d2\u5ea6: position * inv_freq\n        freqs = torch.outer(position, self.inv_freq)  # [seq_len, dim//2]\n\n        # \u62fc\u63a5\uff0c\u5f62\u6210\u5b8c\u6574\u7684\u9891\u7387\u77e9\u9635\n        emb = torch.cat([freqs, freqs], dim=-1)  # [seq_len, dim]\n\n        # \u8ba1\u7b97cos\u548csin\n        cos_cached = emb.cos()\n        sin_cached = emb.sin()\n\n        self.register_buffer('cos_cached', cos_cached)\n        self.register_buffer('sin_cached', sin_cached)\n\n    def rotate_half(self, x):\n        \"\"\"\u5c06\u8f93\u5165\u7684\u540e\u534a\u90e8\u5206\u53d6\u8d1f\u53f7\u5e76\u79fb\u5230\u524d\u9762\"\"\"\n        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n        return torch.cat([-x2, x1], dim=-1)\n\n    def forward(self, q, k, seq_len=None):\n        \"\"\"\n        \u5bf9\u67e5\u8be2\u548c\u952e\u5411\u91cf\u5e94\u7528RoPE\n\n        Args:\n            q: \u67e5\u8be2\u77e9\u9635 [batch, heads, seq_len, dim]\n            k: \u952e\u77e9\u9635 [batch, heads, seq_len, dim]\n            seq_len: \u5e8f\u5217\u957f\u5ea6\n        \"\"\"\n        if seq_len is None:\n            seq_len = q.shape[-2]\n\n        # \u5982\u679c\u5e8f\u5217\u957f\u5ea6\u8d85\u51fa\u7f13\u5b58\uff0c\u91cd\u65b0\u6784\u5efa\n        if seq_len &gt; self.max_seq_len:\n            self._build_cache(seq_len)\n\n        # \u83b7\u53d6\u5bf9\u5e94\u957f\u5ea6\u7684cos\u548csin\n        cos = self.cos_cached[:seq_len]  # [seq_len, dim]\n        sin = self.sin_cached[:seq_len]  # [seq_len, dim]\n\n        # \u5e94\u7528\u65cb\u8f6c\u53d8\u6362\n        q_rot = q * cos + self.rotate_half(q) * sin\n        k_rot = k * cos + self.rotate_half(k) * sin\n\n        return q_rot, k_rot\n\nclass MultiHeadAttentionWithRoPE(nn.Module):\n    \"\"\"\u5e26RoPE\u7684\u591a\u5934\u6ce8\u610f\u529b\"\"\"\n\n    def __init__(self, d_model, num_heads, max_seq_len=2048):\n        super().__init__()\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        # RoPE\u53ea\u5e94\u7528\u5230\u90e8\u5206\u7ef4\u5ea6\uff08\u901a\u5e38\u662f\u524d\u534a\u90e8\u5206\uff09\n        self.rope = RotaryPositionalEmbedding(\n            dim=self.head_dim, \n            max_seq_len=max_seq_len\n        )\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u8ba1\u7b97Q, K, V\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # \u8f6c\u7f6e\u4ee5\u7b26\u5408\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u7ef4\u5ea6\u8981\u6c42\n        Q = Q.transpose(1, 2)  # [batch, heads, seq_len, head_dim]\n        K = K.transpose(1, 2)\n        V = V.transpose(1, 2)\n\n        # \u5e94\u7528RoPE\n        Q, K = self.rope(Q, K, seq_len)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        # \u5e94\u7528\u63a9\u7801\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Softmax\u5f52\u4e00\u5316\n        attn_weights = torch.softmax(scores, dim=-1)\n\n        # \u8ba1\u7b97\u8f93\u51fa\n        out = torch.matmul(attn_weights, V)\n\n        # \u91cd\u5851\u5e76\u5408\u5e76\u591a\u5934\n        out = out.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, d_model\n        )\n\n        return self.W_o(out)\n\n# \u4e0d\u540c\u4f4d\u7f6e\u7f16\u7801\u7684\u5bf9\u6bd4\u6d4b\u8bd5\ndef compare_position_encodings():\n    \"\"\"\u5bf9\u6bd4\u4e0d\u540c\u4f4d\u7f6e\u7f16\u7801\u7684\u6548\u679c\"\"\"\n\n    d_model, seq_len = 512, 64\n    batch_size, num_heads = 2, 8\n\n    print(\"=== \u4f4d\u7f6e\u7f16\u7801\u5bf9\u6bd4\u6d4b\u8bd5 ===\")\n\n    # \u6d4b\u8bd5\u6570\u636e\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # 1. \u65e0\u4f4d\u7f6e\u7f16\u7801\u7684\u6ce8\u610f\u529b\n    attn_no_pos = MultiHeadAttentionWithRoPE(d_model, num_heads)\n    # \u4e34\u65f6\u79fb\u9664RoPE\n    attn_no_pos.rope = lambda q, k, seq_len: (q, k)\n    out_no_pos = attn_no_pos(x)\n\n    # 2. \u5e26RoPE\u7684\u6ce8\u610f\u529b\n    attn_with_rope = MultiHeadAttentionWithRoPE(d_model, num_heads)\n    out_with_rope = attn_with_rope(x)\n\n    print(f\"\u65e0\u4f4d\u7f6e\u7f16\u7801\u8f93\u51fa\u6807\u51c6\u5dee: {out_no_pos.std():.4f}\")\n    print(f\"RoPE\u4f4d\u7f6e\u7f16\u7801\u8f93\u51fa\u6807\u51c6\u5dee: {out_with_rope.std():.4f}\")\n\n    # 3. \u6d4b\u8bd5\u5916\u63a8\u80fd\u529b\n    print(\"\\n=== \u5916\u63a8\u80fd\u529b\u6d4b\u8bd5 ===\")\n\n    # \u77ed\u5e8f\u5217\u8bad\u7ec3\n    short_len = 32\n    x_short = torch.randn(1, short_len, d_model)\n\n    # \u957f\u5e8f\u5217\u63a8\u7406\n    long_len = 128\n    x_long = torch.randn(1, long_len, d_model)\n\n    try:\n        out_short = attn_with_rope(x_short)\n        out_long = attn_with_rope(x_long)\n        print(f\"\u77ed\u5e8f\u5217({short_len})\u5904\u7406\u6210\u529f\")\n        print(f\"\u957f\u5e8f\u5217({long_len})\u5904\u7406\u6210\u529f - RoPE\u652f\u6301\u5916\u63a8\")\n    except Exception as e:\n        print(f\"\u5916\u63a8\u5931\u8d25: {e}\")\n\n# \u624b\u52a8\u9a8c\u8bc1RoPE\u7684\u76f8\u5bf9\u4f4d\u7f6e\u6027\u8d28\ndef verify_rope_property():\n    \"\"\"\u9a8c\u8bc1RoPE\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56\u6027\u8d28\"\"\"\n\n    print(\"=== \u9a8c\u8bc1RoPE\u76f8\u5bf9\u4f4d\u7f6e\u6027\u8d28 ===\")\n\n    dim = 64\n    rope = RotaryPositionalEmbedding(dim, max_seq_len=10)\n\n    # \u521b\u5efa\u4e24\u4e2a\u4f4d\u7f6e\u7684\u67e5\u8be2\u548c\u952e\n    q = torch.randn(1, 1, 1, dim)  # \u4f4d\u7f6e0\u7684\u67e5\u8be2\n    k = torch.randn(1, 1, 1, dim)  # \u4f4d\u7f6e0\u7684\u952e\n\n    # \u5728\u4e0d\u540c\u76f8\u5bf9\u8ddd\u79bb\u4e0b\u6d4b\u8bd5\n    distances = [1, 2, 3]\n\n    for dist in distances:\n        # \u8ba1\u7b97\u4f4d\u7f6e(0, dist)\u7684\u76f8\u5bf9\u6ce8\u610f\u529b\n        q_pos0, k_pos_dist = rope(q, k, seq_len=dist+1)\n        score1 = torch.matmul(q_pos0[:,:,0:1], k_pos_dist[:,:,dist:dist+1].transpose(-2,-1))\n\n        # \u8ba1\u7b97\u4f4d\u7f6e(1, 1+dist)\u7684\u76f8\u5bf9\u6ce8\u610f\u529b  \n        q_pos1, k_pos1_dist = rope(q, k, seq_len=dist+2)\n        score2 = torch.matmul(q_pos1[:,:,1:2], k_pos1_dist[:,:,1+dist:2+dist].transpose(-2,-1))\n\n        print(f\"\u76f8\u5bf9\u8ddd\u79bb{dist}: \u5206\u6570\u5dee\u5f02 = {abs(score1.item() - score2.item()):.6f}\")\n\nif __name__ == \"__main__\":\n    compare_position_encodings()\n    print()\n    verify_rope_property()\n</code></pre>"},{"location":"fundamentals/attention-advanced/positional-encoding/#sinusoidal","title":"Sinusoidal\u4f4d\u7f6e\u7f16\u7801\u5b9e\u73b0","text":"<pre><code>class SinusoidalPositionalEncoding(nn.Module):\n    \"\"\"\u539f\u59cbTransformer\u7684\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\"\"\"\n\n    def __init__(self, d_model, max_seq_len=5000):\n        super().__init__()\n\n        pe = torch.zeros(max_seq_len, d_model)\n        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        return x + self.pe[:, :seq_len]\n</code></pre>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_11","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u548c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u7684\u533a\u522b</li> <li>[ ] \u80fd\u63a8\u5bfcRoPE\u7684\u6570\u5b66\u539f\u7406</li> <li>[ ] \u638c\u63e1RoPE\u7684\u5b9e\u73b0\u7ec6\u8282\u548c\u4f18\u5316\u6280\u5de7</li> <li>[ ] \u5b8c\u6210\u4f4d\u7f6e\u7f16\u7801\u7684\u4ee3\u7801\u5b9e\u73b0\u548c\u6548\u679c\u9a8c\u8bc1</li> </ul>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_12","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u5f52\u4e00\u5316\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1aLLM\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/deepseek-innovations/","title":"\u7b2c4\u8282\uff1aDeepSeek\u6838\u5fc3\u4f18\u5316\u6280\u672f","text":""},{"location":"fundamentals/deepseek-innovations/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3DeepSeek\u56e2\u961f\u63d0\u51fa\u7684\u4e09\u5927\u6838\u5fc3\u6280\u672f\u521b\u65b0\uff0c\u638c\u63e1\u8fd9\u4e9b\u524d\u6cbf\u4f18\u5316\u6280\u672f\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - MLA\u5982\u4f55\u5b9e\u73b010\u500d\u4ee5\u4e0a\u7684KV Cache\u538b\u7f29\uff1f - DeepSeek MoE\u7684\u521b\u65b0\u8def\u7531\u673a\u5236 - MTP\u5982\u4f55\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff1f</p>"},{"location":"fundamentals/deepseek-innovations/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a3\u5929</p> <ul> <li>Day 1: MLA\u6838\u5fc3\u6280\u672f\u6df1\u5ea6\u89e3\u6790 (\u4f4e\u79e9\u538b\u7f29+RoPE\u89e3\u8026+\u6743\u91cd\u5438\u6536)</li> <li>Day 2: DeepSeek MoE\u521b\u65b0\u6280\u672f (\u7ec6\u7c92\u5ea6\u8def\u7531+\u5171\u4eab\u4e13\u5bb6\u673a\u5236)</li> <li>Day 3: MTP\u591atoken\u9884\u6d4b\u6280\u672f + \u4e09\u5927\u6280\u672f\u7efc\u5408\u5bf9\u6bd4\u5206\u6790</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#deepseek","title":"\ud83c\udfc6 DeepSeek\u6280\u672f\u521b\u65b0\u6982\u89c8","text":"<p>DeepSeek\u4f5c\u4e3a\u56fd\u5185\u9886\u5148\u7684AI\u516c\u53f8\uff0c\u5728\u5927\u6a21\u578b\u4f18\u5316\u65b9\u9762\u63d0\u51fa\u4e86\u4e09\u9879\u9769\u547d\u6027\u6280\u672f\uff1a</p>"},{"location":"fundamentals/deepseek-innovations/#_3","title":"\u6838\u5fc3\u6280\u672f\u6808","text":"<pre><code>DeepSeek\u521b\u65b0\u6280\u672f\u4f53\u7cfb\n\u251c\u2500\u2500 MLA (Multi-head Latent Attention)\n\u2502   \u251c\u2500\u2500 \u4f4e\u79e9KV\u8054\u5408\u538b\u7f29\n\u2502   \u251c\u2500\u2500 RoPE\u89e3\u8026\u673a\u5236  \n\u2502   \u2514\u2500\u2500 \u6743\u91cd\u5438\u6536\u4f18\u5316\n\u251c\u2500\u2500 DeepSeek MoE\n\u2502   \u251c\u2500\u2500 \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1\n\u2502   \u251c\u2500\u2500 \u5171\u4eab\u4e13\u5bb6\u673a\u5236\n\u2502   \u2514\u2500\u2500 \u591a\u7ea7\u8d1f\u8f7d\u5747\u8861\n\u2514\u2500\u2500 MTP (Multi-Token Prediction)\n    \u251c\u2500\u2500 \u5e76\u884c\u591atoken\u9884\u6d4b\n    \u251c\u2500\u2500 \u72ec\u7acb\u9884\u6d4b\u5934\u8bbe\u8ba1\n    \u2514\u2500\u2500 \u5bc6\u96c6\u76d1\u7763\u4fe1\u53f7\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/#_4","title":"\u6280\u672f\u5f71\u54cd\u529b","text":"\u6280\u672f \u6838\u5fc3\u521b\u65b0 \u6027\u80fd\u63d0\u5347 \u5e94\u7528\u6a21\u578b MLA KV Cache\u538b\u7f29 \u5185\u5b58\u51cf\u5c1110-20\u00d7 DeepSeek-V2/V3 DeepSeek MoE \u7ec6\u7c92\u5ea6\u8def\u7531 \u53c2\u6570\u6548\u7387\u63d0\u5347 DeepSeek-V2/V3 MTP \u591atoken\u9884\u6d4b \u8bad\u7ec3\u6548\u7387\u63d0\u5347 DeepSeek-V3"},{"location":"fundamentals/deepseek-innovations/#_5","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/deepseek-innovations/#1-mla","title":"1. MLA\u6838\u5fc3\u6280\u672f","text":"<ul> <li>\u4f4e\u79e9\u538b\u7f29\u7684\u6570\u5b66\u539f\u7406</li> <li>RoPE\u89e3\u8026\u7684\u6280\u672f\u7ec6\u8282</li> <li>\u6743\u91cd\u5438\u6536\u7684\u5de5\u7a0b\u4f18\u5316</li> <li>\u4e0e\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u6bd4</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#2-deepseek-moe","title":"2. DeepSeek MoE\u521b\u65b0","text":"<ul> <li>\u7ec6\u7c92\u5ea6\u4e13\u5bb6vs\u7c97\u7c92\u5ea6\u4e13\u5bb6</li> <li>\u5171\u4eab\u4e13\u5bb6\u7684\u8bbe\u8ba1\u7406\u5ff5</li> <li>\u591a\u7ea7\u8d1f\u8f7d\u5747\u8861\u7b56\u7565</li> <li>\u8def\u7531\u673a\u5236\u7684\u6f14\u8fdb\u5386\u7a0b</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#3-mtptoken","title":"3. MTP\u591atoken\u9884\u6d4b","text":"<ul> <li>\u5e76\u884c\u9884\u6d4b\u7684\u67b6\u6784\u8bbe\u8ba1</li> <li>\u591a\u5934\u9884\u6d4b\u7684\u8bad\u7ec3\u7b56\u7565</li> <li>\u5bc6\u96c6\u76d1\u7763\u4fe1\u53f7\u7684\u4f5c\u7528</li> <li>\u4e0e\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u6bd4</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#_6","title":"\ud83d\udd2c \u6280\u672f\u6df1\u5ea6\u5206\u6790","text":""},{"location":"fundamentals/deepseek-innovations/#_7","title":"\u521b\u65b0\u52a8\u673a","text":"<ol> <li>\u5185\u5b58\u74f6\u9888: \u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u5e8f\u5217\u4e0a\u7684\u5185\u5b58\u5f00\u9500\u8fc7\u5927</li> <li>\u8ba1\u7b97\u6548\u7387: MOE\u6a21\u578b\u7684\u8d1f\u8f7d\u5747\u8861\u548c\u4e13\u5bb6\u5229\u7528\u7387\u95ee\u9898</li> <li>\u8bad\u7ec3\u6548\u7387: \u4f20\u7edfnext-token\u9884\u6d4b\u7684\u4fe1\u606f\u5bc6\u5ea6\u4e0d\u8db3</li> </ol>"},{"location":"fundamentals/deepseek-innovations/#_8","title":"\u89e3\u51b3\u65b9\u6848","text":"<ol> <li>MLA: \u901a\u8fc7\u4f4e\u79e9\u538b\u7f29\u548c\u89e3\u8026\u8bbe\u8ba1\u5b9e\u73b0\u5185\u5b58\u4f18\u5316</li> <li>DeepSeek MoE: \u901a\u8fc7\u7ec6\u7c92\u5ea6\u8def\u7531\u548c\u5171\u4eab\u4e13\u5bb6\u63d0\u5347\u6548\u7387</li> <li>MTP: \u901a\u8fc7\u591atoken\u9884\u6d4b\u589e\u52a0\u8bad\u7ec3\u4fe1\u53f7\u5bc6\u5ea6</li> </ol>"},{"location":"fundamentals/deepseek-innovations/#_9","title":"\u534f\u540c\u6548\u5e94","text":"<p>\u8fd9\u4e09\u9879\u6280\u672f\u5728DeepSeek\u6a21\u578b\u4e2d\u534f\u540c\u5de5\u4f5c\uff1a - MLA\u964d\u4f4e\u63a8\u7406\u5185\u5b58\u9700\u6c42 - DeepSeek MoE\u63d0\u4f9b\u53c2\u6570\u89c4\u6a21\u6269\u5c55\u80fd\u529b - MTP\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u8d28\u91cf</p>"},{"location":"fundamentals/deepseek-innovations/#_10","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u9879\u76ee\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u6280\u672f\u7406\u89e3: \u80fd\u6e05\u6670\u89e3\u91ca\u6bcf\u9879\u6280\u672f\u7684\u6838\u5fc3\u539f\u7406</li> <li>\u5bf9\u6bd4\u5206\u6790: \u80fd\u8bf4\u660e\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52bf\u5bf9\u6bd4</li> <li>\u5e94\u7528\u573a\u666f: \u7406\u89e3\u8fd9\u4e9b\u6280\u672f\u7684\u9002\u7528\u573a\u666f\u548c\u9650\u5236</li> <li>\u5de5\u7a0b\u5b9e\u73b0: \u4e86\u89e3\u5173\u952e\u7684\u5b9e\u73b0\u7ec6\u8282\u548c\u5de5\u7a0b\u6311\u6218</li> </ol>"},{"location":"fundamentals/deepseek-innovations/#_11","title":"\ud83c\udf1f \u4e3a\u4ec0\u4e48\u8fd9\u4e9b\u6280\u672f\u91cd\u8981\uff1f","text":""},{"location":"fundamentals/deepseek-innovations/#1","title":"1. \u6280\u672f\u9886\u5148\u6027","text":"<ul> <li>\u4ee3\u8868\u4e86\u5f53\u524d\u5927\u6a21\u578b\u4f18\u5316\u7684\u6700\u524d\u6cbf\u6280\u672f</li> <li>\u5f88\u591a\u6982\u5ff5\u548c\u65b9\u6cd5\u88ab\u540e\u7eed\u7814\u7a76\u5e7f\u6cdb\u91c7\u7528</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#2","title":"2. \u5b9e\u7528\u4ef7\u503c","text":"<ul> <li>\u5df2\u5728\u771f\u5b9e\u7684\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u5f97\u5230\u9a8c\u8bc1</li> <li>\u4e3a\u5de5\u4e1a\u7ea7\u5927\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#3","title":"3. \u9762\u8bd5\u70ed\u70b9","text":"<ul> <li>\u56fd\u5185AI\u516c\u53f8\u9762\u8bd5\u7684\u9ad8\u9891\u8003\u70b9</li> <li>\u4f53\u73b0\u5bf9\u524d\u6cbf\u6280\u672f\u7684\u7406\u89e3\u548c\u5173\u6ce8</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#4","title":"4. \u53d1\u5c55\u8d8b\u52bf","text":"<ul> <li>\u6307\u5f15\u4e86\u5927\u6a21\u578b\u4f18\u5316\u7684\u91cd\u8981\u65b9\u5411</li> <li>\u4e3a\u540e\u7eed\u6280\u672f\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#_12","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u611f\u5174\u8da3\u7684\u6280\u672f\u6a21\u5757\u6df1\u5165\u5b66\u4e60\u3002\u5efa\u8bae\u6309\u987a\u5e8f\u5b66\u4e60\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6280\u672f\u4e4b\u95f4\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002\u6bcf\u4e2a\u6a21\u5757\u90fd\u5305\u542b\u8be6\u7ec6\u7684\u6280\u672f\u539f\u7406\u3001\u4ee3\u7801\u5b9e\u73b0\u548c\u9762\u8bd5\u95ee\u7b54\u3002</p>"},{"location":"fundamentals/deepseek-innovations/#_13","title":"\ud83c\udf93 \u5b66\u4e60\u5efa\u8bae","text":"<ol> <li>\u91cd\u70b9\u7406\u89e3\u539f\u7406: \u4e0d\u8981\u4ec5\u4ec5\u8bb0\u4f4f\u7ed3\u8bba\uff0c\u8981\u7406\u89e3\u4e3a\u4ec0\u4e48\u8fd9\u6837\u8bbe\u8ba1</li> <li>\u5bf9\u6bd4\u4f20\u7edf\u65b9\u6cd5: \u901a\u8fc7\u5bf9\u6bd4\u52a0\u6df1\u5bf9\u521b\u65b0\u70b9\u7684\u7406\u89e3</li> <li>\u5173\u6ce8\u5de5\u7a0b\u7ec6\u8282: \u8fd9\u4e9b\u6280\u672f\u7684\u6210\u529f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5de5\u7a0b\u5b9e\u73b0</li> <li>\u601d\u8003\u5e94\u7528\u573a\u666f: \u8003\u8651\u8fd9\u4e9b\u6280\u672f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027</li> </ol> <p>\u51c6\u5907\u597d\u6df1\u5165\u63a2\u7d22\u8fd9\u4e9b\u9769\u547d\u6027\u7684\u6280\u672f\u521b\u65b0\u4e86\u5417\uff1f</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/","title":"DeepSeek MoE\u521b\u65b0","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3DeepSeek\u5728\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u4e0a\u7684\u521b\u65b0\u8bbe\u8ba1\uff0c\u638c\u63e1\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u548c\u5171\u4eab\u4e13\u5bb6\u7684\u6838\u5fc3\u7406\u5ff5\u3002</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_2","title":"\ud83d\udcdd \u6280\u672f\u521b\u65b0\u89e3\u6790","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#deepseek-moe_1","title":"DeepSeek MoE\u6f14\u8fdb\u5386\u7a0b","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_3","title":"\u7248\u672c\u6f14\u8fdb","text":"<pre><code>DeepSeek MoE\u6280\u672f\u6f14\u8fdb\n\u251c\u2500\u2500 DeepSeek V1 (2023)\n\u2502   \u251c\u2500\u2500 \u57fa\u7840MoE\u67b6\u6784\n\u2502   \u2514\u2500\u2500 \u6807\u51c6Token-Choice\u8def\u7531\n\u251c\u2500\u2500 DeepSeek V2 (2024)\n\u2502   \u251c\u2500\u2500 \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1\n\u2502   \u251c\u2500\u2500 \u5171\u4eab\u4e13\u5bb6\u673a\u5236\n\u2502   \u2514\u2500\u2500 \u591a\u7ea7\u8d1f\u8f7d\u5747\u8861\n\u2514\u2500\u2500 DeepSeek V3 (2024)\n    \u251c\u2500\u2500 \u4f18\u5316\u8def\u7531\u7b56\u7565\n    \u251c\u2500\u2500 \u52a8\u6001\u4e13\u5bb6\u5bb9\u91cf\n    \u2514\u2500\u2500 \u66f4\u7cbe\u7ec6\u7684\u8d1f\u8f7d\u63a7\u5236\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_4","title":"\u6838\u5fc3\u521b\u65b0\u6280\u672f","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#1","title":"1. \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1","text":"<p>\u4f20\u7edf\u95ee\u9898: \u7c97\u7c92\u5ea6\u4e13\u5bb6\u5bfc\u81f4\u7684\u4e13\u4e1a\u5316\u4e0d\u8db3</p> <p>DeepSeek\u89e3\u51b3\u65b9\u6848: \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u5206\u5de5</p> <pre><code># \u4f20\u7edf\u7c97\u7c92\u5ea6\u4e13\u5bb6\nclass CoarseGrainedExpert(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.ffn = FeedForward(d_model, d_ff)  # \u5b8c\u6574FFN\n\n    def forward(self, x):\n        return self.ffn(x)\n\n# DeepSeek\u7ec6\u7c92\u5ea6\u4e13\u5bb6\nclass FineGrainedExpert(nn.Module):\n    def __init__(self, d_model, d_ff, expert_type='gate', shared_gate=None):\n        super().__init__()\n        self.expert_type = expert_type\n\n        if expert_type == 'gate':\n            # \u95e8\u63a7\u4e13\u5bb6\uff1a\u53ea\u8d1f\u8d23\u95e8\u63a7\u6fc0\u6d3b\n            self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n        elif expert_type == 'up':\n            # \u4e0a\u6295\u5f71\u4e13\u5bb6\uff1a\u8d1f\u8d23\u7279\u5f81\u63d0\u53d6\n            self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n        elif expert_type == 'down':\n            # \u4e0b\u6295\u5f71\u4e13\u5bb6\uff1a\u8d1f\u8d23\u8f93\u51fa\u6295\u5f71\n            self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n\n        self.shared_gate = shared_gate\n\n    def forward(self, x):\n        if self.expert_type == 'gate':\n            return self.gate_proj(x)\n        elif self.expert_type == 'up':\n            return self.up_proj(x)\n        elif self.expert_type == 'down':\n            return self.down_proj(x)\n</code></pre> <p>\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u7684\u4f18\u52bf: - \u66f4\u7cbe\u7ec6\u7684\u4e13\u4e1a\u5316: \u6bcf\u79cd\u64cd\u4f5c\u7c7b\u578b\u90fd\u6709\u4e13\u95e8\u7684\u4e13\u5bb6 - \u66f4\u597d\u7684\u53c2\u6570\u5229\u7528: \u907f\u514d\u4e86\u4e13\u5bb6\u5185\u90e8\u7684\u5197\u4f59 - \u7075\u6d3b\u7684\u7ec4\u5408: \u53ef\u4ee5\u52a8\u6001\u7ec4\u5408\u4e0d\u540c\u7c7b\u578b\u7684\u4e13\u5bb6</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#2","title":"2. \u5171\u4eab\u4e13\u5bb6\u673a\u5236","text":"<p>\u8bbe\u8ba1\u7406\u5ff5: \u90e8\u5206\u77e5\u8bc6\u5bf9\u6240\u6709\u8f93\u5165\u90fd\u6709\u7528\uff0c\u5e94\u8be5\u88ab\u5171\u4eab</p> <pre><code>class DeepSeekMoELayer(nn.Module):\n    \"\"\"DeepSeek MoE\u5c42\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, num_experts, num_shared_experts, \n                 expert_capacity, d_ff):\n        super().__init__()\n\n        # \u5171\u4eab\u4e13\u5bb6\uff1a\u59cb\u7ec8\u6fc0\u6d3b\n        self.shared_experts = nn.ModuleList([\n            FeedForward(d_model, d_ff) \n            for _ in range(num_shared_experts)\n        ])\n\n        # \u8def\u7531\u4e13\u5bb6\uff1a\u52a8\u6001\u9009\u62e9\n        self.routed_experts = nn.ModuleList([\n            FeedForward(d_model, d_ff)\n            for _ in range(num_experts)\n        ])\n\n        # \u8def\u7531\u7f51\u7edc\n        self.router = Router(d_model, num_experts)\n\n        # \u4e13\u5bb6\u914d\u7f6e\n        self.num_experts = num_experts\n        self.num_shared_experts = num_shared_experts\n        self.expert_capacity = expert_capacity\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n\n        # 1. \u5171\u4eab\u4e13\u5bb6\u5904\u7406\uff08\u59cb\u7ec8\u6fc0\u6d3b\uff09\n        shared_output = torch.zeros_like(x)\n        for shared_expert in self.shared_experts:\n            shared_output += shared_expert(x) / self.num_shared_experts\n\n        # 2. \u8def\u7531\u4e13\u5bb6\u5904\u7406\uff08\u52a8\u6001\u9009\u62e9\uff09\n        router_probs, expert_indices = self.router(x)\n        routed_output = self.route_to_experts(x, router_probs, expert_indices)\n\n        # 3. \u7ec4\u5408\u5171\u4eab\u548c\u8def\u7531\u8f93\u51fa\n        final_output = shared_output + routed_output\n\n        return final_output\n\n    def route_to_experts(self, x, probs, indices):\n        \"\"\"\u8def\u7531\u5230\u4e13\u5bb6\u7684\u8be6\u7ec6\u5b9e\u73b0\"\"\"\n        output = torch.zeros_like(x)\n\n        # Token-choice\u8def\u7531\u7b56\u7565\n        for i in range(2):  # Top-2\u8def\u7531\n            expert_idx = indices[:, :, i]\n            expert_prob = probs[:, :, i]\n\n            # \u4e13\u5bb6\u5bb9\u91cf\u9650\u5236\n            expert_tokens = self.apply_capacity_limit(x, expert_idx, expert_prob)\n\n            # \u4e13\u5bb6\u5904\u7406\n            for expert_id in range(self.num_experts):\n                mask = (expert_idx == expert_id)\n                if mask.any():\n                    expert_input = x[mask]\n                    expert_output = self.routed_experts[expert_id](expert_input)\n                    output[mask] += expert_prob[mask].unsqueeze(-1) * expert_output\n\n        return output\n</code></pre> <p>\u5171\u4eab\u4e13\u5bb6\u7684\u4f5c\u7528: - \u901a\u7528\u77e5\u8bc6: \u5b58\u50a8\u5bf9\u6240\u6709\u8f93\u5165\u90fd\u6709\u7528\u7684\u57fa\u7840\u77e5\u8bc6 - \u7a33\u5b9a\u57fa\u7ebf: \u4e3a\u6a21\u578b\u63d0\u4f9b\u7a33\u5b9a\u7684\u57fa\u7840\u8f93\u51fa - \u8d1f\u8f7d\u5206\u62c5: \u51cf\u8f7b\u8def\u7531\u4e13\u5bb6\u7684\u8d1f\u62c5</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#3","title":"3. \u591a\u7ea7\u8d1f\u8f7d\u5747\u8861","text":"<p>\u6311\u6218: \u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a</p> <p>DeepSeek\u7684\u591a\u7ea7\u89e3\u51b3\u65b9\u6848:</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_5","title":"\u8bbe\u5907\u7ea7\u8d1f\u8f7d\u5747\u8861","text":"<pre><code>def device_level_load_balancing(expert_assignments, world_size):\n    \"\"\"\u8bbe\u5907\u7ea7\u522b\u7684\u8d1f\u8f7d\u5747\u8861\"\"\"\n\n    # \u7edf\u8ba1\u6bcf\u4e2a\u8bbe\u5907\u4e0a\u7684token\u6570\u91cf\n    device_loads = torch.zeros(world_size)\n\n    for expert_id, tokens in expert_assignments.items():\n        device_id = expert_id % world_size\n        device_loads[device_id] += len(tokens)\n\n    # \u8ba1\u7b97\u8d1f\u8f7d\u5747\u8861\u635f\u5931\n    ideal_load = device_loads.sum() / world_size\n    load_variance = torch.var(device_loads)\n\n    device_balance_loss = load_variance / (ideal_load ** 2)\n\n    return device_balance_loss\n\ndef expert_level_load_balancing(router_probs, expert_indices):\n    \"\"\"\u4e13\u5bb6\u7ea7\u522b\u7684\u8d1f\u8f7d\u5747\u8861\"\"\"\n\n    num_experts = router_probs.size(-1)\n\n    # \u8ba1\u7b97\u6bcf\u4e2a\u4e13\u5bb6\u7684\u9009\u62e9\u9891\u7387\n    expert_frequencies = torch.zeros(num_experts)\n    for expert_id in range(num_experts):\n        expert_frequencies[expert_id] = (expert_indices == expert_id).float().sum()\n\n    # \u7406\u60f3\u9891\u7387\n    total_selections = expert_indices.numel()\n    ideal_frequency = total_selections / num_experts\n\n    # \u8ba1\u7b97\u5747\u8861\u635f\u5931\n    expert_balance_loss = torch.var(expert_frequencies) / (ideal_frequency ** 2)\n\n    return expert_balance_loss\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_6","title":"\u52a8\u6001\u5bb9\u91cf\u8c03\u6574","text":"<pre><code>class DynamicCapacityRouter(nn.Module):\n    \"\"\"\u52a8\u6001\u5bb9\u91cf\u8c03\u6574\u8def\u7531\u5668\"\"\"\n\n    def __init__(self, d_model, num_experts, base_capacity_factor=1.25):\n        super().__init__()\n        self.gate = nn.Linear(d_model, num_experts)\n        self.base_capacity_factor = base_capacity_factor\n        self.expert_utilization = torch.ones(num_experts)  # \u4e13\u5bb6\u5229\u7528\u7387\u8ddf\u8e2a\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n        num_tokens = batch_size * seq_len\n\n        # \u8ba1\u7b97\u57fa\u7840\u5bb9\u91cf\n        base_capacity = int(self.base_capacity_factor * num_tokens / self.num_experts)\n\n        # \u6839\u636e\u5386\u53f2\u5229\u7528\u7387\u52a8\u6001\u8c03\u6574\u5bb9\u91cf\n        adjusted_capacities = []\n        for expert_id in range(self.num_experts):\n            utilization = self.expert_utilization[expert_id]\n\n            if utilization &lt; 0.5:  # \u5229\u7528\u7387\u4f4e\uff0c\u51cf\u5c11\u5bb9\u91cf\n                adjusted_capacity = int(base_capacity * 0.8)\n            elif utilization &gt; 1.5:  # \u5229\u7528\u7387\u9ad8\uff0c\u589e\u52a0\u5bb9\u91cf\n                adjusted_capacity = int(base_capacity * 1.2)\n            else:\n                adjusted_capacity = base_capacity\n\n            adjusted_capacities.append(adjusted_capacity)\n\n        # \u8def\u7531\u8ba1\u7b97\n        logits = self.gate(x)\n        return self.route_with_dynamic_capacity(x, logits, adjusted_capacities)\n\n    def update_utilization(self, expert_assignments):\n        \"\"\"\u66f4\u65b0\u4e13\u5bb6\u5229\u7528\u7387\u7edf\u8ba1\"\"\"\n        current_utilization = torch.zeros(self.num_experts)\n\n        for expert_id, tokens in expert_assignments.items():\n            current_utilization[expert_id] = len(tokens)\n\n        # \u6307\u6570\u79fb\u52a8\u5e73\u5747\u66f4\u65b0\n        alpha = 0.1\n        self.expert_utilization = (1 - alpha) * self.expert_utilization + alpha * current_utilization\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#4","title":"4. \u8def\u7531\u7b56\u7565\u4f18\u5316","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#expert-choice-vs-token-choice","title":"Expert-Choice vs Token-Choice\u6df7\u5408\u8def\u7531","text":"<pre><code>class HybridRouter(nn.Module):\n    \"\"\"\u6df7\u5408\u8def\u7531\u7b56\u7565\"\"\"\n\n    def __init__(self, d_model, num_experts):\n        super().__init__()\n        self.gate = nn.Linear(d_model, num_experts)\n        self.routing_strategy = 'adaptive'  # adaptive, token_choice, expert_choice\n\n    def forward(self, x):\n        logits = self.gate(x)\n\n        if self.routing_strategy == 'token_choice':\n            return self.token_choice_routing(x, logits)\n        elif self.routing_strategy == 'expert_choice':\n            return self.expert_choice_routing(x, logits)\n        else:  # adaptive\n            return self.adaptive_routing(x, logits)\n\n    def adaptive_routing(self, x, logits):\n        \"\"\"\u81ea\u9002\u5e94\u8def\u7531\u7b56\u7565\"\"\"\n        batch_size, seq_len, _ = x.shape\n\n        # \u6839\u636e\u8d1f\u8f7d\u60c5\u51b5\u52a8\u6001\u9009\u62e9\u8def\u7531\u7b56\u7565\n        current_load = self.estimate_current_load()\n\n        if current_load &gt; 0.8:  # \u9ad8\u8d1f\u8f7d\u65f6\u4f7f\u7528expert-choice\n            return self.expert_choice_routing(x, logits)\n        else:  # \u4f4e\u8d1f\u8f7d\u65f6\u4f7f\u7528token-choice\n            return self.token_choice_routing(x, logits)\n\n    def estimate_current_load(self):\n        \"\"\"\u4f30\u8ba1\u5f53\u524d\u7cfb\u7edf\u8d1f\u8f7d\"\"\"\n        # \u7b80\u5316\u5b9e\u73b0\uff1a\u57fa\u4e8e\u5386\u53f2\u7edf\u8ba1\n        return 0.6  # \u5360\u4f4d\u7b26\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#deepseek-moe_2","title":"DeepSeek MoE\u67b6\u6784\u56fe","text":"<pre><code>                 \u8f93\u5165Token\n                     \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                 \u2502\n       \u5171\u4eab\u4e13\u5bb6           \u8def\u7531\u7f51\u7edc\n      (\u59cb\u7ec8\u6fc0\u6d3b)           \u2502\n            \u2502               \u2502\n            \u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502        \u2502           \u2502\n            \u2502     Top-K        \u5bb9\u91cf\n            \u2502     \u9009\u62e9         \u9650\u5236\n            \u2502        \u2502           \u2502\n            \u2502     \u8def\u7531\u4e13\u5bb6     \u8d1f\u8f7d\n            \u2502    (\u52a8\u6001\u9009\u62e9)    \u5747\u8861\n            \u2502        \u2502           \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                  \u6700\u7ec8\u8f93\u51fa\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#q1-deepseek-moemoe","title":"Q1: DeepSeek MoE\u76f8\u6bd4\u4f20\u7edfMoE\u6709\u4ec0\u4e48\u521b\u65b0\uff1f","text":"<p>\u6838\u5fc3\u521b\u65b0:</p> <ol> <li>\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1: \u5c06FFN\u5206\u89e3\u4e3a\u66f4\u4e13\u4e1a\u5316\u7684\u7ec4\u4ef6</li> <li>\u5171\u4eab\u4e13\u5bb6\u673a\u5236: \u90e8\u5206\u4e13\u5bb6\u59cb\u7ec8\u6fc0\u6d3b\uff0c\u63d0\u4f9b\u7a33\u5b9a\u57fa\u7ebf</li> <li>\u591a\u7ea7\u8d1f\u8f7d\u5747\u8861: \u8bbe\u5907\u7ea7\u548c\u4e13\u5bb6\u7ea7\u7684\u53cc\u91cd\u5747\u8861\u7b56\u7565</li> <li>\u52a8\u6001\u5bb9\u91cf\u8c03\u6574: \u6839\u636e\u4e13\u5bb6\u5229\u7528\u7387\u52a8\u6001\u8c03\u6574\u5bb9\u91cf</li> </ol>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#q2","title":"Q2: \u5171\u4eab\u4e13\u5bb6\u673a\u5236\u6709\u4ec0\u4e48\u597d\u5904\uff1f","text":"<p>\u4e3b\u8981\u4f18\u52bf: - \u77e5\u8bc6\u5171\u4eab: \u901a\u7528\u77e5\u8bc6\u4e0d\u9700\u8981\u5728\u6bcf\u4e2a\u4e13\u5bb6\u4e2d\u91cd\u590d - \u8bad\u7ec3\u7a33\u5b9a: \u63d0\u4f9b\u7a33\u5b9a\u7684\u68af\u5ea6\u4fe1\u53f7 - \u8d1f\u8f7d\u5206\u62c5: \u51cf\u5c11\u8def\u7531\u4e13\u5bb6\u7684\u538b\u529b - \u6027\u80fd\u4fdd\u8bc1: \u5373\u4f7f\u8def\u7531\u5931\u8d25\u4e5f\u6709\u57fa\u7840\u8f93\u51fa</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#q3-moe","title":"Q3: \u5982\u4f55\u89e3\u51b3MoE\u7684\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\uff1f","text":"<p>DeepSeek\u7684\u591a\u5c42\u6b21\u65b9\u6848:</p> <ol> <li> <p>\u8f85\u52a9\u635f\u5931\u51fd\u6570:     <pre><code>balance_loss = device_balance_loss + expert_balance_loss\ntotal_loss = task_loss + \u03bb * balance_loss\n</code></pre></p> </li> <li> <p>\u52a8\u6001\u5bb9\u91cf\u8c03\u6574: \u6839\u636e\u4e13\u5bb6\u5386\u53f2\u5229\u7528\u7387\u8c03\u6574\u5bb9\u91cf</p> </li> <li> <p>\u6df7\u5408\u8def\u7531\u7b56\u7565: \u5728token-choice\u548cexpert-choice\u95f4\u81ea\u9002\u5e94\u5207\u6362</p> </li> <li> <p>\u4e13\u5bb6\u5206\u7ec4: \u901a\u8fc7\u5c42\u6b21\u5316\u7ed3\u6784\u63d0\u9ad8\u8d1f\u8f7d\u5206\u5e03</p> </li> </ol>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_8","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u7ec6\u7c92\u5ea6\u4e13\u5bb6vs\u7c97\u7c92\u5ea6\u4e13\u5bb6\u7684\u533a\u522b</li> <li>[ ] \u638c\u63e1\u5171\u4eab\u4e13\u5bb6\u673a\u5236\u7684\u8bbe\u8ba1\u7406\u5ff5</li> <li>[ ] \u4e86\u89e3\u591a\u7ea7\u8d1f\u8f7d\u5747\u8861\u7684\u5b9e\u73b0\u7b56\u7565</li> <li>[ ] \u80fd\u89e3\u91caDeepSeek MoE\u7684\u521b\u65b0\u4ef7\u503c</li> </ul>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_9","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aMLA\u6838\u5fc3\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1aMTP\u591atoken\u9884\u6d4b</li> <li>\u8fd4\u56de\uff1aDeepSeek\u4f18\u5316\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mla/","title":"MLA\u6838\u5fc3\u6280\u672f","text":""},{"location":"fundamentals/deepseek-innovations/mla/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u638c\u63e1Multi-head Latent Attention (MLA)\u7684\u5b8c\u6574\u6280\u672f\u539f\u7406\uff0c\u7406\u89e3\u5176\u5982\u4f55\u5b9e\u73b0\u9769\u547d\u6027\u7684\u5185\u5b58\u4f18\u5316\u3002</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_2","title":"\ud83d\udcdd \u6280\u672f\u6df1\u5ea6\u89e3\u6790","text":""},{"location":"fundamentals/deepseek-innovations/mla/#mla_1","title":"MLA\u8bbe\u8ba1\u80cc\u666f","text":"<p>\u4f20\u7edf\u591a\u5934\u6ce8\u610f\u529b(MHA)\u9762\u4e34\u7684\u6838\u5fc3\u95ee\u9898\uff1a</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_3","title":"\u5185\u5b58\u7206\u70b8\u95ee\u9898","text":"<pre><code># \u4f20\u7edfMHA\u7684KV Cache\u9700\u6c42\nkv_cache_size = num_layers \u00d7 num_heads \u00d7 head_dim \u00d7 seq_len \u00d7 2  # K\u548cV\n\n# \u5177\u4f53\u4f8b\u5b50\uff1aLLaMA-7B\u6a21\u578b\n# 32\u5c42 \u00d7 32\u5934 \u00d7 128\u7ef4 \u00d7 2048\u5e8f\u5217\u957f\u5ea6 \u00d7 2 = 1GB+ \u5185\u5b58\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_4","title":"\u63a8\u7406\u74f6\u9888","text":"<ul> <li>\u957f\u5e8f\u5217\u63a8\u7406\u65f6KV Cache\u5360\u7528\u5927\u91cf\u663e\u5b58</li> <li>\u9650\u5236\u4e86batch size\u548c\u5e8f\u5217\u957f\u5ea6</li> <li>\u63a8\u7406\u6210\u672c\u5c45\u9ad8\u4e0d\u4e0b</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mla/#mla_2","title":"MLA\u6838\u5fc3\u521b\u65b0","text":""},{"location":"fundamentals/deepseek-innovations/mla/#1-kv","title":"1. \u4f4e\u79e9KV\u8054\u5408\u538b\u7f29","text":"<p>\u6838\u5fc3\u601d\u60f3: \u5c06\u9ad8\u7ef4\u7684Key\u548cValue\u77e9\u9635\u8054\u5408\u538b\u7f29\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_5","title":"\u6570\u5b66\u539f\u7406","text":"\\[c_t^{KV} = x_t W^{DKV}\\] <p>\u5176\u4e2d\uff1a - \\(x_t \\in \\mathbb{R}^{d}\\): \u8f93\u5165\u5411\u91cf - \\(W^{DKV} \\in \\mathbb{R}^{d \\times d_c}\\): \u4e0b\u6295\u5f71\u77e9\u9635 - \\(c_t^{KV} \\in \\mathbb{R}^{d_c}\\): \u538b\u7f29\u540e\u7684\u6f5c\u5728\u5411\u91cf - \\(d_c \\ll h \\cdot d_h\\) (\u538b\u7f29\u7ef4\u5ea6\u8fdc\u5c0f\u4e8e\u539f\u59cb\u7ef4\u5ea6)</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_6","title":"\u6062\u590d\u8fc7\u7a0b","text":"<pre><code>def kv_compression_recovery():\n    # 1. \u538b\u7f29\uff1a\u5c06\u8f93\u5165\u538b\u7f29\u5230\u4f4e\u7ef4\u7a7a\u95f4\n    c_kv = x @ W_down_kv  # [batch, seq, d_model] -&gt; [batch, seq, d_c]\n\n    # 2. \u6062\u590d\uff1a\u4ece\u4f4e\u7ef4\u7a7a\u95f4\u6062\u590d\u9ad8\u7ef4K,V\n    k_compressed = c_kv @ W_up_k  # [batch, seq, d_c] -&gt; [batch, seq, d_k]\n    v_compressed = c_kv @ W_up_v  # [batch, seq, d_c] -&gt; [batch, seq, d_v]\n\n    return k_compressed, v_compressed\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_7","title":"\u538b\u7f29\u6548\u679c\u5bf9\u6bd4","text":"\u6a21\u578b\u89c4\u6a21 \u539f\u59cbKV Cache MLA\u538b\u7f29\u540e \u538b\u7f29\u6bd4 7B\u6a21\u578b 8192\u7ef4/token 640\u7ef4/token 12.8\u00d7 67B\u6a21\u578b 16384\u7ef4/token 1024\u7ef4/token 16\u00d7 236B\u6a21\u578b 32768\u7ef4/token 1536\u7ef4/token 21.3\u00d7"},{"location":"fundamentals/deepseek-innovations/mla/#2-rope","title":"2. RoPE\u89e3\u8026\u673a\u5236","text":"<p>\u95ee\u9898: \u4f20\u7edfRoPE\u5728\u538b\u7f29\u7a7a\u95f4\u4e2d\u65e0\u6cd5\u6b63\u786e\u5de5\u4f5c</p> <p>\u89e3\u51b3\u65b9\u6848: \u5c06Query\u548cKey\u5206\u4e3a\u4e24\u4e2a\u72ec\u7acb\u90e8\u5206</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_8","title":"\u5206\u79bb\u7b56\u7565","text":"<pre><code>def rope_decoupling(x, position):\n    # 1. \u751f\u6210\u539f\u59cbQuery\n    q_full = x @ W_q  # [batch, seq, d_model]\n\n    # 2. \u5206\u79bb\u4e3a\u4e24\u90e8\u5206\n    q_compressed = q_full[:, :, :d_c]      # \u8bed\u4e49\u90e8\u5206\uff0c\u53ef\u538b\u7f29\n    q_rope = q_full[:, :, d_c:d_c+d_r]     # \u4f4d\u7f6e\u90e8\u5206\uff0c\u4fdd\u6301\u539f\u7ef4\u5ea6\n\n    # 3. \u5206\u522b\u5904\u7406\n    # \u8bed\u4e49\u90e8\u5206\uff1a\u901a\u8fc7\u538b\u7f29\u7a7a\u95f4\u5904\u7406\n    c_kv = x @ W_down_kv\n    k_compressed = c_kv @ W_up_k\n\n    # \u4f4d\u7f6e\u90e8\u5206\uff1a\u76f4\u63a5\u751f\u6210\u5e76\u5e94\u7528RoPE\n    k_rope = x @ W_k_rope\n    q_rope_rotated = apply_rope(q_rope, position)\n    k_rope_rotated = apply_rope(k_rope, position)\n\n    # 4. \u7ec4\u5408\u6700\u7ec8\u7ed3\u679c\n    q_final = torch.cat([q_compressed, q_rope_rotated], dim=-1)\n    k_final = torch.cat([k_compressed, k_rope_rotated], dim=-1)\n\n    return q_final, k_final\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#rope","title":"RoPE\u89e3\u8026\u7684\u6570\u5b66\u8868\u793a","text":"\\[q_t = [q_t^C; q_t^R], \\quad k_s = [k_s^C; k_s^R]\\] <p>\u5176\u4e2d\uff1a - \\(q_t^C, k_s^C\\): \u8bed\u4e49\u7ec4\u4ef6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u751f\u6210 - \\(q_t^R, k_s^R\\): \u4f4d\u7f6e\u7ec4\u4ef6\uff0c\u5e94\u7528RoPE\u65cb\u8f6c\u7f16\u7801</p> <p>\u6ce8\u610f\u529b\u8ba1\u7b97\uff1a \\(\\(\\text{Attention} = \\text{softmax}\\left(\\frac{q_t^C (k_s^C)^T + q_t^R (k_s^R)^T}{\\sqrt{d_h}}\\right)\\)\\)</p>"},{"location":"fundamentals/deepseek-innovations/mla/#3","title":"3. \u6743\u91cd\u5438\u6536\u4f18\u5316","text":"<p>\u76ee\u6807: \u51cf\u5c11\u63a8\u7406\u65f6\u7684\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_9","title":"\u4f20\u7edf\u8ba1\u7b97\u8def\u5f84","text":"<pre><code># \u9700\u8981\u4e24\u6b21\u77e9\u9635\u4e58\u6cd5\nc_kv = x @ W_down_kv     # \u7b2c\u4e00\u6b21\uff1a\u964d\u7ef4\nk = c_kv @ W_up_k        # \u7b2c\u4e8c\u6b21\uff1a\u5347\u7ef4\u6062\u590d\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_10","title":"\u6743\u91cd\u5438\u6536\u540e","text":"<pre><code># \u9884\u8ba1\u7b97\u5408\u5e76\u6743\u91cd\nW_combined_k = W_down_kv @ W_up_k  # \u79bb\u7ebf\u8ba1\u7b97\nW_combined_v = W_down_kv @ W_up_v\n\n# \u63a8\u7406\u65f6\u53ea\u9700\u4e00\u6b21\u77e9\u9635\u4e58\u6cd5\nk_absorbed = x @ W_combined_k      # \u76f4\u63a5\u5f97\u5230\u7ed3\u679c\nv_absorbed = x @ W_combined_v\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_11","title":"\u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790","text":"\u64cd\u4f5c \u4f20\u7edfMLA \u6743\u91cd\u5438\u6536MLA \u51cf\u5c11\u91cf \u77e9\u9635\u4e58\u6cd5\u6b21\u6570 2\u6b21 1\u6b21 50% \u5185\u5b58\u8bbf\u95ee \u9ad8 \u4f4e ~30% \u63a8\u7406\u5ef6\u8fdf \u57fa\u51c6 \u51cf\u5c1115-20% \u663e\u8457"},{"location":"fundamentals/deepseek-innovations/mla/#mla_3","title":"\u5b8c\u6574MLA\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MLAAttention(nn.Module):\n    \"\"\"Multi-head Latent Attention\u5b8c\u6574\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, num_heads, d_compressed=None, d_rope=None):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        # \u538b\u7f29\u7ef4\u5ea6\u8bbe\u7f6e\n        self.d_compressed = d_compressed or d_model // 8  # \u9ed8\u8ba48\u500d\u538b\u7f29\n        self.d_rope = d_rope or self.head_dim // 2        # RoPE\u7ef4\u5ea6\n\n        # Query\u6295\u5f71\n        self.W_q = nn.Linear(d_model, d_model, bias=False)\n\n        # KV\u8054\u5408\u538b\u7f29\u6295\u5f71\n        self.W_down_kv = nn.Linear(d_model, self.d_compressed, bias=False)\n        self.W_up_k = nn.Linear(self.d_compressed, d_model - self.d_rope * num_heads, bias=False)\n        self.W_up_v = nn.Linear(self.d_compressed, d_model, bias=False)\n\n        # RoPE\u90e8\u5206\u7684Key\u6295\u5f71\n        self.W_k_rope = nn.Linear(d_model, self.d_rope * num_heads, bias=False)\n\n        # \u8f93\u51fa\u6295\u5f71\n        self.W_o = nn.Linear(d_model, d_model, bias=False)\n\n        # RoPE\u521d\u59cb\u5316\n        self.rope = RoPEEmbedding(self.d_rope)\n\n        # \u6743\u91cd\u5438\u6536\u4f18\u5316(\u53ef\u9009)\n        self.enable_weight_absorption = True\n        if self.enable_weight_absorption:\n            self._setup_absorbed_weights()\n\n    def _setup_absorbed_weights(self):\n        \"\"\"\u8bbe\u7f6e\u6743\u91cd\u5438\u6536\u7684\u5408\u5e76\u77e9\u9635\"\"\"\n        # \u9884\u8ba1\u7b97\u5408\u5e76\u6743\u91cd\u77e9\u9635\n        with torch.no_grad():\n            self.W_absorbed_k = nn.Parameter(\n                self.W_down_kv.weight.T @ self.W_up_k.weight.T\n            )\n            self.W_absorbed_v = nn.Parameter(\n                self.W_down_kv.weight.T @ self.W_up_v.weight.T\n            )\n\n    def forward(self, x, position_ids=None, kv_cache=None):\n        batch_size, seq_len, d_model = x.shape\n\n        # 1. \u8ba1\u7b97Query\n        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # 2. \u5206\u79bbQuery\u7684\u538b\u7f29\u90e8\u5206\u548cRoPE\u90e8\u5206\n        q_compressed = q[:, :, :, :-self.d_rope]  # \u8bed\u4e49\u90e8\u5206\n        q_rope = q[:, :, :, -self.d_rope:]        # \u4f4d\u7f6e\u90e8\u5206\n\n        # 3. \u8ba1\u7b97\u538b\u7f29\u7684Key\u548cValue\n        if self.enable_weight_absorption:\n            # \u4f7f\u7528\u6743\u91cd\u5438\u6536\u4f18\u5316\n            k_compressed_flat = x @ self.W_absorbed_k\n            v_flat = x @ self.W_absorbed_v\n        else:\n            # \u6807\u51c6\u4e24\u6b65\u8ba1\u7b97\n            c_kv = x @ self.W_down_kv.weight.T\n            k_compressed_flat = c_kv @ self.W_up_k.weight.T\n            v_flat = c_kv @ self.W_up_v.weight.T\n\n        # \u91cd\u5851\u4e3a\u591a\u5934\u683c\u5f0f\n        k_compressed = k_compressed_flat.view(\n            batch_size, seq_len, self.num_heads, -1\n        )\n        v = v_flat.view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # 4. \u8ba1\u7b97RoPE\u90e8\u5206\u7684Key\n        k_rope_flat = x @ self.W_k_rope.weight.T\n        k_rope = k_rope_flat.view(batch_size, seq_len, self.num_heads, self.d_rope)\n\n        # 5. \u5e94\u7528RoPE\u65cb\u8f6c\u7f16\u7801\n        if position_ids is not None:\n            q_rope = self.rope(q_rope, position_ids)\n            k_rope = self.rope(k_rope, position_ids)\n\n        # 6. \u7ec4\u5408\u5b8c\u6574\u7684Key\n        k = torch.cat([k_compressed, k_rope], dim=-1)\n        q = torch.cat([q_compressed, q_rope], dim=-1)\n\n        # 7. KV Cache\u5904\u7406\n        if kv_cache is not None:\n            # \u66f4\u65b0\u7f13\u5b58 - \u53ea\u7f13\u5b58\u538b\u7f29\u540e\u7684\u8868\u793a\n            compressed_cache = torch.cat([k_compressed, k_rope], dim=-1)\n            k, v = kv_cache.update(compressed_cache, v)\n\n        # 8. \u8ba1\u7b97\u6ce8\u610f\u529b\n        # \u8f6c\u7f6e\u7ef4\u5ea6: [batch, seq, heads, head_dim] -&gt; [batch, heads, seq, head_dim]\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # \u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, v)\n\n        # 9. \u91cd\u5851\u548c\u8f93\u51fa\u6295\u5f71\n        attn_output = attn_output.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, d_model\n        )\n\n        return self.W_o(attn_output)\n\nclass MLAKVCache:\n    \"\"\"MLA\u4e13\u7528\u7684KV Cache\"\"\"\n\n    def __init__(self, max_seq_len, num_heads, compressed_dim, rope_dim):\n        self.max_seq_len = max_seq_len\n        self.num_heads = num_heads\n        self.compressed_dim = compressed_dim\n        self.rope_dim = rope_dim\n\n        # \u53ea\u7f13\u5b58\u538b\u7f29\u540e\u7684\u8868\u793a\n        self.cache_dim = compressed_dim + rope_dim\n        self.cache_k = torch.zeros(max_seq_len, num_heads, self.cache_dim)\n        self.cache_v = torch.zeros(max_seq_len, num_heads, compressed_dim)\n        self.current_len = 0\n\n    def update(self, new_k, new_v):\n        \"\"\"\u66f4\u65b0\u7f13\u5b58\u5e76\u8fd4\u56de\u5b8c\u6574\u7684K,V\"\"\"\n        seq_len = new_k.size(1)\n\n        # \u66f4\u65b0\u7f13\u5b58\n        end_pos = self.current_len + seq_len\n        self.cache_k[:, self.current_len:end_pos] = new_k[0].transpose(0, 1)\n        self.cache_v[:, self.current_len:end_pos] = new_v[0].transpose(0, 1)\n\n        self.current_len = end_pos\n\n        # \u8fd4\u56de\u5b8c\u6574\u7684K,V\n        return (\n            self.cache_k[:self.current_len].transpose(0, 1).unsqueeze(0),\n            self.cache_v[:self.current_len].transpose(0, 1).unsqueeze(0)\n        )\n\n    def get_memory_usage(self):\n        \"\"\"\u83b7\u53d6\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\"\"\"\n        total_elements = self.current_len * self.num_heads * (self.cache_dim + self.compressed_dim)\n        memory_mb = total_elements * 4 / 1024 / 1024  # \u5047\u8bbefloat32\n        return memory_mb\n\nclass RoPEEmbedding(nn.Module):\n    \"\"\"\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\"\"\"\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.dim = dim\n        self.base = base\n\n        # \u9884\u8ba1\u7b97\u9891\u7387\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n    def forward(self, x, position_ids):\n        # x: [batch, seq, heads, dim]\n        # position_ids: [batch, seq]\n\n        seq_len = x.size(1)\n        position = position_ids.float()\n\n        # \u8ba1\u7b97\u89d2\u5ea6\n        freqs = torch.outer(position.flatten(), self.inv_freq)\n\n        # \u751f\u6210cos\u548csin\n        cos = freqs.cos().view(*position.shape, -1)\n        sin = freqs.sin().view(*position.shape, -1)\n\n        # \u5e94\u7528\u65cb\u8f6c\n        x1, x2 = x[..., ::2], x[..., 1::2]\n\n        # \u65cb\u8f6c\u53d8\u6362\n        rotated_x1 = x1 * cos.unsqueeze(2) - x2 * sin.unsqueeze(2)\n        rotated_x2 = x1 * sin.unsqueeze(2) + x2 * cos.unsqueeze(2)\n\n        # \u91cd\u65b0\u7ec4\u5408\n        rotated_x = torch.stack([rotated_x1, rotated_x2], dim=-1).flatten(-2)\n\n        return rotated_x\n\n# \u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5\ndef benchmark_mla_vs_mha():\n    \"\"\"\u5bf9\u6bd4MLA\u548cMHA\u7684\u6027\u80fd\"\"\"\n\n    d_model, num_heads = 4096, 32\n    seq_len, batch_size = 2048, 4\n\n    # \u521b\u5efa\u6d4b\u8bd5\u6570\u636e\n    x = torch.randn(batch_size, seq_len, d_model)\n    position_ids = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1)\n\n    # MLA\u6a21\u578b\n    mla = MLAAttention(d_model, num_heads, d_compressed=512)\n\n    # \u4f20\u7edfMHA\u6a21\u578b (\u7b80\u5316\u7248\u672c)\n    mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n\n    print(\"=== MLA vs MHA \u6027\u80fd\u5bf9\u6bd4 ===\")\n\n    # \u53c2\u6570\u91cf\u5bf9\u6bd4\n    mla_params = sum(p.numel() for p in mla.parameters())\n    mha_params = sum(p.numel() for p in mha.parameters())\n\n    print(f\"MLA\u53c2\u6570\u91cf: {mla_params:,}\")\n    print(f\"MHA\u53c2\u6570\u91cf: {mha_params:,}\")\n    print(f\"\u53c2\u6570\u51cf\u5c11: {(mha_params - mla_params) / mha_params * 100:.1f}%\")\n\n    # KV Cache\u5185\u5b58\u5bf9\u6bd4\n    traditional_kv_cache = num_heads * (d_model // num_heads) * seq_len * 2\n    mla_kv_cache = (512 + 64) * seq_len  # \u538b\u7f29\u7ef4\u5ea6 + RoPE\u7ef4\u5ea6\n\n    print(f\"\u4f20\u7edfKV Cache: {traditional_kv_cache:,} \u5143\u7d20\")\n    print(f\"MLA KV Cache: {mla_kv_cache:,} \u5143\u7d20\")\n    print(f\"\u5185\u5b58\u51cf\u5c11: {traditional_kv_cache / mla_kv_cache:.1f}\u00d7\")\n\n    # \u63a8\u7406\u901f\u5ea6\u6d4b\u8bd5\n    import time\n\n    with torch.no_grad():\n        # MLA\u63a8\u7406\u65f6\u95f4\n        start = time.time()\n        for _ in range(100):\n            _ = mla(x, position_ids)\n        mla_time = time.time() - start\n\n        # MHA\u63a8\u7406\u65f6\u95f4\n        start = time.time()\n        for _ in range(100):\n            _, _ = mha(x, x, x)\n        mha_time = time.time() - start\n\n    print(f\"MLA\u63a8\u7406\u65f6\u95f4: {mla_time:.4f}\u79d2\")\n    print(f\"MHA\u63a8\u7406\u65f6\u95f4: {mha_time:.4f}\u79d2\")\n    print(f\"\u901f\u5ea6\u63d0\u5347: {mha_time / mla_time:.2f}\u00d7\")\n\nif __name__ == \"__main__\":\n    benchmark_mla_vs_mha()\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_12","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/deepseek-innovations/mla/#q1-mla10kv-cache","title":"Q1: MLA\u5982\u4f55\u5b9e\u73b010\u500d\u4ee5\u4e0a\u7684KV Cache\u538b\u7f29\uff1f","text":"<p>\u6838\u5fc3\u673a\u5236:</p> <ol> <li>\u4f4e\u79e9\u8054\u5408\u538b\u7f29: \u5c06\u539f\u672c\u9700\u8981\u5b58\u50a8\u7684 <code>num_heads \u00d7 head_dim \u00d7 2</code> \u7ef4\u5ea6\u538b\u7f29\u5230 <code>compressed_dim</code></li> <li>RoPE\u89e3\u8026: \u53ea\u5bf9\u5c11\u91cf\u7ef4\u5ea6\u4fdd\u6301\u539f\u59cb\u7cbe\u5ea6\uff0c\u5927\u90e8\u5206\u7ef4\u5ea6\u53ef\u4ee5\u538b\u7f29</li> <li>\u667a\u80fd\u8bbe\u8ba1: \u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u4f4e\u79e9\u7279\u6027\u8fdb\u884c\u6709\u635f\u4f46\u5408\u7406\u7684\u538b\u7f29</li> </ol> <p>\u5177\u4f53\u6570\u5b57: <pre><code>\u4f20\u7edfMHA: 32\u5934 \u00d7 128\u7ef4 \u00d7 2 = 8192\u7ef4/token\nMLA\u538b\u7f29: 512\u7ef4(\u538b\u7f29) + 128\u7ef4(RoPE) = 640\u7ef4/token\n\u538b\u7f29\u6bd4: 8192 \u00f7 640 = 12.8\u00d7\n</code></pre></p>"},{"location":"fundamentals/deepseek-innovations/mla/#q2-rope","title":"Q2: RoPE\u89e3\u8026\u4e3a\u4ec0\u4e48\u662f\u5fc5\u8981\u7684\uff1f","text":"<p>\u6838\u5fc3\u95ee\u9898: \u4f4d\u7f6e\u7f16\u7801\u4e0e\u538b\u7f29\u7684\u51b2\u7a81</p> <p>\u8be6\u7ec6\u89e3\u91ca: 1. RoPE\u4f9d\u8d56: \u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u9700\u8981\u5728\u7279\u5b9a\u7ef4\u5ea6\u7a7a\u95f4\u4e2d\u5de5\u4f5c 2. \u538b\u7f29\u7834\u574f: \u4f4e\u79e9\u538b\u7f29\u4f1a\u7834\u574fRoPE\u7684\u6570\u5b66\u7ed3\u6784 3. \u89e3\u8026\u65b9\u6848: \u5c06\u5411\u91cf\u5206\u4e3a\u8bed\u4e49\u90e8\u5206(\u53ef\u538b\u7f29)\u548c\u4f4d\u7f6e\u90e8\u5206(\u4e0d\u538b\u7f29) 4. \u6548\u679c\u4fdd\u8bc1: \u65e2\u83b7\u5f97\u538b\u7f29\u6536\u76ca\uff0c\u53c8\u4fdd\u6301\u4f4d\u7f6e\u654f\u611f\u6027</p>"},{"location":"fundamentals/deepseek-innovations/mla/#q3","title":"Q3: \u6743\u91cd\u5438\u6536\u4f18\u5316\u7684\u5b9e\u9645\u6548\u679c\u5982\u4f55\uff1f","text":"<p>\u6027\u80fd\u63d0\u5347: - \u8ba1\u7b97\u6b21\u6570: \u4ece2\u6b21\u77e9\u9635\u4e58\u6cd5\u51cf\u5c11\u52301\u6b21 - \u5185\u5b58\u8bbf\u95ee: \u51cf\u5c11\u4e2d\u95f4\u7ed3\u679c\u7684\u5b58\u50a8\u548c\u8bfb\u53d6 - \u63a8\u7406\u5ef6\u8fdf: \u901a\u5e38\u51cf\u5c1115-20%</p> <p>\u5de5\u7a0b\u8003\u8651: - \u9700\u8981\u989d\u5916\u7684\u53c2\u6570\u5b58\u50a8\u7a7a\u95f4 - \u9884\u8ba1\u7b97\u5f00\u9500\uff08\u4e00\u6b21\u6027\uff09 - \u6570\u503c\u7cbe\u5ea6\u53ef\u80fd\u6709\u5fae\u5c0f\u635f\u5931</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_13","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3MLA\u7684\u4e09\u5927\u6838\u5fc3\u6280\u672f\u539f\u7406</li> <li>[ ] \u80fd\u8ba1\u7b97MLA\u76f8\u6bd4MHA\u7684\u5185\u5b58\u538b\u7f29\u6bd4</li> <li>[ ] \u638c\u63e1RoPE\u89e3\u8026\u7684\u5fc5\u8981\u6027\u548c\u5b9e\u73b0\u65b9\u6cd5</li> <li>[ ] \u7406\u89e3\u6743\u91cd\u5438\u6536\u4f18\u5316\u7684\u5de5\u7a0b\u4ef7\u503c</li> <li>[ ] \u80fd\u5b9e\u73b0\u7b80\u5316\u7248\u7684MLA\u6ce8\u610f\u529b\u673a\u5236</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mla/#_14","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aDeepSeek MoE\u521b\u65b0</li> <li>\u4e0a\u4e00\u7ae0\uff1a\u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53</li> <li>\u8fd4\u56de\uff1aDeepSeek\u4f18\u5316\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mtp/","title":"MTP\u591atoken\u9884\u6d4b","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3Multi-Token Prediction (MTP)\u6280\u672f\uff0c\u638c\u63e1\u5176\u5982\u4f55\u901a\u8fc7\u5e76\u884c\u9884\u6d4b\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002</p>"},{"location":"fundamentals/deepseek-innovations/mtp/#_2","title":"\ud83d\udcdd \u6280\u672f\u539f\u7406\u89e3\u6790","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#mtp","title":"MTP\u8bbe\u8ba1\u80cc\u666f","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#_3","title":"\u4f20\u7edf\u8bad\u7ec3\u7684\u5c40\u9650\u6027","text":"<p>\u5355token\u9884\u6d4b\u95ee\u9898: <pre><code># \u4f20\u7edfnext-token\u9884\u6d4b\nfor position in sequence:\n    prediction = model(input[:position])\n    loss = cross_entropy(prediction, target[position])\n    # \u6bcf\u6b65\u53ea\u6709\u4e00\u4e2a\u76d1\u7763\u4fe1\u53f7\n</code></pre></p> <p>\u95ee\u9898\u5206\u6790: 1. \u4fe1\u606f\u5bc6\u5ea6\u4f4e: \u6bcf\u4e2a\u524d\u5411\u4f20\u64ad\u53ea\u4ea7\u751f\u4e00\u4e2a\u9884\u6d4b 2. \u957f\u671f\u4f9d\u8d56\u5f31: \u96be\u4ee5\u5efa\u7acb\u8fdc\u8ddd\u79bb\u7684\u4f9d\u8d56\u5173\u7cfb 3. \u8bad\u7ec3\u6548\u7387\u4f4e: \u5e8f\u5217\u8d8a\u957f\uff0c\u6709\u6548\u4fe1\u53f7\u8d8a\u7a00\u758f</p>"},{"location":"fundamentals/deepseek-innovations/mtp/#mtp_1","title":"MTP\u89e3\u51b3\u65b9\u6848","text":"<p>\u6838\u5fc3\u601d\u60f3: \u5728\u6bcf\u4e2a\u4f4d\u7f6e\u540c\u65f6\u9884\u6d4b\u672a\u6765\u591a\u4e2atoken</p> <pre><code># MTP\u591atoken\u9884\u6d4b\nfor position in sequence:\n    predictions = model.multi_head_predict(input[:position])\n    # predictions[0] = \u9884\u6d4bposition+1\u7684token\n    # predictions[1] = \u9884\u6d4bposition+2\u7684token  \n    # predictions[n] = \u9884\u6d4bposition+n+1\u7684token\n\n    multi_loss = sum([\n        cross_entropy(predictions[i], target[position+i+1])\n        for i in range(n_predictions)\n    ])\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#mtp_2","title":"MTP\u67b6\u6784\u8bbe\u8ba1","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#1","title":"1. \u591a\u9884\u6d4b\u5934\u67b6\u6784","text":"<pre><code>class MultiTokenPredictionHead(nn.Module):\n    \"\"\"\u591atoken\u9884\u6d4b\u5934\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, vocab_size, num_predictions, \n                 share_embeddings=True):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.num_predictions = num_predictions\n        self.share_embeddings = share_embeddings\n\n        # \u5171\u4eab\u7684Transformer\u9aa8\u5e72\u7f51\u7edc\n        self.backbone = TransformerBackbone(d_model)\n\n        # \u591a\u4e2a\u72ec\u7acb\u7684\u9884\u6d4b\u5934\n        if share_embeddings:\n            # \u5171\u4eab\u8f93\u51fa\u5d4c\u5165\u5c42\n            self.output_embedding = nn.Linear(d_model, vocab_size)\n            self.prediction_heads = nn.ModuleList([\n                PredictionHead(d_model, self.output_embedding)\n                for _ in range(num_predictions)\n            ])\n        else:\n            # \u72ec\u7acb\u7684\u9884\u6d4b\u5934\n            self.prediction_heads = nn.ModuleList([\n                nn.Linear(d_model, vocab_size)\n                for _ in range(num_predictions)\n            ])\n\n    def forward(self, x):\n        # \u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\n        hidden_states = self.backbone(x)\n\n        # \u591a\u4e2a\u9884\u6d4b\u5934\u5e76\u884c\u9884\u6d4b\n        predictions = []\n        for i, head in enumerate(self.prediction_heads):\n            if self.share_embeddings:\n                # \u6dfb\u52a0\u4f4d\u7f6e\u7279\u5b9a\u7684\u8c03\u5236\n                modulated_hidden = self.position_modulation(hidden_states, i)\n                pred = head(modulated_hidden)\n            else:\n                pred = head(hidden_states)\n\n            predictions.append(pred)\n\n        return predictions\n\n    def position_modulation(self, hidden, prediction_step):\n        \"\"\"\u4f4d\u7f6e\u7279\u5b9a\u7684\u7279\u5f81\u8c03\u5236\"\"\"\n        # \u4e3a\u4e0d\u540c\u9884\u6d4b\u6b65\u9aa4\u6dfb\u52a0\u4f4d\u7f6e\u7279\u5b9a\u7684\u53d8\u6362\n        step_embedding = self.step_embeddings[prediction_step]\n        return hidden + step_embedding\n\nclass PredictionHead(nn.Module):\n    \"\"\"\u5355\u4e2a\u9884\u6d4b\u5934\"\"\"\n\n    def __init__(self, d_model, shared_output_layer=None):\n        super().__init__()\n\n        if shared_output_layer is not None:\n            self.output_proj = shared_output_layer\n        else:\n            self.output_proj = nn.Linear(d_model, vocab_size)\n\n        # \u9884\u6d4b\u6b65\u9aa4\u7279\u5b9a\u7684\u53d8\u6362\n        self.step_transform = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Linear(d_model, d_model)\n        )\n\n    def forward(self, hidden_states):\n        # \u6b65\u9aa4\u7279\u5b9a\u53d8\u6362\n        transformed = self.step_transform(hidden_states)\n\n        # \u6b8b\u5dee\u8fde\u63a5\n        output_hidden = hidden_states + transformed\n\n        # \u8f93\u51fa\u6295\u5f71\n        logits = self.output_proj(output_hidden)\n\n        return logits\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#2","title":"2. \u635f\u5931\u51fd\u6570\u8bbe\u8ba1","text":"<pre><code>class MTPLoss(nn.Module):\n    \"\"\"\u591atoken\u9884\u6d4b\u635f\u5931\u51fd\u6570\"\"\"\n\n    def __init__(self, num_predictions, loss_weights=None, \n                 auxiliary_loss_weight=0.1):\n        super().__init__()\n        self.num_predictions = num_predictions\n        self.auxiliary_loss_weight = auxiliary_loss_weight\n\n        if loss_weights is None:\n            # \u9ed8\u8ba4\u6743\u91cd\uff1a\u8ddd\u79bb\u8d8a\u8fdc\u6743\u91cd\u8d8a\u5c0f\n            self.loss_weights = [1.0 / (i + 1) for i in range(num_predictions)]\n        else:\n            self.loss_weights = loss_weights\n\n    def forward(self, predictions, targets, primary_targets):\n        \"\"\"\n        predictions: List[Tensor] - \u591a\u4e2a\u9884\u6d4b\u5934\u7684\u8f93\u51fa\n        targets: Tensor - \u5bf9\u5e94\u7684\u76ee\u6807\u5e8f\u5217\n        primary_targets: Tensor - \u4e3b\u8981\u4efb\u52a1\u76ee\u6807\uff08next-token\u9884\u6d4b\uff09\n        \"\"\"\n\n        # \u4e3b\u8981\u635f\u5931\uff1a\u4f20\u7edfnext-token\u9884\u6d4b\n        primary_loss = F.cross_entropy(predictions[0], primary_targets)\n\n        # \u8f85\u52a9\u635f\u5931\uff1a\u591atoken\u9884\u6d4b\n        auxiliary_losses = []\n        for i, (pred, weight) in enumerate(zip(predictions, self.loss_weights)):\n            if i &lt; targets.size(1):\n                target_slice = targets[:, i]\n                aux_loss = F.cross_entropy(pred, target_slice)\n                auxiliary_losses.append(weight * aux_loss)\n\n        total_auxiliary_loss = sum(auxiliary_losses) / len(auxiliary_losses)\n\n        # \u7ec4\u5408\u635f\u5931\n        total_loss = primary_loss + self.auxiliary_loss_weight * total_auxiliary_loss\n\n        return {\n            'total_loss': total_loss,\n            'primary_loss': primary_loss,\n            'auxiliary_loss': total_auxiliary_loss\n        }\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#3","title":"3. \u8bad\u7ec3\u7b56\u7565","text":"<pre><code>class MTPTrainer:\n    \"\"\"MTP\u8bad\u7ec3\u5668\"\"\"\n\n    def __init__(self, model, num_predictions=4, \n                 teacher_forcing=True):\n        self.model = model\n        self.num_predictions = num_predictions\n        self.teacher_forcing = teacher_forcing\n        self.loss_fn = MTPLoss(num_predictions)\n\n    def train_step(self, batch):\n        input_ids = batch['input_ids']\n        batch_size, seq_len = input_ids.shape\n\n        # \u751f\u6210\u591a\u4e2a\u9884\u6d4b\u76ee\u6807\n        targets = self.prepare_multi_targets(input_ids)\n\n        # \u524d\u5411\u4f20\u64ad\n        predictions = self.model(input_ids)\n\n        # \u8ba1\u7b97\u635f\u5931\n        loss_dict = self.loss_fn(\n            predictions, \n            targets['multi_targets'],\n            targets['primary_target']\n        )\n\n        return loss_dict\n\n    def prepare_multi_targets(self, input_ids):\n        \"\"\"\u51c6\u5907\u591atoken\u9884\u6d4b\u7684\u76ee\u6807\"\"\"\n        batch_size, seq_len = input_ids.shape\n\n        # \u4e3b\u8981\u76ee\u6807\uff1a\u4e0b\u4e00\u4e2atoken\n        primary_target = input_ids[:, 1:]\n\n        # \u591atoken\u76ee\u6807\uff1a\u672a\u6765n\u4e2atoken\n        multi_targets = []\n        for i in range(self.num_predictions):\n            if i + 1 &lt; seq_len:\n                target = input_ids[:, i+1:]\n                # \u586b\u5145\u5230\u76f8\u540c\u957f\u5ea6\n                if target.size(1) &lt; seq_len - 1:\n                    padding = torch.zeros(\n                        batch_size, \n                        seq_len - 1 - target.size(1),\n                        dtype=input_ids.dtype,\n                        device=input_ids.device\n                    )\n                    target = torch.cat([target, padding], dim=1)\n\n                multi_targets.append(target)\n\n        return {\n            'primary_target': primary_target,\n            'multi_targets': multi_targets\n        }\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#mtp_3","title":"MTP\u7684\u4f18\u52bf\u673a\u5236","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#1_1","title":"1. \u5bc6\u96c6\u76d1\u7763\u4fe1\u53f7","text":"<p>\u4f20\u7edf\u8bad\u7ec3: <pre><code># \u6bcf\u4e2a\u4f4d\u7f6e\u53ea\u6709\u4e00\u4e2a\u76d1\u7763\u4fe1\u53f7\nsupervision_density = 1 / sequence_length\n</code></pre></p> <p>MTP\u8bad\u7ec3: <pre><code># \u6bcf\u4e2a\u4f4d\u7f6e\u6709\u591a\u4e2a\u76d1\u7763\u4fe1\u53f7\nsupervision_density = num_predictions / sequence_length\n# \u901a\u5e38\u63d0\u53472-4\u500d\u7684\u4fe1\u53f7\u5bc6\u5ea6\n</code></pre></p>"},{"location":"fundamentals/deepseek-innovations/mtp/#2_1","title":"2. \u957f\u671f\u4f9d\u8d56\u5efa\u6a21","text":"<pre><code>def analyze_dependency_modeling():\n    \"\"\"\u5206\u6790MTP\u5982\u4f55\u6539\u5584\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\"\"\"\n\n    # \u4f20\u7edf\u65b9\u5f0f\uff1a\u53ea\u80fd\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u5efa\u7acb\u4f9d\u8d56\n    traditional_dependency_range = max_gradient_flow_length\n\n    # MTP\u65b9\u5f0f\uff1a\u76f4\u63a5\u5efa\u7acb\u8fdc\u8ddd\u79bb\u76d1\u7763\n    mtp_dependency_range = num_predictions * traditional_dependency_range\n\n    print(f\"\u4f9d\u8d56\u5efa\u6a21\u8303\u56f4\u63d0\u5347: {mtp_dependency_range / traditional_dependency_range}\u00d7\")\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#3_1","title":"3. \u6837\u672c\u6548\u7387\u63d0\u5347","text":"<pre><code>class SampleEfficiencyAnalyzer:\n    \"\"\"\u6837\u672c\u6548\u7387\u5206\u6790\u5668\"\"\"\n\n    def __init__(self, sequence_length, num_predictions):\n        self.seq_len = sequence_length\n        self.num_pred = num_predictions\n\n    def calculate_effective_samples(self, batch_size):\n        \"\"\"\u8ba1\u7b97\u6709\u6548\u6837\u672c\u6570\u91cf\"\"\"\n\n        # \u4f20\u7edf\u65b9\u5f0f\n        traditional_samples = batch_size * (self.seq_len - 1)\n\n        # MTP\u65b9\u5f0f\n        mtp_samples = batch_size * (self.seq_len - 1) * self.num_pred\n\n        efficiency_gain = mtp_samples / traditional_samples\n\n        return {\n            'traditional_samples': traditional_samples,\n            'mtp_samples': mtp_samples,\n            'efficiency_gain': efficiency_gain\n        }\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#_4","title":"\u63a8\u7406\u65f6\u7684\u5e94\u7528","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#1_2","title":"1. \u6295\u673a\u89e3\u7801\u52a0\u901f","text":"<pre><code>class SpeculativeDecoding:\n    \"\"\"\u57fa\u4e8eMTP\u7684\u6295\u673a\u89e3\u7801\"\"\"\n\n    def __init__(self, model_with_mtp, draft_model):\n        self.main_model = model_with_mtp\n        self.draft_model = draft_model\n\n    def generate(self, input_ids, max_length):\n        \"\"\"\u6295\u673a\u89e3\u7801\u751f\u6210\"\"\"\n        current_ids = input_ids\n\n        while current_ids.size(1) &lt; max_length:\n            # 1. \u4f7f\u7528draft model\u5feb\u901f\u751f\u6210\u5019\u9009\n            draft_predictions = self.draft_model.multi_predict(\n                current_ids, num_tokens=4\n            )\n\n            # 2. \u4f7f\u7528\u4e3b\u6a21\u578b\u9a8c\u8bc1\u5019\u9009\n            main_predictions = self.main_model.multi_predict(\n                current_ids, num_tokens=4\n            )\n\n            # 3. \u627e\u5230\u7b2c\u4e00\u4e2a\u4e0d\u5339\u914d\u7684\u4f4d\u7f6e\n            accepted_length = self.find_acceptance_length(\n                draft_predictions, main_predictions\n            )\n\n            # 4. \u63a5\u53d7\u9a8c\u8bc1\u901a\u8fc7\u7684token\n            if accepted_length &gt; 0:\n                new_tokens = draft_predictions[:accepted_length]\n                current_ids = torch.cat([current_ids, new_tokens], dim=1)\n            else:\n                # \u5982\u679c\u90fd\u4e0d\u5339\u914d\uff0c\u4f7f\u7528\u4e3b\u6a21\u578b\u751f\u6210\u4e00\u4e2atoken\n                next_token = self.main_model.generate_next(current_ids)\n                current_ids = torch.cat([current_ids, next_token], dim=1)\n\n        return current_ids\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#2_2","title":"2. \u5e76\u884c\u89e3\u7801","text":"<pre><code>def parallel_decoding_with_mtp(model, input_ids, beam_width=4):\n    \"\"\"\u57fa\u4e8eMTP\u7684\u5e76\u884c\u89e3\u7801\"\"\"\n\n    batch_size, seq_len = input_ids.shape\n\n    # 1. \u4f7f\u7528MTP\u540c\u65f6\u9884\u6d4b\u591a\u4e2a\u4f4d\u7f6e\n    multi_predictions = model.multi_predict(input_ids, num_tokens=beam_width)\n\n    # 2. \u4e3a\u6bcf\u4e2a\u9884\u6d4b\u4f4d\u7f6e\u751f\u6210\u5019\u9009\n    candidates = []\n    for i, predictions in enumerate(multi_predictions):\n        top_k_tokens = torch.topk(predictions, k=beam_width, dim=-1)\n        candidates.append(top_k_tokens.indices)\n\n    # 3. \u6784\u5efa\u5019\u9009\u5e8f\u5217\n    candidate_sequences = []\n    for seq_candidate in itertools.product(*candidates):\n        candidate_seq = torch.tensor(seq_candidate).unsqueeze(0)\n        candidate_sequences.append(\n            torch.cat([input_ids, candidate_seq], dim=1)\n        )\n\n    # 4. \u8bc4\u4f30\u6240\u6709\u5019\u9009\u5e8f\u5217\n    scores = []\n    for candidate in candidate_sequences:\n        score = model.score_sequence(candidate)\n        scores.append(score)\n\n    # 5. \u9009\u62e9\u6700\u4f73\u5019\u9009\n    best_idx = torch.argmax(torch.tensor(scores))\n    return candidate_sequences[best_idx]\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#_5","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#q1-mtp","title":"Q1: MTP\u5982\u4f55\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff1f","text":"<p>\u6838\u5fc3\u673a\u5236:</p> <ol> <li>\u76d1\u7763\u4fe1\u53f7\u5bc6\u5ea6: \u4ece\u6bcf\u4f4d\u7f6e1\u4e2a\u4fe1\u53f7\u63d0\u5347\u5230n\u4e2a\u4fe1\u53f7</li> <li>\u6837\u672c\u6548\u7387: \u76f8\u540c\u6570\u636e\u4ea7\u751f\u66f4\u591a\u8bad\u7ec3\u4fe1\u53f7</li> <li>\u957f\u671f\u4f9d\u8d56: \u76f4\u63a5\u5efa\u7acb\u8fdc\u8ddd\u79bb\u76d1\u7763\u8fde\u63a5</li> <li>\u5e76\u884c\u8bad\u7ec3: \u591a\u4e2a\u9884\u6d4b\u5934\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97</li> </ol> <p>\u5177\u4f53\u6570\u636e: <pre><code>\u4f20\u7edf\u8bad\u7ec3\uff1a1\u4e2a\u9884\u6d4b/\u4f4d\u7f6e\nMTP\u8bad\u7ec3\uff1a4\u4e2a\u9884\u6d4b/\u4f4d\u7f6e \u2192 4\u00d7\u4fe1\u53f7\u5bc6\u5ea6\n</code></pre></p>"},{"location":"fundamentals/deepseek-innovations/mtp/#q2-mtp","title":"Q2: MTP\u5728\u63a8\u7406\u65f6\u6709\u4ec0\u4e48\u7528\u9014\uff1f","text":"<p>\u4e3b\u8981\u5e94\u7528:</p> <ol> <li>\u6295\u673a\u89e3\u7801: \u4e00\u6b21\u751f\u6210\u591a\u4e2a\u5019\u9009token\uff0c\u901a\u8fc7\u9a8c\u8bc1\u52a0\u901f</li> <li>\u5e76\u884c\u89e3\u7801: \u540c\u65f6\u8003\u8651\u591a\u4e2a\u672a\u6765\u4f4d\u7f6e\u7684\u9884\u6d4b</li> <li>\u8d28\u91cf\u63d0\u5347: \u66f4\u597d\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b</li> <li>beam search\u4f18\u5316: \u66f4\u51c6\u786e\u7684\u5019\u9009\u8bc4\u4f30</li> </ol>"},{"location":"fundamentals/deepseek-innovations/mtp/#q3-mtp","title":"Q3: MTP\u7684\u8bad\u7ec3\u6210\u672c\u5982\u4f55\uff1f","text":"<p>\u6210\u672c\u5206\u6790:</p> <p>\u589e\u52a0\u7684\u6210\u672c: - \u591a\u4e2a\u9884\u6d4b\u5934\u7684\u53c2\u6570\uff08\u901a\u5e38\u589e\u52a010-20%\uff09 - \u989d\u5916\u7684\u524d\u5411\u8ba1\u7b97\uff08\u589e\u52a0\u9884\u6d4b\u5934\u90e8\u5206\uff09 - \u66f4\u590d\u6742\u7684\u635f\u5931\u8ba1\u7b97</p> <p>\u6536\u76ca: - \u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6 - \u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd - \u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b</p> <p>\u603b\u4f53\u8bc4\u4f30: \u867d\u7136\u5355\u6b65\u6210\u672c\u589e\u52a0\uff0c\u4f46\u6536\u655b\u66f4\u5feb\uff0c\u603b\u4f53\u8bad\u7ec3\u6548\u7387\u63d0\u5347</p>"},{"location":"fundamentals/deepseek-innovations/mtp/#_6","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3MTP\u76f8\u6bd4\u4f20\u7edf\u8bad\u7ec3\u7684\u4f18\u52bf</li> <li>[ ] \u638c\u63e1\u591a\u9884\u6d4b\u5934\u7684\u67b6\u6784\u8bbe\u8ba1</li> <li>[ ] \u4e86\u89e3MTP\u5728\u63a8\u7406\u52a0\u901f\u4e2d\u7684\u5e94\u7528</li> <li>[ ] \u80fd\u5206\u6790MTP\u7684\u6210\u672c\u6548\u76ca\u6743\u8861</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mtp/#_7","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aDeepSeek MoE\u521b\u65b0</li> <li>\u76f8\u5173\u6280\u672f\uff1a\u601d\u7ef4\u94fe\u6280\u672f</li> <li>\u8fd4\u56de\uff1aDeepSeek\u4f18\u5316\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/llm-advanced/","title":"\u7b2c3\u8282\uff1aLLM\u5347\u7ea7\u6280\u672f","text":""},{"location":"fundamentals/llm-advanced/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u4e86\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u524d\u6cbf\u4f18\u5316\u6280\u672f\uff0c\u638c\u63e1MOE\u67b6\u6784\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u57fa\u672c\u6982\u5ff5\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - MOE\u662f\u4ec0\u4e48\uff0c\u6709\u4ec0\u4e48\u597d\u5904\uff1f - \u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u57fa\u672c\u7b56\u7565 - \u5927\u6a21\u578b\u8bad\u7ec3\u7684\u5de5\u7a0b\u6311\u6218</p>"},{"location":"fundamentals/llm-advanced/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a1.5\u5929</p> <ul> <li>Day 1: MOE\u67b6\u6784\u539f\u7406\u6df1\u5165\u7406\u89e3</li> <li>\u534a\u5929: \u5206\u5e03\u5f0f\u8bad\u7ec3\u57fa\u7840\u6982\u5ff5</li> </ul>"},{"location":"fundamentals/llm-advanced/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/llm-advanced/#1-moe","title":"1. MOE\u67b6\u6784","text":"<ul> <li>\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u539f\u7406</li> <li>\u7a00\u758f\u6fc0\u6d3b\u7684\u4f18\u52bf</li> <li>\u5de5\u7a0b\u5b9e\u73b0\u6311\u6218</li> </ul>"},{"location":"fundamentals/llm-advanced/#2","title":"2. \u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<ul> <li>\u6570\u636e\u5e76\u884c vs \u6a21\u578b\u5e76\u884c</li> <li>\u6d41\u6c34\u7ebf\u5e76\u884c</li> <li>\u901a\u4fe1\u4f18\u5316\u7b56\u7565</li> </ul>"},{"location":"fundamentals/llm-advanced/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u4e24\u9879\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u95ee\u9898\u89e3\u7b54: \u80fd\u89e3\u91caMOE\u7684\u5de5\u4f5c\u539f\u7406\u548c\u4f18\u52bf</li> <li>\u6982\u5ff5\u7406\u89e3: \u7406\u89e3\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u5206\u5e03\u5f0f\u7b56\u7565</li> </ol>"},{"location":"fundamentals/llm-advanced/#_5","title":"\ud83d\udca1 \u5b66\u4e60\u5efa\u8bae","text":"<p>\u8fd9\u4e00\u8282\u5185\u5bb9\u76f8\u5bf9\u57fa\u7840\uff0c\u4e3b\u8981\u4ee5\u6982\u5ff5\u7406\u89e3\u4e3a\u4e3b\uff1a - MOE\u90e8\u5206: \u91cd\u70b9\u7406\u89e3\u7a00\u758f\u6fc0\u6d3b\u7684\u4f18\u52bf\u548c\u6311\u6218 - \u5206\u5e03\u5f0f\u8bad\u7ec3: \u4e86\u89e3\u57fa\u672c\u6982\u5ff5\u5373\u53ef\uff0c\u65e0\u9700\u6df1\u5165\u5de5\u7a0b\u7ec6\u8282 - \u5b66\u4e60\u91cd\u70b9: \u4e3a\u540e\u7eedDeepSeek MoE\u7b49\u9ad8\u7ea7\u6280\u672f\u6253\u4e0b\u57fa\u7840</p>"},{"location":"fundamentals/llm-advanced/#_6","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u6a21\u5757\u5f00\u59cb\u5b66\u4e60\uff0c\u91cd\u70b9\u638c\u63e1\u6838\u5fc3\u6982\u5ff5\u548c\u539f\u7406\u3002</p>"},{"location":"fundamentals/llm-advanced/distributed/","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3","text":""},{"location":"fundamentals/llm-advanced/distributed/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u4e86\u89e3\u5927\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u57fa\u672c\u7b56\u7565\u548c\u6982\u5ff5\u3002</p>"},{"location":"fundamentals/llm-advanced/distributed/#_3","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/llm-advanced/distributed/#_4","title":"\u4e3b\u8981\u5e76\u884c\u7b56\u7565","text":"<ol> <li>\u6570\u636e\u5e76\u884c: \u4e0d\u540c\u8bbe\u5907\u5904\u7406\u4e0d\u540c\u7684\u6570\u636e\u6279\u6b21</li> <li>\u6a21\u578b\u5e76\u884c: \u5c06\u6a21\u578b\u53c2\u6570\u5206\u5e03\u5230\u4e0d\u540c\u8bbe\u5907</li> <li>\u6d41\u6c34\u7ebf\u5e76\u884c: \u5c06\u6a21\u578b\u5c42\u5206\u5e03\u5230\u4e0d\u540c\u8bbe\u5907\uff0c\u5f62\u6210\u6d41\u6c34\u7ebf</li> </ol>"},{"location":"fundamentals/llm-advanced/distributed/#_5","title":"\u5de5\u7a0b\u6311\u6218","text":"<ul> <li>\u901a\u4fe1\u5f00\u9500\u4f18\u5316</li> <li>\u5185\u5b58\u7ba1\u7406</li> <li>\u8d1f\u8f7d\u5747\u8861</li> </ul>"},{"location":"fundamentals/llm-advanced/distributed/#_6","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/llm-advanced/distributed/#q1","title":"Q1: \u5206\u5e03\u5f0f\u8bad\u7ec3\u6709\u54ea\u4e9b\u4e3b\u8981\u7b56\u7565\uff1f","text":"<p>\u6838\u5fc3\u7b56\u7565: - \u6570\u636e\u5e76\u884c: \u7b80\u5355\u4f46\u901a\u4fe1\u5f00\u9500\u5927 - \u6a21\u578b\u5e76\u884c: \u9002\u5408\u8d85\u5927\u6a21\u578b - \u6df7\u5408\u5e76\u884c: \u7ed3\u5408\u591a\u79cd\u7b56\u7565\u7684\u73b0\u4ee3\u65b9\u6848</p>"},{"location":"fundamentals/llm-advanced/distributed/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u4e86\u89e3\u57fa\u672c\u7684\u5e76\u884c\u8bad\u7ec3\u6982\u5ff5</li> <li>[ ] \u7406\u89e3\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u4e3b\u8981\u6311\u6218</li> </ul>"},{"location":"fundamentals/llm-advanced/distributed/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aMOE\u67b6\u6784</li> <li>\u4e0b\u4e00\u8282\uff1aContext Engineering</li> <li>\u8fd4\u56de\uff1aLLM\u5347\u7ea7\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/llm-advanced/moe/","title":"MOE\u67b6\u6784","text":""},{"location":"fundamentals/llm-advanced/moe/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3\u4e13\u5bb6\u6df7\u5408\u6a21\u578b(Mixture of Experts)\u7684\u6838\u5fc3\u539f\u7406\u3001\u8def\u7531\u673a\u5236\u548c\u5de5\u7a0b\u5b9e\u73b0\u6311\u6218\u3002</p>"},{"location":"fundamentals/llm-advanced/moe/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/llm-advanced/moe/#moe_1","title":"MOE\u57fa\u672c\u6982\u5ff5","text":"<p>Mixture of Experts (MOE) \u662f\u4e00\u79cd\u7a00\u758f\u6fc0\u6d3b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u5c06\u4e0d\u540c\u8f93\u5165\u5206\u914d\u7ed9\u4e13\u95e8\u7684\"\u4e13\u5bb6\"\u5b50\u7f51\u7edc\u5904\u7406\u3002</p>"},{"location":"fundamentals/llm-advanced/moe/#_3","title":"\u6838\u5fc3\u601d\u60f3","text":"<ul> <li>\u6761\u4ef6\u8ba1\u7b97: \u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u8ba1\u7b97\u8def\u5f84</li> <li>\u4e13\u5bb6\u5206\u5de5: \u4e0d\u540c\u4e13\u5bb6\u5b66\u4e60\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u5f0f</li> <li>\u7a00\u758f\u6fc0\u6d3b: \u6bcf\u6b21\u53ea\u6fc0\u6d3b\u5c11\u6570\u4e13\u5bb6\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6</li> </ul>"},{"location":"fundamentals/llm-advanced/moe/#moe_2","title":"MOE\u6838\u5fc3\u7ec4\u4ef6","text":""},{"location":"fundamentals/llm-advanced/moe/#1-experts","title":"1. \u4e13\u5bb6\u7f51\u7edc (Experts)","text":"<pre><code>class Expert(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff)\n        self.w2 = nn.Linear(d_ff, d_model)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        return self.w2(self.activation(self.w1(x)))\n\n# \u591a\u4e2a\u4e13\u5bb6\u7ec4\u6210\u4e13\u5bb6\u6c60\nexperts = nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_experts)])\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#2-routergate","title":"2. \u8def\u7531\u7f51\u7edc (Router/Gate)","text":"<pre><code>class Router(nn.Module):\n    def __init__(self, d_model, num_experts):\n        super().__init__()\n        self.gate = nn.Linear(d_model, num_experts)\n\n    def forward(self, x):\n        # \u8ba1\u7b97\u6bcf\u4e2a\u4e13\u5bb6\u7684\u95e8\u63a7\u5206\u6570\n        logits = self.gate(x)  # [batch, seq_len, num_experts]\n\n        # \u9009\u62e9Top-K\u4e13\u5bb6\n        top_k_logits, top_k_indices = torch.topk(logits, k=2, dim=-1)\n        top_k_probs = F.softmax(top_k_logits, dim=-1)\n\n        return top_k_probs, top_k_indices\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#3","title":"3. \u805a\u5408\u673a\u5236","text":"<pre><code>def moe_forward(x, experts, router):\n    # \u83b7\u53d6\u8def\u7531\u4fe1\u606f\n    probs, indices = router(x)  # [batch, seq_len, k], [batch, seq_len, k]\n\n    # \u521d\u59cb\u5316\u8f93\u51fa\n    output = torch.zeros_like(x)\n\n    # \u5bf9\u6bcf\u4e2a\u9009\u4e2d\u7684\u4e13\u5bb6\u8ba1\u7b97\u8f93\u51fa\n    for i in range(k):\n        expert_idx = indices[:, :, i]\n        expert_prob = probs[:, :, i]\n\n        # \u83b7\u53d6\u5bf9\u5e94\u4e13\u5bb6\u7684\u8f93\u51fa\n        expert_output = experts[expert_idx](x)\n\n        # \u6309\u6982\u7387\u52a0\u6743\n        output += expert_prob.unsqueeze(-1) * expert_output\n\n    return output\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#_4","title":"\u8def\u7531\u7b56\u7565\u8be6\u89e3","text":""},{"location":"fundamentals/llm-advanced/moe/#1-token-choice","title":"1. Token-Choice\u8def\u7531","text":"<p>\u673a\u5236: \u6bcf\u4e2atoken\u9009\u62e9top-k\u4e2a\u4e13\u5bb6 <pre><code>def token_choice_routing(x, num_experts, k=2):\n    \"\"\"\u6bcf\u4e2atoken\u9009\u62e9k\u4e2a\u4e13\u5bb6\"\"\"\n    batch_size, seq_len, d_model = x.shape\n\n    # \u8def\u7531\u6253\u5206\n    router_logits = router(x)  # [batch, seq_len, num_experts]\n\n    # \u9009\u62e9top-k\u4e13\u5bb6\n    top_k_probs, top_k_indices = torch.topk(router_logits, k, dim=-1)\n    top_k_probs = F.softmax(top_k_probs, dim=-1)\n\n    return top_k_probs, top_k_indices\n</code></pre></p> <p>\u4f18\u52bf:  - \u4fdd\u8bc1\u6bcf\u4e2atoken\u90fd\u88ab\u5904\u7406 - \u63a7\u5236\u8ba1\u7b97\u590d\u6742\u5ea6\u7a33\u5b9a</p> <p>\u52a3\u52bf: - \u53ef\u80fd\u5bfc\u81f4\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861 - \u90e8\u5206\u4e13\u5bb6\u53ef\u80fd\u5f97\u4e0d\u5230\u8bad\u7ec3</p>"},{"location":"fundamentals/llm-advanced/moe/#2-expert-choice","title":"2. Expert-Choice\u8def\u7531","text":"<p>\u673a\u5236: \u6bcf\u4e2a\u4e13\u5bb6\u9009\u62e9top-k\u4e2atoken <pre><code>def expert_choice_routing(x, num_experts, capacity):\n    \"\"\"\u6bcf\u4e2a\u4e13\u5bb6\u9009\u62e9\u56fa\u5b9a\u6570\u91cf\u7684token\"\"\"\n    batch_size, seq_len, d_model = x.shape\n\n    # \u8def\u7531\u6253\u5206\n    router_logits = router(x)  # [batch, seq_len, num_experts]\n\n    # \u4e3a\u6bcf\u4e2a\u4e13\u5bb6\u9009\u62e9top tokens\n    expert_assignments = {}\n    for expert_id in range(num_experts):\n        expert_scores = router_logits[:, :, expert_id]\n        top_tokens = torch.topk(expert_scores.flatten(), capacity).indices\n        expert_assignments[expert_id] = top_tokens\n\n    return expert_assignments\n</code></pre></p> <p>\u4f18\u52bf: - \u81ea\u7136\u7684\u8d1f\u8f7d\u5747\u8861 - \u4e13\u5bb6\u80fd\u591f\u9009\u62e9\u6700\u76f8\u5173\u7684\u8f93\u5165</p> <p>\u52a3\u52bf: - \u53ef\u80fd\u6709token\u88ab\u4e22\u5f03 - \u5b9e\u73b0\u66f4\u590d\u6742</p>"},{"location":"fundamentals/llm-advanced/moe/#_5","title":"\u8d1f\u8f7d\u5747\u8861\u6280\u672f","text":""},{"location":"fundamentals/llm-advanced/moe/#1","title":"1. \u8f85\u52a9\u635f\u5931\u51fd\u6570","text":"<pre><code>def load_balancing_loss(router_probs, expert_indices, num_experts):\n    \"\"\"\u8ba1\u7b97\u8d1f\u8f7d\u5747\u8861\u635f\u5931\"\"\"\n    # \u8ba1\u7b97\u6bcf\u4e2a\u4e13\u5bb6\u88ab\u9009\u62e9\u7684\u9891\u7387\n    expert_counts = torch.zeros(num_experts)\n    for expert_id in range(num_experts):\n        expert_counts[expert_id] = (expert_indices == expert_id).float().sum()\n\n    # \u7406\u60f3\u60c5\u51b5\u4e0b\u6bcf\u4e2a\u4e13\u5bb6\u5904\u7406\u76f8\u540c\u6570\u91cf\u7684token\n    ideal_count = expert_indices.numel() / num_experts\n\n    # \u8ba1\u7b97\u8d1f\u8f7d\u4e0d\u5747\u8861\u635f\u5931\n    load_loss = torch.var(expert_counts) / (ideal_count ** 2)\n\n    return load_loss\n\n# \u603b\u635f\u5931 = \u4e3b\u4efb\u52a1\u635f\u5931 + \u03bb * \u8d1f\u8f7d\u5747\u8861\u635f\u5931\ntotal_loss = task_loss + lambda_balance * load_balancing_loss(probs, indices, num_experts)\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#2","title":"2. \u4e13\u5bb6\u5bb9\u91cf\u9650\u5236","text":"<pre><code>def capacity_limited_routing(router_logits, capacity_factor=1.25):\n    \"\"\"\u9650\u5236\u6bcf\u4e2a\u4e13\u5bb6\u7684\u5904\u7406\u5bb9\u91cf\"\"\"\n    num_tokens = router_logits.shape[0] * router_logits.shape[1]\n    expert_capacity = int(capacity_factor * num_tokens / num_experts)\n\n    # \u4e3a\u6bcf\u4e2a\u4e13\u5bb6\u5206\u914d\u56fa\u5b9a\u5bb9\u91cf\n    expert_assignments = []\n    expert_counts = torch.zeros(num_experts)\n\n    for token_idx in range(num_tokens):\n        # \u83b7\u53d6\u5f53\u524dtoken\u7684\u4e13\u5bb6\u504f\u597d\n        token_probs = F.softmax(router_logits.flatten()[token_idx], dim=-1)\n\n        # \u9009\u62e9\u5bb9\u91cf\u672a\u6ee1\u7684\u6700\u4f18\u4e13\u5bb6\n        for expert_id in torch.argsort(token_probs, descending=True):\n            if expert_counts[expert_id] &lt; expert_capacity:\n                expert_assignments.append((token_idx, expert_id))\n                expert_counts[expert_id] += 1\n                break\n\n    return expert_assignments\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#moe_3","title":"MOE\u53d8\u4f53\u548c\u4f18\u5316","text":""},{"location":"fundamentals/llm-advanced/moe/#1-moe-vs-moe","title":"1. \u7a00\u758fMOE vs \u5bc6\u96c6MOE","text":"\u7279\u6027 \u7a00\u758fMOE \u5bc6\u96c6MOE \u6fc0\u6d3b\u4e13\u5bb6\u6570 Top-K (K&lt;&lt;N) \u5168\u90e8\u4e13\u5bb6 \u8ba1\u7b97\u590d\u6742\u5ea6 O(K) O(N) \u53c2\u6570\u5229\u7528\u7387 \u4f4e \u9ad8 \u6269\u5c55\u6027 \u597d \u5dee"},{"location":"fundamentals/llm-advanced/moe/#2-moe","title":"2. \u5c42\u7ea7MOE","text":"<pre><code>class HierarchicalMoE(nn.Module):\n    \"\"\"\u5c42\u7ea7\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\"\"\"\n\n    def __init__(self, d_model, num_coarse_experts, num_fine_experts):\n        super().__init__()\n        # \u7c97\u7c92\u5ea6\u4e13\u5bb6\u9009\u62e9\n        self.coarse_router = Router(d_model, num_coarse_experts)\n\n        # \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u7ec4\n        self.fine_routers = nn.ModuleList([\n            Router(d_model, num_fine_experts) \n            for _ in range(num_coarse_experts)\n        ])\n\n        # \u4e13\u5bb6\u7f51\u7edc\n        self.experts = nn.ModuleList([\n            nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_fine_experts)])\n            for _ in range(num_coarse_experts)\n        ])\n\n    def forward(self, x):\n        # \u7b2c\u4e00\u5c42\uff1a\u9009\u62e9\u7c97\u7c92\u5ea6\u4e13\u5bb6\n        coarse_probs, coarse_indices = self.coarse_router(x)\n\n        output = torch.zeros_like(x)\n\n        # \u7b2c\u4e8c\u5c42\uff1a\u5728\u9009\u4e2d\u7684\u7c97\u7c92\u5ea6\u4e13\u5bb6\u5185\u9009\u62e9\u7ec6\u7c92\u5ea6\u4e13\u5bb6\n        for i, coarse_idx in enumerate(coarse_indices[0, 0]):  # \u7b80\u5316\u5904\u7406\n            fine_probs, fine_indices = self.fine_routers[coarse_idx](x)\n\n            # \u8ba1\u7b97\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8f93\u51fa\n            for j, fine_idx in enumerate(fine_indices[0, 0]):\n                expert_output = self.experts[coarse_idx][fine_idx](x)\n                weight = coarse_probs[0, 0, i] * fine_probs[0, 0, j]\n                output += weight * expert_output\n\n        return output\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#_6","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3\u6311\u6218","text":""},{"location":"fundamentals/llm-advanced/moe/#1_1","title":"1. \u901a\u4fe1\u6a21\u5f0f","text":"<pre><code># All-to-All\u901a\u4fe1\u6a21\u5f0f\ndef all_to_all_communication(tokens, expert_assignments):\n    \"\"\"\n    \u5c06tokens\u5206\u53d1\u5230\u4e0d\u540c\u8bbe\u5907\u4e0a\u7684\u4e13\u5bb6\n    \"\"\"\n    # Token dispatch: \u6839\u636e\u8def\u7531\u7ed3\u679c\u91cd\u65b0\u5206\u5e03token\n    expert_inputs = {}\n    for expert_id, token_list in expert_assignments.items():\n        device_id = expert_id % world_size\n        expert_inputs[device_id] = expert_inputs.get(device_id, []) + token_list\n\n    # \u8de8\u8bbe\u5907\u901a\u4fe1\n    for device_id, tokens in expert_inputs.items():\n        send_to_device(tokens, device_id)\n\n    # Expert processing\n    expert_outputs = process_on_experts(expert_inputs)\n\n    # Token combine: \u6536\u96c6\u4e13\u5bb6\u8f93\u51fa\n    final_outputs = all_gather(expert_outputs)\n\n    return final_outputs\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#2_1","title":"2. \u5185\u5b58\u4f18\u5316","text":"<pre><code>class MemoryEfficientMoE(nn.Module):\n    \"\"\"\u5185\u5b58\u9ad8\u6548\u7684MOE\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, num_experts, expert_capacity):\n        super().__init__()\n        self.num_experts = num_experts\n        self.expert_capacity = expert_capacity\n\n        # \u5171\u4eab\u4e13\u5bb6\u53c2\u6570\u5b58\u50a8\n        self.expert_weights = nn.Parameter(torch.randn(num_experts, d_model, d_ff))\n        self.router = Router(d_model, num_experts)\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u83b7\u53d6\u8def\u7531\u51b3\u7b56\n        probs, indices = self.router(x)\n\n        # \u91cd\u5851\u4e3a\u4e13\u5bb6\u6279\u5904\u7406\u683c\u5f0f\n        flat_x = x.view(-1, d_model)\n        flat_probs = probs.view(-1, 2)\n        flat_indices = indices.view(-1, 2)\n\n        # \u6279\u91cf\u5904\u7406\u51cf\u5c11\u5185\u5b58\u5360\u7528\n        outputs = []\n        for batch_start in range(0, flat_x.shape[0], self.expert_capacity):\n            batch_end = min(batch_start + self.expert_capacity, flat_x.shape[0])\n            batch_output = self._process_batch(\n                flat_x[batch_start:batch_end],\n                flat_probs[batch_start:batch_end],\n                flat_indices[batch_start:batch_end]\n            )\n            outputs.append(batch_output)\n\n        # \u91cd\u7ec4\u8f93\u51fa\n        final_output = torch.cat(outputs, dim=0)\n        return final_output.view(batch_size, seq_len, d_model)\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/llm-advanced/moe/#q1-moe","title":"Q1: MOE\u662f\u4ec0\u4e48\uff0c\u5b83\u6709\u4ec0\u4e48\u597d\u5904\u5462\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54: MOE\u662f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u673a\u5236\u8ba9\u4e0d\u540c\u7684\u5b50\u7f51\u7edc(\u4e13\u5bb6)\u5904\u7406\u4e0d\u540c\u7684\u8f93\u5165\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u91cf\u76f8\u5bf9\u7a33\u5b9a\u7684\u60c5\u51b5\u4e0b\u5927\u5e45\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u3002</p> <p>\u8be6\u7ec6\u89e3\u91ca: - \u5de5\u4f5c\u539f\u7406: \u8f93\u5165\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u9009\u62e9\u6fc0\u6d3b\u5c11\u6570\u51e0\u4e2a\u4e13\u5bb6 - \u6838\u5fc3\u4f18\u52bf: \u53c2\u6570\u91cf\u5927\u4f46\u8ba1\u7b97\u91cf\u53ef\u63a7\uff0c\u5b9e\u73b0\u6761\u4ef6\u8ba1\u7b97 - \u5b9e\u9645\u5e94\u7528: Google\u7684Switch Transformer\u3001GLaM\u3001PaLM\u7b49\u5927\u6a21\u578b</p>"},{"location":"fundamentals/llm-advanced/moe/#q2-moe","title":"Q2: MOE\u7684\u4e3b\u8981\u6311\u6218\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u6311\u6218:</p> <ol> <li>\u8d1f\u8f7d\u5747\u8861: \u9632\u6b62\u6240\u6709\u8f93\u5165\u90fd\u8def\u7531\u5230\u5c11\u6570\u4e13\u5bb6</li> <li>\u901a\u4fe1\u5f00\u9500: \u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684All-to-All\u901a\u4fe1\u6210\u672c\u9ad8</li> <li>\u8bad\u7ec3\u4e0d\u7a33\u5b9a: \u8def\u7531\u7f51\u7edc\u7684\u8bad\u7ec3\u53ef\u80fd\u4e0d\u6536\u655b</li> <li>\u63a8\u7406\u590d\u6742\u5ea6: \u52a8\u6001\u8def\u7531\u589e\u52a0\u63a8\u7406\u65f6\u7684\u8c03\u5ea6\u590d\u6742\u6027</li> </ol>"},{"location":"fundamentals/llm-advanced/moe/#q3-moe","title":"Q3: \u5982\u4f55\u89e3\u51b3MOE\u7684\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\uff1f","text":"<p>\u4e3b\u8981\u65b9\u6cd5:</p> <ol> <li>\u8f85\u52a9\u635f\u5931: \u6dfb\u52a0\u9f13\u52b1\u5747\u5300\u5206\u5e03\u7684\u6b63\u5219\u5316\u9879</li> <li>\u4e13\u5bb6\u5bb9\u91cf\u9650\u5236: \u9650\u5236\u6bcf\u4e2a\u4e13\u5bb6\u5904\u7406\u7684token\u6570\u91cf</li> <li>Expert-Choice\u8def\u7531: \u8ba9\u4e13\u5bb6\u4e3b\u52a8\u9009\u62e9\u8981\u5904\u7406\u7684token</li> <li>\u566a\u58f0\u6ce8\u5165: \u5728\u8def\u7531\u51b3\u7b56\u4e2d\u52a0\u5165\u968f\u673a\u6027</li> </ol>"},{"location":"fundamentals/llm-advanced/moe/#_8","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3MOE\u7684\u57fa\u672c\u67b6\u6784\u548c\u5de5\u4f5c\u539f\u7406</li> <li>[ ] \u638c\u63e1\u4e0d\u540c\u8def\u7531\u7b56\u7565\u7684\u4f18\u52a3</li> <li>[ ] \u4e86\u89e3\u8d1f\u8f7d\u5747\u8861\u7684\u91cd\u8981\u6027\u548c\u89e3\u51b3\u65b9\u6848</li> <li>[ ] \u7406\u89e3MOE\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6311\u6218</li> </ul>"},{"location":"fundamentals/llm-advanced/moe/#_9","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1a\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u4e0b\u4e00\u7ae0\uff1aDeepSeek\u4f18\u5316\u6280\u672f</li> <li>\u8fd4\u56de\uff1aLLM\u5347\u7ea7\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/","title":"\u7b2c1\u8282\uff1aTransformer\u57fa\u7840","text":""},{"location":"fundamentals/transformer/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1Transformer\u67b6\u6784\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u7406\u89e3Attention\u673a\u5236\u539f\u7406\uff0c\u533a\u5206\u4e0d\u540c\u7684\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u4e3a\u9762\u8bd5\u505a\u597d\u57fa\u7840\u51c6\u5907\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - Attention\u8ba1\u7b97\u516c\u5f0f\u548c\u539f\u7406 - FFN\u5728Transformer\u4e2d\u7684\u4f5c\u7528 - Encoder-Only vs Decoder-Only\u67b6\u6784\u5dee\u5f02 - \u4e3a\u4ec0\u4e48\u4e3b\u6d41\u5927\u6a21\u578b\u9009\u62e9Decoder-Only - GPT\u548cBERT\u7684\u67b6\u6784\u533a\u522b - BPE\u548cWordPiece\u7684\u533a\u522b</p>"},{"location":"fundamentals/transformer/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a3\u5929</p> <ul> <li>Day 1: Attention\u673a\u5236 + FFN\u6280\u672f\u6df1\u5165\u7406\u89e3</li> <li>Day 2: \u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784 + \u8bed\u8a00\u6a21\u578b\u67b6\u6784\u5bf9\u6bd4</li> <li>Day 3: Tokenizer\u6280\u672f + \u7efc\u5408\u5b9e\u8df5\u4e0e\u590d\u4e60</li> </ul>"},{"location":"fundamentals/transformer/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/transformer/#1-attention","title":"1. Attention\u673a\u5236","text":"<ul> <li>\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u516c\u5f0f\u63a8\u5bfc</li> <li>Softmax\u548c\u7f29\u653e\u56e0\u5b50\u4f5c\u7528</li> <li>\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236</li> <li>\u4ee3\u7801\u5b9e\u73b0\u7ec3\u4e60</li> </ul>"},{"location":"fundamentals/transformer/#2","title":"2. \u524d\u9988\u795e\u7ecf\u7f51\u7edc","text":"<ul> <li>FFN\u7ed3\u6784\u548c\u529f\u80fd\u539f\u7406</li> <li>\u6fc0\u6d3b\u51fd\u6570\u6f14\u8fdb(ReLU\u2192GELU\u2192SwiGLU)</li> <li>\u77e5\u8bc6\u5b58\u50a8\u673a\u5236</li> <li>\u4e0e\u6ce8\u610f\u529b\u7684\u4e92\u8865\u5173\u7cfb</li> </ul>"},{"location":"fundamentals/transformer/#3-","title":"3. \u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784","text":"<ul> <li>\u539f\u59cbTransformer\u5b8c\u6574\u67b6\u6784</li> <li>\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236\u8be6\u89e3</li> <li>\u63a9\u7801\u673a\u5236\u548c\u4ea4\u53c9\u6ce8\u610f\u529b</li> <li>\u73b0\u4ee3\u67b6\u6784\u6f14\u8fdb\u8d8b\u52bf</li> </ul>"},{"location":"fundamentals/transformer/#4","title":"4. \u8bed\u8a00\u6a21\u578b\u67b6\u6784","text":"<ul> <li>Encoder-Only vs Decoder-Only</li> <li>GPT vs BERT\u67b6\u6784\u5bf9\u6bd4</li> <li>\u4e3b\u6d41\u6a21\u578b\u9009\u62e9\u5206\u6790</li> <li>\u6a21\u578b\u67b6\u6784\u56fe\u89e3</li> </ul>"},{"location":"fundamentals/transformer/#5-tokenizer","title":"5. Tokenizer\u6280\u672f","text":"<ul> <li>BPE/WordPiece/Unigram\u7b97\u6cd5\u5bf9\u6bd4</li> <li>Byte-level BPE\u73b0\u4ee3\u65b9\u6848</li> <li>\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u548c\u4f18\u5316</li> <li>\u591a\u8bed\u8a00\u5904\u7406\u7b56\u7565</li> </ul>"},{"location":"fundamentals/transformer/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u4e09\u9879\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u95ee\u9898\u89e3\u7b54: \u80fd\u7528\u81ea\u5df1\u7684\u8bdd\u56de\u7b54\u6240\u67096\u4e2a\u6838\u5fc3\u9762\u8bd5\u95ee\u9898</li> <li>\u4ee3\u7801\u5b9e\u73b0: \u5b8c\u6210Self-Attention\u3001FFN\u3001Tokenizer\u7684\u7f16\u7a0b\u7ec3\u4e60</li> <li>\u67b6\u6784\u7406\u89e3: \u80fd\u753b\u51fa\u5e76\u89e3\u91ca\u5b8c\u6574\u7684Transformer\u67b6\u6784\u56fe</li> </ol>"},{"location":"fundamentals/transformer/#_5","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u4e00\u4e2a\u5b50\u6a21\u5757\u5f00\u59cb\u4f60\u7684\u5b66\u4e60\u4e4b\u65c5\uff01\u5efa\u8bae\u6309\u987a\u5e8f\u5b66\u4e60\uff0c\u6bcf\u4e2a\u6a21\u5757\u90fd\u5305\u542b\u7cbe\u9009\u7684\u9605\u8bfb\u6750\u6599\u3001\u89c6\u9891\u8d44\u6e90\u548c\u5b9e\u6218\u7ec3\u4e60\u3002</p>"},{"location":"fundamentals/transformer/attention/","title":"Attention\u673a\u5236","text":""},{"location":"fundamentals/transformer/attention/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3Self-Attention\u673a\u5236\u7684\u6570\u5b66\u539f\u7406\u548c\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u638c\u63e1\u9762\u8bd5\u4e2d\u7684\u9ad8\u9891\u95ee\u9898\u3002</p>"},{"location":"fundamentals/transformer/attention/#_2","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/transformer/attention/#_3","title":"\u5fc5\u8bfb\u6587\u7ae0","text":"<ol> <li>\u300aAttention is All You Need\u300b\u6d45\u8bfb\uff08\u7b80\u4ecb+\u4ee3\u7801\uff09 - \u79d1\u5b66\u7a7a\u95f4</li> <li>GPT\u4e0eBERT\u5dee\u522b\u6df1\u5165\u89e3\u6790 - \u77e5\u4e4e</li> <li>\u6df1\u5165\u6d45\u51fa\u7406\u89e3transformer - \u77e5\u4e4e  </li> <li>Transformer\u6a21\u578b\u8be6\u89e3\uff08\u56fe\u89e3\u6700\u5b8c\u6574\u7248\uff09 - \u77e5\u4e4e</li> <li>Transformer\u4f4d\u7f6e\u7f16\u7801\uff08\u57fa\u7840\uff09 - \u77e5\u4e4e</li> </ol>"},{"location":"fundamentals/transformer/attention/#_4","title":"\u539f\u8bba\u6587","text":"<ul> <li>Attention Is All You Need - \u7ecf\u5178\u5fc5\u8bfb</li> </ul>"},{"location":"fundamentals/transformer/attention/#_5","title":"\ud83c\udfac \u89c6\u9891\u6750\u6599","text":"<p>\u5b66\u4e60\u5efa\u8bae\uff1a \u500d\u901f\u89c2\u770b\uff0c\u91cd\u70b9\u7406\u89e3\u6982\u5ff5\u800c\u975e\u7ec6\u8282</p> <ol> <li>GPT\uff0cGPT-2\uff0cGPT-3 \u8bba\u6587\u7cbe\u8bfb - \u54d4\u54e9\u54d4\u54e9</li> <li>Llama 3.1\u8bba\u6587\u7cbe\u8bfb - \u54d4\u54e9\u54d4\u54e9</li> </ol>"},{"location":"fundamentals/transformer/attention/#_6","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/attention/#self-attention","title":"Self-Attention\u8ba1\u7b97\u516c\u5f0f","text":"\\[Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\\] <p>\u6838\u5fc3\u7ec4\u4ef6: - Query (Q): \u67e5\u8be2\u5411\u91cf\uff0c\u51b3\u5b9a\u5f53\u524d\u4f4d\u7f6e\u5173\u6ce8\u4ec0\u4e48 - Key (K): \u952e\u5411\u91cf\uff0c\u88ab\u67e5\u8be2\u7684\u5185\u5bb9 - Value (V): \u503c\u5411\u91cf\uff0c\u5b9e\u9645\u4f20\u9012\u7684\u4fe1\u606f - \u7f29\u653e\u56e0\u5b50: \\(\\sqrt{d_k}\\)\uff0c\u9632\u6b62softmax\u68af\u5ea6\u6d88\u5931</p>"},{"location":"fundamentals/transformer/attention/#multi-head-attention","title":"Multi-Head Attention","text":"<p>\u5c06\u8f93\u5165\u6295\u5f71\u5230\u591a\u4e2a\u4e0d\u540c\u7684\u5b50\u7a7a\u95f4\uff0c\u5e76\u884c\u8ba1\u7b97\u591a\u4e2a\u6ce8\u610f\u529b\u5934\uff1a</p> \\[MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\\] <p>\u5176\u4e2d \\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\\)</p>"},{"location":"fundamentals/transformer/attention/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/attention/#q1-attention","title":"Q1: Attention\u8ba1\u7b97\u516c\u5f0f\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6807\u51c6\u56de\u7b54\uff1a Self-Attention\u7684\u6838\u5fc3\u516c\u5f0f\u662f <code>Attention(Q,K,V) = softmax(QK^T/\u221ad_k)V</code>\u3002</p> <p>\u8be6\u7ec6\u89e3\u91ca\uff1a 1. \u5148\u8ba1\u7b97Query\u548cKey\u7684\u70b9\u79ef\u5f97\u5230\u6ce8\u610f\u529b\u5206\u6570 2. \u9664\u4ee5\u221ad_k\u8fdb\u884c\u7f29\u653e 3. \u901a\u8fc7softmax\u5f52\u4e00\u5316\u5f97\u5230\u6ce8\u610f\u529b\u6743\u91cd 4. \u6700\u540e\u4e0eValue\u76f8\u4e58\u5f97\u5230\u8f93\u51fa</p>"},{"location":"fundamentals/transformer/attention/#q11-d_k","title":"Q1.1: \u4e3a\u4ec0\u4e48\u8981\u9664\u4ee5\u6839\u53f7d_k\uff1f","text":"<p>\u6838\u5fc3\u539f\u56e0\uff1a \u9632\u6b62softmax\u51fd\u6570\u8fdb\u5165\u9971\u548c\u533a\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\u3002</p> <p>\u6280\u672f\u89e3\u91ca\uff1a - \u5f53d_k\u8f83\u5927\u65f6\uff0cQK^T\u7684\u65b9\u5dee\u4f1a\u5f88\u5927 - \u5927\u7684\u6570\u503c\u7ecf\u8fc7softmax\u540e\u68af\u5ea6\u63a5\u8fd10 - \u9664\u4ee5\u221ad_k\u53ef\u4ee5\u63a7\u5236\u65b9\u5dee\u4e3a1\uff0c\u4fdd\u6301\u68af\u5ea6\u7a33\u5b9a</p>"},{"location":"fundamentals/transformer/attention/#q12-softmax","title":"Q1.2: Softmax\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f","text":"<p>\u4e3b\u8981\u4f5c\u7528\uff1a 1. \u5f52\u4e00\u5316: \u786e\u4fdd\u6ce8\u610f\u529b\u6743\u91cd\u4e4b\u548c\u4e3a1 2. \u7a81\u51fa\u91cd\u70b9: \u901a\u8fc7\u6307\u6570\u51fd\u6570\u653e\u5927\u91cd\u8981\u7279\u5f81 3. \u53ef\u5fae\u5206: \u4fdd\u8bc1\u53cd\u5411\u4f20\u64ad\u53ef\u4ee5\u6b63\u5e38\u8fdb\u884c</p>"},{"location":"fundamentals/transformer/attention/#_8","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/attention/#1-self-attention","title":"\u7ec3\u4e601: \u5b9e\u73b0Self-Attention\u673a\u5236","text":"<p>\u5e73\u53f0: Deep-ML Self-Attention</p>"},{"location":"fundamentals/transformer/attention/#2-multi-head-attention","title":"\u7ec3\u4e602: \u5b9e\u73b0Multi-Head Attention","text":"<p>\u5e73\u53f0: Deep-ML Multi-Head Attention</p>"},{"location":"fundamentals/transformer/attention/#_9","title":"\u4ee3\u7801\u6a21\u677f","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass SelfAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # TODO: \u5b9e\u73b0\u6ce8\u610f\u529b\u8ba1\u7b97\n        pass\n\n    def forward(self, x, mask=None):\n        # TODO: \u5b9e\u73b0\u5b8c\u6574\u7684\u524d\u5411\u4f20\u64ad\n        pass\n</code></pre>"},{"location":"fundamentals/transformer/attention/#_10","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u68c0\u9a8c\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ul> <li>[ ] \u80fd\u753b\u51faSelf-Attention\u7684\u8ba1\u7b97\u6d41\u7a0b\u56fe</li> <li>[ ] \u5b8c\u6210Deep-ML\u5e73\u53f0\u7684\u4e24\u4e2a\u7f16\u7a0b\u7ec3\u4e60</li> <li>[ ] \u9762\u8bd5\u95ee\u9898\u80fd\u7528\u81ea\u5df1\u7684\u8bdd\u6d41\u5229\u56de\u7b54</li> </ul>"},{"location":"fundamentals/transformer/attention/#_11","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1a\u8bed\u8a00\u6a21\u578b\u67b6\u6784</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/encoder-decoder/","title":"\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3\u539f\u59cbTransformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u638c\u63e1\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236\u7684\u533a\u522b\u548c\u4f5c\u7528\u3002</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_3","title":"\u6574\u4f53\u67b6\u6784\u6982\u89c8","text":"<p>\u539f\u59cbTransformer\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668(Encoder-Decoder)\u67b6\u6784\uff0c\u4e13\u95e8\u7528\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u8f6c\u6362\u4efb\u52a1\u3002</p> <pre><code>\u8f93\u5165\u5e8f\u5217 \u2192 \u7f16\u7801\u5668 \u2192 \u7f16\u7801\u8868\u793a \u2192 \u89e3\u7801\u5668 \u2192 \u8f93\u51fa\u5e8f\u5217\n</code></pre>"},{"location":"fundamentals/transformer/encoder-decoder/#_4","title":"\u6838\u5fc3\u7ec4\u4ef6","text":"<ol> <li>\u7f16\u7801\u5668(Encoder): \u5c06\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u4e3a\u9ad8\u5c42\u8bed\u4e49\u8868\u793a</li> <li>\u89e3\u7801\u5668(Decoder): \u57fa\u4e8e\u7f16\u7801\u8868\u793a\u81ea\u56de\u5f52\u751f\u6210\u8f93\u51fa\u5e8f\u5217</li> <li>\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236: \u81ea\u6ce8\u610f\u529b\u3001\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u3001\u4ea4\u53c9\u6ce8\u610f\u529b</li> </ol>"},{"location":"fundamentals/transformer/encoder-decoder/#encoder","title":"\u7f16\u7801\u5668 (Encoder) \u8be6\u89e3","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_5","title":"\u7ed3\u6784\u7ec4\u6210","text":"<ul> <li>N\u5c42\u76f8\u540c\u7684\u5c42(\u539f\u8bba\u6587N=6)</li> <li>\u6bcf\u5c42\u5305\u542b\u4e24\u4e2a\u5b50\u5c42\uff1a</li> <li>\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236</li> <li>\u524d\u9988\u795e\u7ecf\u7f51\u7edc</li> <li>\u6bcf\u4e2a\u5b50\u5c42\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316</li> </ul>"},{"location":"fundamentals/transformer/encoder-decoder/#_6","title":"\u6570\u5b66\u8868\u793a","text":"<pre><code># \u7f16\u7801\u5668\u5c42\u7684\u8ba1\u7b97\u8fc7\u7a0b\ndef encoder_layer(x):\n    # \u591a\u5934\u81ea\u6ce8\u610f\u529b\n    attn_output = multi_head_attention(x, x, x)\n    x = layer_norm(x + attn_output)  # \u6b8b\u5dee\u8fde\u63a5 + \u5c42\u5f52\u4e00\u5316\n\n    # \u524d\u9988\u7f51\u7edc\n    ffn_output = feed_forward(x)\n    x = layer_norm(x + ffn_output)   # \u6b8b\u5dee\u8fde\u63a5 + \u5c42\u5f52\u4e00\u5316\n\n    return x\n</code></pre>"},{"location":"fundamentals/transformer/encoder-decoder/#_7","title":"\u6838\u5fc3\u7279\u5f81","text":"<p>1. \u5e76\u884c\u5904\u7406 - \u53ef\u4ee5\u540c\u65f6\u5904\u7406\u6574\u4e2a\u8f93\u5165\u5e8f\u5217 - \u6bcf\u4e2a\u4f4d\u7f6e\u90fd\u80fd\u770b\u5230\u6240\u6709\u5176\u4ed6\u4f4d\u7f6e - \u8bad\u7ec3\u6548\u7387\u9ad8\uff0c\u65e0\u5e8f\u5217\u8ba1\u7b97\u4f9d\u8d56</p> <p>2. \u53cc\u5411\u4e0a\u4e0b\u6587 - \u81ea\u6ce8\u610f\u529b\u673a\u5236\u5141\u8bb8\u6bcf\u4e2a\u4f4d\u7f6e\u5173\u6ce8\u6574\u4e2a\u5e8f\u5217 - \u80fd\u591f\u6355\u83b7\u5168\u5c40\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f - \u9002\u5408\u7406\u89e3\u7c7b\u4efb\u52a1</p> <p>3. \u8bed\u4e49\u63d0\u5347 - \u5c06\u4f4e\u9636\u8bcd\u5411\u91cf\u8f6c\u6362\u4e3a\u9ad8\u9636\u8bed\u4e49\u8868\u793a - \u591a\u5c42\u5806\u53e0\u9010\u6b65\u62bd\u8c61\u8bed\u4e49\u4fe1\u606f - \u6700\u7ec8\u8f93\u51fa\u5305\u542b\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f</p>"},{"location":"fundamentals/transformer/encoder-decoder/#decoder","title":"\u89e3\u7801\u5668 (Decoder) \u8be6\u89e3","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_8","title":"\u7ed3\u6784\u7ec4\u6210","text":"<ul> <li>N\u5c42\u76f8\u540c\u7684\u5c42(\u539f\u8bba\u6587N=6)</li> <li>\u6bcf\u5c42\u5305\u542b\u4e09\u4e2a\u5b50\u5c42\uff1a</li> <li>\u63a9\u7801\u591a\u5934\u81ea\u6ce8\u610f\u529b</li> <li>\u7f16\u7801\u5668-\u89e3\u7801\u5668\u4ea4\u53c9\u6ce8\u610f\u529b</li> <li>\u524d\u9988\u795e\u7ecf\u7f51\u7edc</li> <li>\u6bcf\u4e2a\u5b50\u5c42\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316</li> </ul>"},{"location":"fundamentals/transformer/encoder-decoder/#_9","title":"\u6570\u5b66\u8868\u793a","text":"<pre><code># \u89e3\u7801\u5668\u5c42\u7684\u8ba1\u7b97\u8fc7\u7a0b\ndef decoder_layer(x, encoder_output):\n    # 1. \u63a9\u7801\u81ea\u6ce8\u610f\u529b\n    masked_attn = masked_multi_head_attention(x, x, x)\n    x = layer_norm(x + masked_attn)\n\n    # 2. \u4ea4\u53c9\u6ce8\u610f\u529b\n    cross_attn = multi_head_attention(\n        query=x, \n        key=encoder_output, \n        value=encoder_output\n    )\n    x = layer_norm(x + cross_attn)\n\n    # 3. \u524d\u9988\u7f51\u7edc\n    ffn_output = feed_forward(x)\n    x = layer_norm(x + ffn_output)\n\n    return x\n</code></pre>"},{"location":"fundamentals/transformer/encoder-decoder/#_10","title":"\u6838\u5fc3\u7279\u5f81","text":"<p>1. \u81ea\u56de\u5f52\u751f\u6210 - \u9010\u6b65\u751f\u6210\u8f93\u51fa\u5e8f\u5217 - \u5f53\u524d\u4f4d\u7f6e\u53ea\u80fd\u770b\u5230\u4e4b\u524d\u7684\u4f4d\u7f6e - \u4f7f\u7528\u63a9\u7801\u673a\u5236\u9632\u6b62\u4fe1\u606f\u6cc4\u9732</p> <p>2. \u53cc\u8f93\u5165\u673a\u5236 - \u8f93\u51651\uff1a\u89e3\u7801\u5668\u4e4b\u524d\u7684\u8f93\u51fa(\u81ea\u56de\u5f52) - \u8f93\u51652\uff1a\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8868\u793a(\u4ea4\u53c9\u6ce8\u610f\u529b) - \u7ed3\u5408\u81ea\u8eab\u5386\u53f2\u548c\u6e90\u5e8f\u5217\u4fe1\u606f</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_11","title":"\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236\u8be6\u89e3","text":""},{"location":"fundamentals/transformer/encoder-decoder/#1-encoder-self-attention","title":"1. \u7f16\u7801\u5668\u81ea\u6ce8\u610f\u529b (Encoder Self-Attention)","text":"<p>\u4f5c\u7528: \u8ba9\u7f16\u7801\u5668\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5173\u6ce8\u8f93\u5165\u5e8f\u5217\u7684\u6240\u6709\u4f4d\u7f6e</p> <pre><code># \u7f16\u7801\u5668\u81ea\u6ce8\u610f\u529b\nQ = K = V = encoder_input  # \u90fd\u6765\u81ea\u8f93\u5165\u5e8f\u5217\nattention_output = Attention(Q, K, V)\n</code></pre> <p>\u7279\u70b9: - \u65e0\u63a9\u7801\u9650\u5236\uff0c\u53ef\u4ee5\u770b\u5230\u5168\u5e8f\u5217 - \u5efa\u7acb\u8f93\u5165\u5e8f\u5217\u5185\u90e8\u7684\u4f9d\u8d56\u5173\u7cfb - \u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56</p>"},{"location":"fundamentals/transformer/encoder-decoder/#2-masked-self-attention","title":"2. \u89e3\u7801\u5668\u63a9\u7801\u81ea\u6ce8\u610f\u529b (Masked Self-Attention)","text":"<p>\u4f5c\u7528: \u8ba9\u89e3\u7801\u5668\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u53ea\u5173\u6ce8\u4e4b\u524d\u7684\u4f4d\u7f6e</p> <pre><code># \u63a9\u7801\u81ea\u6ce8\u610f\u529b\nQ = K = V = decoder_input  # \u90fd\u6765\u81ea\u89e3\u7801\u5668\u8f93\u5165\nmask = create_causal_mask(seq_len)  # \u4e0b\u4e09\u89d2\u63a9\u7801\nattention_output = Attention(Q, K, V, mask=mask)\n</code></pre> <p>\u63a9\u7801\u673a\u5236: <pre><code>\u4f4d\u7f6e:  0  1  2  3\n\u63a9\u7801: [1  0  0  0]  # \u4f4d\u7f6e0\u53ea\u80fd\u770b\u81ea\u5df1\n      [1  1  0  0]  # \u4f4d\u7f6e1\u80fd\u770b0,1\n      [1  1  1  0]  # \u4f4d\u7f6e2\u80fd\u770b0,1,2\n      [1  1  1  1]  # \u4f4d\u7f6e3\u80fd\u770b0,1,2,3\n</code></pre></p>"},{"location":"fundamentals/transformer/encoder-decoder/#3-cross-attention","title":"3. \u7f16\u7801\u5668-\u89e3\u7801\u5668\u4ea4\u53c9\u6ce8\u610f\u529b (Cross-Attention)","text":"<p>\u4f5c\u7528: \u8ba9\u89e3\u7801\u5668\u5173\u6ce8\u7f16\u7801\u5668\u7684\u8f93\u51fa\uff0c\u5b9e\u73b0\u5e8f\u5217\u5bf9\u9f50</p> <pre><code># \u4ea4\u53c9\u6ce8\u610f\u529b\nQ = decoder_hidden        # \u67e5\u8be2\u6765\u81ea\u89e3\u7801\u5668\nK = V = encoder_output    # \u952e\u503c\u6765\u81ea\u7f16\u7801\u5668\nattention_output = Attention(Q, K, V)\n</code></pre> <p>\u5de5\u4f5c\u539f\u7406: - Query\uff1a\u89e3\u7801\u5668\u60f3\u8981\u4ec0\u4e48\u4fe1\u606f - Key\uff1a\u7f16\u7801\u5668\u6709\u4ec0\u4e48\u4fe1\u606f - Value\uff1a\u7f16\u7801\u5668\u63d0\u4f9b\u7684\u5177\u4f53\u4fe1\u606f - \u5b9e\u73b0\u6e90\u5e8f\u5217\u548c\u76ee\u6807\u5e8f\u5217\u7684\u5bf9\u9f50</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_12","title":"\u67b6\u6784\u4f18\u52bf\u4e0e\u5e94\u7528","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_13","title":"\u4f18\u52bf\u7279\u70b9","text":"<p>1. \u5e76\u884c\u8bad\u7ec3 - \u7f16\u7801\u5668\u53ef\u4ee5\u5e76\u884c\u5904\u7406\u6574\u4e2a\u8f93\u5165 - \u89e3\u7801\u5668\u5728\u8bad\u7ec3\u65f6\u4e5f\u53ef\u4ee5\u5e76\u884c(Teacher Forcing) - \u76f8\u6bd4RNN\u8bad\u7ec3\u901f\u5ea6\u5927\u5e45\u63d0\u5347</p> <p>2. \u957f\u8ddd\u79bb\u4f9d\u8d56 - \u6ce8\u610f\u529b\u673a\u5236\u76f4\u63a5\u8fde\u63a5\u4efb\u610f\u4e24\u4e2a\u4f4d\u7f6e - \u907f\u514d\u4e86RNN\u7684\u68af\u5ea6\u4f20\u64ad\u95ee\u9898 - \u66f4\u597d\u5730\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb</p> <p>3. \u53ef\u89e3\u91ca\u6027 - \u6ce8\u610f\u529b\u6743\u91cd\u63d0\u4f9b\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89c6\u5316 - \u53ef\u4ee5\u770b\u5230\u6a21\u578b\u5173\u6ce8\u7684\u8f93\u5165\u90e8\u5206 - \u4fbf\u4e8e\u5206\u6790\u548c\u8c03\u8bd5</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_14","title":"\u5178\u578b\u5e94\u7528","text":"<p>1. \u673a\u5668\u7ffb\u8bd1 - \u539f\u59cbTransformer\u7684\u8bbe\u8ba1\u76ee\u6807 - \u7f16\u7801\u5668\u7406\u89e3\u6e90\u8bed\u8a00\uff0c\u89e3\u7801\u5668\u751f\u6210\u76ee\u6807\u8bed\u8a00 - \u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u8bed\u8a00\u5bf9\u9f50</p> <p>2. \u6587\u672c\u6458\u8981 - \u7f16\u7801\u5668\u7406\u89e3\u539f\u6587\uff0c\u89e3\u7801\u5668\u751f\u6210\u6458\u8981 - \u4ea4\u53c9\u6ce8\u610f\u529b\u9009\u62e9\u91cd\u8981\u4fe1\u606f - \u63a7\u5236\u6458\u8981\u957f\u5ea6\u548c\u5185\u5bb9</p> <p>3. \u5bf9\u8bdd\u7cfb\u7edf - \u7f16\u7801\u5668\u7406\u89e3\u7528\u6237\u8f93\u5165 - \u89e3\u7801\u5668\u751f\u6210\u56de\u590d - \u7ef4\u6301\u5bf9\u8bdd\u4e0a\u4e0b\u6587</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_15","title":"\u73b0\u4ee3\u53d1\u5c55\u8d8b\u52bf","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_16","title":"\u67b6\u6784\u6f14\u8fdb","text":"<p>Encoder-Only: - \u4ee3\u8868: BERT, RoBERTa - \u64c5\u957f: \u7406\u89e3\u4efb\u52a1(\u5206\u7c7b\u3001\u9605\u8bfb\u7406\u89e3) - \u7279\u70b9: \u53cc\u5411\u6ce8\u610f\u529b\uff0c\u5e76\u884c\u8bad\u7ec3</p> <p>Decoder-Only: - \u4ee3\u8868: GPT, LLaMA, ChatGPT - \u64c5\u957f: \u751f\u6210\u4efb\u52a1(\u5bf9\u8bdd\u3001\u5199\u4f5c) - \u7279\u70b9: \u56e0\u679c\u6ce8\u610f\u529b\uff0c\u7edf\u4e00\u8303\u5f0f</p> <p>\u4e3a\u4ec0\u4e48Decoder-Only\u6210\u4e3a\u4e3b\u6d41\uff1f 1. \u4efb\u52a1\u7edf\u4e00: \u6240\u6709\u4efb\u52a1\u90fd\u53ef\u4ee5\u8868\u8ff0\u4e3a\u751f\u6210\u95ee\u9898 2. \u6269\u5c55\u6027\u597d: \u66f4\u5bb9\u6613\u6269\u5c55\u5230\u5927\u89c4\u6a21 3. \u6d8c\u73b0\u80fd\u529b: \u5927\u89c4\u6a21\u540e\u5c55\u73b0\u5f3a\u5927\u7684few-shot\u80fd\u529b 4. \u5de5\u7a0b\u7b80\u5316: \u67b6\u6784\u66f4\u7b80\u5355\uff0c\u6613\u4e8e\u4f18\u5316</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_17","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/encoder-decoder/#q1","title":"Q1: \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u4e3b\u8981\u533a\u522b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u533a\u522b:</p> \u7ef4\u5ea6 \u7f16\u7801\u5668 \u89e3\u7801\u5668 \u6ce8\u610f\u529b\u7c7b\u578b \u53cc\u5411\u81ea\u6ce8\u610f\u529b \u5355\u5411\u63a9\u7801\u81ea\u6ce8\u610f\u529b + \u4ea4\u53c9\u6ce8\u610f\u529b \u5904\u7406\u65b9\u5f0f \u5e76\u884c\u5904\u7406 \u81ea\u56de\u5f52\u751f\u6210 \u8f93\u5165\u6765\u6e90 \u539f\u59cb\u8f93\u5165\u5e8f\u5217 \u524d\u4e00\u6b65\u8f93\u51fa + \u7f16\u7801\u5668\u8f93\u51fa \u4e3b\u8981\u529f\u80fd \u7406\u89e3\u548c\u7f16\u7801 \u751f\u6210\u548c\u89e3\u7801"},{"location":"fundamentals/transformer/encoder-decoder/#q2","title":"Q2: \u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u5de5\u4f5c\u539f\u7406\u662f\u4ec0\u4e48\uff1f","text":"<p>\u5de5\u4f5c\u673a\u5236: 1. Query\u6765\u81ea\u89e3\u7801\u5668: \u8868\u793a\"\u6211\u60f3\u8981\u4ec0\u4e48\u4fe1\u606f\" 2. Key/Value\u6765\u81ea\u7f16\u7801\u5668: \u8868\u793a\"\u53ef\u4ee5\u63d0\u4f9b\u4ec0\u4e48\u4fe1\u606f\" 3. \u6ce8\u610f\u529b\u8ba1\u7b97: \u8ba1\u7b97\u89e3\u7801\u5668\u5bf9\u7f16\u7801\u5668\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u5173\u6ce8\u5ea6 4. \u4fe1\u606f\u878d\u5408: \u6839\u636e\u6ce8\u610f\u529b\u6743\u91cd\u805a\u5408\u7f16\u7801\u5668\u4fe1\u606f</p> <p>\u6570\u5b66\u8fc7\u7a0b: \\(\\(CrossAttention = Attention(Q_{decoder}, K_{encoder}, V_{encoder})\\)\\)</p>"},{"location":"fundamentals/transformer/encoder-decoder/#q3","title":"Q3: \u4e3a\u4ec0\u4e48\u89e3\u7801\u5668\u9700\u8981\u63a9\u7801\u81ea\u6ce8\u610f\u529b\uff1f","text":"<p>\u6838\u5fc3\u539f\u56e0: \u9632\u6b62\u4fe1\u606f\u6cc4\u9732</p> <p>\u8be6\u7ec6\u89e3\u91ca: 1. \u8bad\u7ec3\u65f6: \u4f7f\u7528Teacher Forcing\uff0c\u6a21\u578b\u80fd\u770b\u5230\u5b8c\u6574\u76ee\u6807\u5e8f\u5217 2. \u63a8\u7406\u65f6: \u53ea\u80fd\u770b\u5230\u5df2\u751f\u6210\u7684\u90e8\u5206 3. \u4e00\u81f4\u6027\u8981\u6c42: \u8bad\u7ec3\u548c\u63a8\u7406\u7684\u53ef\u89c1\u4fe1\u606f\u5fc5\u987b\u4e00\u81f4 4. \u63a9\u7801\u4f5c\u7528: \u5728\u8bad\u7ec3\u65f6\u4eba\u4e3a\u9650\u5236\u53ef\u89c1\u8303\u56f4</p> <p>\u4ee3\u7801\u793a\u4f8b: <pre><code>def create_causal_mask(seq_len):\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask.masked_fill(mask == 0, float('-inf'))\n</code></pre></p>"},{"location":"fundamentals/transformer/encoder-decoder/#q4-decoder-only","title":"Q4: \u4e3a\u4ec0\u4e48\u73b0\u5728\u66f4\u6d41\u884cDecoder-Only\u67b6\u6784\uff1f","text":"<p>\u4e3b\u8981\u539f\u56e0:</p> <ol> <li>\u7edf\u4e00\u6027: </li> <li>\u6240\u6709\u4efb\u52a1\u90fd\u53ef\u4ee5\u8f6c\u5316\u4e3a\u751f\u6210\u4efb\u52a1</li> <li>\u5206\u7c7b \u2192 \u751f\u6210\u7c7b\u522b\u6807\u7b7e</li> <li> <p>\u95ee\u7b54 \u2192 \u751f\u6210\u7b54\u6848</p> </li> <li> <p>\u6269\u5c55\u6027:</p> </li> <li>\u67b6\u6784\u7b80\u5355\uff0c\u6613\u4e8e\u6269\u5927\u89c4\u6a21</li> <li> <p>\u8bad\u7ec3\u66f4\u7a33\u5b9a\uff0c\u53c2\u6570\u5229\u7528\u7387\u9ad8</p> </li> <li> <p>\u6d8c\u73b0\u80fd\u529b:</p> </li> <li>\u5927\u89c4\u6a21\u8bad\u7ec3\u540e\u5c55\u73b0\u5f3a\u5927\u7684zero/few-shot\u80fd\u529b</li> <li> <p>\u6307\u4ee4\u8ddf\u968f\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b49\u80fd\u529b</p> </li> <li> <p>\u5de5\u7a0b\u4f18\u52bf:</p> </li> <li>\u5b9e\u73b0\u7b80\u5355\uff0c\u4f18\u5316\u6210\u719f</li> <li>\u63a8\u7406\u6548\u7387\u9ad8(KV Cache\u7b49\u6280\u672f)</li> </ol>"},{"location":"fundamentals/transformer/encoder-decoder/#_18","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/encoder-decoder/#encoder-decoder","title":"\u5b8c\u6574Encoder-Decoder\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass EncoderDecoderTransformer(nn.Module):\n    \"\"\"\u5b8c\u6574\u7684Encoder-Decoder Transformer\u5b9e\u73b0\"\"\"\n\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, \n                 num_heads=8, num_layers=6, d_ff=2048, max_seq_len=1000):\n        super().__init__()\n\n        self.d_model = d_model\n        self.max_seq_len = max_seq_len\n\n        # \u8bcd\u5d4c\u5165\u5c42\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n\n        # \u4f4d\u7f6e\u7f16\u7801\n        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n\n        # \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\n        self.encoder = Encoder(d_model, num_heads, num_layers, d_ff)\n        self.decoder = Decoder(d_model, num_heads, num_layers, d_ff)\n\n        # \u8f93\u51fa\u6295\u5f71\u5c42\n        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        # \u6e90\u5e8f\u5217\u7f16\u7801\n        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n        src_emb = self.pos_encoding(src_emb)\n        src_emb = self.dropout(src_emb)\n\n        # \u76ee\u6807\u5e8f\u5217\u7f16\u7801\n        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n        tgt_emb = self.pos_encoding(tgt_emb)\n        tgt_emb = self.dropout(tgt_emb)\n\n        # \u7f16\u7801\u5668\n        encoder_output = self.encoder(src_emb, src_mask)\n\n        # \u89e3\u7801\u5668\n        decoder_output = self.decoder(tgt_emb, encoder_output, tgt_mask, src_mask)\n\n        # \u8f93\u51fa\u6295\u5f71\n        output = self.output_projection(decoder_output)\n\n        return output\n\nclass Encoder(nn.Module):\n    \"\"\"Transformer\u7f16\u7801\u5668\"\"\"\n\n    def __init__(self, d_model, num_heads, num_layers, d_ff):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff)\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\nclass EncoderLayer(nn.Module):\n    \"\"\"\u7f16\u7801\u5668\u5c42\"\"\"\n\n    def __init__(self, d_model, num_heads, d_ff):\n        super().__init__()\n        self.self_attention = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x, mask=None):\n        # \u81ea\u6ce8\u610f\u529b\u5b50\u5c42\n        attn_output = self.self_attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n\n        # \u524d\u9988\u7f51\u7edc\u5b50\u5c42\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n\n        return x\n\nclass Decoder(nn.Module):\n    \"\"\"Transformer\u89e3\u7801\u5668\"\"\"\n\n    def __init__(self, d_model, num_heads, num_layers, d_ff):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff)\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, x, encoder_output, tgt_mask=None, src_mask=None):\n        for layer in self.layers:\n            x = layer(x, encoder_output, tgt_mask, src_mask)\n        return x\n\nclass DecoderLayer(nn.Module):\n    \"\"\"\u89e3\u7801\u5668\u5c42\"\"\"\n\n    def __init__(self, d_model, num_heads, d_ff):\n        super().__init__()\n        self.masked_self_attention = MultiHeadAttention(d_model, num_heads)\n        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x, encoder_output, tgt_mask=None, src_mask=None):\n        # \u63a9\u7801\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\n        masked_attn = self.masked_self_attention(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(masked_attn))\n\n        # \u4ea4\u53c9\u6ce8\u610f\u529b\u5b50\u5c42\n        cross_attn = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n        x = self.norm2(x + self.dropout(cross_attn))\n\n        # \u524d\u9988\u7f51\u7edc\u5b50\u5c42\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n\n        return x\n\ndef create_causal_mask(seq_len):\n    \"\"\"\u521b\u5efa\u56e0\u679c\u63a9\u7801(\u4e0b\u4e09\u89d2\u77e9\u9635)\"\"\"\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask.masked_fill(mask == 0, float('-inf'))\n\ndef create_padding_mask(seq, pad_token=0):\n    \"\"\"\u521b\u5efa\u586b\u5145\u63a9\u7801\"\"\"\n    return (seq != pad_token).unsqueeze(1).unsqueeze(2)\n\n# \u4f7f\u7528\u793a\u4f8b\ndef demo_encoder_decoder():\n    \"\"\"\u6f14\u793a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u4f7f\u7528\"\"\"\n\n    # \u6a21\u578b\u53c2\u6570\n    src_vocab_size = 1000\n    tgt_vocab_size = 1000\n    d_model = 512\n    seq_len = 20\n    batch_size = 2\n\n    # \u521b\u5efa\u6a21\u578b\n    model = EncoderDecoderTransformer(\n        src_vocab_size, tgt_vocab_size, d_model\n    )\n\n    # \u6a21\u62df\u6570\u636e\n    src = torch.randint(1, src_vocab_size, (batch_size, seq_len))\n    tgt = torch.randint(1, tgt_vocab_size, (batch_size, seq_len))\n\n    # \u521b\u5efa\u63a9\u7801\n    tgt_mask = create_causal_mask(seq_len)\n    src_mask = create_padding_mask(src)\n\n    # \u524d\u5411\u4f20\u64ad\n    output = model(src, tgt, src_mask, tgt_mask)\n\n    print(f\"\u8f93\u5165\u5f62\u72b6: {src.shape}\")\n    print(f\"\u8f93\u51fa\u5f62\u72b6: {output.shape}\")\n    print(f\"\u53c2\u6570\u91cf: {sum(p.numel() for p in model.parameters()):,}\")\n\nif __name__ == \"__main__\":\n    demo_encoder_decoder()\n</code></pre>"},{"location":"fundamentals/transformer/encoder-decoder/#_19","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u6574\u4f53\u8bbe\u8ba1</li> <li>[ ] \u638c\u63e1\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236\u7684\u533a\u522b\u548c\u4f5c\u7528</li> <li>[ ] \u7406\u89e3\u63a9\u7801\u673a\u5236\u7684\u5fc5\u8981\u6027\u548c\u5b9e\u73b0</li> <li>[ ] \u80fd\u591f\u5b9e\u73b0\u5b8c\u6574\u7684Encoder-Decoder\u6a21\u578b</li> <li>[ ] \u7406\u89e3\u4e3a\u4ec0\u4e48\u73b0\u4ee3\u6a21\u578b\u504f\u5411Decoder-Only</li> </ul>"},{"location":"fundamentals/transformer/encoder-decoder/#_20","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u524d\u9988\u795e\u7ecf\u7f51\u7edc</li> <li>\u4e0b\u4e00\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/ffn/","title":"\u524d\u9988\u795e\u7ecf\u7f51\u7edc (FFN)","text":""},{"location":"fundamentals/transformer/ffn/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3Transformer\u4e2d\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u4f5c\u7528\u673a\u5236\uff0c\u638c\u63e1\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u7684\u6f14\u8fdb\u548c\u77e5\u8bc6\u5b58\u50a8\u539f\u7406\u3002</p>"},{"location":"fundamentals/transformer/ffn/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/ffn/#ffn_1","title":"FFN\u7684\u57fa\u672c\u7ed3\u6784","text":"<p>\u524d\u9988\u795e\u7ecf\u7f51\u7edc(Feed-Forward Network)\u662fTransformer\u4e2d\u9664\u6ce8\u610f\u529b\u673a\u5236\u5916\u7684\u53e6\u4e00\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u4f4d\u4e8e\u6bcf\u4e2aTransformer\u5c42\u4e2d\u3002</p>"},{"location":"fundamentals/transformer/ffn/#_3","title":"\u6570\u5b66\u8868\u793a","text":"\\[FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2\\] <p>\u5176\u4e2d\uff1a - \\(W_1\\): \u7b2c\u4e00\u5c42\u6743\u91cd\u77e9\u9635 (\u5347\u7ef4) - \\(W_2\\): \u7b2c\u4e8c\u5c42\u6743\u91cd\u77e9\u9635 (\u964d\u7ef4) - \\(b_1, b_2\\): \u504f\u7f6e\u5411\u91cf - \\(\\max(0, \u00b7)\\): ReLU\u6fc0\u6d3b\u51fd\u6570</p>"},{"location":"fundamentals/transformer/ffn/#_4","title":"\u7ef4\u5ea6\u53d8\u5316","text":"<pre><code>\u8f93\u5165: [batch_size, seq_len, d_model]\n  \u2193 W1 (\u7ebf\u6027\u5c421)\n\u4e2d\u95f4: [batch_size, seq_len, d_ff]    # d_ff = 4 * d_model\n  \u2193 \u6fc0\u6d3b\u51fd\u6570\n\u4e2d\u95f4: [batch_size, seq_len, d_ff]    \n  \u2193 W2 (\u7ebf\u6027\u5c422)  \n\u8f93\u51fa: [batch_size, seq_len, d_model]\n</code></pre>"},{"location":"fundamentals/transformer/ffn/#ffn_2","title":"FFN\u7684\u6838\u5fc3\u529f\u80fd","text":""},{"location":"fundamentals/transformer/ffn/#1","title":"1. \u8bed\u4e49\u4fe1\u606f\u63d0\u53d6","text":"<ul> <li>\u9010\u4f4d\u7f6e\u5904\u7406: \u5bf9\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u4f4d\u7f6e\u72ec\u7acb\u8fdb\u884c\u975e\u7ebf\u6027\u53d8\u6362</li> <li>\u7279\u5f81\u6620\u5c04: \u5c06\u6ce8\u610f\u529b\u8f93\u51fa\u6620\u5c04\u5230\u66f4\u9ad8\u7ef4\u7684\u7279\u5f81\u7a7a\u95f4</li> <li>\u6a21\u5f0f\u8bc6\u522b: \u6355\u83b7\u590d\u6742\u7684\u8bed\u4e49\u6a21\u5f0f\u548c\u7279\u5f81\u7ec4\u5408</li> </ul>"},{"location":"fundamentals/transformer/ffn/#2","title":"2. \u77e5\u8bc6\u5b58\u50a8\u673a\u5236","text":"<p>FFN\u88ab\u8ba4\u4e3a\u662fTransformer\u7684\"\u8bb0\u5fc6\u5e93\"\uff1a</p> <p>\u5206\u5e03\u5f0f\u5b58\u50a8: - \u4e0d\u540c\u7684\u795e\u7ecf\u5143\u4e13\u95e8\u5b58\u50a8\u4e0d\u540c\u7c7b\u578b\u7684\u77e5\u8bc6 - \u901a\u8fc7\u6743\u91cd\u77e9\u9635\u7f16\u7801\u8bed\u8a00\u6a21\u5f0f\u548c\u4e16\u754c\u77e5\u8bc6 - \u7c7b\u4f3c\u4e8e\u952e\u503c\u5b58\u50a8\uff0c\u8f93\u5165\u4f5c\u4e3a\"\u952e\"\uff0c\u6fc0\u6d3b\u6a21\u5f0f\u4f5c\u4e3a\"\u503c\"</p> <p>\u77e5\u8bc6\u7535\u8def: - FFN\u4e2d\u7684\u7279\u5b9a\u795e\u7ecf\u5143\u6fc0\u6d3b\u8def\u5f84\u5f62\u6210\"\u77e5\u8bc6\u7535\u8def\" - \u8fd9\u4e9b\u7535\u8def\u7f16\u7801\u7279\u5b9a\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u4e8b\u5b9e\u77e5\u8bc6 - \u591a\u5c42FFN\u534f\u540c\u5de5\u4f5c\uff0c\u6784\u5efa\u590d\u6742\u7684\u77e5\u8bc6\u8868\u793a</p>"},{"location":"fundamentals/transformer/ffn/#3","title":"3. \u8868\u8fbe\u80fd\u529b\u589e\u5f3a","text":"<ul> <li>\u975e\u7ebf\u6027\u53d8\u6362: \u6fc0\u6d3b\u51fd\u6570\u5f15\u5165\u975e\u7ebf\u6027\uff0c\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b</li> <li>\u7ef4\u5ea6\u6269\u5c55: \u4e2d\u95f4\u5c42\u7684\u9ad8\u7ef4\u5ea6\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u7a7a\u95f4</li> <li>\u7279\u5f81\u4ea4\u4e92: \u4fc3\u8fdb\u4e0d\u540c\u7279\u5f81\u7ef4\u5ea6\u4e4b\u95f4\u7684\u4ea4\u4e92</li> </ul>"},{"location":"fundamentals/transformer/ffn/#_5","title":"\u6fc0\u6d3b\u51fd\u6570\u7684\u6f14\u8fdb","text":""},{"location":"fundamentals/transformer/ffn/#1-relu-transformer","title":"1. ReLU (\u65e9\u671fTransformer)","text":"\\[\\text{ReLU}(x) = \\max(0, x)\\] <p>\u7279\u70b9: - \u7b80\u5355\u9ad8\u6548\uff0c\u8ba1\u7b97\u91cf\u5c0f - \u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898 - \u4f46\u5b58\u5728\"\u6b7b\u795e\u7ecf\u5143\"\u95ee\u9898</p>"},{"location":"fundamentals/transformer/ffn/#2-gelu-gpt","title":"2. GELU (GPT\u7b49\u6a21\u578b)","text":"\\[\\text{GELU}(x) = x \\cdot \\Phi(x) = x \\cdot \\frac{1}{2}[1 + \\text{erf}(\\frac{x}{\\sqrt{2}})]\\] <p>\u7279\u70b9: - \u66f4\u5e73\u6ed1\u7684\u6fc0\u6d3b\u51fd\u6570 - \u5728\u8d1f\u503c\u533a\u57df\u6709\u975e\u96f6\u68af\u5ea6 - \u6027\u80fd\u901a\u5e38\u4f18\u4e8eReLU</p>"},{"location":"fundamentals/transformer/ffn/#3-swiglu","title":"3. SwiGLU (\u73b0\u4ee3\u5927\u6a21\u578b)","text":"\\[\\text{SwiGLU}(x) = \\text{Swish}(W_1 x) \\odot (W_2 x)$$ $$\\text{Swish}(x) = x \\cdot \\sigma(x)\\] <p>\u7279\u70b9: - \u95e8\u63a7\u673a\u5236\uff0c\u66f4\u597d\u7684\u7279\u5f81\u9009\u62e9 - \u9700\u8981\u989d\u5916\u53c2\u6570\u4f46\u6027\u80fd\u63d0\u5347\u660e\u663e - LLaMA\u3001PaLM\u7b49\u73b0\u4ee3\u6a21\u578b\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/transformer/ffn/#ffn_3","title":"FFN\u7684\u72ec\u7279\u7279\u6027","text":""},{"location":"fundamentals/transformer/ffn/#1_1","title":"1. \u4f4d\u7f6e\u65e0\u5173\u5904\u7406","text":"<pre><code># FFN\u5bf9\u6bcf\u4e2a\u4f4d\u7f6e\u72ec\u7acb\u5904\u7406\nfor position in sequence:\n    hidden = ffn_layer1(input[position])\n    hidden = activation(hidden)\n    output[position] = ffn_layer2(hidden)\n</code></pre>"},{"location":"fundamentals/transformer/ffn/#2_1","title":"2. \u4e0e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e92\u8865","text":"\u673a\u5236 \u6ce8\u610f\u529b FFN \u529f\u80fd \u5e8f\u5217\u5185\u4fe1\u606f\u6574\u5408 \u4f4d\u7f6e\u5185\u7279\u5f81\u63d0\u53d6 \u4f9d\u8d56 \u5168\u5e8f\u5217 \u5355\u4e2a\u4f4d\u7f6e \u4f5c\u7528 \u5efa\u6a21\u5173\u7cfb \u5b58\u50a8\u77e5\u8bc6 \u8ba1\u7b97 \u5e8f\u5217\u957f\u5ea6\u76f8\u5173 \u5e8f\u5217\u957f\u5ea6\u65e0\u5173"},{"location":"fundamentals/transformer/ffn/#ffn_4","title":"\u73b0\u4ee3FFN\u4f18\u5316\u6280\u672f","text":""},{"location":"fundamentals/transformer/ffn/#1-mixture-of-experts-moe","title":"1. Mixture of Experts (MoE)","text":"<ul> <li>\u5c06FFN\u66ff\u6362\u4e3a\u591a\u4e2a\u4e13\u5bb6\u7f51\u7edc</li> <li>\u901a\u8fc7\u8def\u7531\u673a\u5236\u52a8\u6001\u9009\u62e9\u4e13\u5bb6</li> <li>\u5728\u4fdd\u6301\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u5927\u5e45\u589e\u52a0\u53c2\u6570</li> </ul>"},{"location":"fundamentals/transformer/ffn/#2-memory-layers","title":"2. Memory Layers","text":"<ul> <li>\u5f15\u5165\u5916\u90e8\u8bb0\u5fc6\u673a\u5236</li> <li>\u7f13\u5b58\u548c\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6</li> <li>\u63d0\u9ad8\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b</li> </ul>"},{"location":"fundamentals/transformer/ffn/#3-kan-kolmogorov-arnold-networks","title":"3. KAN (Kolmogorov-Arnold Networks)","text":"<ul> <li>\u66ff\u4ee3\u4f20\u7edf\u7684\u7ebf\u6027\u5c42\u7ed3\u6784</li> <li>\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6fc0\u6d3b\u51fd\u6570</li> <li>\u7406\u8bba\u4e0a\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b</li> </ul>"},{"location":"fundamentals/transformer/ffn/#_6","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/ffn/#q1-ffntransformer","title":"Q1: FFN\u5728Transformer\u4e2d\u8d77\u4ec0\u4e48\u4f5c\u7528\uff1f","text":"<p>\u6838\u5fc3\u4f5c\u7528: 1. \u77e5\u8bc6\u5b58\u50a8: \u4f5c\u4e3a\u6a21\u578b\u7684\"\u8bb0\u5fc6\u5e93\"\uff0c\u5b58\u50a8\u8bed\u8a00\u6a21\u5f0f\u548c\u4e16\u754c\u77e5\u8bc6 2. \u7279\u5f81\u63d0\u53d6: \u5bf9\u6bcf\u4e2a\u4f4d\u7f6e\u8fdb\u884c\u975e\u7ebf\u6027\u7279\u5f81\u53d8\u6362 3. \u8868\u8fbe\u589e\u5f3a: \u901a\u8fc7\u9ad8\u7ef4\u6620\u5c04\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b 4. \u4e0e\u6ce8\u610f\u529b\u4e92\u8865: \u63d0\u4f9b\u4f4d\u7f6e\u5185\u7684\u6df1\u5ea6\u5904\u7406</p> <p>\u6280\u672f\u7ec6\u8282: - \u9010\u4f4d\u7f6e\u72ec\u7acb\u5904\u7406\uff0c\u4e0e\u6ce8\u610f\u529b\u7684\u5e8f\u5217\u5efa\u6a21\u5f62\u6210\u4e92\u8865 - \u901a\u8fc7\u5347\u7ef4-\u6fc0\u6d3b-\u964d\u7ef4\u7684\u8fc7\u7a0b\u589e\u5f3a\u7279\u5f81\u8868\u793a - \u53c2\u6570\u91cf\u901a\u5e38\u5360Transformer\u6a21\u578b\u603b\u53c2\u6570\u76842/3</p>"},{"location":"fundamentals/transformer/ffn/#q2-ffn","title":"Q2: \u4e3a\u4ec0\u4e48FFN\u8981\u5148\u5347\u7ef4\u518d\u964d\u7ef4\uff1f","text":"<p>\u8bbe\u8ba1\u539f\u7406: 1. \u8868\u793a\u7a7a\u95f4\u6269\u5c55: \u5347\u7ef4\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a\u7a7a\u95f4 2. \u975e\u7ebf\u6027\u5efa\u6a21: \u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u66f4\u5bb9\u6613\u62df\u5408\u590d\u6742\u51fd\u6570 3. \u7279\u5f81\u4ea4\u4e92: \u66f4\u591a\u7ef4\u5ea6\u5141\u8bb8\u66f4\u590d\u6742\u7684\u7279\u5f81\u7ec4\u5408 4. \u4fe1\u606f\u74f6\u9888: \u6700\u7ec8\u964d\u7ef4\u8d77\u5230\u4fe1\u606f\u7b5b\u9009\u7684\u4f5c\u7528</p> <p>\u6570\u5b66\u76f4\u89c9: <pre><code>d_model \u2192 d_ff \u2192 d_model\n512 \u2192 2048 \u2192 512\n</code></pre> \u4e2d\u95f4\u7684\u9ad8\u7ef4\u7a7a\u95f4\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u975e\u7ebf\u6027\u5efa\u6a21\u80fd\u529b\u3002</p>"},{"location":"fundamentals/transformer/ffn/#q3","title":"Q3: \u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u4ec0\u4e48\u5f71\u54cd\uff1f","text":"<p>\u6027\u80fd\u5bf9\u6bd4:</p> \u6fc0\u6d3b\u51fd\u6570 \u4f18\u52bf \u52a3\u52bf \u9002\u7528\u573a\u666f ReLU \u8ba1\u7b97\u7b80\u5355\uff0c\u8bad\u7ec3\u5feb \u6b7b\u795e\u7ecf\u5143\u95ee\u9898 \u65e9\u671f\u6a21\u578b GELU \u5e73\u6ed1\uff0c\u6027\u80fd\u597d \u8ba1\u7b97\u7a0d\u590d\u6742 \u4e2d\u7b49\u89c4\u6a21\u6a21\u578b SwiGLU \u6027\u80fd\u6700\u4f73\uff0c\u95e8\u63a7\u673a\u5236 \u53c2\u6570\u91cf\u589e\u52a0 \u73b0\u4ee3\u5927\u6a21\u578b <p>\u9009\u62e9\u7b56\u7565: - \u8ba1\u7b97\u8d44\u6e90\u5145\u8db3\uff1a\u9009\u62e9SwiGLU - \u5e73\u8861\u6027\u80fd\u548c\u6548\u7387\uff1a\u9009\u62e9GELU - \u6781\u5ea6\u5173\u6ce8\u901f\u5ea6\uff1a\u9009\u62e9ReLU</p>"},{"location":"fundamentals/transformer/ffn/#q4-ffn","title":"Q4: FFN\u5982\u4f55\u5b58\u50a8\u548c\u68c0\u7d22\u77e5\u8bc6\uff1f","text":"<p>\u5b58\u50a8\u673a\u5236: 1. \u5206\u5e03\u5f0f\u8868\u793a: \u77e5\u8bc6\u5206\u5e03\u5728\u4e0d\u540c\u795e\u7ecf\u5143\u7684\u6743\u91cd\u4e2d 2. \u6fc0\u6d3b\u6a21\u5f0f: \u7279\u5b9a\u8f93\u5165\u89e6\u53d1\u7279\u5b9a\u7684\u795e\u7ecf\u5143\u7ec4\u5408 3. \u5c42\u6b21\u7ed3\u6784: \u4e0d\u540c\u5c42\u7684FFN\u5b58\u50a8\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u7684\u77e5\u8bc6</p> <p>\u68c0\u7d22\u8fc7\u7a0b: <pre><code># \u7b80\u5316\u7684\u77e5\u8bc6\u68c0\u7d22\u8fc7\u7a0b\ninput_features = attention_output  # \u67e5\u8be2\"\u952e\"\nactivated_neurons = ffn_layer1(input_features)  # \u6fc0\u6d3b\u76f8\u5173\u795e\u7ecf\u5143\nknowledge_pattern = activation_function(activated_neurons)  # \u77e5\u8bc6\u6a21\u5f0f\noutput_knowledge = ffn_layer2(knowledge_pattern)  # \u68c0\u7d22\"\u503c\"\n</code></pre></p>"},{"location":"fundamentals/transformer/ffn/#_7","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/ffn/#ffn_5","title":"\u6807\u51c6FFN\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FeedForward(nn.Module):\n    \"\"\"\u6807\u51c6Transformer FFN\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1, activation='relu'):\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_ff\n\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n        # \u9009\u62e9\u6fc0\u6d3b\u51fd\u6570\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'gelu':\n            self.activation = nn.GELU()\n        else:\n            raise ValueError(f\"Unsupported activation: {activation}\")\n\n    def forward(self, x):\n        # x shape: [batch_size, seq_len, d_model]\n\n        # \u7b2c\u4e00\u5c42\uff1a\u5347\u7ef4 + \u6fc0\u6d3b\n        hidden = self.activation(self.linear1(x))\n        hidden = self.dropout(hidden)\n\n        # \u7b2c\u4e8c\u5c42\uff1a\u964d\u7ef4\n        output = self.linear2(hidden)\n\n        return output\n\nclass SwiGLU(nn.Module):\n    \"\"\"SwiGLU\u6fc0\u6d3b\u51fd\u6570\u7684FFN\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_ff\n\n        # SwiGLU\u9700\u8981\u4e24\u4e2a\u7ebf\u6027\u5c42\u7528\u4e8e\u95e8\u63a7\n        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n        self.w2 = nn.Linear(d_model, d_ff, bias=False) \n        self.w3 = nn.Linear(d_ff, d_model, bias=False)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # SwiGLU: swish(W1*x) \u2299 (W2*x)\n        swish_gate = F.silu(self.w1(x))  # Swish activation\n        linear_part = self.w2(x)\n\n        # \u95e8\u63a7\u673a\u5236\n        gated = swish_gate * linear_part\n        gated = self.dropout(gated)\n\n        # \u8f93\u51fa\u6295\u5f71\n        output = self.w3(gated)\n\n        return output\n\n# \u6027\u80fd\u5bf9\u6bd4\u793a\u4f8b\ndef compare_ffn_activations():\n    \"\"\"\u5bf9\u6bd4\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u7684FFN\u6027\u80fd\"\"\"\n\n    d_model, d_ff = 512, 2048\n    batch_size, seq_len = 32, 128\n\n    # \u6d4b\u8bd5\u6570\u636e\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # \u4e0d\u540cFFN\u5b9e\u73b0\n    ffn_relu = FeedForward(d_model, d_ff, activation='relu')\n    ffn_gelu = FeedForward(d_model, d_ff, activation='gelu')\n    ffn_swiglu = SwiGLU(d_model, d_ff)\n\n    print(\"=== FFN\u6fc0\u6d3b\u51fd\u6570\u5bf9\u6bd4 ===\")\n\n    # \u53c2\u6570\u91cf\u5bf9\u6bd4\n    relu_params = sum(p.numel() for p in ffn_relu.parameters())\n    gelu_params = sum(p.numel() for p in ffn_gelu.parameters())\n    swiglu_params = sum(p.numel() for p in ffn_swiglu.parameters())\n\n    print(f\"ReLU FFN\u53c2\u6570\u91cf: {relu_params:,}\")\n    print(f\"GELU FFN\u53c2\u6570\u91cf: {gelu_params:,}\")\n    print(f\"SwiGLU FFN\u53c2\u6570\u91cf: {swiglu_params:,}\")\n\n    # \u8ba1\u7b97\u65f6\u95f4\u5bf9\u6bd4\n    import time\n\n    with torch.no_grad():\n        # ReLU\n        start = time.time()\n        for _ in range(100):\n            _ = ffn_relu(x)\n        relu_time = time.time() - start\n\n        # GELU\n        start = time.time()\n        for _ in range(100):\n            _ = ffn_gelu(x)\n        gelu_time = time.time() - start\n\n        # SwiGLU\n        start = time.time()\n        for _ in range(100):\n            _ = ffn_swiglu(x)\n        swiglu_time = time.time() - start\n\n    print(f\"ReLU\u63a8\u7406\u65f6\u95f4: {relu_time:.4f}\u79d2\")\n    print(f\"GELU\u63a8\u7406\u65f6\u95f4: {gelu_time:.4f}\u79d2\")\n    print(f\"SwiGLU\u63a8\u7406\u65f6\u95f4: {swiglu_time:.4f}\u79d2\")\n\n# \u77e5\u8bc6\u5b58\u50a8\u53ef\u89c6\u5316\nclass KnowledgeAnalyzer:\n    \"\"\"\u5206\u6790FFN\u4e2d\u7684\u77e5\u8bc6\u5b58\u50a8\u6a21\u5f0f\"\"\"\n\n    def __init__(self, ffn_model):\n        self.ffn = ffn_model\n\n    def analyze_neuron_activation(self, inputs, texts):\n        \"\"\"\u5206\u6790\u4e0d\u540c\u8f93\u5165\u5bf9\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u6a21\u5f0f\"\"\"\n\n        activations = []\n        with torch.no_grad():\n            for input_tensor in inputs:\n                # \u83b7\u53d6\u7b2c\u4e00\u5c42\u7684\u6fc0\u6d3b\n                hidden = torch.relu(self.ffn.linear1(input_tensor))\n                activations.append(hidden.mean(dim=1))  # \u5e73\u5747\u6c60\u5316\n\n        # \u5206\u6790\u6fc0\u6d3b\u6a21\u5f0f\n        activations = torch.stack(activations)\n\n        # \u627e\u51fa\u6700\u6d3b\u8dc3\u7684\u795e\u7ecf\u5143\n        neuron_activity = activations.mean(dim=0)\n        top_neurons = torch.topk(neuron_activity, k=10).indices\n\n        print(\"\u6700\u6d3b\u8dc3\u7684\u795e\u7ecf\u5143\u7d22\u5f15:\", top_neurons.tolist())\n\n        # \u5206\u6790\u4e0d\u540c\u8f93\u5165\u7684\u6fc0\u6d3b\u76f8\u4f3c\u6027\n        similarity_matrix = torch.cosine_similarity(\n            activations.unsqueeze(1), \n            activations.unsqueeze(0), \n            dim=2\n        )\n\n        return {\n            'activations': activations,\n            'top_neurons': top_neurons,\n            'similarity_matrix': similarity_matrix\n        }\n\nif __name__ == \"__main__\":\n    compare_ffn_activations()\n</code></pre>"},{"location":"fundamentals/transformer/ffn/#_8","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3FFN\u7684\u57fa\u672c\u7ed3\u6784\u548c\u6570\u5b66\u539f\u7406</li> <li>[ ] \u638c\u63e1\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u7684\u7279\u70b9\u548c\u9002\u7528\u573a\u666f</li> <li>[ ] \u7406\u89e3FFN\u7684\u77e5\u8bc6\u5b58\u50a8\u673a\u5236</li> <li>[ ] \u80fd\u5b9e\u73b0\u548c\u5bf9\u6bd4\u4e0d\u540c\u7684FFN\u53d8\u4f53</li> <li>[ ] \u7406\u89e3FFN\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e92\u8865\u5173\u7cfb</li> </ul>"},{"location":"fundamentals/transformer/ffn/#_9","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/language-models/","title":"\u8bed\u8a00\u6a21\u578b\u67b6\u6784","text":""},{"location":"fundamentals/transformer/language-models/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4e0d\u540cTransformer\u67b6\u6784\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u638c\u63e1GPT\u4e0eBERT\u7684\u6838\u5fc3\u5dee\u5f02\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5927\u6a21\u578b\u504f\u7231Decoder-Only\u67b6\u6784\u3002</p>"},{"location":"fundamentals/transformer/language-models/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/transformer/language-models/#_4","title":"\u6838\u5fc3\u6982\u5ff5\u6587\u7ae0","text":"<p>\u53c2\u8003Attention\u673a\u5236\u9875\u9762\u7684\u9605\u8bfb\u6750\u6599\uff0c\u91cd\u70b9\u5173\u6ce8\uff1a - Encoder-Only vs Decoder-Only\u67b6\u6784\u5bf9\u6bd4 - GPT\u7cfb\u5217\u6a21\u578b\u7684\u6f14\u8fdb - BERT\u7684\u53cc\u5411\u7f16\u7801\u7279\u6027</p>"},{"location":"fundamentals/transformer/language-models/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/language-models/#_6","title":"\u4e09\u5927\u67b6\u6784\u7c7b\u578b","text":"\u67b6\u6784\u7c7b\u578b \u4ee3\u8868\u6a21\u578b \u7279\u70b9 \u5e94\u7528\u573a\u666f Encoder-Only BERT, RoBERTa \u53cc\u5411\u6ce8\u610f\u529b\uff0c\u9002\u5408\u7406\u89e3\u4efb\u52a1 \u6587\u672c\u5206\u7c7b\u3001\u9605\u8bfb\u7406\u89e3\u3001\u60c5\u611f\u5206\u6790 Decoder-Only GPT, LLaMA, ChatGPT \u56e0\u679c\u6ce8\u610f\u529b\uff0c\u9002\u5408\u751f\u6210\u4efb\u52a1 \u6587\u672c\u751f\u6210\u3001\u5bf9\u8bdd\u3001\u4ee3\u7801\u751f\u6210 Encoder-Decoder T5, BART \u7f16\u7801-\u89e3\u7801\u7ed3\u6784 \u7ffb\u8bd1\u3001\u6458\u8981\u3001\u95ee\u7b54"},{"location":"fundamentals/transformer/language-models/#attention-mask","title":"Attention Mask\u5bf9\u6bd4","text":"<p>BERT (\u53cc\u5411\u6ce8\u610f\u529b): <pre><code>Token:  [CLS] I    love  AI   [SEP]\nMask:   \u2713     \u2713    \u2713     \u2713    \u2713\n\u6bcf\u4e2atoken\u90fd\u53ef\u4ee5\u770b\u5230\u6240\u6709\u5176\u4ed6token\n</code></pre></p> <p>GPT (\u56e0\u679c\u6ce8\u610f\u529b): <pre><code>Token:  I    love  AI    very  much\nMask:   \u2713    \n        \u2713    \u2713     \n        \u2713    \u2713     \u2713\n        \u2713    \u2713     \u2713     \u2713\n        \u2713    \u2713     \u2713     \u2713     \u2713\n\u6bcf\u4e2atoken\u53ea\u80fd\u770b\u5230\u5b83\u4e4b\u524d\u7684token\uff08\u5305\u62ec\u81ea\u5df1\uff09\n</code></pre></p>"},{"location":"fundamentals/transformer/language-models/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/language-models/#q1-encoder-onlydecoder-onlygptbert","title":"Q1: Encoder-Only\u548cDecoder-Only\u67b6\u6784\u662f\u4ec0\u4e48\uff1fGPT\u548cBERT\u5206\u522b\u662f\u4ec0\u4e48\u67b6\u6784\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54\uff1a - Encoder-Only: \u4f7f\u7528\u53cc\u5411\u6ce8\u610f\u529b\uff0c\u53ef\u4ee5\u540c\u65f6\u770b\u5230\u524d\u540e\u6587\u672c\uff0cBERT\u5c31\u662f\u8fd9\u79cd\u67b6\u6784 - Decoder-Only: \u4f7f\u7528\u56e0\u679c\u6ce8\u610f\u529b\uff0c\u53ea\u80fd\u770b\u5230\u5f53\u524d\u4f4d\u7f6e\u4e4b\u524d\u7684\u6587\u672c\uff0cGPT\u7cfb\u5217\u90fd\u662f\u8fd9\u79cd\u67b6\u6784</p>"},{"location":"fundamentals/transformer/language-models/#q11","title":"Q1.1: \u67b6\u6784\u533a\u522b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u533a\u522b\uff1a</p> <ol> <li>\u6ce8\u610f\u529b\u673a\u5236</li> <li>Encoder-Only: \u53cc\u5411\u6ce8\u610f\u529b\uff0c\u65e0\u63a9\u7801\u9650\u5236</li> <li> <p>Decoder-Only: \u56e0\u679c\u6ce8\u610f\u529b\uff0c\u4f7f\u7528\u4e0b\u4e09\u89d2\u63a9\u7801</p> </li> <li> <p>\u8bad\u7ec3\u76ee\u6807</p> </li> <li>BERT: \u63a9\u7801\u8bed\u8a00\u6a21\u578b(MLM) + \u4e0b\u4e00\u53e5\u9884\u6d4b(NSP)</li> <li> <p>GPT: \u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\uff0c\u9884\u6d4b\u4e0b\u4e00\u4e2atoken</p> </li> <li> <p>\u4f4d\u7f6e\u7f16\u7801</p> </li> <li>BERT: \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801</li> <li>GPT: \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff08\u65e9\u671f\uff09\u2192 \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff08\u73b0\u4ee3\uff09</li> </ol>"},{"location":"fundamentals/transformer/language-models/#q12","title":"Q1.2: \u7528\u9014\u5dee\u5f02\uff1f","text":"<p>BERT\u64c5\u957f\u7406\u89e3\u4efb\u52a1\uff1a - \u6587\u672c\u5206\u7c7b - \u60c5\u611f\u5206\u6790 - \u9605\u8bfb\u7406\u89e3 - \u5b9e\u4f53\u8bc6\u522b</p> <p>GPT\u64c5\u957f\u751f\u6210\u4efb\u52a1\uff1a - \u6587\u672c\u7eed\u5199 - \u5bf9\u8bdd\u751f\u6210 - \u4ee3\u7801\u751f\u6210 - \u521b\u610f\u5199\u4f5c</p>"},{"location":"fundamentals/transformer/language-models/#q13","title":"Q1.3: \u4f18\u52a3\u6bd4\u8f83","text":"\u65b9\u9762 BERT\u4f18\u52bf GPT\u4f18\u52bf \u7406\u89e3\u80fd\u529b \u53cc\u5411\u4e0a\u4e0b\u6587\uff0c\u7406\u89e3\u66f4\u6df1\u5165 \u9002\u5408\u5e8f\u5217\u751f\u6210\uff0c\u903b\u8f91\u8fde\u8d2f \u8bad\u7ec3\u6548\u7387 \u5e76\u884c\u8bad\u7ec3\u6240\u6709\u4f4d\u7f6e \u81ea\u56de\u5f52\u8bad\u7ec3\uff0c\u7b80\u5355\u7a33\u5b9a \u63a8\u7406\u901f\u5ea6 \u5e76\u884c\u63a8\u7406 \u9700\u8981\u9010\u6b65\u751f\u6210 \u5e94\u7528\u7075\u6d3b\u6027 \u9700\u8981\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03 Zero-shot\u80fd\u529b\u5f3a"},{"location":"fundamentals/transformer/language-models/#q2-decoder-only","title":"Q2: \u4e3a\u4ec0\u4e48\u4e3b\u6d41\u5927\u6a21\u578b\u90fd\u7528Decoder-Only\u67b6\u6784\uff1f","text":"<p>\u4e3b\u8981\u539f\u56e0\uff1a</p> <ol> <li>\u7edf\u4e00\u7684\u751f\u6210\u8303\u5f0f</li> <li>\u6240\u6709\u4efb\u52a1\u90fd\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6587\u672c\u751f\u6210</li> <li> <p>\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u7279\u6b8a\u67b6\u6784</p> </li> <li> <p>\u66f4\u5f3a\u7684\u6d8c\u73b0\u80fd\u529b</p> </li> <li>\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u540e\u5c55\u73b0\u51fa\u5f3a\u5927\u7684few-shot\u5b66\u4e60\u80fd\u529b</li> <li> <p>\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u5929\u7136\u9002\u5408\u5bf9\u8bdd\u573a\u666f</p> </li> <li> <p>\u6269\u5c55\u6027\u66f4\u597d</p> </li> <li>\u67b6\u6784\u7b80\u5355\uff0c\u6613\u4e8e\u6269\u5927\u6a21\u578b\u89c4\u6a21</li> <li> <p>\u8bad\u7ec3\u7a33\u5b9a\u6027\u66f4\u597d</p> </li> <li> <p>\u5e94\u7528\u573a\u666f\u66f4\u5e7f</p> </li> <li>\u4ece\u5bf9\u8bdd\u5230\u4ee3\u7801\u751f\u6210\uff0c\u4e00\u4e2a\u6a21\u578b\u641e\u5b9a</li> <li>\u5546\u4e1a\u4ef7\u503c\u66f4\u9ad8</li> </ol>"},{"location":"fundamentals/transformer/language-models/#q3-qwenllama","title":"Q3: \u7b80\u5355\u8bb2\u89e3Qwen/LLaMA\u6a21\u578b\u67b6\u6784","text":"<p>LLaMA\u67b6\u6784\u7279\u70b9\uff1a</p> <pre><code>\u8f93\u5165 \u2192 Token Embedding + \u4f4d\u7f6e\u7f16\u7801\n     \u2193\n   N \u00d7 Decoder Layer:\n     \u251c\u2500\u2500 RMSNorm\n     \u251c\u2500\u2500 Multi-Head Attention (RoPE\u4f4d\u7f6e\u7f16\u7801)\n     \u251c\u2500\u2500 \u6b8b\u5dee\u8fde\u63a5\n     \u251c\u2500\u2500 RMSNorm  \n     \u251c\u2500\u2500 SwiGLU FFN\n     \u2514\u2500\u2500 \u6b8b\u5dee\u8fde\u63a5\n     \u2193\n   RMSNorm \u2192 \u8f93\u51fa\u5c42 \u2192 \u9884\u6d4b\u4e0b\u4e00\u4e2atoken\n</code></pre> <p>\u5173\u952e\u6280\u672f\u6539\u8fdb\uff1a - RMSNorm: \u66ff\u4ee3LayerNorm\uff0c\u8ba1\u7b97\u66f4\u9ad8\u6548 - RoPE\u4f4d\u7f6e\u7f16\u7801: \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u652f\u6301\u66f4\u957f\u5e8f\u5217 - SwiGLU\u6fc0\u6d3b: \u66ff\u4ee3ReLU\uff0c\u6548\u679c\u66f4\u597d - Pre-Norm: \u5f52\u4e00\u5316\u524d\u7f6e\uff0c\u8bad\u7ec3\u66f4\u7a33\u5b9a</p>"},{"location":"fundamentals/transformer/language-models/#_8","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/language-models/#_9","title":"\u67b6\u6784\u5bf9\u6bd4\u793a\u4f8b","text":"<pre><code># BERT\u98ce\u683c\u7684\u53cc\u5411\u6ce8\u610f\u529b\nclass BidirectionalAttention(nn.Module):\n    def forward(self, x):\n        # \u53ef\u4ee5\u770b\u5230\u5168\u90e8\u5e8f\u5217\n        attn_mask = torch.ones(seq_len, seq_len)  # \u51681\u77e9\u9635\n        return self.attention(x, mask=attn_mask)\n\n# GPT\u98ce\u683c\u7684\u56e0\u679c\u6ce8\u610f\u529b  \nclass CausalAttention(nn.Module):\n    def forward(self, x):\n        # \u53ea\u80fd\u770b\u5230\u5f53\u524d\u4f4d\u7f6e\u4e4b\u524d\n        seq_len = x.size(1)\n        attn_mask = torch.tril(torch.ones(seq_len, seq_len))  # \u4e0b\u4e09\u89d2\u77e9\u9635\n        return self.attention(x, mask=attn_mask)\n</code></pre>"},{"location":"fundamentals/transformer/language-models/#_10","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u80fd\u753b\u51faBERT\u548cGPT\u7684\u67b6\u6784\u5bf9\u6bd4\u56fe</li> <li>[ ] \u7406\u89e3\u4e3a\u4ec0\u4e48GPT\u9700\u8981\u56e0\u679c\u63a9\u7801</li> <li>[ ] \u80fd\u89e3\u91ca\u4e3b\u6d41\u5927\u6a21\u578b\u9009\u62e9Decoder-Only\u7684\u539f\u56e0</li> <li>[ ] \u638c\u63e1LLaMA/Qwen\u7684\u5173\u952e\u6280\u672f\u6539\u8fdb\u70b9</li> </ul>"},{"location":"fundamentals/transformer/language-models/#_11","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aAttention\u673a\u5236</li> <li>\u4e0b\u4e00\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/tokenizer/","title":"Tokenizer\u6280\u672f","text":""},{"location":"fundamentals/transformer/tokenizer/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3Tokenizer\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u638c\u63e1\u4e0d\u540ctokenization\u7b97\u6cd5\u7684\u7279\u70b9\u548c\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u5927\u6a21\u578b\u7684\u6587\u672c\u5904\u7406\u5960\u5b9a\u57fa\u7840\u3002</p>"},{"location":"fundamentals/transformer/tokenizer/#_2","title":"\ud83d\udcdd \u6280\u672f\u539f\u7406\u89e3\u6790","text":""},{"location":"fundamentals/transformer/tokenizer/#tokenizer_1","title":"Tokenizer\u57fa\u672c\u6982\u5ff5","text":"<p>Tokenizer\u662f\u5c06\u539f\u59cb\u6587\u672c\u8f6c\u6362\u4e3a\u6a21\u578b\u53ef\u5904\u7406\u7684\u6570\u503c\u8868\u793a\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u662f\u8fde\u63a5\u4eba\u7c7b\u8bed\u8a00\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6865\u6881\u3002</p>"},{"location":"fundamentals/transformer/tokenizer/#_3","title":"\u6838\u5fc3\u529f\u80fd","text":"<pre><code># Tokenizer\u7684\u57fa\u672c\u6d41\u7a0b\ntext = \"Hello, world! This is a test.\"\n     \u2193 1. \u6587\u672c\u89c4\u8303\u5316(Normalization)\nnormalized = \"hello world this is a test\"\n     \u2193 2. \u9884\u5206\u8bcd(Pre-tokenization)  \npre_tokens = [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\"]\n     \u2193 3. \u6a21\u578b\u5904\u7406(Model Processing)\ntokens = [\"hello\", \"wor\", \"##ld\", \"this\", \"is\", \"a\", \"test\"]\n     \u2193 4. \u540e\u5904\u7406(Post-processing)\nfinal_tokens = [\"[CLS]\", \"hello\", \"wor\", \"##ld\", \"this\", \"is\", \"a\", \"test\", \"[SEP]\"]\n     \u2193 5. \u6570\u503c\u6620\u5c04\ntoken_ids = [101, 7592, 24829, 2094, 2023, 2003, 1037, 3231, 102]\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#tokenization","title":"\u4e09\u5927Tokenization\u8303\u5f0f","text":""},{"location":"fundamentals/transformer/tokenizer/#1-word-level","title":"1. \u8bcd\u7ea7\u522b\u5206\u8bcd (Word-level)","text":"<p>\u539f\u7406: \u5c06\u6587\u672c\u6309\u5b8c\u6574\u5355\u8bcd\u5206\u5272</p> <pre><code>class WordLevelTokenizer:\n    def __init__(self, vocab_path):\n        self.word_to_id = self.load_vocab(vocab_path)\n        self.id_to_word = {v: k for k, v in self.word_to_id.items()}\n        self.unk_token = \"[UNK]\"\n\n    def tokenize(self, text):\n        words = text.lower().split()\n        tokens = []\n        for word in words:\n            if word in self.word_to_id:\n                tokens.append(word)\n            else:\n                tokens.append(self.unk_token)  # \u672a\u77e5\u8bcd\u5904\u7406\n        return tokens\n\n    def encode(self, text):\n        tokens = self.tokenize(text)\n        return [self.word_to_id.get(token, self.word_to_id[self.unk_token]) \n                for token in tokens]\n</code></pre> <p>\u4f18\u52bf: - \u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027 - \u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u4e60\u60ef - \u5b9e\u73b0\u7b80\u5355\u76f4\u89c2</p> <p>\u52a3\u52bf: - \u8bcd\u6c47\u8868\u5e9e\u5927(\u901a\u5e38&gt;100k) - \u672a\u767b\u5f55\u8bcd(OOV)\u95ee\u9898\u4e25\u91cd - \u65e0\u6cd5\u5904\u7406\u5f62\u6001\u53d8\u5316\u4e30\u5bcc\u7684\u8bed\u8a00</p>"},{"location":"fundamentals/transformer/tokenizer/#2-character-level","title":"2. \u5b57\u7b26\u7ea7\u522b\u5206\u8bcd (Character-level)","text":"<p>\u539f\u7406: \u5c06\u6587\u672c\u5206\u89e3\u4e3a\u5355\u4e2a\u5b57\u7b26</p> <pre><code>class CharacterLevelTokenizer:\n    def __init__(self):\n        # \u57fa\u672c\u5b57\u7b26\u96c6\n        self.chars = list(\"abcdefghijklmnopqrstuvwxyz0123456789 .,!?'\")\n        self.char_to_id = {char: i for i, char in enumerate(self.chars)}\n        self.id_to_char = {i: char for i, char in enumerate(self.chars)}\n\n    def tokenize(self, text):\n        return list(text.lower())\n\n    def encode(self, text):\n        chars = self.tokenize(text)\n        return [self.char_to_id.get(char, 0) for char in chars]  # 0 for unknown\n\n    def decode(self, token_ids):\n        chars = [self.id_to_char.get(id, \"\") for id in token_ids]\n        return \"\".join(chars)\n</code></pre> <p>\u4f18\u52bf: - \u8bcd\u6c47\u8868\u6781\u5c0f(\u901a\u5e38&lt;100) - \u65e0OOV\u95ee\u9898 - \u5bf9\u62fc\u5199\u9519\u8bef\u9c81\u68d2</p> <p>\u52a3\u52bf: - \u5e8f\u5217\u957f\u5ea6\u5927\u5e45\u589e\u52a0 - \u4e22\u5931\u8bcd\u8fb9\u754c\u4fe1\u606f - \u8bed\u4e49\u7406\u89e3\u56f0\u96be</p>"},{"location":"fundamentals/transformer/tokenizer/#3-subword-level","title":"3. \u5b50\u8bcd\u7ea7\u522b\u5206\u8bcd (Subword-level)","text":"<p>\u6838\u5fc3\u601d\u60f3: \u5728\u8bcd\u6c47\u8868\u5927\u5c0f\u548c\u8bed\u4e49\u4fdd\u6301\u4e4b\u95f4\u627e\u5230\u5e73\u8861</p>"},{"location":"fundamentals/transformer/tokenizer/#subword","title":"\ud83d\udd2c \u4e3b\u6d41Subword\u7b97\u6cd5\u8be6\u89e3","text":""},{"location":"fundamentals/transformer/tokenizer/#1-bpe-byte-pair-encoding","title":"1. BPE (Byte Pair Encoding)","text":"<p>\u7b97\u6cd5\u539f\u7406: \u8fed\u4ee3\u5408\u5e76\u6700\u9891\u7e41\u7684\u5b57\u7b26\u5bf9</p>"},{"location":"fundamentals/transformer/tokenizer/#_4","title":"\u8bad\u7ec3\u8fc7\u7a0b","text":"<pre><code>class BPETokenizer:\n    def __init__(self):\n        self.vocab = {}\n        self.merges = []\n\n    def train(self, corpus, vocab_size):\n        # 1. \u521d\u59cb\u5316\uff1a\u5c06\u6240\u6709\u5b57\u7b26\u4f5c\u4e3a\u57fa\u7840\u8bcd\u6c47\n        word_freqs = self.get_word_frequencies(corpus)\n\n        # \u5c06\u6bcf\u4e2a\u8bcd\u5206\u89e3\u4e3a\u5b57\u7b26\n        vocab = set()\n        for word in word_freqs:\n            for char in word:\n                vocab.add(char)\n\n        vocab = list(vocab)\n\n        # 2. \u8fed\u4ee3\u5408\u5e76\u6700\u9891\u7e41\u7684\u5b57\u7b26\u5bf9\n        while len(vocab) &lt; vocab_size:\n            # \u7edf\u8ba1\u6240\u6709\u5b57\u7b26\u5bf9\u7684\u9891\u7387\n            pairs = self.get_all_pairs(word_freqs)\n\n            if not pairs:\n                break\n\n            # \u627e\u5230\u6700\u9891\u7e41\u7684\u5b57\u7b26\u5bf9\n            best_pair = max(pairs, key=pairs.get)\n\n            # \u5408\u5e76\u8fd9\u4e2a\u5b57\u7b26\u5bf9\n            vocab.append(''.join(best_pair))\n            self.merges.append(best_pair)\n\n            # \u66f4\u65b0\u8bcd\u9891\u5b57\u5178\n            word_freqs = self.merge_vocab(best_pair, word_freqs)\n\n        self.vocab = {token: i for i, token in enumerate(vocab)}\n\n    def get_all_pairs(self, word_freqs):\n        \"\"\"\u83b7\u53d6\u6240\u6709\u76f8\u90bb\u5b57\u7b26\u5bf9\u53ca\u5176\u9891\u7387\"\"\"\n        pairs = {}\n        for word, freq in word_freqs.items():\n            symbols = word.split()\n            for i in range(len(symbols) - 1):\n                pair = (symbols[i], symbols[i + 1])\n                pairs[pair] = pairs.get(pair, 0) + freq\n        return pairs\n\n    def merge_vocab(self, pair, word_freqs):\n        \"\"\"\u5408\u5e76\u6307\u5b9a\u5b57\u7b26\u5bf9\"\"\"\n        new_word_freqs = {}\n        bigram = ' '.join(pair)\n        replacement = ''.join(pair)\n\n        for word in word_freqs:\n            new_word = word.replace(bigram, replacement)\n            new_word_freqs[new_word] = word_freqs[word]\n\n        return new_word_freqs\n\n    def tokenize(self, text):\n        \"\"\"\u4f7f\u7528\u8bad\u7ec3\u597d\u7684BPE\u8fdb\u884c\u5206\u8bcd\"\"\"\n        words = text.split()\n        result = []\n\n        for word in words:\n            # \u5c06\u8bcd\u5206\u89e3\u4e3a\u5b57\u7b26\n            word_tokens = list(word)\n\n            # \u5e94\u7528\u6240\u6709\u5b66\u5230\u7684\u5408\u5e76\u89c4\u5219\n            for merge in self.merges:\n                word_tokens = self.apply_merge(word_tokens, merge)\n\n            result.extend(word_tokens)\n\n        return result\n\n    def apply_merge(self, tokens, merge):\n        \"\"\"\u5e94\u7528\u5355\u4e2a\u5408\u5e76\u89c4\u5219\"\"\"\n        new_tokens = []\n        i = 0\n        while i &lt; len(tokens):\n            if (i &lt; len(tokens) - 1 and \n                tokens[i] == merge[0] and \n                tokens[i + 1] == merge[1]):\n                # \u627e\u5230\u5339\u914d\u7684\u5bf9\uff0c\u5408\u5e76\n                new_tokens.append(''.join(merge))\n                i += 2\n            else:\n                new_tokens.append(tokens[i])\n                i += 1\n        return new_tokens\n\n# \u4f7f\u7528\u793a\u4f8b\ndef demo_bpe():\n    corpus = [\"low lower newest widest\", \"low lower newest widest\"] * 1000\n\n    bpe = BPETokenizer()\n    bpe.train(corpus, vocab_size=1000)\n\n    # \u6d4b\u8bd5\u5206\u8bcd\n    text = \"lowest\"\n    tokens = bpe.tokenize(text)\n    print(f\"'{text}' -&gt; {tokens}\")\n    # \u53ef\u80fd\u8f93\u51fa: ['low', 'est'] \u6216\u7c7b\u4f3c\u7684\u5b50\u8bcd\u7ec4\u5408\n\ndemo_bpe()\n</code></pre> <p>BPE\u7279\u70b9: - \u6570\u636e\u9a71\u52a8\uff0c\u65e0\u9700\u8bed\u8a00\u5b66\u77e5\u8bc6 - \u80fd\u5904\u7406\u672a\u89c1\u8fc7\u7684\u8bcd - GPT\u7cfb\u5217\u6a21\u578b\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/transformer/tokenizer/#2-wordpiece","title":"2. WordPiece","text":"<p>\u6838\u5fc3\u6539\u8fdb: \u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u4f3c\u7136\u5ea6\u9009\u62e9\u5408\u5e76</p> <pre><code>class WordPieceTokenizer:\n    def __init__(self):\n        self.vocab = {}\n        self.unk_token = \"[UNK]\"\n\n    def train(self, corpus, vocab_size):\n        # 1. \u521d\u59cb\u5316\u57fa\u7840\u8bcd\u6c47(\u5b57\u7b26 + \u7279\u6b8atoken)\n        base_vocab = set()\n        for text in corpus:\n            for char in text:\n                base_vocab.add(char)\n\n        base_vocab.update([\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n        vocab = list(base_vocab)\n\n        # 2. \u8fed\u4ee3\u6dfb\u52a0\u6700\u4f18\u5b50\u8bcd\n        while len(vocab) &lt; vocab_size:\n            best_subword = self.find_best_subword(corpus, vocab)\n            if best_subword is None:\n                break\n            vocab.append(best_subword)\n\n        self.vocab = {token: i for i, token in enumerate(vocab)}\n\n    def find_best_subword(self, corpus, current_vocab):\n        \"\"\"\u627e\u5230\u80fd\u6700\u5927\u5316\u8bed\u8a00\u6a21\u578b\u4f3c\u7136\u5ea6\u7684\u5b50\u8bcd\"\"\"\n        candidates = self.generate_candidates(corpus, current_vocab)\n\n        best_score = float('-inf')\n        best_subword = None\n\n        for candidate in candidates:\n            # \u8ba1\u7b97\u6dfb\u52a0\u8fd9\u4e2a\u5b50\u8bcd\u540e\u7684\u8bed\u8a00\u6a21\u578b\u5f97\u5206\n            score = self.calculate_lm_score(corpus, current_vocab + [candidate])\n\n            if score &gt; best_score:\n                best_score = score\n                best_subword = candidate\n\n        return best_subword\n\n    def generate_candidates(self, corpus, vocab):\n        \"\"\"\u751f\u6210\u5019\u9009\u5b50\u8bcd\"\"\"\n        candidates = set()\n\n        # \u57fa\u4e8e\u73b0\u6709\u8bcd\u6c47\u751f\u6210\u5019\u9009\n        for text in corpus:\n            tokens = self.basic_tokenize(text)\n            for token in tokens:\n                for i in range(len(token)):\n                    for j in range(i + 1, len(token) + 1):\n                        subword = token[i:j]\n                        if len(subword) &gt; 1 and subword not in vocab:\n                            candidates.add(subword)\n\n        return list(candidates)\n\n    def tokenize(self, text):\n        \"\"\"WordPiece\u5206\u8bcd\u7b97\u6cd5\"\"\"\n        tokens = []\n\n        for word in text.split():\n            # \u8d2a\u5fc3\u6700\u957f\u5339\u914d\n            start = 0\n            sub_tokens = []\n\n            while start &lt; len(word):\n                end = len(word)\n                cur_substr = None\n\n                # \u4ece\u6700\u957f\u5b50\u4e32\u5f00\u59cb\u5c1d\u8bd5\n                while start &lt; end:\n                    substr = word[start:end]\n                    if start &gt; 0:\n                        substr = \"##\" + substr  # WordPiece\u524d\u7f00\n\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n\n                if cur_substr is None:\n                    # \u65e0\u6cd5\u5206\u8bcd\uff0c\u6807\u8bb0\u4e3a\u672a\u77e5\n                    sub_tokens.append(self.unk_token)\n                    break\n\n                sub_tokens.append(cur_substr)\n                start = end\n\n            tokens.extend(sub_tokens)\n\n        return tokens\n\n# BERT\u4f7f\u7528\u7684WordPiece\u793a\u4f8b\ndef demo_wordpiece():\n    text = \"unaffable\"\n    # WordPiece\u53ef\u80fd\u5206\u8bcd\u4e3a: [\"un\", \"##aff\", \"##able\"]\n\n    wp = WordPieceTokenizer()\n    # \u5047\u8bbe\u5df2\u8bad\u7ec3\n    wp.vocab = {\"un\": 1, \"##aff\": 2, \"##able\": 3, \"[UNK]\": 0}\n\n    tokens = wp.tokenize(text)\n    print(f\"'{text}' -&gt; {tokens}\")\n\ndemo_wordpiece()\n</code></pre> <p>WordPiece\u4f18\u52bf: - \u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u4f18\u5316\uff0c\u7406\u8bba\u66f4\u4e25\u683c - BERT\u7b49\u6a21\u578b\u7684\u6807\u51c6\u9009\u62e9 - \u80fd\u66f4\u597d\u5730\u4fdd\u6301\u8bed\u4e49\u76f8\u5173\u6027</p>"},{"location":"fundamentals/transformer/tokenizer/#3-unigram-language-model","title":"3. Unigram Language Model","text":"<p>\u6838\u5fc3\u601d\u60f3: \u4ece\u5927\u8bcd\u6c47\u8868\u5f00\u59cb\uff0c\u9010\u6b65\u79fb\u9664\u4e0d\u91cd\u8981\u7684token</p> <pre><code>class UnigramTokenizer:\n    def __init__(self):\n        self.vocab = {}\n        self.token_probs = {}\n\n    def train(self, corpus, vocab_size):\n        # 1. \u521d\u59cb\u5316\u5927\u8bcd\u6c47\u8868\uff08\u5305\u542b\u6240\u6709\u53ef\u80fd\u7684\u5b50\u4e32\uff09\n        initial_vocab = self.build_initial_vocab(corpus)\n\n        # 2. \u521d\u59cb\u5316token\u6982\u7387\n        self.token_probs = self.initialize_probabilities(initial_vocab, corpus)\n\n        # 3. \u8fed\u4ee3\u51cf\u5c11\u8bcd\u6c47\u8868\n        current_vocab = list(initial_vocab)\n\n        while len(current_vocab) &gt; vocab_size:\n            # \u8ba1\u7b97\u79fb\u9664\u6bcf\u4e2atoken\u7684\u635f\u5931\n            losses = {}\n            for token in current_vocab:\n                if self.is_removable(token):  # \u4fdd\u62a4\u7279\u6b8atoken\n                    losses[token] = self.calculate_removal_loss(token, corpus)\n\n            # \u79fb\u9664\u635f\u5931\u6700\u5c0f\u7684token\n            if losses:\n                token_to_remove = min(losses, key=losses.get)\n                current_vocab.remove(token_to_remove)\n                del self.token_probs[token_to_remove]\n\n                # \u91cd\u65b0\u8ba1\u7b97\u6982\u7387\n                self.update_probabilities(current_vocab, corpus)\n\n        self.vocab = {token: i for i, token in enumerate(current_vocab)}\n\n    def calculate_removal_loss(self, token, corpus):\n        \"\"\"\u8ba1\u7b97\u79fb\u9664token\u7684\u4f3c\u7136\u5ea6\u635f\u5931\"\"\"\n        total_loss = 0\n\n        for text in corpus:\n            # \u8ba1\u7b97\u6709token\u65f6\u7684\u6700\u4f18\u5206\u8bcd\u4f3c\u7136\u5ea6\n            with_token = self.get_best_segmentation(text, include_token=token)\n\n            # \u8ba1\u7b97\u65e0token\u65f6\u7684\u6700\u4f18\u5206\u8bcd\u4f3c\u7136\u5ea6  \n            without_token = self.get_best_segmentation(text, exclude_token=token)\n\n            # \u635f\u5931 = \u65e0token\u4f3c\u7136\u5ea6 - \u6709token\u4f3c\u7136\u5ea6\n            loss = without_token['log_prob'] - with_token['log_prob']\n            total_loss += loss\n\n        return total_loss\n\n    def get_best_segmentation(self, text, include_token=None, exclude_token=None):\n        \"\"\"\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u627e\u6700\u4f18\u5206\u8bcd\"\"\"\n        n = len(text)\n\n        # dp[i] = (\u6700\u4f18\u5bf9\u6570\u4f3c\u7136\u5ea6, \u5206\u8bcd\u65b9\u6848)\n        dp = [(-float('inf'), [])] * (n + 1)\n        dp[0] = (0.0, [])\n\n        for i in range(n + 1):\n            if dp[i][0] == -float('inf'):\n                continue\n\n            for j in range(i + 1, n + 1):\n                token = text[i:j]\n\n                # \u68c0\u67e5token\u662f\u5426\u53ef\u7528\n                if exclude_token and token == exclude_token:\n                    continue\n                if include_token and token not in self.token_probs and token != include_token:\n                    continue\n                if token not in self.token_probs:\n                    continue\n\n                # \u8ba1\u7b97\u65b0\u7684\u4f3c\u7136\u5ea6\n                token_prob = self.token_probs.get(token, 1e-10)\n                new_prob = dp[i][0] + math.log(token_prob)\n\n                if new_prob &gt; dp[j][0]:\n                    dp[j] = (new_prob, dp[i][1] + [token])\n\n        return {'log_prob': dp[n][0], 'tokens': dp[n][1]}\n\n    def tokenize(self, text):\n        \"\"\"\u4f7f\u7528\u8bad\u7ec3\u597d\u7684Unigram\u6a21\u578b\u5206\u8bcd\"\"\"\n        result = self.get_best_segmentation(text)\n        return result['tokens']\n\n# SentencePiece\u4f7f\u7528\u7684Unigram\u793a\u4f8b\ndef demo_unigram():\n    corpus = [\"hello world\", \"hello universe\", \"hi world\"]\n\n    unigram = UnigramTokenizer()\n    unigram.train(corpus, vocab_size=20)\n\n    text = \"hello world\"\n    tokens = unigram.tokenize(text)\n    print(f\"'{text}' -&gt; {tokens}\")\n\ndemo_unigram()\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#4-bpe-byte-level-bpe","title":"4. \u5b57\u8282\u7ea7BPE (Byte-level BPE)","text":"<p>\u521b\u65b0\u70b9: \u5728UTF-8\u5b57\u8282\u7ea7\u522b\u8fdb\u884cBPE</p> <pre><code>class ByteLevelBPE:\n    def __init__(self):\n        # UTF-8\u5b57\u8282\u5230\u53ef\u6253\u5370\u5b57\u7b26\u7684\u6620\u5c04\n        self.byte_encoder = self.bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n\n    def bytes_to_unicode(self):\n        \"\"\"\u521b\u5efa\u5b57\u8282\u5230Unicode\u5b57\u7b26\u7684\u6620\u5c04\"\"\"\n        bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"\u00a1\"), ord(\"\u00ac\")+1))+list(range(ord(\"\u00ae\"), ord(\"\u00ff\")+1))\n        cs = bs[:]\n        n = 0\n        for b in range(2**8):\n            if b not in bs:\n                bs.append(b)\n                cs.append(2**8+n)\n                n += 1\n        cs = [chr(n) for n in cs]\n        return dict(zip(bs, cs))\n\n    def encode_text_to_bytes(self, text):\n        \"\"\"\u5c06\u6587\u672c\u7f16\u7801\u4e3a\u5b57\u8282\u7ea7\u8868\u793a\"\"\"\n        byte_encoded = []\n        for char in text:\n            # \u83b7\u53d6UTF-8\u5b57\u8282\n            utf8_bytes = char.encode('utf-8')\n            for byte in utf8_bytes:\n                byte_encoded.append(self.byte_encoder[byte])\n        return ''.join(byte_encoded)\n\n    def decode_bytes_to_text(self, byte_string):\n        \"\"\"\u5c06\u5b57\u8282\u7ea7\u8868\u793a\u89e3\u7801\u56de\u6587\u672c\"\"\"\n        bytes_list = []\n        for char in byte_string:\n            bytes_list.append(self.byte_decoder[char])\n\n        byte_array = bytes(bytes_list)\n        return byte_array.decode('utf-8', errors='replace')\n\n    def train(self, corpus, vocab_size):\n        \"\"\"\u5728\u5b57\u8282\u7ea7\u522b\u8bad\u7ec3BPE\"\"\"\n        # 1. \u5c06\u6240\u6709\u6587\u672c\u8f6c\u6362\u4e3a\u5b57\u8282\u7ea7\u8868\u793a\n        byte_corpus = []\n        for text in corpus:\n            byte_text = self.encode_text_to_bytes(text)\n            byte_corpus.append(byte_text)\n\n        # 2. \u5728\u5b57\u8282\u7ea7\u522b\u5e94\u7528\u6807\u51c6BPE\n        self.bpe = BPETokenizer()\n        self.bpe.train(byte_corpus, vocab_size)\n\n    def tokenize(self, text):\n        \"\"\"\u5b57\u8282\u7ea7BPE\u5206\u8bcd\"\"\"\n        # \u8f6c\u6362\u4e3a\u5b57\u8282\u7ea7\u8868\u793a\n        byte_text = self.encode_text_to_bytes(text)\n\n        # \u5e94\u7528BPE\n        byte_tokens = self.bpe.tokenize(byte_text)\n\n        return byte_tokens\n\n# GPT-2\u4f7f\u7528\u7684Byte-level BPE\ndef demo_byte_bpe():\n    text = \"Hello, \u4e16\u754c!\"  # \u5305\u542b\u4e2d\u6587\n\n    bbpe = ByteLevelBPE()\n    byte_text = bbpe.encode_text_to_bytes(text)\n    print(f\"\u5b57\u8282\u7ea7\u7f16\u7801: {byte_text}\")\n\n    decoded = bbpe.decode_bytes_to_text(byte_text)\n    print(f\"\u89e3\u7801\u7ed3\u679c: {decoded}\")\n\ndemo_byte_bpe()\n</code></pre> <p>\u5b57\u8282\u7ea7BPE\u4f18\u52bf: - \u901a\u7528\u6027\u5f3a\uff0c\u652f\u6301\u6240\u6709\u8bed\u8a00 - \u8bcd\u6c47\u8868\u7d27\u51d1 - GPT-2/3/4\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/transformer/tokenizer/#tokenizer_2","title":"\ud83d\udcac \u73b0\u4ee3Tokenizer\u53d1\u5c55\u8d8b\u52bf","text":""},{"location":"fundamentals/transformer/tokenizer/#1-large-concept-models","title":"1. \u5927\u6982\u5ff5\u6a21\u578b (Large Concept Models)","text":"<p>\u7a81\u7834\u6027\u601d\u8def: \u8d85\u8d8atoken\u7ea7\u522b\uff0c\u76f4\u63a5\u5904\u7406\u6982\u5ff5\u7ea7\u522b</p> <pre><code>class ConceptLevelTokenizer:\n    \"\"\"\u6982\u5ff5\u7ea7\u522b\u7684tokenizer (\u7406\u8bba\u6a21\u578b)\"\"\"\n\n    def __init__(self, concept_encoder):\n        self.concept_encoder = concept_encoder  # \u9884\u8bad\u7ec3\u7684\u6982\u5ff5\u7f16\u7801\u5668\n\n    def encode_to_concepts(self, text):\n        \"\"\"\u5c06\u6587\u672c\u76f4\u63a5\u7f16\u7801\u4e3a\u6982\u5ff5\u5411\u91cf\"\"\"\n        # \u4f7f\u7528\u53e5\u5b50\u7ea7\u522b\u7684\u7f16\u7801\u5668\n        sentences = self.split_to_sentences(text)\n\n        concept_vectors = []\n        for sentence in sentences:\n            # \u5c06\u53e5\u5b50\u7f16\u7801\u4e3a\u6982\u5ff5\u5411\u91cf\u800c\u975etoken\u5e8f\u5217\n            concept_vec = self.concept_encoder.encode(sentence)\n            concept_vectors.append(concept_vec)\n\n        return concept_vectors\n\n    def decode_from_concepts(self, concept_vectors):\n        \"\"\"\u4ece\u6982\u5ff5\u5411\u91cf\u89e3\u7801\u56de\u6587\u672c\"\"\"\n        sentences = []\n        for concept_vec in concept_vectors:\n            sentence = self.concept_encoder.decode(concept_vec)\n            sentences.append(sentence)\n\n        return ' '.join(sentences)\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#2","title":"2. \u52a8\u6001\u4e0a\u4e0b\u6587\u5206\u8bcd","text":"<p>\u6838\u5fc3\u601d\u60f3: \u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u5206\u8bcd\u7b56\u7565</p> <pre><code>class ContextAwareTokenizer:\n    \"\"\"\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u52a8\u6001\u5206\u8bcd\u5668\"\"\"\n\n    def __init__(self, base_tokenizer, context_model):\n        self.base_tokenizer = base_tokenizer\n        self.context_model = context_model\n\n    def tokenize_with_context(self, text, context=\"\"):\n        \"\"\"\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u52a8\u6001\u5206\u8bcd\"\"\"\n\n        # 1. \u5206\u6790\u4e0a\u4e0b\u6587\u786e\u5b9a\u5206\u8bcd\u7b56\u7565\n        context_features = self.analyze_context(context)\n\n        # 2. \u6839\u636e\u4e0a\u4e0b\u6587\u9009\u62e9\u5206\u8bcd\u7c92\u5ea6\n        if context_features['domain'] == 'technical':\n            # \u6280\u672f\u6587\u672c\uff1a\u66f4\u7ec6\u7c92\u5ea6\u5206\u8bcd\n            granularity = 'fine'\n        elif context_features['domain'] == 'casual':\n            # \u65e5\u5e38\u5bf9\u8bdd\uff1a\u8f83\u7c97\u7c92\u5ea6\u5206\u8bcd\n            granularity = 'coarse'\n        else:\n            granularity = 'medium'\n\n        # 3. \u5e94\u7528\u52a8\u6001\u5206\u8bcd\n        tokens = self.adaptive_tokenize(text, granularity)\n\n        return tokens\n\n    def adaptive_tokenize(self, text, granularity):\n        \"\"\"\u81ea\u9002\u5e94\u5206\u8bcd\"\"\"\n        if granularity == 'fine':\n            # \u66f4\u591a\u5b50\u8bcd\u5206\u5272\n            return self.base_tokenizer.tokenize(text, merge_threshold=0.3)\n        elif granularity == 'coarse':\n            # \u66f4\u5c11\u5b50\u8bcd\u5206\u5272\n            return self.base_tokenizer.tokenize(text, merge_threshold=0.8)\n        else:\n            # \u6807\u51c6\u5206\u8bcd\n            return self.base_tokenizer.tokenize(text)\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#3-tokenizer","title":"3. \u591a\u6a21\u6001Tokenizer","text":"<p>\u6269\u5c55\u601d\u8def: \u7edf\u4e00\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u7b49\u591a\u79cd\u6a21\u6001</p> <pre><code>class MultimodalTokenizer:\n    \"\"\"\u591a\u6a21\u6001\u7edf\u4e00tokenizer\"\"\"\n\n    def __init__(self):\n        self.text_tokenizer = BPETokenizer()\n        self.image_tokenizer = ImagePatchTokenizer()\n        self.audio_tokenizer = AudioSegmentTokenizer()\n\n        # \u7edf\u4e00\u8bcd\u6c47\u8868\n        self.unified_vocab = self.build_unified_vocab()\n\n    def tokenize_multimodal(self, inputs):\n        \"\"\"\u591a\u6a21\u6001\u8f93\u5165\u7684\u7edf\u4e00\u5206\u8bcd\"\"\"\n        unified_tokens = []\n\n        for modality, data in inputs.items():\n            if modality == 'text':\n                tokens = self.text_tokenizer.tokenize(data)\n                # \u6dfb\u52a0\u6a21\u6001\u6807\u8bc6\n                unified_tokens.extend([f\"&lt;text&gt;{token}\" for token in tokens])\n\n            elif modality == 'image':\n                patches = self.image_tokenizer.tokenize(data)\n                unified_tokens.extend([f\"&lt;image&gt;{patch}\" for patch in patches])\n\n            elif modality == 'audio':\n                segments = self.audio_tokenizer.tokenize(data)\n                unified_tokens.extend([f\"&lt;audio&gt;{segment}\" for segment in segments])\n\n        return unified_tokens\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#_5","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/tokenizer/#q1-bpewordpieceunigram","title":"Q1: BPE\u3001WordPiece\u3001Unigram\u7684\u6838\u5fc3\u533a\u522b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u7b80\u6d01\u5bf9\u6bd4:</p> \u7ef4\u5ea6 BPE WordPiece Unigram \u6838\u5fc3\u7b56\u7565 \u9891\u7387\u9a71\u52a8\u5408\u5e76 \u4f3c\u7136\u5ea6\u9a71\u52a8\u5408\u5e76 \u6982\u7387\u9a71\u52a8\u526a\u679d \u8bad\u7ec3\u65b9\u5411 \u4ece\u5b57\u7b26\u5411\u4e0a\u6784\u5efa \u4ece\u5b57\u7b26\u5411\u4e0a\u6784\u5efa \u4ece\u5b8c\u6574\u8bcd\u6c47\u5411\u4e0b\u526a\u679d \u9009\u62e9\u6807\u51c6 \u5b57\u7b26\u5bf9\u9891\u7387 \u8bed\u8a00\u6a21\u578b\u4f3c\u7136\u5ea6 Token\u79fb\u9664\u635f\u5931 \u4ee3\u8868\u6a21\u578b GPT\u7cfb\u5217 BERT\u7cfb\u5217 T5\u3001mT5"},{"location":"fundamentals/transformer/tokenizer/#q2-byte-level-bpe","title":"Q2: \u4e3a\u4ec0\u4e48\u73b0\u4ee3\u5927\u6a21\u578b\u591a\u91c7\u7528Byte-level BPE\uff1f","text":"<p>\u6838\u5fc3\u4f18\u52bf:</p> <ol> <li>\u901a\u7528\u6027: \u652f\u6301\u6240\u6709\u8bed\u8a00\u548c\u5b57\u7b26\uff0c\u65e0\u9700\u7279\u6b8a\u9884\u5904\u7406</li> <li>\u7d27\u51d1\u6027: \u57fa\u7840\u8bcd\u6c47\u8868\u53ea\u9700256\u4e2a\u5b57\u8282</li> <li>\u9c81\u68d2\u6027: \u80fd\u5904\u7406\u4efb\u4f55UTF-8\u7f16\u7801\u7684\u6587\u672c</li> <li>\u4e00\u81f4\u6027: \u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u7684\u5904\u7406\u5b8c\u5168\u4e00\u81f4</li> </ol> <p>\u6280\u672f\u7ec6\u8282: <pre><code># \u4f20\u7edfBPE\u53ef\u80fd\u9047\u5230\u7684\u95ee\u9898\ntext = \"caf\u00e9\"  # \u5305\u542b\u91cd\u97f3\u5b57\u7b26\n# \u53ef\u80fd\u5bfc\u81f4\u7f16\u7801\u4e0d\u4e00\u81f4\u6216OOV\u95ee\u9898\n\n# Byte-level BPE\u7684\u89e3\u51b3\u65b9\u6848\nbytes_representation = text.encode('utf-8')  # [99, 97, 102, 195, 169]\n# \u6bcf\u4e2a\u5b57\u8282\u90fd\u6709\u56fa\u5b9a\u7684\u6620\u5c04\uff0c\u4fdd\u8bc1\u4e00\u81f4\u6027\n</code></pre></p>"},{"location":"fundamentals/transformer/tokenizer/#q3-tokenizer","title":"Q3: Tokenizer\u7684\u8bcd\u6c47\u8868\u5927\u5c0f\u5982\u4f55\u9009\u62e9\uff1f","text":"<p>\u6743\u8861\u8003\u8651:</p> <p>\u8bcd\u6c47\u8868\u8fc7\u5c0f: - \u5e8f\u5217\u53d8\u957f\uff0c\u8ba1\u7b97\u6210\u672c\u589e\u52a0 - \u8bed\u4e49\u4fe1\u606f\u4e22\u5931 - \u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u56f0\u96be</p> <p>\u8bcd\u6c47\u8868\u8fc7\u5927: - \u5d4c\u5165\u5c42\u53c2\u6570\u6fc0\u589e - \u7a00\u6709token\u8bad\u7ec3\u4e0d\u5145\u5206 - \u63a8\u7406\u65f6\u5185\u5b58\u5360\u7528\u5927</p> <p>\u7ecf\u9a8c\u6cd5\u5219: <pre><code># \u5e38\u89c1\u914d\u7f6e\nmodel_configs = {\n    'GPT-2': {'vocab_size': 50257, 'algorithm': 'Byte-level BPE'},\n    'BERT': {'vocab_size': 30522, 'algorithm': 'WordPiece'},\n    'T5': {'vocab_size': 32128, 'algorithm': 'SentencePiece Unigram'},\n    'LLaMA': {'vocab_size': 32000, 'algorithm': 'SentencePiece BPE'}\n}\n\n# \u9009\u62e9\u539f\u5219\uff1a\n# 1. \u82f1\u6587\uff1a20k-50k\u8f83\u4e3a\u5408\u9002\n# 2. \u591a\u8bed\u8a00\uff1a50k-100k\n# 3. \u4ee3\u7801\uff1a\u7279\u6b8a\u8003\u8651\uff0c\u53ef\u80fd\u9700\u8981\u66f4\u5927\u8bcd\u6c47\u8868\n</code></pre></p>"},{"location":"fundamentals/transformer/tokenizer/#q4-tokenizer","title":"Q4: \u5982\u4f55\u8bc4\u4f30Tokenizer\u7684\u597d\u574f\uff1f","text":"<p>\u8bc4\u4f30\u7ef4\u5ea6:</p> <ol> <li>\u538b\u7f29\u6548\u7387: <code>compression_ratio = original_chars / num_tokens</code></li> <li>\u8bcd\u6c47\u8986\u76d6\u7387: \u6d4b\u8bd5\u96c6\u4e2dUNK token\u7684\u6bd4\u4f8b</li> <li>\u8bed\u4e49\u4fdd\u6301\u5ea6: \u91cd\u8981\u8bcd\u6c47\u662f\u5426\u88ab\u5408\u7406\u5206\u5272</li> <li>fertility: \u5e73\u5747\u6bcf\u4e2a\u8bcd\u88ab\u5206\u6210\u591a\u5c11\u4e2atoken</li> </ol> <pre><code>def evaluate_tokenizer(tokenizer, test_corpus):\n    \"\"\"tokenizer\u8bc4\u4f30\u51fd\u6570\"\"\"\n\n    total_chars = sum(len(text) for text in test_corpus)\n    total_tokens = sum(len(tokenizer.tokenize(text)) for text in test_corpus)\n\n    # \u538b\u7f29\u6bd4\n    compression_ratio = total_chars / total_tokens\n\n    # UNK\u6bd4\u4f8b\n    unk_count = sum(text.count('[UNK]') for text in \n                   [' '.join(tokenizer.tokenize(text)) for text in test_corpus])\n    unk_ratio = unk_count / total_tokens\n\n    # Fertility (\u8bcd\u6c47\u5206\u5272\u5ea6)\n    word_tokens = []\n    for text in test_corpus:\n        words = text.split()\n        for word in words:\n            tokens = tokenizer.tokenize(word)\n            word_tokens.append(len(tokens))\n\n    fertility = sum(word_tokens) / len(word_tokens)\n\n    return {\n        'compression_ratio': compression_ratio,\n        'unk_ratio': unk_ratio,\n        'fertility': fertility\n    }\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#_6","title":"\ud83d\udcbb \u5b8c\u6574\u5b9e\u73b0\u793a\u4f8b","text":"<pre><code># \u73b0\u4ee3Tokenizer\u7684\u5b8c\u6574\u5b9e\u73b0\u793a\u4f8b\nfrom transformers import AutoTokenizer\n\nclass ModernTokenizerExample:\n    \"\"\"\u73b0\u4ee3tokenizer\u4f7f\u7528\u793a\u4f8b\"\"\"\n\n    def __init__(self, model_name=\"gpt2\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def demonstrate_tokenization(self):\n        \"\"\"\u6f14\u793a\u5404\u79cdtokenization\u573a\u666f\"\"\"\n\n        test_cases = [\n            \"Hello, world!\",\n            \"artificial intelligence\",\n            \"\u672a\u6765\u7684\u4eba\u5de5\u667a\u80fd\",\n            \"caf\u00e9 r\u00e9sum\u00e9 na\u00efve\",\n            \"COVID-19 vaccination\",\n            \"user@example.com\",\n            \"print('Hello, World!')\"\n        ]\n\n        print(\"=== Tokenization\u6f14\u793a ===\")\n        for text in test_cases:\n            tokens = self.tokenizer.tokenize(text)\n            token_ids = self.tokenizer.encode(text)\n            decoded = self.tokenizer.decode(token_ids)\n\n            print(f\"\\n\u539f\u6587: {text}\")\n            print(f\"Tokens: {tokens}\")\n            print(f\"Token IDs: {token_ids}\")\n            print(f\"\u89e3\u7801: {decoded}\")\n            print(f\"\u538b\u7f29\u6bd4: {len(text)/len(tokens):.2f}\")\n\n    def analyze_special_tokens(self):\n        \"\"\"\u5206\u6790\u7279\u6b8atoken\u7684\u5904\u7406\"\"\"\n\n        special_cases = [\n            \"Mr. Smith went to the U.S.A.\",  # \u7f29\u5199\n            \"She said, \\\"Hello!\\\"\",          # \u5f15\u53f7\n            \"Visit https://example.com\",      # URL\n            \"Temperature: 25.6\u00b0C\",           # \u7b26\u53f7\u548c\u6570\u5b57\n            \"   extra    spaces   \",         # \u591a\u4f59\u7a7a\u683c\n        ]\n\n        print(\"\\n=== \u7279\u6b8a\u60c5\u51b5\u5904\u7406 ===\")\n        for text in special_cases:\n            tokens = self.tokenizer.tokenize(text)\n            print(f\"'{text}' -&gt; {tokens}\")\n\n    def compare_tokenizers(self):\n        \"\"\"\u5bf9\u6bd4\u4e0d\u540ctokenizer\"\"\"\n\n        models = ['bert-base-uncased', 'gpt2', 'facebook/bart-base']\n        test_text = \"The quick brown fox jumps over the lazy dog.\"\n\n        print(\"\\n=== Tokenizer\u5bf9\u6bd4 ===\")\n        for model_name in models:\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(model_name)\n                tokens = tokenizer.tokenize(test_text)\n\n                print(f\"\\n{model_name}:\")\n                print(f\"Vocab size: {tokenizer.vocab_size}\")\n                print(f\"Tokens: {tokens}\")\n                print(f\"Token count: {len(tokens)}\")\n\n            except Exception as e:\n                print(f\"Error loading {model_name}: {e}\")\n\n# \u8fd0\u884c\u6f14\u793a\nif __name__ == \"__main__\":\n    demo = ModernTokenizerExample()\n    demo.demonstrate_tokenization()\n    demo.analyze_special_tokens()\n    demo.compare_tokenizers()\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3Tokenizer\u7684\u57fa\u672c\u5de5\u4f5c\u6d41\u7a0b</li> <li>[ ] \u638c\u63e1BPE\u3001WordPiece\u3001Unigram\u7684\u6838\u5fc3\u7b97\u6cd5</li> <li>[ ] \u4e86\u89e3Byte-level BPE\u7684\u4f18\u52bf</li> <li>[ ] \u80fd\u5206\u6790\u4e0d\u540ctokenizer\u7684\u9002\u7528\u573a\u666f</li> <li>[ ] \u7406\u89e3tokenizer\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd</li> </ul>"},{"location":"fundamentals/transformer/tokenizer/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u8bed\u8a00\u6a21\u578b\u67b6\u6784</li> <li>\u4e0b\u4e00\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"getting-started/","title":"\ud83c\udfaf LLM\u5b66\u4e60\u6307\u5357","text":"<p>\u6b22\u8fce\u6765\u5230\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u81ea\u5b66\u6307\u5357\uff01\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u6280\u672f\u9762\u8bd5\u8bbe\u8ba1\u768415\u5929\u7cfb\u7edf\u5316\u5b66\u4e60\u8def\u5f84\u3002</p>"},{"location":"getting-started/#_1","title":"\ud83d\ude80 \u4e3a\u4ec0\u4e48\u9009\u62e9\u8fd9\u4e2a\u5b66\u4e60\u6307\u5357\uff1f","text":""},{"location":"getting-started/#_2","title":"\u2728 \u9762\u8bd5\u5bfc\u5411\u8bbe\u8ba1","text":"<ul> <li>\ud83c\udfaf \u9ad8\u9891\u8003\u70b9\u8986\u76d6\uff1a\u91cd\u70b9\u5173\u6ce8\u6280\u672f\u9762\u8bd5\u4e2d\u7684\u6838\u5fc3\u95ee\u9898</li> <li>\ud83d\udcdd \u95ee\u7b54\u5f0f\u5b66\u4e60\uff1a\u6bcf\u4e2a\u7ae0\u8282\u90fd\u5305\u542b\u9762\u8bd5\u95ee\u9898\u548c\u6807\u51c6\u7b54\u6848</li> <li>\ud83e\udde0 \u7406\u8bba+\u5b9e\u8df5\uff1a\u65e2\u6709\u539f\u7406\u89e3\u91ca\u53c8\u6709\u4ee3\u7801\u5b9e\u73b0</li> </ul>"},{"location":"getting-started/#_3","title":"\ud83d\udd25 \u6280\u672f\u5185\u5bb9\u5168\u9762","text":"<ul> <li>\u57fa\u7840\u624e\u5b9e\uff1a\u4eceTransformer\u57fa\u7840\u5230\u73b0\u4ee3\u67b6\u6784\u6f14\u8fdb</li> <li>\u524d\u6cbf\u6280\u672f\uff1a\u5305\u542bDeepSeek\u7b49\u6700\u65b0\u6280\u672f\u521b\u65b0</li> <li>\u5de5\u7a0b\u5bfc\u5411\uff1a\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u548c\u6027\u80fd\u4f18\u5316</li> </ul>"},{"location":"getting-started/#_4","title":"\u23f1\ufe0f \u5b66\u4e60\u65f6\u95f4\u5408\u7406","text":"<ul> <li>15\u5929\u89c4\u5212\uff1a\u79d1\u5b66\u5206\u914d\u5b66\u4e60\u65f6\u95f4\uff0c\u5e73\u8861\u6df1\u5ea6\u4e0e\u5e7f\u5ea6</li> <li>\u7075\u6d3b\u5b89\u6392\uff1a\u53ef\u6839\u636e\u4e2a\u4eba\u65f6\u95f4\u8c03\u6574\u5b66\u4e60\u8282\u594f</li> <li>\u68c0\u9a8c\u6807\u51c6\uff1a\u6bcf\u8282\u90fd\u6709\u660e\u786e\u7684\u638c\u63e1\u6807\u51c6</li> </ul>"},{"location":"getting-started/#_5","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84\u5bfc\u822a","text":""},{"location":"getting-started/#105","title":"\ud83c\udfd7\ufe0f \u6a21\u578b\u57fa\u7840 (10.5\u5929)","text":"\u7ae0\u8282 \u6838\u5fc3\u5185\u5bb9 \u65f6\u95f4 \u96be\u5ea6 \u91cd\u8981\u6027 \u7b2c1\u8282 Transformer\u57fa\u7840 Attention + FFN + \u7f16\u7801\u5668-\u89e3\u7801\u5668 + Tokenizer 3\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25\ud83d\udd25 \u7b2c2\u8282 Attention\u5347\u7ea7 MHA\u2192MQA\u2192GQA\u2192MLA + KV Cache + RoPE 3\u5929 \u2b50\u2b50\u2b50 \ud83d\udd25\ud83d\udd25\ud83d\udd25 \u7b2c3\u8282 LLM\u5347\u7ea7\u6280\u672f MOE\u67b6\u6784 + \u5206\u5e03\u5f0f\u8bad\u7ec3 1.5\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25 \u7b2c4\u8282 DeepSeek\u6280\u672f MLA + DeepSeek MoE + MTP 3\u5929 \u2b50\u2b50\u2b50\u2b50 \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25"},{"location":"getting-started/#45","title":"\ud83d\udee0\ufe0f \u5e94\u7528\u5b9e\u6218 (4.5\u5929)","text":"\u7ae0\u8282 \u6838\u5fc3\u5185\u5bb9 \u65f6\u95f4 \u96be\u5ea6 \u91cd\u8981\u6027 \u7b2c5\u8282 Context Engineering \u4e0a\u4e0b\u6587\u5de5\u7a0b + \u63d0\u793a\u8bcd\u8bbe\u8ba1 1.5\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25\ud83d\udd25 \u7b2c6\u8282 RAG\u4e0eAgent RAG\u6280\u672f + AI Agent\u6846\u67b6 1.5\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25 \u7b2c7\u8282 CoT\u4e0e\u8bc4\u6d4b \u601d\u7ef4\u94fe + LangChain + \u6a21\u578b\u8bc4\u6d4b 1.5\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25"},{"location":"getting-started/#_6","title":"\ud83c\udfaf \u5b66\u4e60\u7b56\u7565\u5efa\u8bae","text":""},{"location":"getting-started/#7","title":"\ud83d\udcd6 \u7b2c\u4e00\u9636\u6bb5\uff1a\u57fa\u7840\u7406\u8bba (\u524d7\u5929)","text":"<ol> <li>\u91cd\u70b9\u638c\u63e1\uff1a\u7b2c1-2\u8282\u7684\u6838\u5fc3\u6982\u5ff5</li> <li>\u5b66\u4e60\u65b9\u6cd5\uff1a\u5148\u7406\u8bba\u540e\u4ee3\u7801\uff0c\u591a\u505a\u7b14\u8bb0</li> <li>\u65f6\u95f4\u5206\u914d\uff1a\u7406\u8bba\u7406\u89e360% + \u4ee3\u7801\u5b9e\u8df540%</li> </ol>"},{"location":"getting-started/#8-11","title":"\ud83d\udd2c \u7b2c\u4e8c\u9636\u6bb5\uff1a\u524d\u6cbf\u6280\u672f (\u7b2c8-11\u5929)","text":"<ol> <li>\u91cd\u70b9\u638c\u63e1\uff1a\u7b2c4\u8282DeepSeek\u6280\u672f\uff08\u9762\u8bd5\u70ed\u70b9\uff09</li> <li>\u5b66\u4e60\u65b9\u6cd5\uff1a\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7406\u89e3\u6280\u672f\u6f14\u8fdb\u903b\u8f91</li> <li>\u65f6\u95f4\u5206\u914d\uff1a\u6280\u672f\u539f\u740670% + \u5e94\u7528\u573a\u666f30%</li> </ol>"},{"location":"getting-started/#12-15","title":"\ud83d\udee0\ufe0f \u7b2c\u4e09\u9636\u6bb5\uff1a\u5e94\u7528\u5b9e\u6218 (\u7b2c12-15\u5929)","text":"<ol> <li>\u91cd\u70b9\u638c\u63e1\uff1a\u7b2c5\u8282\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff08\u5b9e\u7528\u6027\u5f3a\uff09</li> <li>\u5b66\u4e60\u65b9\u6cd5\uff1a\u5b9e\u8df5\u5bfc\u5411\uff0c\u591a\u505a\u9879\u76ee\u7ec3\u4e60</li> <li>\u65f6\u95f4\u5206\u914d\uff1a\u6982\u5ff5\u7406\u89e340% + \u5b9e\u9645\u5e94\u752860%</li> </ol>"},{"location":"getting-started/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":""},{"location":"getting-started/#_8","title":"\ud83e\udde0 \u7406\u8bba\u638c\u63e1","text":"<ul> <li>[ ] \u80fd\u7528\u81ea\u5df1\u7684\u8bdd\u89e3\u91ca\u6bcf\u4e2a\u6280\u672f\u7684\u6838\u5fc3\u539f\u7406</li> <li>[ ] \u80fd\u753b\u51faTransformer\u5b8c\u6574\u67b6\u6784\u56fe</li> <li>[ ] \u80fd\u8bf4\u660e\u4e0d\u540c\u6280\u672f\u7684\u4f18\u7f3a\u70b9\u548c\u9002\u7528\u573a\u666f</li> </ul>"},{"location":"getting-started/#_9","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u8df5","text":"<ul> <li>[ ] \u80fd\u72ec\u7acb\u5b9e\u73b0Self-Attention\u673a\u5236</li> <li>[ ] \u80fd\u5b8c\u6210KV Cache\u4f18\u5316\u4ee3\u7801</li> <li>[ ] \u80fd\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\u6a21\u677f</li> </ul>"},{"location":"getting-started/#_10","title":"\ud83c\udfa4 \u9762\u8bd5\u51c6\u5907","text":"<ul> <li>[ ] \u80fd\u56de\u7b54\u6240\u6709\u7ae0\u8282\u7684\u6838\u5fc3\u9762\u8bd5\u95ee\u9898</li> <li>[ ] \u80fd\u89e3\u91ca\u6280\u672f\u9009\u62e9\u80cc\u540e\u7684\u5de5\u7a0b\u8003\u91cf</li> <li>[ ] \u80fd\u63cf\u8ff0\u5177\u4f53\u7684\u9879\u76ee\u5e94\u7528\u7ecf\u9a8c</li> </ul>"},{"location":"getting-started/#_11","title":"\ud83d\udca1 \u5b66\u4e60\u5efa\u8bae","text":""},{"location":"getting-started/#_12","title":"\ud83d\udd0d \u6df1\u5ea6\u4f18\u5148","text":"<ul> <li>\u91cd\u70b9\u6280\u672f\u6df1\u5165\u7406\u89e3\uff1aAttention\u673a\u5236\u3001DeepSeek\u521b\u65b0</li> <li>\u5173\u952e\u6982\u5ff5\u53cd\u590d\u7ec3\u4e60\uff1a\u901a\u8fc7\u4ee3\u7801\u52a0\u6df1\u7406\u89e3</li> <li>\u9762\u8bd5\u95ee\u9898\u591a\u6b21\u6f14\u7ec3\uff1a\u786e\u4fdd\u8868\u8fbe\u6e05\u6670\u6d41\u7545</li> </ul>"},{"location":"getting-started/#_13","title":"\ud83d\udd17 \u5173\u8054\u5b66\u4e60","text":"<ul> <li>\u6280\u672f\u6f14\u8fdb\u8109\u7edc\uff1a\u7406\u89e3\u4eceMHA\u5230MLA\u7684\u53d1\u5c55\u903b\u8f91</li> <li>\u5bf9\u6bd4\u5206\u6790\u65b9\u6cd5\uff1a\u901a\u8fc7\u5bf9\u6bd4\u52a0\u6df1\u4e0d\u540c\u6280\u672f\u7684\u7406\u89e3</li> <li>\u5b9e\u9645\u5e94\u7528\u601d\u8003\uff1a\u6bcf\u4e2a\u6280\u672f\u90fd\u8981\u601d\u8003\u5e94\u7528\u573a\u666f</li> </ul>"},{"location":"getting-started/#_14","title":"\u26a1 \u9ad8\u6548\u5b66\u4e60","text":"<ul> <li>\u65f6\u95f4\u7ba1\u7406\uff1a\u8bbe\u5b9a\u660e\u786e\u7684\u6bcf\u65e5\u5b66\u4e60\u76ee\u6807</li> <li>\u7b14\u8bb0\u6574\u7406\uff1a\u5efa\u7acb\u4e2a\u4eba\u7684\u77e5\u8bc6\u4f53\u7cfb</li> <li>\u5b9a\u671f\u590d\u4e60\uff1a\u5de9\u56fa\u5df2\u5b66\u5185\u5bb9\uff0c\u907f\u514d\u9057\u5fd8</li> </ul>"},{"location":"getting-started/#llm_1","title":"\ud83c\udf89 \u5f00\u59cb\u4f60\u7684LLM\u5b66\u4e60\u4e4b\u65c5","text":"<p>\u9009\u62e9\u4e00\u4e2a\u611f\u5174\u8da3\u7684\u7ae0\u8282\u5f00\u59cb\u5b66\u4e60\u5427\uff01\u5efa\u8bae\u4ece\u7b2c1\u8282 Transformer\u57fa\u7840\u5f00\u59cb\uff0c\u5faa\u5e8f\u6e10\u8fdb\u5730\u638c\u63e1\u6574\u4e2a\u77e5\u8bc6\u4f53\u7cfb\u3002</p> <p>\u8bb0\u4f4f\uff1a\u771f\u77e5\u8bc6\u6765\u81ea\u4e8e\u7406\u89e3\u539f\u7406\uff0c\u800c\u4e0d\u662f\u6b7b\u8bb0\u786c\u80cc\u3002 </p> <p>\u795d\u4f60\u5b66\u4e60\u987a\u5229\uff0c\u9762\u8bd5\u6210\u529f\uff01\ud83c\udfaf</p>"},{"location":"interview/","title":"\u9762\u8bd5\u9898\u5e93","text":""},{"location":"interview/#_2","title":"\ud83c\udfaf \u76ee\u6807","text":"<p>\u6c47\u603bLLM\u76f8\u5173\u7684\u9ad8\u9891\u9762\u8bd5\u95ee\u9898\uff0c\u5e2e\u52a9\u5feb\u901f\u590d\u4e60\u548c\u51c6\u5907\u3002</p>"},{"location":"interview/#_3","title":"\ud83d\udcda \u6309\u4e3b\u9898\u5206\u7c7b","text":""},{"location":"interview/#_4","title":"\u6a21\u578b\u57fa\u7840","text":"<ul> <li>Attention\u8ba1\u7b97\u516c\u5f0f\u548c\u539f\u7406</li> <li>Encoder-Only vs Decoder-Only\u67b6\u6784</li> <li>MHA/MQA/GQA/MLA\u7684\u533a\u522b</li> <li>KV Cache\u5de5\u4f5c\u539f\u7406</li> <li>RoPE\u4f4d\u7f6e\u7f16\u7801\u63a8\u5bfc</li> </ul>"},{"location":"interview/#_5","title":"\u6a21\u578b\u5e94\u7528","text":"<ul> <li>RAG\u7684\u5de5\u4f5c\u6d41\u7a0b</li> <li>Agent\u7684\u7279\u70b9\u548c\u80fd\u529b</li> <li>CoT\u63d0\u5347\u63a8\u7406\u7684\u539f\u7406</li> <li>\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6838\u5fc3\u6280\u5de7</li> </ul>"},{"location":"interview/#_6","title":"\u5de5\u7a0b\u5b9e\u8df5","text":"<ul> <li>\u6a21\u578b\u90e8\u7f72\u4f18\u5316</li> <li>\u63a8\u7406\u52a0\u901f\u6280\u672f</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3\u7b56\u7565</li> <li>\u6a21\u578b\u8bc4\u6d4b\u65b9\u6cd5</li> </ul>"},{"location":"interview/#top-10","title":"\ud83d\udd25 \u9ad8\u9891\u95ee\u9898 Top 10","text":"<ol> <li>Attention\u673a\u5236\u7684\u6570\u5b66\u516c\u5f0f\u662f\u4ec0\u4e48\uff1f</li> <li>\u4e3a\u4ec0\u4e48\u8981\u9664\u4ee5\u221ad_k\uff1f</li> <li>GPT\u548cBERT\u7684\u67b6\u6784\u533a\u522b\uff1f</li> <li>KV Cache\u5982\u4f55\u52a0\u901f\u63a8\u7406\uff1f</li> <li>\u4ec0\u4e48\u662fRoPE\uff1f\u5982\u4f55\u63a8\u5bfc\uff1f</li> <li>MQA\u76f8\u6bd4MHA\u7684\u4f18\u52bf\uff1f</li> <li>Pre-Norm vs Post-Norm\uff1f</li> <li>RAG\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1f</li> <li>Agent\u548c\u666e\u901aLLM\u7684\u533a\u522b\uff1f</li> <li>\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\uff1f</li> </ol>"},{"location":"interview/#_7","title":"\ud83d\udca1 \u7b54\u9898\u6280\u5de7","text":"<ol> <li>\u5148\u8bf4\u6838\u5fc3\uff0c\u518d\u5c55\u5f00\u7ec6\u8282</li> <li>\u7ed3\u5408\u6570\u5b66\u516c\u5f0f\u548c\u4ee3\u7801\u5b9e\u73b0</li> <li>\u63d0\u53ca\u5b9e\u9645\u5e94\u7528\u548c\u5de5\u7a0b\u8003\u8651</li> <li>\u5bf9\u6bd4\u4e0d\u540c\u65b9\u6848\u7684\u4f18\u52a3</li> </ol> <p>\u6bcf\u4e2a\u95ee\u9898\u90fd\u5728\u5bf9\u5e94\u7ae0\u8282\u6709\u8be6\u7ec6\u89e3\u7b54\uff0c\u5efa\u8bae\u7ed3\u5408\u5b66\u4e60\uff01</p>"},{"location":"reference/markdown/","title":"Markdown \u8bed\u6cd5\u53c2\u8003","text":"<p>\u5b8c\u6574\u7684 Markdown \u8bed\u6cd5\u53c2\u8003\u6307\u5357\u3002</p>"},{"location":"reference/markdown/#_1","title":"\u57fa\u7840\u8bed\u6cd5","text":""},{"location":"reference/markdown/#_2","title":"\u6807\u9898","text":"<pre><code># H1 \u6807\u9898\n## H2 \u6807\u9898\n### H3 \u6807\u9898\n#### H4 \u6807\u9898\n##### H5 \u6807\u9898\n###### H6 \u6807\u9898\n</code></pre>"},{"location":"reference/markdown/#_3","title":"\u6587\u672c\u6837\u5f0f","text":"\u6837\u5f0f \u8bed\u6cd5 \u793a\u4f8b \u7c97\u4f53 <code>**\u6587\u672c**</code> \u6216 <code>__\u6587\u672c__</code> \u7c97\u4f53\u6587\u672c \u659c\u4f53 <code>*\u6587\u672c*</code> \u6216 <code>_\u6587\u672c_</code> \u659c\u4f53\u6587\u672c \u5220\u9664\u7ebf <code>~~\u6587\u672c~~</code> ~~\u5220\u9664\u7ebf\u6587\u672c~~ \u884c\u5185\u4ee3\u7801 <code>`\u4ee3\u7801`</code> <code>\u4ee3\u7801</code>"},{"location":"reference/markdown/#_4","title":"\u5217\u8868","text":""},{"location":"reference/markdown/#_5","title":"\u65e0\u5e8f\u5217\u8868","text":"<pre><code>- \u9879\u76ee 1\n- \u9879\u76ee 2\n  - \u5b50\u9879\u76ee 2.1\n  - \u5b50\u9879\u76ee 2.2\n- \u9879\u76ee 3\n</code></pre>"},{"location":"reference/markdown/#_6","title":"\u6709\u5e8f\u5217\u8868","text":"<pre><code>1. \u7b2c\u4e00\u9879\n2. \u7b2c\u4e8c\u9879\n   1. \u5b50\u9879\u76ee 2.1\n   2. \u5b50\u9879\u76ee 2.2\n3. \u7b2c\u4e09\u9879\n</code></pre>"},{"location":"reference/markdown/#_7","title":"\u94fe\u63a5\u548c\u56fe\u7247","text":"<pre><code>[\u94fe\u63a5\u6587\u672c](https://example.com)\n[\u5185\u90e8\u94fe\u63a5](../index.md)\n![\u56fe\u7247\u63cf\u8ff0](image.png)\n</code></pre>"},{"location":"reference/markdown/#_8","title":"\u8868\u683c","text":"<pre><code>| \u8868\u59341 | \u8868\u59342 | \u8868\u59343 |\n|-------|-------|-------|\n| \u5355\u5143\u683c1 | \u5355\u5143\u683c2 | \u5355\u5143\u683c3 |\n| \u5355\u5143\u683c4 | \u5355\u5143\u683c5 | \u5355\u5143\u683c6 |\n</code></pre>"},{"location":"reference/markdown/#_9","title":"\u6269\u5c55\u8bed\u6cd5","text":""},{"location":"reference/markdown/#_10","title":"\u4ee3\u7801\u5757","text":"<pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n</code></pre>"},{"location":"reference/markdown/#_11","title":"\u544a\u793a\u6846\uff08\u9700\u8981\u6269\u5c55\uff09","text":"<pre><code>!!! note \"\u6ce8\u91ca\"\n    \u8fd9\u662f\u4e00\u4e2a\u6ce8\u91ca\u6846\u3002\n\n!!! tip \"\u63d0\u793a\"\n    \u8fd9\u662f\u4e00\u4e2a\u63d0\u793a\u6846\u3002\n\n!!! warning \"\u8b66\u544a\"\n    \u8fd9\u662f\u4e00\u4e2a\u8b66\u544a\u6846\u3002\n\n!!! danger \"\u5371\u9669\"\n    \u8fd9\u662f\u4e00\u4e2a\u5371\u9669\u8b66\u544a\u6846\u3002\n</code></pre>"},{"location":"reference/markdown/#_12","title":"\u4efb\u52a1\u5217\u8868","text":"<pre><code>- [x] \u5df2\u5b8c\u6210\u7684\u4efb\u52a1\n- [ ] \u672a\u5b8c\u6210\u7684\u4efb\u52a1\n- [ ] \u53e6\u4e00\u4e2a\u672a\u5b8c\u6210\u7684\u4efb\u52a1\n</code></pre>"},{"location":"reference/markdown/#_13","title":"\u811a\u6ce8","text":"<pre><code>\u8fd9\u91cc\u6709\u4e00\u4e2a\u811a\u6ce8\u5f15\u7528[^1]\u3002\n\n[^1]: \u8fd9\u662f\u811a\u6ce8\u5185\u5bb9\u3002\n</code></pre>"},{"location":"reference/markdown/#_14","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u4f7f\u7528\u63cf\u8ff0\u6027\u6807\u9898 - \u8ba9\u8bfb\u8005\u5feb\u901f\u4e86\u89e3\u5185\u5bb9</li> <li>\u4fdd\u6301\u4e00\u81f4\u7684\u683c\u5f0f - \u7edf\u4e00\u7684\u6837\u5f0f\u8ba9\u6587\u6863\u66f4\u4e13\u4e1a</li> <li>\u9002\u5ea6\u4f7f\u7528\u683c\u5f0f - \u8fc7\u591a\u7684\u683c\u5f0f\u4f1a\u5f71\u54cd\u53ef\u8bfb\u6027</li> <li>\u6dfb\u52a0\u76ee\u5f55\u94fe\u63a5 - \u5e2e\u52a9\u7528\u6237\u5feb\u901f\u5bfc\u822a</li> </ol> <p>!!! tip \"\u7f16\u5199\u6280\u5de7\"     \u5b9a\u671f\u9884\u89c8\u60a8\u7684\u6587\u6863\uff0c\u786e\u4fdd\u683c\u5f0f\u6b63\u786e\u6e32\u67d3\u3002</p>"},{"location":"tutorials/first-document/","title":"\u7f16\u5199\u7b2c\u4e00\u7bc7\u6587\u6863","text":"<p>\u5b66\u4e60\u5982\u4f55\u521b\u5efa\u548c\u7f16\u8f91\u60a8\u7684\u7b2c\u4e00\u7bc7 Markdown \u6587\u6863\u3002</p>"},{"location":"tutorials/first-document/#_2","title":"\u521b\u5efa\u65b0\u9875\u9762","text":"<ol> <li>\u5728 <code>docs/</code> \u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 <code>.md</code> \u6587\u4ef6</li> <li>\u4f7f\u7528 Markdown \u8bed\u6cd5\u7f16\u5199\u5185\u5bb9</li> <li>\u4fdd\u5b58\u6587\u4ef6\u540e\uff0c\u5f00\u53d1\u670d\u52a1\u5668\u4f1a\u81ea\u52a8\u5237\u65b0</li> </ol>"},{"location":"tutorials/first-document/#markdown","title":"\u57fa\u7840 Markdown \u8bed\u6cd5","text":""},{"location":"tutorials/first-document/#_3","title":"\u6807\u9898","text":"<pre><code># \u4e00\u7ea7\u6807\u9898\n## \u4e8c\u7ea7\u6807\u9898\n### \u4e09\u7ea7\u6807\u9898\n</code></pre>"},{"location":"tutorials/first-document/#_4","title":"\u6587\u672c\u683c\u5f0f","text":"<pre><code>**\u7c97\u4f53\u6587\u672c**\n*\u659c\u4f53\u6587\u672c*\n`\u884c\u5185\u4ee3\u7801`\n</code></pre>"},{"location":"tutorials/first-document/#_5","title":"\u5217\u8868","text":"<pre><code>- \u65e0\u5e8f\u5217\u8868\u9879 1\n- \u65e0\u5e8f\u5217\u8868\u9879 2\n\n1. \u6709\u5e8f\u5217\u8868\u9879 1\n2. \u6709\u5e8f\u5217\u8868\u9879 2\n</code></pre>"},{"location":"tutorials/first-document/#_6","title":"\u94fe\u63a5","text":"<pre><code>[\u94fe\u63a5\u6587\u672c](URL)\n[\u5185\u90e8\u94fe\u63a5](../reference/markdown.md)\n</code></pre>"},{"location":"tutorials/first-document/#_7","title":"\u4ee3\u7801\u5757","text":"<pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n</code></pre>"},{"location":"tutorials/first-document/#_8","title":"\u5b9e\u8df5\u7ec3\u4e60","text":"<p>\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9\u7684\u65b0\u9875\u9762\uff1a</p> <ul> <li>[ ] \u6807\u9898\u548c\u5b50\u6807\u9898</li> <li>[ ] \u4e00\u4e9b\u683c\u5f0f\u5316\u6587\u672c</li> <li>[ ] \u4e00\u4e2a\u5217\u8868</li> <li>[ ] \u4e00\u4e2a\u4ee3\u7801\u793a\u4f8b</li> </ul>"},{"location":"tutorials/first-document/#_9","title":"\u63d0\u793a","text":"<p>!!! tip \"\u4e13\u4e1a\u63d0\u793a\"     \u4f7f\u7528 MkDocs Material \u7684\u544a\u793a\u6846\u529f\u80fd\u6765\u7a81\u51fa\u91cd\u8981\u4fe1\u606f\uff01</p> <p>!!! warning \"\u6ce8\u610f\"     \u8bb0\u5f97\u4fdd\u5b58\u60a8\u7684\u66f4\u6539\uff0c\u5f00\u53d1\u670d\u52a1\u5668\u4f1a\u81ea\u52a8\u91cd\u65b0\u52a0\u8f7d\u3002</p>"}]}