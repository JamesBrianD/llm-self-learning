{"config":{"lang":["zh","en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u6b22\u8fce\u6765\u5230\u6211\u7684\u6587\u6863\u5e93","text":"<p>\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e MkDocs Material \u6784\u5efa\u7684\u73b0\u4ee3\u5316\u6587\u6863\u7f51\u7ad9\u3002</p>"},{"location":"#_2","title":"\u7279\u6027","text":"<ul> <li>\u2728 \u73b0\u4ee3\u5316\u8bbe\u8ba1: \u4f7f\u7528 Material Design \u98ce\u683c\u7684\u754c\u9762</li> <li>\ud83c\udf19 \u6df1\u8272/\u6d45\u8272\u6a21\u5f0f: \u652f\u6301\u4e3b\u9898\u5207\u6362</li> <li>\ud83d\udd0d \u5168\u6587\u641c\u7d22: \u5f3a\u5927\u7684\u6587\u6863\u641c\u7d22\u529f\u80fd</li> <li>\ud83d\udcf1 \u54cd\u5e94\u5f0f\u5e03\u5c40: \u5b8c\u7f8e\u9002\u914d\u5404\u79cd\u8bbe\u5907</li> <li>\u26a1 \u5feb\u901f\u52a0\u8f7d: \u4f18\u5316\u7684\u9759\u6001\u7ad9\u70b9\u751f\u6210</li> </ul>"},{"location":"#_3","title":"\u5feb\u901f\u5f00\u59cb","text":""},{"location":"#_4","title":"\u672c\u5730\u5f00\u53d1","text":"<pre><code># \u542f\u52a8\u5f00\u53d1\u670d\u52a1\u5668\nmkdocs serve\n\n# \u6784\u5efa\u9759\u6001\u7f51\u7ad9\nmkdocs build\n</code></pre>"},{"location":"#_5","title":"\u5e38\u7528\u547d\u4ee4","text":"<ul> <li><code>mkdocs new [dir-name]</code> - \u521b\u5efa\u65b0\u9879\u76ee</li> <li><code>mkdocs serve</code> - \u542f\u52a8\u5b9e\u65f6\u9884\u89c8\u670d\u52a1\u5668</li> <li><code>mkdocs build</code> - \u6784\u5efa\u6587\u6863\u7ad9\u70b9</li> <li><code>mkdocs gh-deploy</code> - \u90e8\u7f72\u5230 GitHub Pages</li> </ul>"},{"location":"#_6","title":"\u6587\u6863\u7ed3\u6784","text":"<pre><code>mkdocs.yml          # \u914d\u7f6e\u6587\u4ef6\ndocs/\n    index.md        # \u6587\u6863\u9996\u9875\n    getting-started/# \u5165\u95e8\u6307\u5357\n    tutorials/      # \u6559\u7a0b\n    reference/      # \u53c2\u8003\u6587\u6863\n</code></pre> <p>\u5f00\u59cb\u63a2\u7d22\u60a8\u7684\u6587\u6863\u4e4b\u65c5\u5427\uff01</p>"},{"location":"applications/context-engineering/","title":"\u7b2c4\u8282\uff1aContext Engineering","text":""},{"location":"applications/context-engineering/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6838\u5fc3\u6280\u5de7\uff0c\u5b66\u4f1a\u8bbe\u8ba1\u9ad8\u6548\u7684\u63d0\u793a\u8bcd\u6765\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - \u4ec0\u4e48\u662f\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff1f - \u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\uff1f - \u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u7684\u5e94\u5bf9\u7b56\u7565</p>"},{"location":"applications/context-engineering/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a2\u5929</p> <ul> <li>Day 1: \u4e0a\u4e0b\u6587\u5de5\u7a0b\u57fa\u7840\u7406\u8bba</li> <li>Day 2: \u5b9e\u8df5\u6280\u5de7\u548c\u6848\u4f8b\u5206\u6790</li> </ul>"},{"location":"applications/context-engineering/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"applications/context-engineering/#1","title":"1. \u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5","text":"<ul> <li>\u63d0\u793a\u8bcd\u8bbe\u8ba1\u539f\u5219</li> <li>Few-shot\u5b66\u4e60\u6280\u5de7</li> <li>\u4e0a\u4e0b\u6587\u4f18\u5316\u7b56\u7565</li> </ul>"},{"location":"applications/context-engineering/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u9879\u76ee\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u7406\u8bba\u638c\u63e1: \u7406\u89e3\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6838\u5fc3\u539f\u7406</li> <li>\u5b9e\u8df5\u5e94\u7528: \u80fd\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\u6a21\u677f</li> </ol>"},{"location":"applications/context-engineering/#_5","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u8fd9\u662f\u5927\u6a21\u578b\u5e94\u7528\u7684\u6838\u5fc3\u6280\u80fd\uff0c\u5b9e\u7528\u6027\u5f88\u5f3a\uff01</p>"},{"location":"applications/context-engineering/practices/","title":"\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5","text":""},{"location":"applications/context-engineering/practices/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u638c\u63e1\u5b9e\u7528\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6280\u5de7\u548c\u6700\u4f73\u5b9e\u8df5\u3002</p>"},{"location":"applications/context-engineering/practices/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"applications/context-engineering/practices/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Manus \u5185\u90e8\u7684 Context \u5de5\u7a0b\u7ecf\u9a8c - \u77e5\u4e4e</li> <li>Context Engineering GitHub\u9879\u76ee - \u5b9e\u8df5\u6848\u4f8b</li> </ol>"},{"location":"applications/context-engineering/practices/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/context-engineering/practices/#_6","title":"\u63d0\u793a\u8bcd\u8bbe\u8ba1\u539f\u5219","text":"<ol> <li>\u660e\u786e\u6027: \u6e05\u6670\u8868\u8fbe\u4efb\u52a1\u8981\u6c42</li> <li>\u7ed3\u6784\u5316: \u4f7f\u7528\u4e00\u81f4\u7684\u683c\u5f0f</li> <li>\u793a\u4f8b\u5f15\u5bfc: \u63d0\u4f9bFew-shot\u793a\u4f8b</li> <li>\u89d2\u8272\u8bbe\u5b9a: \u660e\u786eAI\u7684\u89d2\u8272\u5b9a\u4f4d</li> </ol>"},{"location":"applications/context-engineering/practices/#_7","title":"\u5e38\u7528\u6280\u5de7","text":"<ul> <li>\u94fe\u5f0f\u601d\u7ef4(CoT): \u5f15\u5bfc\u9010\u6b65\u63a8\u7406</li> <li>\u6a21\u677f\u5316: \u6807\u51c6\u5316\u63d0\u793a\u8bcd\u7ed3\u6784</li> <li>\u4e0a\u4e0b\u6587\u538b\u7f29: \u4f18\u5316\u4fe1\u606f\u5bc6\u5ea6</li> </ul>"},{"location":"applications/context-engineering/practices/#_8","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/context-engineering/practices/#q1","title":"Q1: \u4ec0\u4e48\u662f\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff1f","text":"<p>\u6838\u5fc3\u5b9a\u4e49: \u4e0a\u4e0b\u6587\u5de5\u7a0b\u662f\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u8f93\u5165\u63d0\u793a\u8bcd\u6765\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u671f\u671b\u8f93\u51fa\u7684\u6280\u672f\u3002</p> <p>\u5173\u952e\u8981\u7d20: - \u4efb\u52a1\u63cf\u8ff0\u6e05\u6670 - \u63d0\u4f9b\u76f8\u5173\u793a\u4f8b - \u8bbe\u7f6e\u5408\u9002\u7684\u7ea6\u675f\u6761\u4ef6</p>"},{"location":"applications/context-engineering/practices/#_9","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u63d0\u793a\u8bcd\u8bbe\u8ba1\u7684\u6838\u5fc3\u539f\u5219</li> <li>[ ] \u80fd\u8bbe\u8ba1\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u63d0\u793a\u8bcd\u6a21\u677f</li> </ul>"},{"location":"applications/context-engineering/practices/#_10","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aRAG\u4e0eAgent</li> <li>\u8fd4\u56de\uff1aContext Engineering\u6982\u89c8</li> </ul>"},{"location":"applications/cot-evaluation/","title":"\u7b2c6\u8282\uff1aCoT\u4e0e\u8bc4\u6d4b","text":""},{"location":"applications/cot-evaluation/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u601d\u7ef4\u94fe(Chain of Thought)\u6280\u672f\u548c\u5927\u6a21\u578b\u8bc4\u6d4b\u65b9\u6cd5\u3002</p>"},{"location":"applications/cot-evaluation/#_2","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"applications/cot-evaluation/#1","title":"1. \u601d\u7ef4\u94fe\u6280\u672f","text":"<ul> <li>CoT\u7684\u539f\u7406\u548c\u5e94\u7528</li> <li>\u63d0\u5347\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5</li> </ul>"},{"location":"applications/cot-evaluation/#2-langchain","title":"2. LangChain\u6846\u67b6","text":"<ul> <li>LangChain\u7684\u6838\u5fc3\u6982\u5ff5</li> <li>\u94fe\u5f0f\u8c03\u7528\u548c\u5de5\u5177\u96c6\u6210</li> </ul>"},{"location":"applications/cot-evaluation/#3","title":"3. \u6a21\u578b\u8bc4\u6d4b","text":"<ul> <li>\u8bc4\u6d4b\u6307\u6807\u548c\u65b9\u6cd5</li> <li>\u57fa\u51c6\u6d4b\u8bd5\u4ecb\u7ecd</li> </ul>"},{"location":"applications/cot-evaluation/#_3","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<ul> <li>[ ] \u7406\u89e3CoT\u7684\u5de5\u4f5c\u539f\u7406</li> <li>[ ] \u4e86\u89e3\u6a21\u578b\u8bc4\u6d4b\u7684\u57fa\u672c\u65b9\u6cd5</li> </ul>"},{"location":"applications/cot-evaluation/cot/","title":"\u601d\u7ef4\u94fe\u6280\u672f","text":""},{"location":"applications/cot-evaluation/cot/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u601d\u7ef4\u94fe(Chain of Thought)\u6280\u672f\u5982\u4f55\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002</p>"},{"location":"applications/cot-evaluation/cot/#_3","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/cot-evaluation/cot/#cot","title":"CoT\u662f\u4ec0\u4e48\uff1f","text":"<p>Chain of Thought (CoT) \u662f\u4e00\u79cd\u63d0\u793a\u6280\u672f\uff0c\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u5c55\u793a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u6765\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002</p>"},{"location":"applications/cot-evaluation/cot/#_4","title":"\u6838\u5fc3\u539f\u7406","text":"<ol> <li>\u5206\u6b65\u63a8\u7406: \u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u6b65\u9aa4</li> <li>\u663e\u5f0f\u601d\u8003: \u8ba9\u6a21\u578b\u5c55\u793a\u601d\u8003\u8fc7\u7a0b</li> <li>\u9010\u6b65\u5f15\u5bfc: \u901a\u8fc7\u793a\u4f8b\u6559\u4f1a\u6a21\u578b\u63a8\u7406\u6a21\u5f0f</li> </ol>"},{"location":"applications/cot-evaluation/cot/#_5","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/cot-evaluation/cot/#q1-cot","title":"Q1: \u4ec0\u4e48\u662f\u601d\u7ef4\u94fe(CoT)\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54: CoT\u662f\u901a\u8fc7\"\u8ba9\u6211\u60f3\u60f3...\"\u7684\u65b9\u5f0f\u5f15\u5bfc\u5927\u6a21\u578b\u8fdb\u884c\u5206\u6b65\u63a8\u7406\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u8868\u73b0\u7684\u6280\u672f\u3002</p> <p>\u6838\u5fc3\u673a\u5236: - \u4e0d\u76f4\u63a5\u7ed9\u7b54\u6848\uff0c\u800c\u662f\u5c55\u793a\u63a8\u7406\u8fc7\u7a0b - \u901a\u8fc7\u4e2d\u95f4\u6b65\u9aa4\u63d0\u5347\u6700\u7ec8\u7b54\u6848\u8d28\u91cf - \u5bf9\u6570\u5b66\u3001\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u7279\u522b\u6709\u6548</p>"},{"location":"applications/cot-evaluation/cot/#_6","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3CoT\u7684\u57fa\u672c\u539f\u7406</li> <li>[ ] \u77e5\u9053CoT\u9002\u7528\u7684\u573a\u666f</li> </ul>"},{"location":"applications/cot-evaluation/cot/#_7","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aLangChain\u6846\u67b6</li> <li>\u8fd4\u56de\uff1aCoT\u4e0e\u8bc4\u6d4b\u6982\u89c8</li> </ul>"},{"location":"applications/cot-evaluation/evaluation/","title":"\u6a21\u578b\u8bc4\u6d4b","text":""},{"location":"applications/cot-evaluation/evaluation/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u4e86\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u6d4b\u65b9\u6cd5\u548c\u4e3b\u8981\u57fa\u51c6\u3002</p>"},{"location":"applications/cot-evaluation/evaluation/#_3","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/cot-evaluation/evaluation/#_4","title":"\u8bc4\u6d4b\u7684\u91cd\u8981\u6027","text":"<p>\u6a21\u578b\u8bc4\u6d4b\u662f\u8861\u91cfLLM\u6027\u80fd\u3001\u6307\u5bfc\u6a21\u578b\u6539\u8fdb\u7684\u91cd\u8981\u624b\u6bb5\u3002</p>"},{"location":"applications/cot-evaluation/evaluation/#_5","title":"\u4e3b\u8981\u8bc4\u6d4b\u7ef4\u5ea6","text":"<ol> <li>\u80fd\u529b\u8bc4\u6d4b: \u63a8\u7406\u3001\u77e5\u8bc6\u3001\u8bed\u8a00\u7406\u89e3</li> <li>\u5b89\u5168\u8bc4\u6d4b: \u6709\u5bb3\u8f93\u51fa\u3001\u504f\u89c1\u68c0\u6d4b</li> <li>\u6548\u7387\u8bc4\u6d4b: \u901f\u5ea6\u3001\u8d44\u6e90\u6d88\u8017</li> <li>\u53ef\u9760\u6027: \u4e00\u81f4\u6027\u3001\u7a33\u5b9a\u6027</li> </ol>"},{"location":"applications/cot-evaluation/evaluation/#_6","title":"\u5e38\u89c1\u57fa\u51c6","text":"<ul> <li>MMLU: \u591a\u5b66\u79d1\u77e5\u8bc6\u7406\u89e3</li> <li>HellaSwag: \u5e38\u8bc6\u63a8\u7406</li> <li>HumanEval: \u4ee3\u7801\u751f\u6210\u80fd\u529b</li> <li>GSM8K: \u6570\u5b66\u63a8\u7406</li> </ul>"},{"location":"applications/cot-evaluation/evaluation/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/cot-evaluation/evaluation/#q1","title":"Q1: \u5927\u6a21\u578b\u8bc4\u6d4b\u6709\u54ea\u4e9b\u4e3b\u8981\u65b9\u6cd5\uff1f","text":"<p>\u4e3b\u8981\u65b9\u6cd5: - \u81ea\u52a8\u8bc4\u6d4b: \u57fa\u4e8e\u6807\u51c6\u7b54\u6848\u7684\u5ba2\u89c2\u6307\u6807 - \u4eba\u5de5\u8bc4\u6d4b: \u4e3b\u89c2\u8d28\u91cf\u8bc4\u4f30 - \u5bf9\u6bd4\u8bc4\u6d4b: \u6a21\u578b\u95f4\u76f8\u5bf9\u8868\u73b0 - \u5728\u7ebf\u8bc4\u6d4b: \u5b9e\u9645\u5e94\u7528\u573a\u666f\u6d4b\u8bd5</p>"},{"location":"applications/cot-evaluation/evaluation/#_8","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u4e86\u89e3\u6a21\u578b\u8bc4\u6d4b\u7684\u57fa\u672c\u6982\u5ff5</li> <li>[ ] \u77e5\u9053\u4e3b\u8981\u7684\u8bc4\u6d4b\u57fa\u51c6</li> </ul>"},{"location":"applications/cot-evaluation/evaluation/#_9","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aLangChain\u6846\u67b6</li> <li>\u9762\u8bd5\u9898\u5e93</li> <li>\u8fd4\u56de\uff1aCoT\u4e0e\u8bc4\u6d4b\u6982\u89c8</li> </ul>"},{"location":"applications/cot-evaluation/langchain/","title":"LangChain\u6846\u67b6","text":""},{"location":"applications/cot-evaluation/langchain/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u4e86\u89e3LangChain\u6846\u67b6\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u5e94\u7528\u573a\u666f\u3002</p>"},{"location":"applications/cot-evaluation/langchain/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/cot-evaluation/langchain/#langchain_1","title":"LangChain\u662f\u4ec0\u4e48\uff1f","text":"<p>LangChain \u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u94fe\u5f0f\u8c03\u7528\u3001\u5de5\u5177\u96c6\u6210\u7b49\u529f\u80fd\u3002</p>"},{"location":"applications/cot-evaluation/langchain/#_3","title":"\u6838\u5fc3\u7ec4\u4ef6","text":"<ol> <li>Chains: \u94fe\u5f0f\u8c03\u7528\u591a\u4e2a\u7ec4\u4ef6</li> <li>Agents: \u667a\u80fd\u4f53\u548c\u5de5\u5177\u4f7f\u7528</li> <li>Memory: \u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406</li> <li>Tools: \u5916\u90e8\u5de5\u5177\u96c6\u6210</li> </ol>"},{"location":"applications/cot-evaluation/langchain/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/cot-evaluation/langchain/#q1-langchain","title":"Q1: LangChain\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u4f5c\u7528: - \u7b80\u5316LLM\u5e94\u7528\u5f00\u53d1 - \u63d0\u4f9b\u6807\u51c6\u5316\u7684\u7ec4\u4ef6\u548c\u63a5\u53e3 - \u652f\u6301\u590d\u6742\u7684\u5de5\u4f5c\u6d41\u7f16\u6392</p>"},{"location":"applications/cot-evaluation/langchain/#_5","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u4e86\u89e3LangChain\u7684\u57fa\u672c\u6982\u5ff5</li> <li>[ ] \u7406\u89e3\u94fe\u5f0f\u8c03\u7528\u7684\u4f18\u52bf</li> </ul>"},{"location":"applications/cot-evaluation/langchain/#_6","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u601d\u7ef4\u94fe\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1a\u6a21\u578b\u8bc4\u6d4b</li> <li>\u8fd4\u56de\uff1aCoT\u4e0e\u8bc4\u6d4b\u6982\u89c8</li> </ul>"},{"location":"applications/rag-agent/","title":"\u7b2c5\u8282\uff1aRAG\u4e0eAgent","text":""},{"location":"applications/rag-agent/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u7406\u89e3\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548cAI Agent\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u638c\u63e1\u8fd9\u4e24\u4e2a\u91cd\u8981\u7684LLM\u5e94\u7528\u8303\u5f0f\u3002</p>"},{"location":"applications/rag-agent/#_2","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"applications/rag-agent/#1-rag","title":"1. RAG\u6280\u672f","text":"<ul> <li>\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u539f\u7406</li> <li>RAG\u7cfb\u7edf\u67b6\u6784</li> <li>\u5411\u91cf\u6570\u636e\u5e93\u5e94\u7528</li> </ul>"},{"location":"applications/rag-agent/#2-ai-agent","title":"2. AI Agent","text":"<ul> <li>Agent\u7684\u5b9a\u4e49\u548c\u7279\u70b9</li> <li>\u591a\u6b65\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528</li> <li>Agent\u6846\u67b6\u548c\u5b9e\u73b0</li> </ul>"},{"location":"applications/rag-agent/#_3","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<ul> <li>[ ] \u7406\u89e3RAG\u7684\u5de5\u4f5c\u6d41\u7a0b</li> <li>[ ] \u638c\u63e1Agent\u7684\u57fa\u672c\u6982\u5ff5</li> </ul>"},{"location":"applications/rag-agent/agent/","title":"AI Agent","text":""},{"location":"applications/rag-agent/agent/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3AI Agent\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u5b9e\u73b0\u65b9\u5f0f\u3002</p>"},{"location":"applications/rag-agent/agent/#_2","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"applications/rag-agent/agent/#_3","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>AI-Agent\u7cfb\u5217(\u4e00)\uff1a\u667a\u80fd\u4f53\u8d77\u6e90\u63a2\u7a76 - \u7406\u8bba\u57fa\u7840</li> <li>\u5927\u6a21\u578b-Agent \u9762\u8bd5\u516b\u80a1\u6587 - \u77e5\u4e4e</li> </ol>"},{"location":"applications/rag-agent/agent/#_4","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/rag-agent/agent/#agent","title":"Agent\u662f\u4ec0\u4e48\uff1f","text":"<p>AI Agent \u662f\u80fd\u591f\u611f\u77e5\u73af\u5883\u3001\u505a\u51fa\u51b3\u7b56\u5e76\u6267\u884c\u884c\u52a8\u4ee5\u5b9e\u73b0\u76ee\u6807\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002</p>"},{"location":"applications/rag-agent/agent/#_5","title":"\u6838\u5fc3\u7279\u5f81","text":"<ol> <li>\u81ea\u4e3b\u6027: \u80fd\u591f\u72ec\u7acb\u505a\u51b3\u7b56</li> <li>\u53cd\u5e94\u6027: \u5bf9\u73af\u5883\u53d8\u5316\u505a\u51fa\u54cd\u5e94  </li> <li>\u4e3b\u52a8\u6027: \u4e3b\u52a8\u91c7\u53d6\u884c\u52a8\u5b9e\u73b0\u76ee\u6807</li> <li>\u5b66\u4e60\u6027: \u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u6539\u8fdb</li> </ol>"},{"location":"applications/rag-agent/agent/#_6","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/rag-agent/agent/#q1-ai-agent","title":"Q1: AI Agent\u6709\u4ec0\u4e48\u7279\u70b9\uff1f","text":"<p>\u6838\u5fc3\u7279\u70b9: - \u591a\u6b65\u63a8\u7406: \u80fd\u591f\u5206\u89e3\u590d\u6742\u4efb\u52a1 - \u5de5\u5177\u4f7f\u7528: \u53ef\u4ee5\u8c03\u7528\u5916\u90e8\u5de5\u5177\u548cAPI - \u8bb0\u5fc6\u673a\u5236: \u7ef4\u62a4\u5bf9\u8bdd\u5386\u53f2\u548c\u72b6\u6001 - \u76ee\u6807\u5bfc\u5411: \u671d\u7740\u7279\u5b9a\u76ee\u6807\u6267\u884c\u8ba1\u5212</p>"},{"location":"applications/rag-agent/agent/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3Agent\u7684\u57fa\u672c\u6982\u5ff5\u548c\u7279\u5f81</li> <li>[ ] \u4e86\u89e3Agent\u4e0e\u666e\u901aLLM\u7684\u533a\u522b</li> </ul>"},{"location":"applications/rag-agent/agent/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aRAG\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1aCoT\u4e0e\u8bc4\u6d4b</li> <li>\u8fd4\u56de\uff1aRAG\u4e0eAgent\u6982\u89c8</li> </ul>"},{"location":"applications/rag-agent/rag/","title":"RAG\u6280\u672f","text":""},{"location":"applications/rag-agent/rag/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u5de5\u4f5c\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\u3002</p>"},{"location":"applications/rag-agent/rag/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/rag-agent/rag/#rag_1","title":"RAG\u662f\u4ec0\u4e48\uff1f","text":"<p>Retrieval-Augmented Generation (RAG) \u662f\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u548c\u751f\u6210\u7684\u6280\u672f\uff0c\u901a\u8fc7\u68c0\u7d22\u5916\u90e8\u77e5\u8bc6\u5e93\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56de\u7b54\u80fd\u529b\u3002</p>"},{"location":"applications/rag-agent/rag/#_3","title":"\u6838\u5fc3\u4f18\u52bf","text":"<ol> <li>\u77e5\u8bc6\u66f4\u65b0: \u53ef\u4ee5\u8bbf\u95ee\u6700\u65b0\u4fe1\u606f</li> <li>\u51cf\u5c11\u5e7b\u89c9: \u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u56de\u7b54</li> <li>\u53ef\u8ffd\u6eaf\u6027: \u53ef\u4ee5\u63d0\u4f9b\u4fe1\u606f\u6765\u6e90</li> </ol>"},{"location":"applications/rag-agent/rag/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/rag-agent/rag/#q1-rag","title":"Q1: RAG\u7684\u5de5\u4f5c\u6d41\u7a0b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u6d41\u7a0b: 1. \u7d22\u5f15\u6784\u5efa: \u5c06\u6587\u6863\u5411\u91cf\u5316\u5b58\u50a8 2. \u67e5\u8be2\u68c0\u7d22: \u6839\u636e\u95ee\u9898\u68c0\u7d22\u76f8\u5173\u6587\u6863 3. \u589e\u5f3a\u751f\u6210: \u7ed3\u5408\u68c0\u7d22\u7ed3\u679c\u751f\u6210\u7b54\u6848</p>"},{"location":"applications/rag-agent/rag/#_5","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3RAG\u7684\u57fa\u672c\u5de5\u4f5c\u6d41\u7a0b</li> <li>[ ] \u77e5\u9053RAG\u76f8\u6bd4\u7eafLLM\u7684\u4f18\u52bf</li> </ul>"},{"location":"applications/rag-agent/rag/#_6","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aAI Agent</li> <li>\u8fd4\u56de\uff1aRAG\u4e0eAgent\u6982\u89c8</li> </ul>"},{"location":"code-examples/","title":"\u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"code-examples/#_2","title":"\ud83c\udfaf \u76ee\u6807","text":"<p>\u63d0\u4f9bLLM\u76f8\u5173\u6280\u672f\u7684\u53ef\u6267\u884c\u4ee3\u7801\u793a\u4f8b\uff0c\u5e2e\u52a9\u52a0\u6df1\u7406\u89e3\u3002</p>"},{"location":"code-examples/#_3","title":"\ud83d\udcbb \u4ee3\u7801\u5206\u7c7b","text":""},{"location":"code-examples/#_4","title":"\u57fa\u7840\u5b9e\u73b0","text":"<ul> <li>Self-Attention\u673a\u5236</li> <li>Multi-Head Attention</li> <li>Softmax\u51fd\u6570</li> </ul>"},{"location":"code-examples/#_5","title":"\u4f18\u5316\u6280\u672f","text":"<ul> <li>GQA (Grouped-Query Attention) \u5b9e\u73b0</li> <li>KV Cache\u6f14\u793a\u4ee3\u7801</li> <li>RoPE\u4f4d\u7f6e\u7f16\u7801\u5b8c\u6574\u5b9e\u73b0</li> <li>\u4e09\u79cd\u5f52\u4e00\u5316\u6280\u672f\u5bf9\u6bd4</li> </ul>"},{"location":"code-examples/#_6","title":"\u5e94\u7528\u793a\u4f8b","text":"<ul> <li>RAG\u7cfb\u7edf\u7b80\u5316\u7248</li> <li>\u7b80\u5355Agent\u6846\u67b6</li> <li>\u63d0\u793a\u8bcd\u5de5\u7a0b\u6a21\u677f</li> </ul>"},{"location":"code-examples/#_7","title":"\ud83d\udee0\ufe0f \u5b9e\u8df5\u5efa\u8bae","text":"<ol> <li>\u52a8\u624b\u7f16\u5199: \u4e0d\u8981\u53ea\u770b\u4ee3\u7801\uff0c\u8981\u81ea\u5df1\u5b9e\u73b0</li> <li>\u7406\u89e3\u539f\u7406: \u6bcf\u884c\u4ee3\u7801\u90fd\u8981\u77e5\u9053\u4e3a\u4ec0\u4e48\u8fd9\u6837\u5199</li> <li>\u8c03\u8bd5\u8fd0\u884c: \u786e\u4fdd\u4ee3\u7801\u80fd\u6b63\u786e\u6267\u884c</li> <li>\u6027\u80fd\u5bf9\u6bd4: \u6d4b\u8bd5\u4e0d\u540c\u5b9e\u73b0\u7684\u6548\u679c\u5dee\u5f02</li> </ol>"},{"location":"code-examples/#_8","title":"\ud83d\udccb \u68c0\u9a8c\u6e05\u5355","text":"<ul> <li>[ ] \u5b8c\u6210Self-Attention\u7f16\u7a0b\u7ec3\u4e60</li> <li>[ ] \u5b9e\u73b0KV Cache\u6f14\u793a</li> <li>[ ] \u7f16\u5199RoPE\u4f4d\u7f6e\u7f16\u7801</li> <li>[ ] \u5bf9\u6bd4\u4e0d\u540c\u5f52\u4e00\u5316\u6280\u672f</li> <li>[ ] \u8bbe\u8ba1\u63d0\u793a\u8bcd\u6a21\u677f</li> </ul> <p>\u6240\u6709\u4ee3\u7801\u90fd\u5728\u5bf9\u5e94\u6280\u672f\u7ae0\u8282\u4e2d\u63d0\u4f9b\uff0c\u8fd9\u91cc\u4f5c\u4e3a\u603b\u5165\u53e3\uff01</p>"},{"location":"fundamentals/attention-advanced/","title":"\u7b2c2\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f","text":""},{"location":"fundamentals/attention-advanced/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u73b0\u4ee3\u5927\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u6280\u672f\uff0c\u7406\u89e3\u63a8\u7406\u52a0\u901f\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - MHA/MQA/GQA/MLA\u7684\u533a\u522b\u548c\u4f18\u52bf - KV Cache\u7684\u5de5\u4f5c\u539f\u7406\u548c\u52a0\u901f\u6548\u679c - LayerNorm vs RMSNorm\u7684\u9009\u62e9 - RoPE\u4f4d\u7f6e\u7f16\u7801\u7684\u6570\u5b66\u539f\u7406</p>"},{"location":"fundamentals/attention-advanced/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a2\u5929</p> <ul> <li>Day 1: \u6ce8\u610f\u529b\u53d8\u4f53 + KV Cache\u6280\u672f</li> <li>Day 2: \u5f52\u4e00\u5316\u6280\u672f + \u4f4d\u7f6e\u7f16\u7801\u5347\u7ea7</li> </ul>"},{"location":"fundamentals/attention-advanced/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/attention-advanced/#1","title":"1. \u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53","text":"<ul> <li>MHA \u2192 MQA \u2192 GQA \u2192 MLA\u6f14\u8fdb</li> <li>\u6ce8\u610f\u529b\u5934\u6570\u4f18\u5316\u7b56\u7565</li> <li>\u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790</li> </ul>"},{"location":"fundamentals/attention-advanced/#2-kv-cache","title":"2. KV Cache\u6280\u672f","text":"<ul> <li>\u63a8\u7406\u52a0\u901f\u539f\u7406</li> <li>\u5185\u5b58\u4f18\u5316\u7b56\u7565</li> <li>\u5b9e\u73b0\u7ec6\u8282\u548c\u4ee3\u7801\u793a\u4f8b</li> </ul>"},{"location":"fundamentals/attention-advanced/#3","title":"3. \u5f52\u4e00\u5316\u6280\u672f","text":"<ul> <li>BatchNorm vs LayerNorm vs RMSNorm</li> <li>Pre-Norm vs Post-Norm</li> <li>\u8bad\u7ec3\u7a33\u5b9a\u6027\u5206\u6790</li> </ul>"},{"location":"fundamentals/attention-advanced/#4","title":"4. \u4f4d\u7f6e\u7f16\u7801","text":"<ul> <li>\u7edd\u5bf9\u4f4d\u7f6e vs \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801</li> <li>RoPE\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u63a8\u5bfc</li> <li>\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b</li> </ul>"},{"location":"fundamentals/attention-advanced/#_4","title":"\ud83d\udcd6 \u6838\u5fc3\u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/#_5","title":"\u5fc5\u8bfb\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Transformer\u7684Attention\u53ca\u5176\u5404\u79cd\u53d8\u4f53 - \u51b7\u7738\u535a\u5ba2</li> <li>\u7f13\u5b58\u4e0e\u6548\u679c\u7684\u6781\u9650\u62c9\u626f\uff1a\u4eceMHA\u3001MQA\u3001GQA\u5230MLA - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u5927\u6a21\u578b\u4e2d\u5e38\u89c1\u76843\u79cdNorm - \u77e5\u4e4e</li> <li>\u4e3a\u4ec0\u4e48\u5f53\u524d\u4e3b\u6d41\u7684\u5927\u6a21\u578b\u90fd\u4f7f\u7528RMS-Norm\uff1f - \u77e5\u4e4e</li> <li>\u4e3a\u4ec0\u4e48Pre Norm\u7684\u6548\u679c\u4e0d\u5982Post Norm\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> <li>Sinusoidal\u4f4d\u7f6e\u7f16\u7801\u8ffd\u6839\u6eaf\u6e90 - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u535a\u91c7\u4f17\u957f\u7684\u65cb\u8f6c\u5f0f\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4</li> </ol>"},{"location":"fundamentals/attention-advanced/#_6","title":"\u9009\u8bfb\u6df1\u5165\u6750\u6599","text":"<ul> <li>BN\u7a76\u7adf\u8d77\u4e86\u4ec0\u4e48\u4f5c\u7528\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> </ul>"},{"location":"fundamentals/attention-advanced/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u9879\u76ee\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u6982\u5ff5\u7406\u89e3: \u6e05\u6670\u533a\u5206\u5404\u79cd\u6ce8\u610f\u529b\u53d8\u4f53</li> <li>\u4ee3\u7801\u5b9e\u73b0: \u5b8c\u6210GQA\u9002\u914d\u548cKV Cache\u6f14\u793a</li> <li>\u9762\u8bd5\u51c6\u5907: \u80fd\u89e3\u91ca\u6280\u672f\u9009\u62e9\u7684\u5de5\u7a0b\u539f\u56e0</li> </ol>"},{"location":"fundamentals/attention-advanced/#_8","title":"\ud83d\udca1 \u5b66\u4e60\u63d0\u793a","text":"<p>\u8fd9\u4e00\u8282\u6280\u672f\u542b\u91cf\u8f83\u9ad8\uff0c\u5efa\u8bae\uff1a - \u5148\u7406\u89e3\u539f\u7406\uff0c\u518d\u770b\u5177\u4f53\u5b9e\u73b0 - \u91cd\u70b9\u5173\u6ce8\u5de5\u7a0b\u4f18\u5316\u7684motivation - \u901a\u8fc7\u5bf9\u6bd4\u7406\u89e3\u4e0d\u540c\u6280\u672f\u7684trade-off</p>"},{"location":"fundamentals/attention-advanced/#_9","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u611f\u5174\u8da3\u7684\u6280\u672f\u6a21\u5757\u6df1\u5165\u5b66\u4e60\uff0c\u6bcf\u4e2a\u90fd\u662f\u73b0\u4ee3\u5927\u6a21\u578b\u7684\u6838\u5fc3\u6280\u672f\uff01</p>"},{"location":"fundamentals/attention-advanced/kv-cache/","title":"KV Cache\u6280\u672f","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3KV Cache\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u638c\u63e1\u8fd9\u4e2a\u8ba9\u5927\u6a21\u578b\u63a8\u7406\u63d0\u901f\u6570\u500d\u7684\u5173\u952e\u6280\u672f\u3002</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_1","title":"KV Cache\u662f\u4ec0\u4e48\uff1f","text":"<p>\u5b9a\u4e49: KV Cache\u662f\u4e00\u79cd\u63a8\u7406\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u7f13\u5b58\u4e4b\u524d\u8ba1\u7b97\u8fc7\u7684Key\u548cValue\u77e9\u9635\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97\uff0c\u5927\u5e45\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_2","title":"\u4e3a\u4ec0\u4e48\u9700\u8981KV Cache\uff1f","text":"<p>\u95ee\u9898\u80cc\u666f: \u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u91cd\u590d\u8ba1\u7b97</p> <pre><code># \u751f\u6210\"\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\"\u7684\u8fc7\u7a0b\nStep 1: \u8f93\u5165[\"\u6211\"]        \u2192 \u9884\u6d4b\"\u7231\" \nStep 2: \u8f93\u5165[\"\u6211\",\"\u7231\"]    \u2192 \u9884\u6d4b\"\u5317\"\nStep 3: \u8f93\u5165[\"\u6211\",\"\u7231\",\"\u5317\"] \u2192 \u9884\u6d4b\"\u4eac\"\n...\n</code></pre> <p>\u91cd\u590d\u8ba1\u7b97\u95ee\u9898: - \u6bcf\u4e00\u6b65\u90fd\u8981\u91cd\u65b0\u8ba1\u7b97\u6240\u6709previous tokens\u7684K,V\u77e9\u9635 - \u8ba1\u7b97\u590d\u6742\u5ea6: O(n\u00b2)\uff0c\u5176\u4e2dn\u662f\u5e8f\u5217\u957f\u5ea6 - \u5927\u91cf\u91cd\u590d\u8ba1\u7b97\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_3","title":"KV Cache\u5de5\u4f5c\u539f\u7406","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#1","title":"1. \u4f20\u7edf\u65b9\u5f0f (\u65e0\u7f13\u5b58)","text":"<pre><code># \u6bcf\u6b21\u90fd\u91cd\u65b0\u8ba1\u7b97\u5168\u90e8K,V\ndef generate_token_naive(tokens):\n    # \u91cd\u65b0\u8ba1\u7b97\u6240\u6709token\u7684K,V - \u975e\u5e38\u4f4e\u6548\uff01\n    K = compute_K(tokens)  # \u5305\u542b\u6240\u6709\u5386\u53f2token\n    V = compute_V(tokens)  # \u5305\u542b\u6240\u6709\u5386\u53f2token\n    Q = compute_Q(tokens[-1])  # \u53ea\u9700\u8981\u6700\u540e\u4e00\u4e2atoken\u7684Q\n\n    attention_output = attention(Q, K, V)\n    return next_token\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#2-kv-cache","title":"2. KV Cache\u4f18\u5316\u65b9\u5f0f","text":"<pre><code># \u53ea\u8ba1\u7b97\u65b0token\u7684K,V\uff0c\u590d\u7528\u5386\u53f2\u7f13\u5b58\ndef generate_token_with_cache(new_token, kv_cache):\n    # \u53ea\u8ba1\u7b97\u65b0token\u7684K,V\n    new_K = compute_K(new_token)  \n    new_V = compute_V(new_token)\n\n    # \u66f4\u65b0\u7f13\u5b58\n    kv_cache.append(new_K, new_V)\n\n    # \u4f7f\u7528\u5b8c\u6574\u7684K,V (\u5386\u53f2+\u65b0\u589e)\n    Q = compute_Q(new_token)\n    attention_output = attention(Q, kv_cache.K, kv_cache.V)\n\n    return next_token\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#_3","title":"\u52a0\u901f\u6548\u679c\u5206\u6790","text":"<p>\u65f6\u95f4\u590d\u6742\u5ea6\u5bf9\u6bd4:</p> \u751f\u6210\u6b65\u9aa4 \u65e0Cache \u6709Cache \u52a0\u901f\u6bd4 \u7b2c1\u6b65 O(1) O(1) 1x \u7b2c2\u6b65 O(4) O(1) 4x \u7b2c3\u6b65 O(9) O(1) 9x \u7b2cn\u6b65 O(n\u00b2) O(1) n\u00b2x <p>\u5185\u5b58\u4f7f\u7528: - \u7a7a\u95f4\u6362\u65f6\u95f4\u7684\u7b56\u7565 - \u9700\u8981\u5b58\u50a8: <code>seq_len \u00d7 num_heads \u00d7 head_dim \u00d7 2</code> (K\u548cV) - \u957f\u5e8f\u5217\u65f6\u5185\u5b58\u9700\u6c42\u663e\u8457\u589e\u52a0</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#q1-kv-cachekv-cache","title":"Q1: KV Cache\u662f\u4ec0\u4e48\uff0c\u4e3a\u4ec0\u4e48KV Cache\u80fd\u52a0\u901f\u6a21\u578b\u63a8\u7406\uff1f","text":"<p>\u6838\u5fc3\u7b54\u6848:  KV Cache\u662f\u7f13\u5b58\u6ce8\u610f\u529b\u673a\u5236\u4e2dKey\u548cValue\u77e9\u9635\u7684\u6280\u672f\uff0c\u901a\u8fc7\u907f\u514d\u91cd\u590d\u8ba1\u7b97\u5386\u53f2token\u7684K,V\u6765\u52a0\u901f\u63a8\u7406\u3002</p> <p>\u8be6\u7ec6\u89e3\u91ca:</p> <ol> <li>\u95ee\u9898\u6839\u6e90: </li> <li>\u81ea\u56de\u5f52\u751f\u6210\u6bcf\u6b65\u90fd\u9700\u8981\u5b8c\u6574\u7684attention\u8ba1\u7b97</li> <li>\u5386\u53f2token\u7684K,V\u77e9\u9635\u5728\u6bcf\u6b65\u4e2d\u4fdd\u6301\u4e0d\u53d8</li> <li> <p>\u91cd\u590d\u8ba1\u7b97\u9020\u6210O(n\u00b2)\u7684\u65f6\u95f4\u590d\u6742\u5ea6</p> </li> <li> <p>\u89e3\u51b3\u65b9\u6848:</p> </li> <li>\u7f13\u5b58\u5df2\u8ba1\u7b97\u7684K,V\u77e9\u9635</li> <li>\u65b0token\u53ea\u9700\u8ba1\u7b97\u81ea\u5df1\u7684K,V\u5e76\u8ffd\u52a0\u5230\u7f13\u5b58</li> <li> <p>\u5c06\u65f6\u95f4\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u4f4e\u5230O(1)</p> </li> <li> <p>\u52a0\u901f\u539f\u7406:    <pre><code>\u4f20\u7edf\u65b9\u5f0f: \u6bcf\u6b65\u8ba1\u7b97\u5b8c\u6574\u5e8f\u5217\u7684K,V\n\u7f13\u5b58\u65b9\u5f0f: \u53ea\u8ba1\u7b97\u65b0\u589etoken\u7684K,V\n</code></pre></p> </li> </ol>"},{"location":"fundamentals/attention-advanced/kv-cache/#q2-kv-cache","title":"Q2: KV Cache\u7684\u5185\u5b58\u5f00\u9500\u5982\u4f55\uff1f","text":"<p>\u5185\u5b58\u9700\u6c42\u8ba1\u7b97: <pre><code>memory_per_token = num_layers \u00d7 num_heads \u00d7 head_dim \u00d7 2 \u00d7 dtype_size\ntotal_memory = memory_per_token \u00d7 max_seq_length\n</code></pre></p> <p>\u5177\u4f53\u4f8b\u5b50 (LLaMA-7B): <pre><code>\u53c2\u6570: 32\u5c42, 32\u5934, 128\u7ef4\u5ea6, FP16\n\u6bcf\u4e2atoken: 32 \u00d7 32 \u00d7 128 \u00d7 2 \u00d7 2 bytes = 524KB\n2048\u957f\u5ea6: 524KB \u00d7 2048 \u2248 1GB\n</code></pre></p> <p>\u5185\u5b58\u4f18\u5316\u7b56\u7565: - \u4f7f\u7528\u66f4\u4f4e\u7cbe\u5ea6(FP16/INT8) - \u5206\u5c42\u7f13\u5b58\uff0c\u53ea\u4fdd\u7559\u6700\u8fd1\u7684token - \u6ed1\u52a8\u7a97\u53e3\uff0c\u4e22\u5f03\u8fc7\u65e7\u7684\u7f13\u5b58</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#q3-kv-cache","title":"Q3: KV Cache\u5728\u4e0d\u540c\u6ce8\u610f\u529b\u53d8\u4f53\u4e2d\u7684\u8868\u73b0\uff1f","text":"<p>\u5404\u53d8\u4f53\u7684KV Cache\u9700\u6c42:</p> \u6ce8\u610f\u529b\u7c7b\u578b KV Cache\u5927\u5c0f \u8bf4\u660e MHA <code>h \u00d7 d \u00d7 L</code> \u6bcf\u4e2a\u5934\u72ec\u7acb\u5b58\u50a8K,V MQA <code>d \u00d7 L</code> \u6240\u6709\u5934\u5171\u4eabK,V\uff0c\u5185\u5b58\u51cf\u5c11h\u500d GQA <code>g \u00d7 d \u00d7 L</code> \u5206\u7ec4\u5171\u4eab\uff0c\u5185\u5b58\u9700\u6c42\u5728MHA\u548cMQA\u4e4b\u95f4 MLA \u6700\u5c0f \u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u8fdb\u4e00\u6b65\u538b\u7f29 <p>h=\u5934\u6570, d=\u7ef4\u5ea6, L=\u5e8f\u5217\u957f\u5ea6, g=\u7ec4\u6570</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#_5","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_4","title":"\u5b8c\u6574KV Cache\u6f14\u793a","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass KVCache:\n    \"\"\"\u7b80\u5316\u7684KV Cache\u5b9e\u73b0\"\"\"\n\n    def __init__(self, max_seq_len, num_heads, head_dim, device='cpu'):\n        self.max_seq_len = max_seq_len\n        self.num_heads = num_heads\n        self.head_dim = head_dim\n        self.device = device\n\n        # \u9884\u5206\u914d\u7f13\u5b58\u7a7a\u95f4\n        self.cache_k = torch.zeros(\n            max_seq_len, num_heads, head_dim, \n            device=device, dtype=torch.float16\n        )\n        self.cache_v = torch.zeros(\n            max_seq_len, num_heads, head_dim,\n            device=device, dtype=torch.float16\n        )\n\n        self.current_length = 0\n\n    def update_cache(self, new_k, new_v):\n        \"\"\"\u66f4\u65b0\u7f13\u5b58\u5e76\u8fd4\u56de\u5b8c\u6574\u7684K,V\"\"\"\n        batch_size, seq_len, num_heads, head_dim = new_k.shape\n\n        # \u68c0\u67e5\u662f\u5426\u8d85\u51fa\u7f13\u5b58\u5bb9\u91cf\n        if self.current_length + seq_len &gt; self.max_seq_len:\n            raise ValueError(\"Sequence length exceeds cache capacity\")\n\n        # \u66f4\u65b0\u7f13\u5b58\n        end_pos = self.current_length + seq_len\n        self.cache_k[self.current_length:end_pos] = new_k[0]  # \u5047\u8bbebatch_size=1\n        self.cache_v[self.current_length:end_pos] = new_v[0]\n\n        self.current_length = end_pos\n\n        # \u8fd4\u56de\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u5b8c\u6574K,V\n        return (\n            self.cache_k[:self.current_length].unsqueeze(0),  # \u6dfb\u52a0batch\u7ef4\u5ea6\n            self.cache_v[:self.current_length].unsqueeze(0)\n        )\n\n    def clear(self):\n        \"\"\"\u6e05\u7a7a\u7f13\u5b58\"\"\"\n        self.current_length = 0\n\n    def get_cache_info(self):\n        \"\"\"\u83b7\u53d6\u7f13\u5b58\u72b6\u6001\u4fe1\u606f\"\"\"\n        return {\n            'current_length': self.current_length,\n            'capacity': self.max_seq_len,\n            'usage_ratio': self.current_length / self.max_seq_len,\n            'memory_mb': self.cache_k.numel() * 2 * 2 / 1024 / 1024  # FP16\n        }\n\n\nclass AttentionWithKVCache(nn.Module):\n    \"\"\"\u5e26KV Cache\u7684\u6ce8\u610f\u529b\u5c42\"\"\"\n\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.kv_cache = None\n\n    def setup_cache(self, max_seq_len, device):\n        \"\"\"\u521d\u59cb\u5316KV Cache\"\"\"\n        self.kv_cache = KVCache(max_seq_len, self.num_heads, self.head_dim, device)\n\n    def forward(self, x, use_cache=False):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u8ba1\u7b97Q, K, V\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        if use_cache and self.kv_cache is not None:\n            # \u4f7f\u7528\u7f13\u5b58\u6a21\u5f0f\uff1a\u66f4\u65b0\u7f13\u5b58\u5e76\u83b7\u53d6\u5b8c\u6574\u7684K,V\n            K, V = self.kv_cache.update_cache(K, V)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attn_weights = torch.softmax(scores, dim=-1)\n        out = torch.matmul(attn_weights, V)\n\n        # \u5408\u5e76\u591a\u5934\u8f93\u51fa\n        out = out.view(batch_size, seq_len, d_model)\n        return self.W_o(out)\n\n\n# \u4f7f\u7528\u793a\u4f8b\ndef demo_kv_cache():\n    \"\"\"KV Cache\u4f7f\u7528\u6f14\u793a\"\"\"\n\n    # \u521d\u59cb\u5316\u6a21\u578b\n    attention = AttentionWithKVCache(d_model=512, num_heads=8)\n    attention.setup_cache(max_seq_len=1024, device='cpu')\n\n    print(\"=== KV Cache\u6f14\u793a ===\")\n\n    # \u6a21\u62df\u751f\u6210\u8fc7\u7a0b\n    vocab_size = 1000\n    sequence = []\n\n    for step in range(5):\n        if step == 0:\n            # \u7b2c\u4e00\u6b65\uff1a\u8f93\u5165\u5b8c\u6574\u7684prompt\n            current_input = torch.randint(0, vocab_size, (1, 3, 512))  # 3\u4e2atoken\u7684prompt\n            print(f\"Step {step}: \u8f93\u5165prompt (3 tokens)\")\n        else:\n            # \u540e\u7eed\u6b65\u9aa4\uff1a\u53ea\u8f93\u5165\u65b0\u751f\u6210\u7684token\n            current_input = torch.randint(0, vocab_size, (1, 1, 512))  # 1\u4e2a\u65b0token\n            print(f\"Step {step}: \u8f93\u5165\u65b0token (1 token)\")\n\n        # \u524d\u5411\u4f20\u64ad\uff08\u4f7f\u7528\u7f13\u5b58\uff09\n        output = attention(current_input, use_cache=True)\n\n        # \u663e\u793a\u7f13\u5b58\u72b6\u6001\n        cache_info = attention.kv_cache.get_cache_info()\n        print(f\"  \u7f13\u5b58\u957f\u5ea6: {cache_info['current_length']}\")\n        print(f\"  \u5185\u5b58\u4f7f\u7528: {cache_info['memory_mb']:.2f} MB\")\n        print()\n\nif __name__ == \"__main__\":\n    demo_kv_cache()\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#_6","title":"\u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5","text":"<pre><code>import time\n\ndef benchmark_with_without_cache():\n    \"\"\"\u5bf9\u6bd4\u6709\u65e0KV Cache\u7684\u6027\u80fd\"\"\"\n\n    d_model, num_heads = 768, 12\n    max_seq_len = 512\n\n    # \u521d\u59cb\u5316\u6a21\u578b\n    attention_with_cache = AttentionWithKVCache(d_model, num_heads)\n    attention_with_cache.setup_cache(max_seq_len, 'cpu')\n\n    attention_without_cache = AttentionWithKVCache(d_model, num_heads)\n\n    # \u6a21\u62df\u5e8f\u5217\u751f\u6210\n    prompt_len = 50\n    generate_len = 100\n\n    print(\"=== \u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5 ===\")\n\n    # \u6d4b\u8bd5\u65e0\u7f13\u5b58\u7248\u672c\n    start_time = time.time()\n    sequence_input = torch.randn(1, prompt_len, d_model)\n\n    for i in range(generate_len):\n        # \u6bcf\u6b21\u90fd\u8f93\u5165\u5b8c\u6574\u5e8f\u5217\uff08\u65e0\u7f13\u5b58\uff09\n        full_input = torch.randn(1, prompt_len + i + 1, d_model)\n        _ = attention_without_cache(full_input, use_cache=False)\n\n    no_cache_time = time.time() - start_time\n    print(f\"\u65e0\u7f13\u5b58\u751f\u6210\u65f6\u95f4: {no_cache_time:.3f}\u79d2\")\n\n    # \u6d4b\u8bd5\u6709\u7f13\u5b58\u7248\u672c  \n    start_time = time.time()\n\n    # \u5904\u7406prompt\n    _ = attention_with_cache(sequence_input, use_cache=True)\n\n    # \u9010\u6b65\u751f\u6210\n    for i in range(generate_len):\n        # \u6bcf\u6b21\u53ea\u8f93\u5165\u65b0token\uff08\u6709\u7f13\u5b58\uff09\n        new_token = torch.randn(1, 1, d_model)\n        _ = attention_with_cache(new_token, use_cache=True)\n\n    with_cache_time = time.time() - start_time\n    print(f\"\u6709\u7f13\u5b58\u751f\u6210\u65f6\u95f4: {with_cache_time:.3f}\u79d2\")\n\n    speedup = no_cache_time / with_cache_time\n    print(f\"\u52a0\u901f\u500d\u6570: {speedup:.1f}x\")\n\nif __name__ == \"__main__\":\n    benchmark_with_without_cache()\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3KV Cache\u7684\u5de5\u4f5c\u539f\u7406\u548c\u52a0\u901f\u673a\u5236</li> <li>[ ] \u80fd\u8ba1\u7b97KV Cache\u7684\u5185\u5b58\u9700\u6c42</li> <li>[ ] \u5b8c\u6210KV Cache\u6f14\u793a\u4ee3\u7801\u7684\u7f16\u5199\u548c\u6d4b\u8bd5</li> <li>[ ] \u7406\u89e3\u4e0d\u540c\u6ce8\u610f\u529b\u53d8\u4f53\u5bf9KV Cache\u7684\u5f71\u54cd</li> </ul>"},{"location":"fundamentals/attention-advanced/kv-cache/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53</li> <li>\u4e0b\u4e00\u8282\uff1a\u5f52\u4e00\u5316\u6280\u672f</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/","title":"\u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4eceMHA\u5230MLA\u7684\u6280\u672f\u6f14\u8fdb\uff0c\u638c\u63e1\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u5316\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\u3002</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Transformer\u7684Attention\u53ca\u5176\u5404\u79cd\u53d8\u4f53 - \u8be6\u7ec6\u5bf9\u6bd4\u5206\u6790</li> <li>\u7f13\u5b58\u4e0e\u6548\u679c\u7684\u6781\u9650\u62c9\u626f\uff1a\u4eceMHA\u3001MQA\u3001GQA\u5230MLA - \u79d1\u5b66\u7a7a\u95f4\u6df1\u5ea6\u89e3\u6790</li> </ol>"},{"location":"fundamentals/attention-advanced/mha-variants/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#_6","title":"\u6280\u672f\u6f14\u8fdb\u8def\u5f84","text":"<pre><code>MHA (\u6807\u51c6\u591a\u5934) \u2192 MQA (\u5171\u4eabKV) \u2192 GQA (\u5206\u7ec4\u5171\u4eab) \u2192 MLA (\u6f5c\u5728\u7a7a\u95f4)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_7","title":"\u5404\u53d8\u4f53\u8be6\u7ec6\u5bf9\u6bd4","text":"\u53d8\u4f53 KV Cache\u9700\u6c42 \u8ba1\u7b97\u590d\u6742\u5ea6 \u6027\u80fd\u8868\u73b0 \u4e3b\u8981\u5e94\u7528 MHA O(h\u00d7d\u00d7L) \u6700\u9ad8 \u57fa\u51c6\u6027\u80fd \u6807\u51c6Transformer MQA O(d\u00d7L) \u6700\u4f4e \u8f7b\u5fae\u4e0b\u964d \u5feb\u901f\u63a8\u7406 GQA O(g\u00d7d\u00d7L) \u4e2d\u7b49 \u5e73\u8861\u4f18\u79c0 \u4e3b\u6d41\u5927\u6a21\u578b MLA \u6700\u4f18\u5316 \u4f18\u5316 \u8d85\u8d8aMHA \u957f\u4e0a\u4e0b\u6587 <p>\u5176\u4e2d\uff1ah=\u5934\u6570\uff0cd=\u7ef4\u5ea6\uff0cL=\u5e8f\u5217\u957f\u5ea6\uff0cg=\u7ec4\u6570</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_8","title":"\u6838\u5fc3\u6280\u672f\u7ec6\u8282","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#1-mha-multi-head-attention","title":"1. MHA (Multi-Head Attention)","text":"<pre><code># \u6bcf\u4e2a\u5934\u90fd\u6709\u72ec\u7acb\u7684Q\u3001K\u3001V\nfor i in range(num_heads):\n    Q_i = input @ W_Q_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u67e5\u8be2\u77e9\u9635\n    K_i = input @ W_K_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u952e\u77e9\u9635  \n    V_i = input @ W_V_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u503c\u77e9\u9635\n    head_i = attention(Q_i, K_i, V_i)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#2-mqa-multi-query-attention","title":"2. MQA (Multi-Query Attention)","text":"<pre><code># \u6240\u6709\u5934\u5171\u4eabK\u3001V\uff0c\u53ea\u6709Q\u72ec\u7acb\nK_shared = input @ W_K  # \u5171\u4eab\u7684\u952e\u77e9\u9635\nV_shared = input @ W_V  # \u5171\u4eab\u7684\u503c\u77e9\u9635\n\nfor i in range(num_heads):\n    Q_i = input @ W_Q_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u67e5\u8be2\u77e9\u9635\n    head_i = attention(Q_i, K_shared, V_shared)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3-gqa-grouped-query-attention","title":"3. GQA (Grouped-Query Attention)","text":"<pre><code># \u5206\u7ec4\u5171\u4eab\uff1a\u6bcf\u7ec4\u5185\u5171\u4eabK\u3001V\nnum_groups = num_heads // group_size\n\nfor g in range(num_groups):\n    K_g = input @ W_K_g  # \u7ec4\u5171\u4eab\u7684\u952e\u77e9\u9635\n    V_g = input @ W_V_g  # \u7ec4\u5171\u4eab\u7684\u503c\u77e9\u9635\n\n    for i in range(group_size):\n        Q_i = input @ W_Q_i\n        head_i = attention(Q_i, K_g, V_g)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_9","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#q1-mhamqagqamla","title":"Q1: MHA\u3001MQA\u3001GQA\u3001MLA\u90fd\u662f\u4ec0\u4e48\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54\uff1a \u8fd9\u662fTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u56db\u4e2a\u6f14\u8fdb\u9636\u6bb5\uff0c\u4e3b\u8981\u4f18\u5316KV Cache\u7684\u5b58\u50a8\u9700\u6c42\uff1a</p> <ul> <li>MHA: \u6807\u51c6\u591a\u5934\u6ce8\u610f\u529b\uff0c\u6bcf\u4e2a\u5934\u72ec\u7acbQKV</li> <li>MQA: \u591a\u67e5\u8be2\u6ce8\u610f\u529b\uff0c\u6240\u6709\u5934\u5171\u4eabKV</li> <li>GQA: \u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\uff0c\u5206\u7ec4\u5185\u5171\u4eabKV  </li> <li>MLA: \u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u4f18\u5316</li> </ul> <p>\u6280\u672f\u7ec6\u8282\uff1a</p> <p>MHA\u95ee\u9898: KV Cache\u968f\u5934\u6570\u7ebf\u6027\u589e\u957f\uff0c\u5185\u5b58\u5f00\u9500\u5927 <pre><code>\u5185\u5b58\u9700\u6c42 = \u5934\u6570 \u00d7 \u7ef4\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6\n</code></pre></p> <p>MQA\u89e3\u51b3\u65b9\u6848: \u5171\u4eabKV\u77e9\u9635\uff0c\u5185\u5b58\u9700\u6c42\u964d\u4f4eh\u500d <pre><code># \u4ece h\u00d7(d_k + d_v) \u964d\u4f4e\u5230 (d_k + d_v)\n</code></pre></p> <p>GQA\u5e73\u8861\u65b9\u6848: \u5206\u7ec4\u5171\u4eab\uff0c\u517c\u987e\u6027\u80fd\u548c\u6548\u7387 <pre><code># \u5185\u5b58\u9700\u6c42 = \u7ec4\u6570 \u00d7 \u7ef4\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6  \n# \u5176\u4e2d\uff1a\u7ec4\u6570 = \u5934\u6570 / \u6bcf\u7ec4\u5934\u6570\n</code></pre></p> <p>MLA\u7ec8\u6781\u4f18\u5316: \u6f5c\u5728\u7a7a\u95f4\u6295\u5f71\uff0c\u6700\u5c0f\u5316KV Cache</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#q2","title":"Q2: \u4e3a\u4ec0\u4e48\u9700\u8981\u8fd9\u4e9b\u4f18\u5316\uff1f","text":"<p>\u6838\u5fc3\u52a8\u673a\uff1a</p> <ol> <li>\u5185\u5b58\u74f6\u9888</li> <li>\u957f\u5e8f\u5217\u63a8\u7406\u65f6KV Cache\u5360\u7528\u5927\u91cf\u663e\u5b58</li> <li> <p>\u9650\u5236\u4e86\u6a21\u578b\u7684\u90e8\u7f72\u548c\u6269\u5c55\u80fd\u529b</p> </li> <li> <p>\u63a8\u7406\u901f\u5ea6</p> </li> <li>\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387</li> <li> <p>\u652f\u6301\u66f4\u5927\u7684batch size</p> </li> <li> <p>\u6210\u672c\u8003\u8651</p> </li> <li>\u964d\u4f4e\u786c\u4ef6\u8981\u6c42</li> <li>\u63d0\u9ad8\u670d\u52a1\u5e76\u53d1\u80fd\u529b</li> </ol>"},{"location":"fundamentals/attention-advanced/mha-variants/#q3","title":"Q3: \u5404\u53d8\u4f53\u7684\u4f18\u7f3a\u70b9\u5bf9\u6bd4\uff1f","text":"\u7ef4\u5ea6 MHA MQA GQA MLA \u6027\u80fd \ud83d\udfe2 \u57fa\u51c6\u6700\u597d \ud83d\udfe1 \u8f7b\u5fae\u4e0b\u964d \ud83d\udfe2 \u63a5\u8fd1MHA \ud83d\udfe2 \u8d85\u8d8aMHA \u5185\u5b58 \ud83d\udd34 \u9700\u6c42\u6700\u9ad8 \ud83d\udfe2 \u663e\u8457\u964d\u4f4e \ud83d\udfe1 \u9002\u4e2d \ud83d\udfe2 \u6700\u4f18 \u901f\u5ea6 \ud83d\udfe1 \u6807\u51c6 \ud83d\udfe2 \u6700\u5feb \ud83d\udfe2 \u8f83\u5feb \ud83d\udfe2 \u4f18\u79c0 \u5b9e\u73b0 \ud83d\udfe2 \u7b80\u5355 \ud83d\udfe2 \u7b80\u5355 \ud83d\udfe1 \u4e2d\u7b49 \ud83d\udd34 \u590d\u6742"},{"location":"fundamentals/attention-advanced/mha-variants/#q4","title":"Q4: \u5982\u4f55\u9009\u62e9\u5408\u9002\u7684\u6ce8\u610f\u529b\u673a\u5236\uff1f","text":"<p>\u9009\u62e9\u7b56\u7565\uff1a</p> <pre><code>if \u8d44\u6e90\u5145\u8db3 and \u8ffd\u6c42\u6700\u4f73\u6027\u80fd:\n    \u9009\u62e9 MHA\nelif \u8d44\u6e90\u4e25\u91cd\u53d7\u9650 and \u53ef\u63a5\u53d7\u6027\u80fd\u635f\u5931:\n    \u9009\u62e9 MQA  \nelif \u9700\u8981\u5e73\u8861\u6027\u80fd\u548c\u6548\u7387:\n    \u9009\u62e9 GQA  # \u4e3b\u6d41\u9009\u62e9\nelif \u957f\u4e0a\u4e0b\u6587 and \u5185\u5b58\u654f\u611f:\n    \u9009\u62e9 MLA\n</code></pre> <p>\u5b9e\u9645\u8003\u8651\u56e0\u7d20\uff1a - \u786c\u4ef6\u5185\u5b58\u9650\u5236 - \u5e8f\u5217\u957f\u5ea6\u9700\u6c42 - \u5ef6\u8fdf\u8981\u6c42 - \u5f00\u53d1\u590d\u6742\u5ea6</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_10","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#1-softmax","title":"\u7ec3\u4e601: \u5b9e\u73b0Softmax\u51fd\u6570","text":"<p>\u5e73\u53f0: Deep-ML Softmax</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#2-mhagqa","title":"\u7ec3\u4e602: MHA\u5230GQA\u7684\u9002\u914d","text":"<pre><code>class GroupedQueryAttention(nn.Module):\n    def __init__(self, d_model, num_heads, num_groups):\n        super().__init__()\n        assert num_heads % num_groups == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.num_groups = num_groups\n        self.heads_per_group = num_heads // num_groups\n        self.d_k = d_model // num_heads\n\n        # Q\u77e9\u9635\uff1a\u6bcf\u4e2a\u5934\u72ec\u7acb\n        self.W_q = nn.Linear(d_model, d_model)\n\n        # K,V\u77e9\u9635\uff1a\u6309\u7ec4\u5171\u4eab\n        self.W_k = nn.Linear(d_model, num_groups * self.d_k)\n        self.W_v = nn.Linear(d_model, num_groups * self.d_k)\n\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u751f\u6210Q\uff1a\u6bcf\u4e2a\u5934\u72ec\u7acb\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k)\n\n        # \u751f\u6210K,V\uff1a\u6309\u7ec4\u5171\u4eab\n        K = self.W_k(x).view(batch_size, seq_len, self.num_groups, self.d_k)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_groups, self.d_k)\n\n        # \u91cd\u590dK,V\u4ee5\u5339\u914dQ\u7684\u5934\u6570\n        K = K.repeat_interleave(self.heads_per_group, dim=2)\n        V = V.repeat_interleave(self.heads_per_group, dim=2)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        attn_weights = F.softmax(scores, dim=-1)\n        out = torch.matmul(attn_weights, V)\n\n        # \u5408\u5e76\u591a\u5934\u8f93\u51fa\n        out = out.view(batch_size, seq_len, d_model)\n        return self.W_o(out)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3-kv-cache","title":"\u7ec3\u4e603: KV Cache\u5b9e\u73b0\u9884\u89c8","text":"<pre><code>class KVCache:\n    def __init__(self, max_seq_len, num_heads, d_k):\n        self.max_seq_len = max_seq_len\n        self.cache_k = torch.zeros(max_seq_len, num_heads, d_k)\n        self.cache_v = torch.zeros(max_seq_len, num_heads, d_k) \n        self.current_len = 0\n\n    def update(self, new_k, new_v):\n        \"\"\"\u66f4\u65b0\u7f13\u5b58\u5e76\u8fd4\u56de\u5b8c\u6574\u7684K,V\"\"\"\n        seq_len = new_k.size(0)\n\n        # \u5b58\u50a8\u65b0\u7684K,V\n        self.cache_k[self.current_len:self.current_len+seq_len] = new_k\n        self.cache_v[self.current_len:self.current_len+seq_len] = new_v\n\n        self.current_len += seq_len\n\n        # \u8fd4\u56de\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u5b8c\u6574K,V\n        return (self.cache_k[:self.current_len], \n                self.cache_v[:self.current_len])\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_11","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u80fd\u89e3\u91ca\u5404\u53d8\u4f53\u7684\u6838\u5fc3\u533a\u522b</li> <li>[ ] \u7406\u89e3KV Cache\u4f18\u5316\u7684\u539f\u7406</li> <li>[ ] \u5b8c\u6210GQA\u4ee3\u7801\u5b9e\u73b0</li> <li>[ ] \u80fd\u6839\u636e\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u6ce8\u610f\u529b\u673a\u5236</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/#_12","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aKV Cache\u6280\u672f</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/attention-advanced/normalization/","title":"\u5f52\u4e00\u5316\u6280\u672f","text":""},{"location":"fundamentals/attention-advanced/normalization/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u638c\u63e1\u6df1\u5ea6\u5b66\u4e60\u4e2d\u4e09\u5927\u5f52\u4e00\u5316\u6280\u672f\u7684\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\uff0c\u7406\u89e3Pre-Norm\u4e0ePost-Norm\u5728Transformer\u4e2d\u7684\u5f71\u54cd\u3002</p>"},{"location":"fundamentals/attention-advanced/normalization/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/normalization/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>\u5927\u6a21\u578b\u4e2d\u5e38\u89c1\u76843\u79cdNorm - \u77e5\u4e4e</li> <li>\u4e3a\u4ec0\u4e48\u5f53\u524d\u4e3b\u6d41\u7684\u5927\u6a21\u578b\u90fd\u4f7f\u7528RMS-Norm\uff1f - \u77e5\u4e4e  </li> <li>\u4e3a\u4ec0\u4e48Pre Norm\u7684\u6548\u679c\u4e0d\u5982Post Norm\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> </ol>"},{"location":"fundamentals/attention-advanced/normalization/#_5","title":"\u9009\u8bfb\u6df1\u5165\u6750\u6599","text":"<ul> <li>BN\u7a76\u7adf\u8d77\u4e86\u4ec0\u4e48\u4f5c\u7528\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> </ul>"},{"location":"fundamentals/attention-advanced/normalization/#_6","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/normalization/#_7","title":"\u4e09\u5927\u5f52\u4e00\u5316\u6280\u672f\u5bf9\u6bd4","text":"\u6280\u672f \u5f52\u4e00\u5316\u7ef4\u5ea6 \u9002\u7528\u573a\u666f \u4e3b\u8981\u4f18\u52bf \u8ba1\u7b97\u6210\u672c BatchNorm \u8de8\u6837\u672c\u7279\u5f81\u7ef4\u5ea6 CNN\u3001\u5927batch \u8bad\u7ec3\u52a0\u901f\u3001\u9632\u8fc7\u62df\u5408 \u4e2d\u7b49 LayerNorm \u5355\u6837\u672c\u6240\u6709\u7279\u5f81 RNN\u3001Transformer \u4e0d\u4f9d\u8d56batch\u5927\u5c0f \u8f83\u9ad8 RMSNorm \u5355\u6837\u672cRMS\u5f52\u4e00\u5316 \u5927\u578b\u8bed\u8a00\u6a21\u578b \u8ba1\u7b97\u9ad8\u6548\u3001\u6548\u679c\u76f8\u5f53 \u6700\u4f4e"},{"location":"fundamentals/attention-advanced/normalization/#_8","title":"\u6570\u5b66\u516c\u5f0f\u8be6\u89e3","text":""},{"location":"fundamentals/attention-advanced/normalization/#1-batch-normalization","title":"1. Batch Normalization","text":"<pre><code>BN(x) = \u03b3 \u00d7 \\frac{x - \u03bc_B}{\\sqrt{\u03c3_B^2 + \u03b5}} + \u03b2\n</code></pre> <p>\u6838\u5fc3\u7279\u70b9: - \u03bc_B, \u03c3_B: \u5728batch\u7ef4\u5ea6\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee - \u8bad\u7ec3\u65f6\u4f7f\u7528\u5f53\u524dbatch\u7edf\u8ba1\u91cf\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u79fb\u52a8\u5e73\u5747 - \u9700\u8981\u03b3(\u7f29\u653e)\u548c\u03b2(\u504f\u79fb)\u53ef\u5b66\u4e60\u53c2\u6570</p> <p>\u95ee\u9898: - \u4f9d\u8d56batch\u5927\u5c0f\uff0c\u5c0fbatch\u6548\u679c\u5dee - \u8bad\u7ec3\u548c\u63a8\u7406\u4e0d\u4e00\u81f4 - \u5728\u5e8f\u5217\u6a21\u578b\u4e2d\u6548\u679c\u4e0d\u4f73</p>"},{"location":"fundamentals/attention-advanced/normalization/#2-layer-normalization","title":"2. Layer Normalization","text":"<pre><code>LN(x) = \u03b3 \u00d7 \\frac{x - \u03bc_L}{\\sqrt{\u03c3_L^2 + \u03b5}} + \u03b2\n</code></pre> <p>\u6838\u5fc3\u7279\u70b9: - \u03bc_L, \u03c3_L: \u5728\u7279\u5f81\u7ef4\u5ea6\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee - \u6bcf\u4e2a\u6837\u672c\u72ec\u7acb\u5f52\u4e00\u5316\uff0c\u4e0d\u4f9d\u8d56\u5176\u4ed6\u6837\u672c - \u8bad\u7ec3\u548c\u63a8\u7406\u4e00\u81f4</p> <p>\u4f18\u52bf: - \u9002\u5408\u53d8\u957f\u5e8f\u5217 - \u4e0d\u53d7batch\u5927\u5c0f\u5f71\u54cd - Transformer\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/attention-advanced/normalization/#3-rms-normalization","title":"3. RMS Normalization","text":"<pre><code>RMSNorm(x) = \u03b3 \u00d7 \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2 + \u03b5}}\n</code></pre> <p>\u6838\u5fc3\u7279\u70b9: - \u53ea\u8ba1\u7b97RMS\uff0c\u4e0d\u51cf\u53bb\u5747\u503c - \u7b80\u5316\u4e86LayerNorm\u7684\u8ba1\u7b97 - \u53ea\u9700\u8981\u03b3\u53c2\u6570\uff0c\u65e0\u9700\u03b2</p> <p>\u4f18\u52bf: - \u8ba1\u7b97\u6210\u672c\u66f4\u4f4e - \u5728\u5927\u6a21\u578b\u4e2d\u6548\u679c\u4e0d\u8f93LayerNorm - \u5185\u5b58\u53cb\u597d</p>"},{"location":"fundamentals/attention-advanced/normalization/#pre-norm-vs-post-norm","title":"Pre-Norm vs Post-Norm","text":""},{"location":"fundamentals/attention-advanced/normalization/#post-norm-transformer","title":"Post-Norm (\u539f\u59cbTransformer)","text":"<pre><code>Input \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 LayerNorm \u2192 Output\n</code></pre> <p>\u7279\u70b9: - \u5f52\u4e00\u5316\u5728\u6b8b\u5dee\u8fde\u63a5\u4e4b\u540e - \u9700\u8981\u5b66\u4e60\u7387warmup\u624d\u80fd\u7a33\u5b9a\u8bad\u7ec3 - \u6d45\u5c42\u6a21\u578b(\u22646\u5c42)\u6548\u679c\u66f4\u597d - \u68af\u5ea6\u4f20\u64ad\u53ef\u80fd\u4e0d\u7a33\u5b9a</p>"},{"location":"fundamentals/attention-advanced/normalization/#pre-norm","title":"Pre-Norm (\u73b0\u4ee3\u4e3b\u6d41)","text":"<pre><code>Input \u2192 LayerNorm \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 Output\n</code></pre> <p>\u7279\u70b9: - \u5f52\u4e00\u5316\u5728\u6b8b\u5dee\u8fde\u63a5\u4e4b\u524d - \u8bad\u7ec3\u66f4\u7a33\u5b9a\uff0c\u65e0\u9700warmup - \u6df1\u5c42\u6a21\u578b\u8bad\u7ec3\u66f4\u5bb9\u6613 - \u73b0\u4ee3\u5927\u6a21\u578b\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/attention-advanced/normalization/#_9","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/normalization/#q1-batch-normlayer-norm","title":"Q1: Batch Norm\u548cLayer Norm\u7684\u533a\u522b\uff1f","text":"<p>\u6838\u5fc3\u533a\u522b:</p> <ol> <li>\u5f52\u4e00\u5316\u7ef4\u5ea6\u4e0d\u540c:</li> <li>BatchNorm: \u5728batch\u7ef4\u5ea6\u5f52\u4e00\u5316\uff0c\u6bcf\u4e2a\u7279\u5f81\u72ec\u7acb</li> <li> <p>LayerNorm: \u5728\u7279\u5f81\u7ef4\u5ea6\u5f52\u4e00\u5316\uff0c\u6bcf\u4e2a\u6837\u672c\u72ec\u7acb</p> </li> <li> <p>\u5e94\u7528\u573a\u666f:</p> </li> <li>BatchNorm: CNN\u3001\u89c6\u89c9\u4efb\u52a1\u3001\u5927batch\u8bad\u7ec3</li> <li> <p>LayerNorm: NLP\u3001\u5e8f\u5217\u6a21\u578b\u3001\u5c0fbatch\u6216\u53d8\u957f\u5e8f\u5217</p> </li> <li> <p>\u4f9d\u8d56\u6027:</p> </li> <li>BatchNorm: \u4f9d\u8d56batch\u5927\u5c0f\u548c\u5176\u4ed6\u6837\u672c</li> <li>LayerNorm: \u53ea\u4f9d\u8d56\u5f53\u524d\u6837\u672c\uff0c\u66f4\u7a33\u5b9a</li> </ol> <p>\u6280\u672f\u7ec6\u8282: <pre><code># BatchNorm: \u5728batch\u7ef4\u5ea6\u8ba1\u7b97\u7edf\u8ba1\u91cf\nbatch_mean = x.mean(dim=0)  # [features]\nbatch_var = x.var(dim=0)    # [features]\n\n# LayerNorm: \u5728\u7279\u5f81\u7ef4\u5ea6\u8ba1\u7b97\u7edf\u8ba1\u91cf  \nlayer_mean = x.mean(dim=-1, keepdim=True)  # [batch, 1]\nlayer_var = x.var(dim=-1, keepdim=True)    # [batch, 1]\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/normalization/#q2-rmsnorm","title":"Q2: \u4e3a\u4ec0\u4e48\u73b0\u5728\u7528RMSNorm\uff1f","text":"<p>\u4e3b\u8981\u539f\u56e0:</p> <ol> <li>\u8ba1\u7b97\u6548\u7387:</li> <li>\u7701\u7565\u4e86\u5747\u503c\u8ba1\u7b97\uff0c\u51cf\u5c11\u4e86\u7ea615%\u7684\u8ba1\u7b97\u91cf</li> <li> <p>\u5185\u5b58\u8bbf\u95ee\u66f4\u5c11\uff0c\u5bf9GPU\u66f4\u53cb\u597d</p> </li> <li> <p>\u6548\u679c\u76f8\u5f53:</p> </li> <li>\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eRMSNorm\u6548\u679c\u4e0d\u8f93LayerNorm</li> <li> <p>\u5728\u5927\u6a21\u578b\u4e2d\u8868\u73b0\u751a\u81f3\u66f4\u597d</p> </li> <li> <p>\u7b80\u5316\u5b9e\u73b0:</p> </li> <li>\u4e0d\u9700\u8981\u03b2\u53c2\u6570\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u91cf</li> <li>\u6570\u503c\u7a33\u5b9a\u6027\u66f4\u597d</li> </ol> <p>\u6280\u672f\u539f\u7406: <pre><code># LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\nmean = x.mean(dim=-1, keepdim=True)\nvar = x.var(dim=-1, keepdim=True)  \nln_out = gamma * (x - mean) / sqrt(var + eps) + beta\n\n# RMSNorm\u53ea\u9700\u8981\u8ba1\u7b97RMS\nrms = sqrt(x.pow(2).mean(dim=-1, keepdim=True) + eps)\nrms_out = gamma * x / rms\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/normalization/#q3-pre-normpost-norm","title":"Q3: Pre-Norm\u548cPost-Norm\u7684\u4f4d\u7f6e\u533a\u522b\uff1f","text":"<p>\u67b6\u6784\u5bf9\u6bd4:</p> <p>Post-Norm (\u539f\u59cb): <pre><code>x \u2192 Attention \u2192 (+) \u2192 LayerNorm \u2192 FFN \u2192 (+) \u2192 LayerNorm\n    \u2191_______________|              \u2191_______|\n</code></pre></p> <p>Pre-Norm (\u73b0\u4ee3): <pre><code>x \u2192 LayerNorm \u2192 Attention \u2192 (+) \u2192 LayerNorm \u2192 FFN \u2192 (+)\n    \u2191________________________|              \u2191______|\n</code></pre></p> <p>\u8bad\u7ec3\u7a33\u5b9a\u6027\u5dee\u5f02:</p> \u65b9\u9762 Post-Norm Pre-Norm \u5b66\u4e60\u7387warmup \u5fc5\u9700 \u53ef\u9009 \u6df1\u5c42\u8bad\u7ec3 \u5bb9\u6613\u5931\u8d25 \u7a33\u5b9a \u68af\u5ea6\u4f20\u64ad \u53ef\u80fd\u4e0d\u7a33\u5b9a \u66f4\u5e73\u6ed1 \u6536\u655b\u901f\u5ea6 \u8f83\u6162 \u8f83\u5feb \u6700\u7ec8\u6027\u80fd \u6d45\u5c42\u66f4\u597d \u6df1\u5c42\u66f4\u4f18"},{"location":"fundamentals/attention-advanced/normalization/#q4-pre-norm","title":"Q4: \u4e3a\u4ec0\u4e48Pre-Norm\u8bad\u7ec3\u66f4\u7a33\u5b9a\uff1f","text":"<p>\u68af\u5ea6\u4f20\u64ad\u5206\u6790:</p> <ol> <li>Post-Norm\u95ee\u9898:</li> <li>\u68af\u5ea6\u9700\u8981\u7ecf\u8fc7LayerNorm\u7684\u53cd\u5411\u4f20\u64ad</li> <li>LayerNorm\u7684\u5bfc\u6570\u53ef\u80fd\u653e\u5927\u6216\u7f29\u5c0f\u68af\u5ea6</li> <li> <p>\u6df1\u5c42\u7f51\u7edc\u5bb9\u6613\u51fa\u73b0\u68af\u5ea6\u7206\u70b8/\u6d88\u5931</p> </li> <li> <p>Pre-Norm\u4f18\u52bf:</p> </li> <li>\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u6052\u7b49\u8def\u5f84(identity path)</li> <li>\u68af\u5ea6\u53ef\u4ee5\u66f4\u76f4\u63a5\u5730\u53cd\u5411\u4f20\u64ad</li> <li>\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u68af\u5ea6\u7684\u6570\u91cf\u7ea7\u4e3a\u221aL (L\u4e3a\u5c42\u6570)</li> </ol> <p>\u6570\u5b66\u76f4\u89c9: <pre><code>Post-Norm: \u68af\u5ea6\u9700\u8981\u7a7f\u8fc7LayerNorm\n\u2207L/\u2202x = \u2207L/\u2202norm \u00d7 \u2202norm/\u2202x  (\u4e0d\u7a33\u5b9a)\n\nPre-Norm: \u6052\u7b49\u8def\u5f84\u66f4\u5f3a  \n\u2207L/\u2202x = \u2207L/\u2202residual + \u2207L/\u2202processed  (\u66f4\u7a33\u5b9a)\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/normalization/#_10","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/normalization/#normpytorch","title":"\u4e09\u79cdNorm\u7684PyTorch\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass BatchNorm1d(nn.Module):\n    \"\"\"\u81ea\u5b9e\u73b0BatchNorm\"\"\"\n    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n\n        # \u53ef\u5b66\u4e60\u53c2\u6570\n        self.gamma = nn.Parameter(torch.ones(num_features))\n        self.beta = nn.Parameter(torch.zeros(num_features))\n\n        # \u79fb\u52a8\u5e73\u5747\u7edf\u8ba1\u91cf\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n\n    def forward(self, x):\n        # x shape: [batch_size, num_features]\n        if self.training:\n            # \u8ba1\u7b97\u5f53\u524dbatch\u7684\u7edf\u8ba1\u91cf\n            batch_mean = x.mean(dim=0)\n            batch_var = x.var(dim=0, unbiased=False)\n\n            # \u66f4\u65b0\u79fb\u52a8\u5e73\u5747\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n\n            # \u4f7f\u7528\u5f53\u524dbatch\u7edf\u8ba1\u91cf\u5f52\u4e00\u5316\n            mean, var = batch_mean, batch_var\n        else:\n            # \u63a8\u7406\u65f6\u4f7f\u7528\u79fb\u52a8\u5e73\u5747\n            mean, var = self.running_mean, self.running_var\n\n        # \u5f52\u4e00\u5316\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        return self.gamma * x_norm + self.beta\n\nclass LayerNorm(nn.Module):\n    \"\"\"\u81ea\u5b9e\u73b0LayerNorm\"\"\"\n    def __init__(self, normalized_shape, eps=1e-5):\n        super().__init__()\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n\n    def forward(self, x):\n        # \u5728\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8ba1\u7b97\u7edf\u8ba1\u91cf\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n\n        # \u5f52\u4e00\u5316\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        return self.gamma * x_norm + self.beta\n\nclass RMSNorm(nn.Module):\n    \"\"\"\u81ea\u5b9e\u73b0RMSNorm\"\"\"\n    def __init__(self, normalized_shape, eps=1e-5):\n        super().__init__()\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n\n    def forward(self, x):\n        # \u8ba1\u7b97RMS\n        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n\n        # RMS\u5f52\u4e00\u5316\n        x_norm = x / rms\n        return self.gamma * x_norm\n\n# \u4f7f\u7528\u793a\u4f8b\u548c\u6027\u80fd\u5bf9\u6bd4\ndef compare_normalization():\n    \"\"\"\u5bf9\u6bd4\u4e09\u79cd\u5f52\u4e00\u5316\u7684\u8ba1\u7b97\u6210\u672c\"\"\"\n\n    batch_size, seq_len, d_model = 32, 512, 768\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # \u521d\u59cb\u5316\u4e09\u79cdnorm\n    bn = BatchNorm1d(d_model)\n    ln = LayerNorm(d_model)\n    rms = RMSNorm(d_model)\n\n    print(\"=== \u5f52\u4e00\u5316\u6280\u672f\u5bf9\u6bd4 ===\")\n\n    # \u6d4b\u8bd5LayerNorm\n    import time\n    start_time = time.time()\n    for _ in range(1000):\n        _ = ln(x)\n    ln_time = time.time() - start_time\n    print(f\"LayerNorm\u8017\u65f6: {ln_time:.4f}\u79d2\")\n\n    # \u6d4b\u8bd5RMSNorm\n    start_time = time.time()\n    for _ in range(1000):\n        _ = rms(x)\n    rms_time = time.time() - start_time\n    print(f\"RMSNorm\u8017\u65f6: {rms_time:.4f}\u79d2\")\n\n    speedup = ln_time / rms_time\n    print(f\"RMSNorm\u52a0\u901f\u500d\u6570: {speedup:.2f}x\")\n\n    # \u53c2\u6570\u91cf\u5bf9\u6bd4\n    ln_params = sum(p.numel() for p in ln.parameters())\n    rms_params = sum(p.numel() for p in rms.parameters())\n    print(f\"LayerNorm\u53c2\u6570\u91cf: {ln_params}\")\n    print(f\"RMSNorm\u53c2\u6570\u91cf: {rms_params}\")\n    print(f\"\u53c2\u6570\u51cf\u5c11: {(ln_params - rms_params) / ln_params * 100:.1f}%\")\n\n# Pre-Norm vs Post-Norm\u5b9e\u73b0\nclass PostNormBlock(nn.Module):\n    \"\"\"Post-Norm Transformer Block\"\"\"\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model)\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        # Post-Norm: Attention \u2192 Add \u2192 Norm\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)\n\n        # FFN \u2192 Add \u2192 Norm\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)\n        return x\n\nclass PreNormBlock(nn.Module):\n    \"\"\"Pre-Norm Transformer Block\"\"\"\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model)\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        # Pre-Norm: Norm \u2192 Attention \u2192 Add\n        norm_x = self.norm1(x)\n        attn_out, _ = self.attention(norm_x, norm_x, norm_x)\n        x = x + attn_out\n\n        # Norm \u2192 FFN \u2192 Add\n        norm_x = self.norm2(x)\n        ffn_out = self.ffn(norm_x)\n        x = x + ffn_out\n        return x\n\nif __name__ == \"__main__\":\n    compare_normalization()\n</code></pre>"},{"location":"fundamentals/attention-advanced/normalization/#_11","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u4e09\u79cd\u5f52\u4e00\u5316\u7684\u6570\u5b66\u539f\u7406\u548c\u8ba1\u7b97\u65b9\u5f0f</li> <li>[ ] \u80fd\u89e3\u91ca\u4e3a\u4ec0\u4e48Transformer\u9009\u62e9LayerNorm\u800c\u975eBatchNorm</li> <li>[ ] \u638c\u63e1Pre-Norm\u76f8\u6bd4Post-Norm\u7684\u8bad\u7ec3\u4f18\u52bf</li> <li>[ ] \u5b8c\u6210\u5f52\u4e00\u5316\u6280\u672f\u7684\u4ee3\u7801\u5b9e\u73b0\u548c\u6027\u80fd\u5bf9\u6bd4</li> </ul>"},{"location":"fundamentals/attention-advanced/normalization/#_12","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aKV Cache\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1a\u4f4d\u7f6e\u7f16\u7801</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/attention-advanced/positional-encoding/","title":"\u4f4d\u7f6e\u7f16\u7801","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4f4d\u7f6e\u7f16\u7801\u5728Transformer\u4e2d\u7684\u4f5c\u7528\uff0c\u638c\u63e1\u4ece\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u5230RoPE\u7684\u6280\u672f\u6f14\u8fdb\uff0c\u80fd\u591f\u63a8\u5bfcRoPE\u7684\u6570\u5b66\u539f\u7406\u3002</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Sinusoidal\u4f4d\u7f6e\u7f16\u7801\u8ffd\u6839\u6eaf\u6e90 - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u535a\u91c7\u4f17\u957f\u7684\u65cb\u8f6c\u5f0f\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u8ba9\u7814\u7a76\u4eba\u5458\u7ede\u5c3d\u8111\u6c41\u7684Transformer\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4</li> </ol>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#_6","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u4f4d\u7f6e\u7f16\u7801\uff1f","text":"<p>\u6838\u5fc3\u95ee\u9898: Transformer\u7684Self-Attention\u673a\u5236\u662f\u7f6e\u6362\u4e0d\u53d8\u7684\uff08permutation invariant\uff09\uff0c\u65e0\u6cd5\u533a\u5206token\u7684\u987a\u5e8f\u3002</p> <pre><code># \u6ca1\u6709\u4f4d\u7f6e\u4fe1\u606f\u65f6\uff0c\u8fd9\u4e24\u4e2a\u5e8f\u5217\u662f\u7b49\u4ef7\u7684\nsequence1 = [\"\u6211\", \"\u7231\", \"\u5317\u4eac\"]\nsequence2 = [\"\u7231\", \"\u5317\u4eac\", \"\u6211\"]\n# Self-Attention\u4f1a\u7ed9\u51fa\u76f8\u540c\u7684\u7ed3\u679c\uff01\n</code></pre> <p>\u89e3\u51b3\u65b9\u6848: \u5728\u8f93\u5165\u4e2d\u6ce8\u5165\u4f4d\u7f6e\u4fe1\u606f\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u7406\u89e3token\u4e4b\u95f4\u7684\u76f8\u5bf9\u6216\u7edd\u5bf9\u4f4d\u7f6e\u5173\u7cfb\u3002</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_7","title":"\u4f4d\u7f6e\u7f16\u7801\u5206\u7c7b","text":"<pre><code>\u4f4d\u7f6e\u7f16\u7801\n\u251c\u2500\u2500 \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 (APE)\n\u2502   \u251c\u2500\u2500 \u53ef\u8bad\u7ec3\u4f4d\u7f6e\u7f16\u7801 (Learned PE)\n\u2502   \u2514\u2500\u2500 \u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801 (Sinusoidal PE)\n\u2514\u2500\u2500 \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 (RPE)\n    \u251c\u2500\u2500 \u7ecf\u5178\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\n    \u251c\u2500\u2500 \u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 (RoPE)\n    \u2514\u2500\u2500 \u5176\u4ed6\u53d8\u4f53 (ALiBi\u7b49)\n</code></pre>"},{"location":"fundamentals/attention-advanced/positional-encoding/#vs","title":"\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 vs \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801","text":"\u7ef4\u5ea6 \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 \u7f16\u7801\u5bf9\u8c61 token\u7684\u7edd\u5bf9\u4f4d\u7f6e token\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb \u64cd\u4f5c\u4f4d\u7f6e \u8f93\u5165\u5c42\u6dfb\u52a0\u4f4d\u7f6e\u5411\u91cf \u6ce8\u610f\u529b\u5c42\u4fee\u6539\u8ba1\u7b97\u65b9\u5f0f \u5b9e\u73b0\u590d\u6742\u5ea6 \u7b80\u5355 \u76f8\u5bf9\u590d\u6742 \u957f\u5ea6\u5916\u63a8 \u8f83\u5dee \u8f83\u597d \u6027\u80fd\u8868\u73b0 \u77ed\u5e8f\u5217\u8db3\u591f \u957f\u5e8f\u5217\u66f4\u4f18"},{"location":"fundamentals/attention-advanced/positional-encoding/#_8","title":"\u6838\u5fc3\u6280\u672f\u8be6\u89e3","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#1-sinusoidal-transformer","title":"1. Sinusoidal\u4f4d\u7f6e\u7f16\u7801 (\u539f\u59cbTransformer)","text":"<p>\u6570\u5b66\u516c\u5f0f: <pre><code>PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}})\nPE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}})\n</code></pre></p> <p>\u6838\u5fc3\u7279\u70b9: - \u4f7f\u7528\u6b63\u5f26\u4f59\u5f26\u51fd\u6570\u751f\u6210\u4f4d\u7f6e\u7f16\u7801 - \u4e0d\u540c\u7ef4\u5ea6\u4f7f\u7528\u4e0d\u540c\u7684\u9891\u7387 - \u56fa\u5b9a\u7f16\u7801\uff0c\u4e0d\u9700\u8981\u8bad\u7ec3 - \u7406\u8bba\u4e0a\u652f\u6301\u4efb\u610f\u957f\u5ea6\u5e8f\u5217</p> <p>\u4f18\u52bf: - \u8ba1\u7b97\u7b80\u5355\uff0c\u4e0d\u5360\u7528\u53c2\u6570 - \u5177\u6709\u4e00\u5b9a\u7684\u5916\u63a8\u80fd\u529b - \u76f8\u5bf9\u4f4d\u7f6e\u6709\u4e00\u5b9a\u7684\u89c4\u5f8b\u6027</p> <p>\u7f3a\u70b9: - \u4f4d\u7f6e\u4fe1\u606f\u5728\u6df1\u5c42\u53ef\u80fd\u8870\u51cf - \u5bf9\u76f8\u5bf9\u4f4d\u7f6e\u7684\u5efa\u6a21\u4e0d\u591f\u76f4\u63a5</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#2","title":"2. \u53ef\u8bad\u7ec3\u4f4d\u7f6e\u7f16\u7801","text":"<p>\u5b9e\u73b0\u65b9\u5f0f: <pre><code># \u4e3a\u6bcf\u4e2a\u4f4d\u7f6e\u5b66\u4e60\u4e00\u4e2a\u5411\u91cf\nposition_embeddings = nn.Embedding(max_seq_len, d_model)\npos_emb = position_embeddings(position_ids)\ninput_emb = token_emb + pos_emb\n</code></pre></p> <p>\u7279\u70b9: - \u6bcf\u4e2a\u4f4d\u7f6e\u5bf9\u5e94\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u5411\u91cf - \u901a\u8fc7\u8bad\u7ec3\u4f18\u5316\u4f4d\u7f6e\u8868\u793a - \u5728\u8bad\u7ec3\u957f\u5ea6\u8303\u56f4\u5185\u6548\u679c\u901a\u5e38\u66f4\u597d</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#3-rope","title":"3. RoPE (\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801)","text":"<p>\u6838\u5fc3\u601d\u60f3: \u901a\u8fc7\u65cb\u8f6c\u53d8\u6362\u5c06\u4f4d\u7f6e\u4fe1\u606f\u7f16\u7801\u5230\u67e5\u8be2\u548c\u952e\u5411\u91cf\u4e2d\uff0c\u4f7f\u5f97\u6ce8\u610f\u529b\u5206\u6570\u81ea\u7136\u5730\u4f9d\u8d56\u4e8e\u76f8\u5bf9\u4f4d\u7f6e\u3002</p> <p>\u6570\u5b66\u63a8\u5bfc:</p> <p>\u6b65\u9aa41: \u5c06\u7279\u5f81\u5206\u4e3apairs\uff0c\u6bcf\u5bf9\u7279\u5f81\u770b\u4f5c2D\u5e73\u9762\u7684\u5750\u6807 <pre><code>x = [x_1, x_2, x_3, x_4, ...] \u2192 [(x_1, x_2), (x_3, x_4), ...]\n</code></pre></p> <p>\u6b65\u9aa42: \u5bf9\u6bcf\u4e00\u5bf9\u7279\u5f81\u5e94\u7528\u65cb\u8f6c\u77e9\u9635 <pre><code>\\begin{pmatrix}\nx_{m}^{(1)} \\\\\nx_{m}^{(2)}\n\\end{pmatrix}\n\u2192\n\\begin{pmatrix}\n\\cos(m\\theta) &amp; -\\sin(m\\theta) \\\\\n\\sin(m\\theta) &amp; \\cos(m\\theta)\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{m}^{(1)} \\\\\nx_{m}^{(2)}\n\\end{pmatrix}\n</code></pre></p> <p>\u6b65\u9aa43: \u65cb\u8f6c\u540e\u7684\u5411\u91cf <pre><code>\\begin{pmatrix}\nx_{m}^{(1)} \\cos(m\\theta) - x_{m}^{(2)} \\sin(m\\theta) \\\\\nx_{m}^{(2)} \\cos(m\\theta) + x_{m}^{(1)} \\sin(m\\theta)\n\\end{pmatrix}\n</code></pre></p> <p>\u6838\u5fc3\u6027\u8d28: \u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56 <pre><code>\\langle RoPE(q_m), RoPE(k_n) \\rangle = \\langle q_m, k_n \\rangle \\cos((m-n)\\theta) + \\text{\u5176\u4ed6\u9879}\n</code></pre></p> <p>\u6ce8\u610f\u529b\u5206\u6570\u53ea\u4f9d\u8d56\u4e8e\u76f8\u5bf9\u8ddd\u79bb (m-n)\uff01</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_9","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#q1","title":"Q1: \u4ec0\u4e48\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff1f","text":"<p>\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 (APE): - \u5b9a\u4e49: \u4e3a\u6bcf\u4e2atoken\u7684\u7edd\u5bf9\u4f4d\u7f6e\u5206\u914d\u4e00\u4e2a\u4f4d\u7f6e\u5411\u91cf - \u5b9e\u73b0: \u5728\u8f93\u5165\u5c42\u5c06\u4f4d\u7f6e\u5411\u91cf\u52a0\u5230token embedding\u4e0a - \u7279\u70b9: \u7b80\u5355\u76f4\u63a5\uff0c\u6bcf\u4e2a\u4f4d\u7f6e\u6709\u56fa\u5b9a\u7684\u7f16\u7801</p> <p>\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 (RPE): - \u5b9a\u4e49: \u5728\u8ba1\u7b97\u6ce8\u610f\u529b\u65f6\u8003\u8651token\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb - \u5b9e\u73b0: \u4fee\u6539\u6ce8\u610f\u529b\u8ba1\u7b97\u516c\u5f0f\uff0c\u52a0\u5165\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e - \u7279\u70b9: \u66f4\u7b26\u5408\u76f4\u89c9\uff0c\u5916\u63a8\u80fd\u529b\u66f4\u5f3a</p> <p>\u6280\u672f\u7ec6\u8282\u5bf9\u6bd4: <pre><code># \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\ninput_emb = token_emb + position_emb[pos]\n\n# \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801  \nattention_score = QK^T + relative_position_bias[i-j]\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#q2-rope","title":"Q2: \u63a8\u5bfcRoPE\u7684\u6570\u5b66\u539f\u7406","text":"<p>\u63a8\u5bfc\u6b65\u9aa4:</p> <p>\u76ee\u6807: \u8bbe\u8ba1\u4e00\u4e2a\u51fd\u6570f\uff0c\u4f7f\u5f97\uff1a <pre><code>\\langle f(q, m), f(k, n) \\rangle = g(q, k, m-n)\n</code></pre> \u5373\u6ce8\u610f\u529b\u5206\u6570\u53ea\u4f9d\u8d56\u76f8\u5bf9\u4f4d\u7f6e m-n\u3002</p> <p>\u89e3\u51b3\u65b9\u6848: \u590d\u6570\u57df\u7684\u65cb\u8f6c\u53d8\u6362</p> <p>\u6b65\u9aa41: \u5c06\u5b9e\u6570\u5411\u91cf\u6620\u5c04\u5230\u590d\u6570 <pre><code>q_{1} + i q_{2} \u2192 q_{complex}\n</code></pre></p> <p>\u6b65\u9aa42: \u5e94\u7528\u590d\u6570\u65cb\u8f6c <pre><code>f(q, m) = q_{complex} \\cdot e^{im\\theta} = q_{complex} \\cdot (\\cos(m\\theta) + i\\sin(m\\theta))\n</code></pre></p> <p>\u6b65\u9aa43: \u9a8c\u8bc1\u76f8\u5bf9\u4f4d\u7f6e\u6027\u8d28 <pre><code>\\langle f(q,m), f(k,n) \\rangle^* = \\langle q \\cdot e^{im\\theta}, k \\cdot e^{in\\theta} \\rangle\n= \\langle q, k \\rangle \\cdot e^{i(m-n)\\theta}\n</code></pre></p> <p>\u53ea\u4f9d\u8d56\u4e8e (m-n)\uff01</p> <p>\u6b65\u9aa44: \u8f6c\u6362\u56de\u5b9e\u6570\u57df <pre><code>\\begin{pmatrix}\nq_1 \\cos(m\\theta) - q_2 \\sin(m\\theta) \\\\\nq_1 \\sin(m\\theta) + q_2 \\cos(m\\theta)\n\\end{pmatrix}\n</code></pre></p> <p>\u5173\u952e\u6d1e\u5bdf: \u901a\u8fc7\u65cb\u8f6c\u53d8\u6362\uff0c\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u81ea\u7136\u5730\u7f16\u7801\u5728\u4e86\u5411\u91cf\u7684\u51e0\u4f55\u5173\u7cfb\u4e2d\u3002</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#q3-rope","title":"Q3: RoPE\u76f8\u6bd4\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801\u7684\u4f18\u52bf\uff1f","text":"<p>\u6838\u5fc3\u4f18\u52bf:</p> <ol> <li>\u81ea\u7136\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56</li> <li>\u6ce8\u610f\u529b\u5206\u6570\u76f4\u63a5\u4f9d\u8d56\u76f8\u5bf9\u8ddd\u79bb</li> <li> <p>\u65e0\u9700\u989d\u5916\u7684\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e\u9879</p> </li> <li> <p>\u4f18\u79c0\u7684\u5916\u63a8\u80fd\u529b</p> </li> <li>\u8bad\u7ec3\u65f6\u7684\u76f8\u5bf9\u4f4d\u7f6e\u6a21\u5f0f\u53ef\u4ee5\u6cdb\u5316\u5230\u66f4\u957f\u5e8f\u5217</li> <li> <p>\u7406\u8bba\u4e0a\u652f\u6301\u65e0\u9650\u957f\u5ea6\u5916\u63a8</p> </li> <li> <p>\u8ba1\u7b97\u9ad8\u6548</p> </li> <li>\u65e0\u9700\u5b58\u50a8\u4f4d\u7f6e\u5d4c\u5165\u8868</li> <li> <p>\u65cb\u8f6c\u64cd\u4f5c\u53ef\u4ee5\u9ad8\u6548\u5b9e\u73b0</p> </li> <li> <p>\u7406\u8bba\u4f18\u96c5</p> </li> <li>\u6570\u5b66\u57fa\u7840\u624e\u5b9e</li> <li>\u57fa\u4e8e\u590d\u6570\u65cb\u8f6c\u7684\u51e0\u4f55\u76f4\u89c9</li> </ol> <p>\u5b9e\u9a8c\u9a8c\u8bc1: - \u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801 - \u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u8868\u73b0\u7279\u522b\u7a81\u51fa - \u5df2\u88ab\u591a\u4e2a\u5927\u6a21\u578b\u91c7\u7528(LLaMA\u3001PaLM\u7b49)</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#q4-rope","title":"Q4: RoPE\u5728\u5b9e\u9645\u5b9e\u73b0\u4e2d\u6709\u4ec0\u4e48\u6280\u5de7\uff1f","text":"<p>\u5b9e\u73b0\u4f18\u5316:</p> <ol> <li> <p>\u9891\u7387\u9009\u62e9 <pre><code># \u4e0d\u540c\u7ef4\u5ea6\u4f7f\u7528\u4e0d\u540c\u9891\u7387\ntheta = 10000 ** (-2 * torch.arange(0, dim, 2) / dim)\n</code></pre></p> </li> <li> <p>\u9884\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635 <pre><code># \u907f\u514d\u91cd\u590d\u8ba1\u7b97sin/cos\ncos_cached = torch.cos(position * theta)\nsin_cached = torch.sin(position * theta)\n</code></pre></p> </li> <li> <p>\u5411\u91cf\u5316\u5b9e\u73b0 <pre><code># \u540c\u65f6\u5904\u7406\u6240\u6709\u4f4d\u7f6e\u548c\u7ef4\u5ea6\nq_rot = q * cos_cached - q_shifted * sin_cached\n</code></pre></p> </li> </ol>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_10","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#rope","title":"RoPE\u5b8c\u6574\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass RotaryPositionalEmbedding(nn.Module):\n    \"\"\"RoPE (Rotary Position Embedding) \u5b9e\u73b0\"\"\"\n\n    def __init__(self, dim, max_seq_len=2048, base=10000):\n        super().__init__()\n        self.dim = dim\n        self.max_seq_len = max_seq_len\n        self.base = base\n\n        # \u8ba1\u7b97\u65cb\u8f6c\u9891\u7387\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n        # \u9884\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801\n        self._build_cache(max_seq_len)\n\n    def _build_cache(self, seq_len):\n        \"\"\"\u9884\u8ba1\u7b97\u5e76\u7f13\u5b58\u65cb\u8f6c\u77e9\u9635\"\"\"\n        # \u751f\u6210\u4f4d\u7f6e\u5e8f\u5217\n        position = torch.arange(seq_len).float()\n\n        # \u8ba1\u7b97\u89d2\u5ea6: position * inv_freq\n        freqs = torch.outer(position, self.inv_freq)  # [seq_len, dim//2]\n\n        # \u62fc\u63a5\uff0c\u5f62\u6210\u5b8c\u6574\u7684\u9891\u7387\u77e9\u9635\n        emb = torch.cat([freqs, freqs], dim=-1)  # [seq_len, dim]\n\n        # \u8ba1\u7b97cos\u548csin\n        cos_cached = emb.cos()\n        sin_cached = emb.sin()\n\n        self.register_buffer('cos_cached', cos_cached)\n        self.register_buffer('sin_cached', sin_cached)\n\n    def rotate_half(self, x):\n        \"\"\"\u5c06\u8f93\u5165\u7684\u540e\u534a\u90e8\u5206\u53d6\u8d1f\u53f7\u5e76\u79fb\u5230\u524d\u9762\"\"\"\n        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n        return torch.cat([-x2, x1], dim=-1)\n\n    def forward(self, q, k, seq_len=None):\n        \"\"\"\n        \u5bf9\u67e5\u8be2\u548c\u952e\u5411\u91cf\u5e94\u7528RoPE\n\n        Args:\n            q: \u67e5\u8be2\u77e9\u9635 [batch, heads, seq_len, dim]\n            k: \u952e\u77e9\u9635 [batch, heads, seq_len, dim]\n            seq_len: \u5e8f\u5217\u957f\u5ea6\n        \"\"\"\n        if seq_len is None:\n            seq_len = q.shape[-2]\n\n        # \u5982\u679c\u5e8f\u5217\u957f\u5ea6\u8d85\u51fa\u7f13\u5b58\uff0c\u91cd\u65b0\u6784\u5efa\n        if seq_len &gt; self.max_seq_len:\n            self._build_cache(seq_len)\n\n        # \u83b7\u53d6\u5bf9\u5e94\u957f\u5ea6\u7684cos\u548csin\n        cos = self.cos_cached[:seq_len]  # [seq_len, dim]\n        sin = self.sin_cached[:seq_len]  # [seq_len, dim]\n\n        # \u5e94\u7528\u65cb\u8f6c\u53d8\u6362\n        q_rot = q * cos + self.rotate_half(q) * sin\n        k_rot = k * cos + self.rotate_half(k) * sin\n\n        return q_rot, k_rot\n\nclass MultiHeadAttentionWithRoPE(nn.Module):\n    \"\"\"\u5e26RoPE\u7684\u591a\u5934\u6ce8\u610f\u529b\"\"\"\n\n    def __init__(self, d_model, num_heads, max_seq_len=2048):\n        super().__init__()\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        # RoPE\u53ea\u5e94\u7528\u5230\u90e8\u5206\u7ef4\u5ea6\uff08\u901a\u5e38\u662f\u524d\u534a\u90e8\u5206\uff09\n        self.rope = RotaryPositionalEmbedding(\n            dim=self.head_dim, \n            max_seq_len=max_seq_len\n        )\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u8ba1\u7b97Q, K, V\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # \u8f6c\u7f6e\u4ee5\u7b26\u5408\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u7ef4\u5ea6\u8981\u6c42\n        Q = Q.transpose(1, 2)  # [batch, heads, seq_len, head_dim]\n        K = K.transpose(1, 2)\n        V = V.transpose(1, 2)\n\n        # \u5e94\u7528RoPE\n        Q, K = self.rope(Q, K, seq_len)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        # \u5e94\u7528\u63a9\u7801\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Softmax\u5f52\u4e00\u5316\n        attn_weights = torch.softmax(scores, dim=-1)\n\n        # \u8ba1\u7b97\u8f93\u51fa\n        out = torch.matmul(attn_weights, V)\n\n        # \u91cd\u5851\u5e76\u5408\u5e76\u591a\u5934\n        out = out.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, d_model\n        )\n\n        return self.W_o(out)\n\n# \u4e0d\u540c\u4f4d\u7f6e\u7f16\u7801\u7684\u5bf9\u6bd4\u6d4b\u8bd5\ndef compare_position_encodings():\n    \"\"\"\u5bf9\u6bd4\u4e0d\u540c\u4f4d\u7f6e\u7f16\u7801\u7684\u6548\u679c\"\"\"\n\n    d_model, seq_len = 512, 64\n    batch_size, num_heads = 2, 8\n\n    print(\"=== \u4f4d\u7f6e\u7f16\u7801\u5bf9\u6bd4\u6d4b\u8bd5 ===\")\n\n    # \u6d4b\u8bd5\u6570\u636e\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # 1. \u65e0\u4f4d\u7f6e\u7f16\u7801\u7684\u6ce8\u610f\u529b\n    attn_no_pos = MultiHeadAttentionWithRoPE(d_model, num_heads)\n    # \u4e34\u65f6\u79fb\u9664RoPE\n    attn_no_pos.rope = lambda q, k, seq_len: (q, k)\n    out_no_pos = attn_no_pos(x)\n\n    # 2. \u5e26RoPE\u7684\u6ce8\u610f\u529b\n    attn_with_rope = MultiHeadAttentionWithRoPE(d_model, num_heads)\n    out_with_rope = attn_with_rope(x)\n\n    print(f\"\u65e0\u4f4d\u7f6e\u7f16\u7801\u8f93\u51fa\u6807\u51c6\u5dee: {out_no_pos.std():.4f}\")\n    print(f\"RoPE\u4f4d\u7f6e\u7f16\u7801\u8f93\u51fa\u6807\u51c6\u5dee: {out_with_rope.std():.4f}\")\n\n    # 3. \u6d4b\u8bd5\u5916\u63a8\u80fd\u529b\n    print(\"\\n=== \u5916\u63a8\u80fd\u529b\u6d4b\u8bd5 ===\")\n\n    # \u77ed\u5e8f\u5217\u8bad\u7ec3\n    short_len = 32\n    x_short = torch.randn(1, short_len, d_model)\n\n    # \u957f\u5e8f\u5217\u63a8\u7406\n    long_len = 128\n    x_long = torch.randn(1, long_len, d_model)\n\n    try:\n        out_short = attn_with_rope(x_short)\n        out_long = attn_with_rope(x_long)\n        print(f\"\u77ed\u5e8f\u5217({short_len})\u5904\u7406\u6210\u529f\")\n        print(f\"\u957f\u5e8f\u5217({long_len})\u5904\u7406\u6210\u529f - RoPE\u652f\u6301\u5916\u63a8\")\n    except Exception as e:\n        print(f\"\u5916\u63a8\u5931\u8d25: {e}\")\n\n# \u624b\u52a8\u9a8c\u8bc1RoPE\u7684\u76f8\u5bf9\u4f4d\u7f6e\u6027\u8d28\ndef verify_rope_property():\n    \"\"\"\u9a8c\u8bc1RoPE\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56\u6027\u8d28\"\"\"\n\n    print(\"=== \u9a8c\u8bc1RoPE\u76f8\u5bf9\u4f4d\u7f6e\u6027\u8d28 ===\")\n\n    dim = 64\n    rope = RotaryPositionalEmbedding(dim, max_seq_len=10)\n\n    # \u521b\u5efa\u4e24\u4e2a\u4f4d\u7f6e\u7684\u67e5\u8be2\u548c\u952e\n    q = torch.randn(1, 1, 1, dim)  # \u4f4d\u7f6e0\u7684\u67e5\u8be2\n    k = torch.randn(1, 1, 1, dim)  # \u4f4d\u7f6e0\u7684\u952e\n\n    # \u5728\u4e0d\u540c\u76f8\u5bf9\u8ddd\u79bb\u4e0b\u6d4b\u8bd5\n    distances = [1, 2, 3]\n\n    for dist in distances:\n        # \u8ba1\u7b97\u4f4d\u7f6e(0, dist)\u7684\u76f8\u5bf9\u6ce8\u610f\u529b\n        q_pos0, k_pos_dist = rope(q, k, seq_len=dist+1)\n        score1 = torch.matmul(q_pos0[:,:,0:1], k_pos_dist[:,:,dist:dist+1].transpose(-2,-1))\n\n        # \u8ba1\u7b97\u4f4d\u7f6e(1, 1+dist)\u7684\u76f8\u5bf9\u6ce8\u610f\u529b  \n        q_pos1, k_pos1_dist = rope(q, k, seq_len=dist+2)\n        score2 = torch.matmul(q_pos1[:,:,1:2], k_pos1_dist[:,:,1+dist:2+dist].transpose(-2,-1))\n\n        print(f\"\u76f8\u5bf9\u8ddd\u79bb{dist}: \u5206\u6570\u5dee\u5f02 = {abs(score1.item() - score2.item()):.6f}\")\n\nif __name__ == \"__main__\":\n    compare_position_encodings()\n    print()\n    verify_rope_property()\n</code></pre>"},{"location":"fundamentals/attention-advanced/positional-encoding/#sinusoidal","title":"Sinusoidal\u4f4d\u7f6e\u7f16\u7801\u5b9e\u73b0","text":"<pre><code>class SinusoidalPositionalEncoding(nn.Module):\n    \"\"\"\u539f\u59cbTransformer\u7684\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\"\"\"\n\n    def __init__(self, d_model, max_seq_len=5000):\n        super().__init__()\n\n        pe = torch.zeros(max_seq_len, d_model)\n        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        return x + self.pe[:, :seq_len]\n</code></pre>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_11","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u548c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u7684\u533a\u522b</li> <li>[ ] \u80fd\u63a8\u5bfcRoPE\u7684\u6570\u5b66\u539f\u7406</li> <li>[ ] \u638c\u63e1RoPE\u7684\u5b9e\u73b0\u7ec6\u8282\u548c\u4f18\u5316\u6280\u5de7</li> <li>[ ] \u5b8c\u6210\u4f4d\u7f6e\u7f16\u7801\u7684\u4ee3\u7801\u5b9e\u73b0\u548c\u6548\u679c\u9a8c\u8bc1</li> </ul>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_12","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u5f52\u4e00\u5316\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1aLLM\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/llm-advanced/","title":"\u7b2c3\u8282\uff1aLLM\u5347\u7ea7\u6280\u672f","text":""},{"location":"fundamentals/llm-advanced/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u4e86\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u524d\u6cbf\u4f18\u5316\u6280\u672f\uff0c\u638c\u63e1MOE\u67b6\u6784\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u57fa\u672c\u6982\u5ff5\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - MOE\u662f\u4ec0\u4e48\uff0c\u6709\u4ec0\u4e48\u597d\u5904\uff1f - \u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u57fa\u672c\u7b56\u7565 - \u5927\u6a21\u578b\u8bad\u7ec3\u7684\u5de5\u7a0b\u6311\u6218</p>"},{"location":"fundamentals/llm-advanced/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a2\u5929</p> <ul> <li>Day 1: MOE\u67b6\u6784\u539f\u7406</li> <li>Day 2: \u5206\u5e03\u5f0f\u8bad\u7ec3\u6982\u5ff5</li> </ul>"},{"location":"fundamentals/llm-advanced/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/llm-advanced/#1-moe","title":"1. MOE\u67b6\u6784","text":"<ul> <li>\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u539f\u7406</li> <li>\u7a00\u758f\u6fc0\u6d3b\u7684\u4f18\u52bf</li> <li>\u5de5\u7a0b\u5b9e\u73b0\u6311\u6218</li> </ul>"},{"location":"fundamentals/llm-advanced/#2","title":"2. \u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<ul> <li>\u6570\u636e\u5e76\u884c vs \u6a21\u578b\u5e76\u884c</li> <li>\u6d41\u6c34\u7ebf\u5e76\u884c</li> <li>\u901a\u4fe1\u4f18\u5316\u7b56\u7565</li> </ul>"},{"location":"fundamentals/llm-advanced/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u4e24\u9879\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u95ee\u9898\u89e3\u7b54: \u80fd\u89e3\u91caMOE\u7684\u5de5\u4f5c\u539f\u7406\u548c\u4f18\u52bf</li> <li>\u6982\u5ff5\u7406\u89e3: \u7406\u89e3\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u5206\u5e03\u5f0f\u7b56\u7565</li> </ol>"},{"location":"fundamentals/llm-advanced/#_5","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u8fd9\u4e00\u8282\u662f\u9009\u4fee\u5185\u5bb9\uff0c\u91cd\u70b9\u4e86\u89e3\u6982\u5ff5\u5373\u53ef\u3002</p>"},{"location":"fundamentals/llm-advanced/distributed/","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3","text":""},{"location":"fundamentals/llm-advanced/distributed/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u4e86\u89e3\u5927\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u57fa\u672c\u7b56\u7565\u548c\u6982\u5ff5\u3002</p>"},{"location":"fundamentals/llm-advanced/distributed/#_3","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/llm-advanced/distributed/#_4","title":"\u4e3b\u8981\u5e76\u884c\u7b56\u7565","text":"<ol> <li>\u6570\u636e\u5e76\u884c: \u4e0d\u540c\u8bbe\u5907\u5904\u7406\u4e0d\u540c\u7684\u6570\u636e\u6279\u6b21</li> <li>\u6a21\u578b\u5e76\u884c: \u5c06\u6a21\u578b\u53c2\u6570\u5206\u5e03\u5230\u4e0d\u540c\u8bbe\u5907</li> <li>\u6d41\u6c34\u7ebf\u5e76\u884c: \u5c06\u6a21\u578b\u5c42\u5206\u5e03\u5230\u4e0d\u540c\u8bbe\u5907\uff0c\u5f62\u6210\u6d41\u6c34\u7ebf</li> </ol>"},{"location":"fundamentals/llm-advanced/distributed/#_5","title":"\u5de5\u7a0b\u6311\u6218","text":"<ul> <li>\u901a\u4fe1\u5f00\u9500\u4f18\u5316</li> <li>\u5185\u5b58\u7ba1\u7406</li> <li>\u8d1f\u8f7d\u5747\u8861</li> </ul>"},{"location":"fundamentals/llm-advanced/distributed/#_6","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/llm-advanced/distributed/#q1","title":"Q1: \u5206\u5e03\u5f0f\u8bad\u7ec3\u6709\u54ea\u4e9b\u4e3b\u8981\u7b56\u7565\uff1f","text":"<p>\u6838\u5fc3\u7b56\u7565: - \u6570\u636e\u5e76\u884c: \u7b80\u5355\u4f46\u901a\u4fe1\u5f00\u9500\u5927 - \u6a21\u578b\u5e76\u884c: \u9002\u5408\u8d85\u5927\u6a21\u578b - \u6df7\u5408\u5e76\u884c: \u7ed3\u5408\u591a\u79cd\u7b56\u7565\u7684\u73b0\u4ee3\u65b9\u6848</p>"},{"location":"fundamentals/llm-advanced/distributed/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u4e86\u89e3\u57fa\u672c\u7684\u5e76\u884c\u8bad\u7ec3\u6982\u5ff5</li> <li>[ ] \u7406\u89e3\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u4e3b\u8981\u6311\u6218</li> </ul>"},{"location":"fundamentals/llm-advanced/distributed/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aMOE\u67b6\u6784</li> <li>\u4e0b\u4e00\u8282\uff1aContext Engineering</li> <li>\u8fd4\u56de\uff1aLLM\u5347\u7ea7\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/llm-advanced/moe/","title":"MOE\u67b6\u6784","text":""},{"location":"fundamentals/llm-advanced/moe/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4e13\u5bb6\u6df7\u5408\u6a21\u578b(Mixture of Experts)\u7684\u6838\u5fc3\u539f\u7406\u548c\u5e94\u7528\u4f18\u52bf\u3002</p>"},{"location":"fundamentals/llm-advanced/moe/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/llm-advanced/moe/#moe_1","title":"MOE\u662f\u4ec0\u4e48\uff1f","text":"<p>Mixture of Experts (MOE) \u662f\u4e00\u79cd\u7a00\u758f\u6fc0\u6d3b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u8def\u7531\u673a\u5236\u5c06\u4e0d\u540c\u7684\u8f93\u5165\u5206\u914d\u7ed9\u4e0d\u540c\u7684\"\u4e13\u5bb6\"\u5b50\u7f51\u7edc\u5904\u7406\u3002</p>"},{"location":"fundamentals/llm-advanced/moe/#_3","title":"\u6838\u5fc3\u4f18\u52bf","text":"<ol> <li>\u53c2\u6570\u6548\u7387: \u867d\u7136\u603b\u53c2\u6570\u91cf\u5927\uff0c\u4f46\u6bcf\u6b21\u524d\u5411\u4f20\u64ad\u53ea\u6fc0\u6d3b\u90e8\u5206\u53c2\u6570</li> <li>\u6269\u5c55\u80fd\u529b: \u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u4e13\u5bb6\u6570\u91cf\u6765\u6269\u5c55\u6a21\u578b\u5bb9\u91cf</li> <li>\u4e13\u4e1a\u5206\u5de5: \u4e0d\u540c\u4e13\u5bb6\u53ef\u4ee5\u5b66\u4e60\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u8f93\u5165</li> </ol>"},{"location":"fundamentals/llm-advanced/moe/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/llm-advanced/moe/#q1-moe","title":"Q1: MOE\u662f\u4ec0\u4e48\uff0c\u5b83\u6709\u4ec0\u4e48\u597d\u5904\u5462\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54: MOE\u662f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u673a\u5236\u8ba9\u4e0d\u540c\u7684\u5b50\u7f51\u7edc(\u4e13\u5bb6)\u5904\u7406\u4e0d\u540c\u7684\u8f93\u5165\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u91cf\u76f8\u5bf9\u7a33\u5b9a\u7684\u60c5\u51b5\u4e0b\u5927\u5e45\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u3002</p> <p>\u8be6\u7ec6\u89e3\u91ca: - \u5de5\u4f5c\u539f\u7406: \u8f93\u5165\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u9009\u62e9\u6fc0\u6d3b\u5c11\u6570\u51e0\u4e2a\u4e13\u5bb6 - \u6838\u5fc3\u4f18\u52bf: \u53c2\u6570\u91cf\u5927\u4f46\u8ba1\u7b97\u91cf\u53ef\u63a7 - \u5b9e\u9645\u5e94\u7528: Google\u7684Switch Transformer\u7b49\u5927\u6a21\u578b</p>"},{"location":"fundamentals/llm-advanced/moe/#_5","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3MOE\u7684\u7a00\u758f\u6fc0\u6d3b\u539f\u7406</li> <li>[ ] \u80fd\u89e3\u91caMOE\u76f8\u6bd4\u7a20\u5bc6\u6a21\u578b\u7684\u4f18\u52bf</li> </ul>"},{"location":"fundamentals/llm-advanced/moe/#_6","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1a\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u8fd4\u56de\uff1aLLM\u5347\u7ea7\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/","title":"\u7b2c1\u8282\uff1aTransformer\u57fa\u7840","text":""},{"location":"fundamentals/transformer/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1Transformer\u67b6\u6784\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u7406\u89e3Attention\u673a\u5236\u539f\u7406\uff0c\u533a\u5206\u4e0d\u540c\u7684\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u4e3a\u9762\u8bd5\u505a\u597d\u57fa\u7840\u51c6\u5907\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - Attention\u8ba1\u7b97\u516c\u5f0f\u548c\u539f\u7406 - Encoder-Only vs Decoder-Only\u67b6\u6784\u5dee\u5f02 - \u4e3a\u4ec0\u4e48\u4e3b\u6d41\u5927\u6a21\u578b\u9009\u62e9Decoder-Only - GPT\u548cBERT\u7684\u67b6\u6784\u533a\u522b</p>"},{"location":"fundamentals/transformer/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a2\u5929</p> <ul> <li>Day 1: Attention\u673a\u5236\u6df1\u5165\u7406\u89e3</li> <li>Day 2: \u8bed\u8a00\u6a21\u578b\u67b6\u6784\u5bf9\u6bd4\u5206\u6790</li> </ul>"},{"location":"fundamentals/transformer/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/transformer/#1-attention","title":"1. Attention\u673a\u5236","text":"<ul> <li>\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u516c\u5f0f\u63a8\u5bfc</li> <li>Softmax\u548c\u7f29\u653e\u56e0\u5b50\u4f5c\u7528</li> <li>\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236</li> <li>\u4ee3\u7801\u5b9e\u73b0\u7ec3\u4e60</li> </ul>"},{"location":"fundamentals/transformer/#2","title":"2. \u8bed\u8a00\u6a21\u578b\u67b6\u6784","text":"<ul> <li>Encoder-Only vs Decoder-Only</li> <li>GPT vs BERT\u67b6\u6784\u5bf9\u6bd4</li> <li>\u4e3b\u6d41\u6a21\u578b\u9009\u62e9\u5206\u6790</li> <li>\u6a21\u578b\u67b6\u6784\u56fe\u89e3</li> </ul>"},{"location":"fundamentals/transformer/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u4e24\u9879\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u95ee\u9898\u89e3\u7b54: \u80fd\u7528\u81ea\u5df1\u7684\u8bdd\u56de\u7b54\u6240\u6709\u9762\u8bd5\u95ee\u9898</li> <li>\u4ee3\u7801\u5b9e\u73b0: \u5b8c\u6210Self-Attention\u548cMulti-Head Attention\u7684\u7f16\u7a0b\u7ec3\u4e60</li> </ol>"},{"location":"fundamentals/transformer/#_5","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u4e00\u4e2a\u5b50\u6a21\u5757\u5f00\u59cb\u4f60\u7684\u5b66\u4e60\u4e4b\u65c5\uff01\u5efa\u8bae\u6309\u987a\u5e8f\u5b66\u4e60\uff0c\u6bcf\u4e2a\u6a21\u5757\u90fd\u5305\u542b\u7cbe\u9009\u7684\u9605\u8bfb\u6750\u6599\u3001\u89c6\u9891\u8d44\u6e90\u548c\u5b9e\u6218\u7ec3\u4e60\u3002</p>"},{"location":"fundamentals/transformer/attention/","title":"Attention\u673a\u5236","text":""},{"location":"fundamentals/transformer/attention/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3Self-Attention\u673a\u5236\u7684\u6570\u5b66\u539f\u7406\u548c\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u638c\u63e1\u9762\u8bd5\u4e2d\u7684\u9ad8\u9891\u95ee\u9898\u3002</p>"},{"location":"fundamentals/transformer/attention/#_2","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/transformer/attention/#_3","title":"\u5fc5\u8bfb\u6587\u7ae0","text":"<ol> <li>\u300aAttention is All You Need\u300b\u6d45\u8bfb\uff08\u7b80\u4ecb+\u4ee3\u7801\uff09 - \u79d1\u5b66\u7a7a\u95f4</li> <li>GPT\u4e0eBERT\u5dee\u522b\u6df1\u5165\u89e3\u6790 - \u77e5\u4e4e</li> <li>\u6df1\u5165\u6d45\u51fa\u7406\u89e3transformer - \u77e5\u4e4e  </li> <li>Transformer\u6a21\u578b\u8be6\u89e3\uff08\u56fe\u89e3\u6700\u5b8c\u6574\u7248\uff09 - \u77e5\u4e4e</li> <li>Transformer\u4f4d\u7f6e\u7f16\u7801\uff08\u57fa\u7840\uff09 - \u77e5\u4e4e</li> </ol>"},{"location":"fundamentals/transformer/attention/#_4","title":"\u539f\u8bba\u6587","text":"<ul> <li>Attention Is All You Need - \u7ecf\u5178\u5fc5\u8bfb</li> </ul>"},{"location":"fundamentals/transformer/attention/#_5","title":"\ud83c\udfac \u89c6\u9891\u6750\u6599","text":"<p>\u5b66\u4e60\u5efa\u8bae\uff1a \u500d\u901f\u89c2\u770b\uff0c\u91cd\u70b9\u7406\u89e3\u6982\u5ff5\u800c\u975e\u7ec6\u8282</p> <ol> <li>GPT\uff0cGPT-2\uff0cGPT-3 \u8bba\u6587\u7cbe\u8bfb - \u54d4\u54e9\u54d4\u54e9</li> <li>Llama 3.1\u8bba\u6587\u7cbe\u8bfb - \u54d4\u54e9\u54d4\u54e9</li> </ol>"},{"location":"fundamentals/transformer/attention/#_6","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/attention/#self-attention","title":"Self-Attention\u8ba1\u7b97\u516c\u5f0f","text":"<pre><code>Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n</code></pre> <p>\u6838\u5fc3\u7ec4\u4ef6: - Query (Q): \u67e5\u8be2\u5411\u91cf\uff0c\u51b3\u5b9a\u5f53\u524d\u4f4d\u7f6e\u5173\u6ce8\u4ec0\u4e48 - Key (K): \u952e\u5411\u91cf\uff0c\u88ab\u67e5\u8be2\u7684\u5185\u5bb9 - Value (V): \u503c\u5411\u91cf\uff0c\u5b9e\u9645\u4f20\u9012\u7684\u4fe1\u606f - \u7f29\u653e\u56e0\u5b50: $\\sqrt{d_k}$\uff0c\u9632\u6b62softmax\u68af\u5ea6\u6d88\u5931</p>"},{"location":"fundamentals/transformer/attention/#multi-head-attention","title":"Multi-Head Attention","text":"<p>\u5c06\u8f93\u5165\u6295\u5f71\u5230\u591a\u4e2a\u4e0d\u540c\u7684\u5b50\u7a7a\u95f4\uff0c\u5e76\u884c\u8ba1\u7b97\u591a\u4e2a\u6ce8\u610f\u529b\u5934\uff1a</p> <pre><code>MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\n</code></pre> <p>\u5176\u4e2d $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$</p>"},{"location":"fundamentals/transformer/attention/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/attention/#q1-attention","title":"Q1: Attention\u8ba1\u7b97\u516c\u5f0f\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6807\u51c6\u56de\u7b54\uff1a Self-Attention\u7684\u6838\u5fc3\u516c\u5f0f\u662f <code>Attention(Q,K,V) = softmax(QK^T/\u221ad_k)V</code>\u3002</p> <p>\u8be6\u7ec6\u89e3\u91ca\uff1a 1. \u5148\u8ba1\u7b97Query\u548cKey\u7684\u70b9\u79ef\u5f97\u5230\u6ce8\u610f\u529b\u5206\u6570 2. \u9664\u4ee5\u221ad_k\u8fdb\u884c\u7f29\u653e 3. \u901a\u8fc7softmax\u5f52\u4e00\u5316\u5f97\u5230\u6ce8\u610f\u529b\u6743\u91cd 4. \u6700\u540e\u4e0eValue\u76f8\u4e58\u5f97\u5230\u8f93\u51fa</p>"},{"location":"fundamentals/transformer/attention/#q11-d_k","title":"Q1.1: \u4e3a\u4ec0\u4e48\u8981\u9664\u4ee5\u6839\u53f7d_k\uff1f","text":"<p>\u6838\u5fc3\u539f\u56e0\uff1a \u9632\u6b62softmax\u51fd\u6570\u8fdb\u5165\u9971\u548c\u533a\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\u3002</p> <p>\u6280\u672f\u89e3\u91ca\uff1a - \u5f53d_k\u8f83\u5927\u65f6\uff0cQK^T\u7684\u65b9\u5dee\u4f1a\u5f88\u5927 - \u5927\u7684\u6570\u503c\u7ecf\u8fc7softmax\u540e\u68af\u5ea6\u63a5\u8fd10 - \u9664\u4ee5\u221ad_k\u53ef\u4ee5\u63a7\u5236\u65b9\u5dee\u4e3a1\uff0c\u4fdd\u6301\u68af\u5ea6\u7a33\u5b9a</p>"},{"location":"fundamentals/transformer/attention/#q12-softmax","title":"Q1.2: Softmax\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f","text":"<p>\u4e3b\u8981\u4f5c\u7528\uff1a 1. \u5f52\u4e00\u5316: \u786e\u4fdd\u6ce8\u610f\u529b\u6743\u91cd\u4e4b\u548c\u4e3a1 2. \u7a81\u51fa\u91cd\u70b9: \u901a\u8fc7\u6307\u6570\u51fd\u6570\u653e\u5927\u91cd\u8981\u7279\u5f81 3. \u53ef\u5fae\u5206: \u4fdd\u8bc1\u53cd\u5411\u4f20\u64ad\u53ef\u4ee5\u6b63\u5e38\u8fdb\u884c</p>"},{"location":"fundamentals/transformer/attention/#_8","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/attention/#1-self-attention","title":"\u7ec3\u4e601: \u5b9e\u73b0Self-Attention\u673a\u5236","text":"<p>\u5e73\u53f0: Deep-ML Self-Attention</p>"},{"location":"fundamentals/transformer/attention/#2-multi-head-attention","title":"\u7ec3\u4e602: \u5b9e\u73b0Multi-Head Attention","text":"<p>\u5e73\u53f0: Deep-ML Multi-Head Attention</p>"},{"location":"fundamentals/transformer/attention/#_9","title":"\u4ee3\u7801\u6a21\u677f","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass SelfAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # TODO: \u5b9e\u73b0\u6ce8\u610f\u529b\u8ba1\u7b97\n        pass\n\n    def forward(self, x, mask=None):\n        # TODO: \u5b9e\u73b0\u5b8c\u6574\u7684\u524d\u5411\u4f20\u64ad\n        pass\n</code></pre>"},{"location":"fundamentals/transformer/attention/#_10","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u68c0\u9a8c\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ul> <li>[ ] \u80fd\u753b\u51faSelf-Attention\u7684\u8ba1\u7b97\u6d41\u7a0b\u56fe</li> <li>[ ] \u53ef\u4ee5\u624b\u7b97\u7b80\u5355\u7684Attention\u6743\u91cd</li> <li>[ ] \u5b8c\u6210Deep-ML\u5e73\u53f0\u7684\u4e24\u4e2a\u7f16\u7a0b\u7ec3\u4e60</li> <li>[ ] \u9762\u8bd5\u95ee\u9898\u80fd\u7528\u81ea\u5df1\u7684\u8bdd\u6d41\u5229\u56de\u7b54</li> </ul>"},{"location":"fundamentals/transformer/attention/#_11","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1a\u8bed\u8a00\u6a21\u578b\u67b6\u6784</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/language-models/","title":"\u8bed\u8a00\u6a21\u578b\u67b6\u6784","text":""},{"location":"fundamentals/transformer/language-models/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4e0d\u540cTransformer\u67b6\u6784\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u638c\u63e1GPT\u4e0eBERT\u7684\u6838\u5fc3\u5dee\u5f02\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5927\u6a21\u578b\u504f\u7231Decoder-Only\u67b6\u6784\u3002</p>"},{"location":"fundamentals/transformer/language-models/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/transformer/language-models/#_4","title":"\u6838\u5fc3\u6982\u5ff5\u6587\u7ae0","text":"<p>\u53c2\u8003Attention\u673a\u5236\u9875\u9762\u7684\u9605\u8bfb\u6750\u6599\uff0c\u91cd\u70b9\u5173\u6ce8\uff1a - Encoder-Only vs Decoder-Only\u67b6\u6784\u5bf9\u6bd4 - GPT\u7cfb\u5217\u6a21\u578b\u7684\u6f14\u8fdb - BERT\u7684\u53cc\u5411\u7f16\u7801\u7279\u6027</p>"},{"location":"fundamentals/transformer/language-models/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/language-models/#_6","title":"\u4e09\u5927\u67b6\u6784\u7c7b\u578b","text":"\u67b6\u6784\u7c7b\u578b \u4ee3\u8868\u6a21\u578b \u7279\u70b9 \u5e94\u7528\u573a\u666f Encoder-Only BERT, RoBERTa \u53cc\u5411\u6ce8\u610f\u529b\uff0c\u9002\u5408\u7406\u89e3\u4efb\u52a1 \u6587\u672c\u5206\u7c7b\u3001\u9605\u8bfb\u7406\u89e3\u3001\u60c5\u611f\u5206\u6790 Decoder-Only GPT, LLaMA, ChatGPT \u56e0\u679c\u6ce8\u610f\u529b\uff0c\u9002\u5408\u751f\u6210\u4efb\u52a1 \u6587\u672c\u751f\u6210\u3001\u5bf9\u8bdd\u3001\u4ee3\u7801\u751f\u6210 Encoder-Decoder T5, BART \u7f16\u7801-\u89e3\u7801\u7ed3\u6784 \u7ffb\u8bd1\u3001\u6458\u8981\u3001\u95ee\u7b54"},{"location":"fundamentals/transformer/language-models/#attention-mask","title":"Attention Mask\u5bf9\u6bd4","text":"<p>BERT (\u53cc\u5411\u6ce8\u610f\u529b): <pre><code>Token:  [CLS] I    love  AI   [SEP]\nMask:   \u2713     \u2713    \u2713     \u2713    \u2713\n\u6bcf\u4e2atoken\u90fd\u53ef\u4ee5\u770b\u5230\u6240\u6709\u5176\u4ed6token\n</code></pre></p> <p>GPT (\u56e0\u679c\u6ce8\u610f\u529b): <pre><code>Token:  I    love  AI    very  much\nMask:   \u2713    \n        \u2713    \u2713     \n        \u2713    \u2713     \u2713\n        \u2713    \u2713     \u2713     \u2713\n        \u2713    \u2713     \u2713     \u2713     \u2713\n\u6bcf\u4e2atoken\u53ea\u80fd\u770b\u5230\u5b83\u4e4b\u524d\u7684token\uff08\u5305\u62ec\u81ea\u5df1\uff09\n</code></pre></p>"},{"location":"fundamentals/transformer/language-models/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/language-models/#q1-encoder-onlydecoder-onlygptbert","title":"Q1: Encoder-Only\u548cDecoder-Only\u67b6\u6784\u662f\u4ec0\u4e48\uff1fGPT\u548cBERT\u5206\u522b\u662f\u4ec0\u4e48\u67b6\u6784\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54\uff1a - Encoder-Only: \u4f7f\u7528\u53cc\u5411\u6ce8\u610f\u529b\uff0c\u53ef\u4ee5\u540c\u65f6\u770b\u5230\u524d\u540e\u6587\u672c\uff0cBERT\u5c31\u662f\u8fd9\u79cd\u67b6\u6784 - Decoder-Only: \u4f7f\u7528\u56e0\u679c\u6ce8\u610f\u529b\uff0c\u53ea\u80fd\u770b\u5230\u5f53\u524d\u4f4d\u7f6e\u4e4b\u524d\u7684\u6587\u672c\uff0cGPT\u7cfb\u5217\u90fd\u662f\u8fd9\u79cd\u67b6\u6784</p>"},{"location":"fundamentals/transformer/language-models/#q11","title":"Q1.1: \u67b6\u6784\u533a\u522b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u533a\u522b\uff1a</p> <ol> <li>\u6ce8\u610f\u529b\u673a\u5236</li> <li>Encoder-Only: \u53cc\u5411\u6ce8\u610f\u529b\uff0c\u65e0\u63a9\u7801\u9650\u5236</li> <li> <p>Decoder-Only: \u56e0\u679c\u6ce8\u610f\u529b\uff0c\u4f7f\u7528\u4e0b\u4e09\u89d2\u63a9\u7801</p> </li> <li> <p>\u8bad\u7ec3\u76ee\u6807</p> </li> <li>BERT: \u63a9\u7801\u8bed\u8a00\u6a21\u578b(MLM) + \u4e0b\u4e00\u53e5\u9884\u6d4b(NSP)</li> <li> <p>GPT: \u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\uff0c\u9884\u6d4b\u4e0b\u4e00\u4e2atoken</p> </li> <li> <p>\u4f4d\u7f6e\u7f16\u7801</p> </li> <li>BERT: \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801</li> <li>GPT: \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff08\u65e9\u671f\uff09\u2192 \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff08\u73b0\u4ee3\uff09</li> </ol>"},{"location":"fundamentals/transformer/language-models/#q12","title":"Q1.2: \u7528\u9014\u5dee\u5f02\uff1f","text":"<p>BERT\u64c5\u957f\u7406\u89e3\u4efb\u52a1\uff1a - \u6587\u672c\u5206\u7c7b - \u60c5\u611f\u5206\u6790 - \u9605\u8bfb\u7406\u89e3 - \u5b9e\u4f53\u8bc6\u522b</p> <p>GPT\u64c5\u957f\u751f\u6210\u4efb\u52a1\uff1a - \u6587\u672c\u7eed\u5199 - \u5bf9\u8bdd\u751f\u6210 - \u4ee3\u7801\u751f\u6210 - \u521b\u610f\u5199\u4f5c</p>"},{"location":"fundamentals/transformer/language-models/#q13","title":"Q1.3: \u4f18\u52a3\u6bd4\u8f83","text":"\u65b9\u9762 BERT\u4f18\u52bf GPT\u4f18\u52bf \u7406\u89e3\u80fd\u529b \u53cc\u5411\u4e0a\u4e0b\u6587\uff0c\u7406\u89e3\u66f4\u6df1\u5165 \u9002\u5408\u5e8f\u5217\u751f\u6210\uff0c\u903b\u8f91\u8fde\u8d2f \u8bad\u7ec3\u6548\u7387 \u5e76\u884c\u8bad\u7ec3\u6240\u6709\u4f4d\u7f6e \u81ea\u56de\u5f52\u8bad\u7ec3\uff0c\u7b80\u5355\u7a33\u5b9a \u63a8\u7406\u901f\u5ea6 \u5e76\u884c\u63a8\u7406 \u9700\u8981\u9010\u6b65\u751f\u6210 \u5e94\u7528\u7075\u6d3b\u6027 \u9700\u8981\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03 Zero-shot\u80fd\u529b\u5f3a"},{"location":"fundamentals/transformer/language-models/#q2-decoder-only","title":"Q2: \u4e3a\u4ec0\u4e48\u4e3b\u6d41\u5927\u6a21\u578b\u90fd\u7528Decoder-Only\u67b6\u6784\uff1f","text":"<p>\u4e3b\u8981\u539f\u56e0\uff1a</p> <ol> <li>\u7edf\u4e00\u7684\u751f\u6210\u8303\u5f0f</li> <li>\u6240\u6709\u4efb\u52a1\u90fd\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6587\u672c\u751f\u6210</li> <li> <p>\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u7279\u6b8a\u67b6\u6784</p> </li> <li> <p>\u66f4\u5f3a\u7684\u6d8c\u73b0\u80fd\u529b</p> </li> <li>\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u540e\u5c55\u73b0\u51fa\u5f3a\u5927\u7684few-shot\u5b66\u4e60\u80fd\u529b</li> <li> <p>\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u5929\u7136\u9002\u5408\u5bf9\u8bdd\u573a\u666f</p> </li> <li> <p>\u6269\u5c55\u6027\u66f4\u597d</p> </li> <li>\u67b6\u6784\u7b80\u5355\uff0c\u6613\u4e8e\u6269\u5927\u6a21\u578b\u89c4\u6a21</li> <li> <p>\u8bad\u7ec3\u7a33\u5b9a\u6027\u66f4\u597d</p> </li> <li> <p>\u5e94\u7528\u573a\u666f\u66f4\u5e7f</p> </li> <li>\u4ece\u5bf9\u8bdd\u5230\u4ee3\u7801\u751f\u6210\uff0c\u4e00\u4e2a\u6a21\u578b\u641e\u5b9a</li> <li>\u5546\u4e1a\u4ef7\u503c\u66f4\u9ad8</li> </ol>"},{"location":"fundamentals/transformer/language-models/#q3-qwenllama","title":"Q3: \u7b80\u5355\u8bb2\u89e3Qwen/LLaMA\u6a21\u578b\u67b6\u6784","text":"<p>LLaMA\u67b6\u6784\u7279\u70b9\uff1a</p> <pre><code>\u8f93\u5165 \u2192 Token Embedding + \u4f4d\u7f6e\u7f16\u7801\n     \u2193\n   N \u00d7 Decoder Layer:\n     \u251c\u2500\u2500 RMSNorm\n     \u251c\u2500\u2500 Multi-Head Attention (RoPE\u4f4d\u7f6e\u7f16\u7801)\n     \u251c\u2500\u2500 \u6b8b\u5dee\u8fde\u63a5\n     \u251c\u2500\u2500 RMSNorm  \n     \u251c\u2500\u2500 SwiGLU FFN\n     \u2514\u2500\u2500 \u6b8b\u5dee\u8fde\u63a5\n     \u2193\n   RMSNorm \u2192 \u8f93\u51fa\u5c42 \u2192 \u9884\u6d4b\u4e0b\u4e00\u4e2atoken\n</code></pre> <p>\u5173\u952e\u6280\u672f\u6539\u8fdb\uff1a - RMSNorm: \u66ff\u4ee3LayerNorm\uff0c\u8ba1\u7b97\u66f4\u9ad8\u6548 - RoPE\u4f4d\u7f6e\u7f16\u7801: \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u652f\u6301\u66f4\u957f\u5e8f\u5217 - SwiGLU\u6fc0\u6d3b: \u66ff\u4ee3ReLU\uff0c\u6548\u679c\u66f4\u597d - Pre-Norm: \u5f52\u4e00\u5316\u524d\u7f6e\uff0c\u8bad\u7ec3\u66f4\u7a33\u5b9a</p>"},{"location":"fundamentals/transformer/language-models/#_8","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/language-models/#_9","title":"\u67b6\u6784\u5bf9\u6bd4\u793a\u4f8b","text":"<pre><code># BERT\u98ce\u683c\u7684\u53cc\u5411\u6ce8\u610f\u529b\nclass BidirectionalAttention(nn.Module):\n    def forward(self, x):\n        # \u53ef\u4ee5\u770b\u5230\u5168\u90e8\u5e8f\u5217\n        attn_mask = torch.ones(seq_len, seq_len)  # \u51681\u77e9\u9635\n        return self.attention(x, mask=attn_mask)\n\n# GPT\u98ce\u683c\u7684\u56e0\u679c\u6ce8\u610f\u529b  \nclass CausalAttention(nn.Module):\n    def forward(self, x):\n        # \u53ea\u80fd\u770b\u5230\u5f53\u524d\u4f4d\u7f6e\u4e4b\u524d\n        seq_len = x.size(1)\n        attn_mask = torch.tril(torch.ones(seq_len, seq_len))  # \u4e0b\u4e09\u89d2\u77e9\u9635\n        return self.attention(x, mask=attn_mask)\n</code></pre>"},{"location":"fundamentals/transformer/language-models/#_10","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u80fd\u753b\u51faBERT\u548cGPT\u7684\u67b6\u6784\u5bf9\u6bd4\u56fe</li> <li>[ ] \u7406\u89e3\u4e3a\u4ec0\u4e48GPT\u9700\u8981\u56e0\u679c\u63a9\u7801</li> <li>[ ] \u80fd\u89e3\u91ca\u4e3b\u6d41\u5927\u6a21\u578b\u9009\u62e9Decoder-Only\u7684\u539f\u56e0</li> <li>[ ] \u638c\u63e1LLaMA/Qwen\u7684\u5173\u952e\u6280\u672f\u6539\u8fdb\u70b9</li> </ul>"},{"location":"fundamentals/transformer/language-models/#_11","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aAttention\u673a\u5236</li> <li>\u4e0b\u4e00\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"getting-started/","title":"\u5165\u95e8\u6307\u5357","text":"<p>\u6b22\u8fce\u5f00\u59cb\u4f7f\u7528\u6211\u4eec\u7684\u6587\u6863\u7cfb\u7edf\uff01</p>"},{"location":"getting-started/#_2","title":"\u5b89\u88c5\u8981\u6c42","text":"<p>\u5728\u5f00\u59cb\u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u60a8\u7684\u7cfb\u7edf\u6ee1\u8db3\u4ee5\u4e0b\u8981\u6c42\uff1a</p> <ul> <li>Python 3.7+</li> <li>pip \u5305\u7ba1\u7406\u5668</li> </ul>"},{"location":"getting-started/#_3","title":"\u5b89\u88c5\u6b65\u9aa4","text":""},{"location":"getting-started/#1-mkdocs-material","title":"1. \u5b89\u88c5 MkDocs Material","text":"<pre><code>pip install mkdocs-material\n</code></pre>"},{"location":"getting-started/#2","title":"2. \u521b\u5efa\u65b0\u9879\u76ee","text":"<pre><code>mkdocs new my-project\ncd my-project\n</code></pre>"},{"location":"getting-started/#3","title":"3. \u914d\u7f6e\u4e3b\u9898","text":"<p>\u7f16\u8f91 <code>mkdocs.yml</code> \u6587\u4ef6\uff1a</p> <pre><code>site_name: \u6211\u7684\u9879\u76ee\ntheme:\n  name: material\n</code></pre>"},{"location":"getting-started/#4","title":"4. \u542f\u52a8\u5f00\u53d1\u670d\u52a1\u5668","text":"<pre><code>mkdocs serve\n</code></pre> <p>\u73b0\u5728\u60a8\u53ef\u4ee5\u5728\u6d4f\u89c8\u5668\u4e2d\u8bbf\u95ee <code>http://127.0.0.1:8000</code> \u67e5\u770b\u60a8\u7684\u6587\u6863\u4e86\uff01</p>"},{"location":"getting-started/#_4","title":"\u4e0b\u4e00\u6b65","text":"<ul> <li>\u7f16\u5199\u7b2c\u4e00\u7bc7\u6587\u6863</li> <li>\u4e86\u89e3 Markdown \u8bed\u6cd5</li> </ul>"},{"location":"interview/","title":"\u9762\u8bd5\u9898\u5e93","text":""},{"location":"interview/#_2","title":"\ud83c\udfaf \u76ee\u6807","text":"<p>\u6c47\u603bLLM\u76f8\u5173\u7684\u9ad8\u9891\u9762\u8bd5\u95ee\u9898\uff0c\u5e2e\u52a9\u5feb\u901f\u590d\u4e60\u548c\u51c6\u5907\u3002</p>"},{"location":"interview/#_3","title":"\ud83d\udcda \u6309\u4e3b\u9898\u5206\u7c7b","text":""},{"location":"interview/#_4","title":"\u6a21\u578b\u57fa\u7840","text":"<ul> <li>Attention\u8ba1\u7b97\u516c\u5f0f\u548c\u539f\u7406</li> <li>Encoder-Only vs Decoder-Only\u67b6\u6784</li> <li>MHA/MQA/GQA/MLA\u7684\u533a\u522b</li> <li>KV Cache\u5de5\u4f5c\u539f\u7406</li> <li>RoPE\u4f4d\u7f6e\u7f16\u7801\u63a8\u5bfc</li> </ul>"},{"location":"interview/#_5","title":"\u6a21\u578b\u5e94\u7528","text":"<ul> <li>RAG\u7684\u5de5\u4f5c\u6d41\u7a0b</li> <li>Agent\u7684\u7279\u70b9\u548c\u80fd\u529b</li> <li>CoT\u63d0\u5347\u63a8\u7406\u7684\u539f\u7406</li> <li>\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6838\u5fc3\u6280\u5de7</li> </ul>"},{"location":"interview/#_6","title":"\u5de5\u7a0b\u5b9e\u8df5","text":"<ul> <li>\u6a21\u578b\u90e8\u7f72\u4f18\u5316</li> <li>\u63a8\u7406\u52a0\u901f\u6280\u672f</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3\u7b56\u7565</li> <li>\u6a21\u578b\u8bc4\u6d4b\u65b9\u6cd5</li> </ul>"},{"location":"interview/#top-10","title":"\ud83d\udd25 \u9ad8\u9891\u95ee\u9898 Top 10","text":"<ol> <li>Attention\u673a\u5236\u7684\u6570\u5b66\u516c\u5f0f\u662f\u4ec0\u4e48\uff1f</li> <li>\u4e3a\u4ec0\u4e48\u8981\u9664\u4ee5\u221ad_k\uff1f</li> <li>GPT\u548cBERT\u7684\u67b6\u6784\u533a\u522b\uff1f</li> <li>KV Cache\u5982\u4f55\u52a0\u901f\u63a8\u7406\uff1f</li> <li>\u4ec0\u4e48\u662fRoPE\uff1f\u5982\u4f55\u63a8\u5bfc\uff1f</li> <li>MQA\u76f8\u6bd4MHA\u7684\u4f18\u52bf\uff1f</li> <li>Pre-Norm vs Post-Norm\uff1f</li> <li>RAG\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1f</li> <li>Agent\u548c\u666e\u901aLLM\u7684\u533a\u522b\uff1f</li> <li>\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\uff1f</li> </ol>"},{"location":"interview/#_7","title":"\ud83d\udca1 \u7b54\u9898\u6280\u5de7","text":"<ol> <li>\u5148\u8bf4\u6838\u5fc3\uff0c\u518d\u5c55\u5f00\u7ec6\u8282</li> <li>\u7ed3\u5408\u6570\u5b66\u516c\u5f0f\u548c\u4ee3\u7801\u5b9e\u73b0</li> <li>\u63d0\u53ca\u5b9e\u9645\u5e94\u7528\u548c\u5de5\u7a0b\u8003\u8651</li> <li>\u5bf9\u6bd4\u4e0d\u540c\u65b9\u6848\u7684\u4f18\u52a3</li> </ol> <p>\u6bcf\u4e2a\u95ee\u9898\u90fd\u5728\u5bf9\u5e94\u7ae0\u8282\u6709\u8be6\u7ec6\u89e3\u7b54\uff0c\u5efa\u8bae\u7ed3\u5408\u5b66\u4e60\uff01</p>"},{"location":"reference/markdown/","title":"Markdown \u8bed\u6cd5\u53c2\u8003","text":"<p>\u5b8c\u6574\u7684 Markdown \u8bed\u6cd5\u53c2\u8003\u6307\u5357\u3002</p>"},{"location":"reference/markdown/#_1","title":"\u57fa\u7840\u8bed\u6cd5","text":""},{"location":"reference/markdown/#_2","title":"\u6807\u9898","text":"<pre><code># H1 \u6807\u9898\n## H2 \u6807\u9898\n### H3 \u6807\u9898\n#### H4 \u6807\u9898\n##### H5 \u6807\u9898\n###### H6 \u6807\u9898\n</code></pre>"},{"location":"reference/markdown/#_3","title":"\u6587\u672c\u6837\u5f0f","text":"\u6837\u5f0f \u8bed\u6cd5 \u793a\u4f8b \u7c97\u4f53 <code>**\u6587\u672c**</code> \u6216 <code>__\u6587\u672c__</code> \u7c97\u4f53\u6587\u672c \u659c\u4f53 <code>*\u6587\u672c*</code> \u6216 <code>_\u6587\u672c_</code> \u659c\u4f53\u6587\u672c \u5220\u9664\u7ebf <code>~~\u6587\u672c~~</code> ~~\u5220\u9664\u7ebf\u6587\u672c~~ \u884c\u5185\u4ee3\u7801 <code>`\u4ee3\u7801`</code> <code>\u4ee3\u7801</code>"},{"location":"reference/markdown/#_4","title":"\u5217\u8868","text":""},{"location":"reference/markdown/#_5","title":"\u65e0\u5e8f\u5217\u8868","text":"<pre><code>- \u9879\u76ee 1\n- \u9879\u76ee 2\n  - \u5b50\u9879\u76ee 2.1\n  - \u5b50\u9879\u76ee 2.2\n- \u9879\u76ee 3\n</code></pre>"},{"location":"reference/markdown/#_6","title":"\u6709\u5e8f\u5217\u8868","text":"<pre><code>1. \u7b2c\u4e00\u9879\n2. \u7b2c\u4e8c\u9879\n   1. \u5b50\u9879\u76ee 2.1\n   2. \u5b50\u9879\u76ee 2.2\n3. \u7b2c\u4e09\u9879\n</code></pre>"},{"location":"reference/markdown/#_7","title":"\u94fe\u63a5\u548c\u56fe\u7247","text":"<pre><code>[\u94fe\u63a5\u6587\u672c](https://example.com)\n[\u5185\u90e8\u94fe\u63a5](../index.md)\n![\u56fe\u7247\u63cf\u8ff0](image.png)\n</code></pre>"},{"location":"reference/markdown/#_8","title":"\u8868\u683c","text":"<pre><code>| \u8868\u59341 | \u8868\u59342 | \u8868\u59343 |\n|-------|-------|-------|\n| \u5355\u5143\u683c1 | \u5355\u5143\u683c2 | \u5355\u5143\u683c3 |\n| \u5355\u5143\u683c4 | \u5355\u5143\u683c5 | \u5355\u5143\u683c6 |\n</code></pre>"},{"location":"reference/markdown/#_9","title":"\u6269\u5c55\u8bed\u6cd5","text":""},{"location":"reference/markdown/#_10","title":"\u4ee3\u7801\u5757","text":"<pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n</code></pre>"},{"location":"reference/markdown/#_11","title":"\u544a\u793a\u6846\uff08\u9700\u8981\u6269\u5c55\uff09","text":"<pre><code>!!! note \"\u6ce8\u91ca\"\n    \u8fd9\u662f\u4e00\u4e2a\u6ce8\u91ca\u6846\u3002\n\n!!! tip \"\u63d0\u793a\"\n    \u8fd9\u662f\u4e00\u4e2a\u63d0\u793a\u6846\u3002\n\n!!! warning \"\u8b66\u544a\"\n    \u8fd9\u662f\u4e00\u4e2a\u8b66\u544a\u6846\u3002\n\n!!! danger \"\u5371\u9669\"\n    \u8fd9\u662f\u4e00\u4e2a\u5371\u9669\u8b66\u544a\u6846\u3002\n</code></pre>"},{"location":"reference/markdown/#_12","title":"\u4efb\u52a1\u5217\u8868","text":"<pre><code>- [x] \u5df2\u5b8c\u6210\u7684\u4efb\u52a1\n- [ ] \u672a\u5b8c\u6210\u7684\u4efb\u52a1\n- [ ] \u53e6\u4e00\u4e2a\u672a\u5b8c\u6210\u7684\u4efb\u52a1\n</code></pre>"},{"location":"reference/markdown/#_13","title":"\u811a\u6ce8","text":"<pre><code>\u8fd9\u91cc\u6709\u4e00\u4e2a\u811a\u6ce8\u5f15\u7528[^1]\u3002\n\n[^1]: \u8fd9\u662f\u811a\u6ce8\u5185\u5bb9\u3002\n</code></pre>"},{"location":"reference/markdown/#_14","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u4f7f\u7528\u63cf\u8ff0\u6027\u6807\u9898 - \u8ba9\u8bfb\u8005\u5feb\u901f\u4e86\u89e3\u5185\u5bb9</li> <li>\u4fdd\u6301\u4e00\u81f4\u7684\u683c\u5f0f - \u7edf\u4e00\u7684\u6837\u5f0f\u8ba9\u6587\u6863\u66f4\u4e13\u4e1a</li> <li>\u9002\u5ea6\u4f7f\u7528\u683c\u5f0f - \u8fc7\u591a\u7684\u683c\u5f0f\u4f1a\u5f71\u54cd\u53ef\u8bfb\u6027</li> <li>\u6dfb\u52a0\u76ee\u5f55\u94fe\u63a5 - \u5e2e\u52a9\u7528\u6237\u5feb\u901f\u5bfc\u822a</li> </ol> <p>!!! tip \"\u7f16\u5199\u6280\u5de7\"     \u5b9a\u671f\u9884\u89c8\u60a8\u7684\u6587\u6863\uff0c\u786e\u4fdd\u683c\u5f0f\u6b63\u786e\u6e32\u67d3\u3002</p>"},{"location":"tutorials/first-document/","title":"\u7f16\u5199\u7b2c\u4e00\u7bc7\u6587\u6863","text":"<p>\u5b66\u4e60\u5982\u4f55\u521b\u5efa\u548c\u7f16\u8f91\u60a8\u7684\u7b2c\u4e00\u7bc7 Markdown \u6587\u6863\u3002</p>"},{"location":"tutorials/first-document/#_2","title":"\u521b\u5efa\u65b0\u9875\u9762","text":"<ol> <li>\u5728 <code>docs/</code> \u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 <code>.md</code> \u6587\u4ef6</li> <li>\u4f7f\u7528 Markdown \u8bed\u6cd5\u7f16\u5199\u5185\u5bb9</li> <li>\u4fdd\u5b58\u6587\u4ef6\u540e\uff0c\u5f00\u53d1\u670d\u52a1\u5668\u4f1a\u81ea\u52a8\u5237\u65b0</li> </ol>"},{"location":"tutorials/first-document/#markdown","title":"\u57fa\u7840 Markdown \u8bed\u6cd5","text":""},{"location":"tutorials/first-document/#_3","title":"\u6807\u9898","text":"<pre><code># \u4e00\u7ea7\u6807\u9898\n## \u4e8c\u7ea7\u6807\u9898\n### \u4e09\u7ea7\u6807\u9898\n</code></pre>"},{"location":"tutorials/first-document/#_4","title":"\u6587\u672c\u683c\u5f0f","text":"<pre><code>**\u7c97\u4f53\u6587\u672c**\n*\u659c\u4f53\u6587\u672c*\n`\u884c\u5185\u4ee3\u7801`\n</code></pre>"},{"location":"tutorials/first-document/#_5","title":"\u5217\u8868","text":"<pre><code>- \u65e0\u5e8f\u5217\u8868\u9879 1\n- \u65e0\u5e8f\u5217\u8868\u9879 2\n\n1. \u6709\u5e8f\u5217\u8868\u9879 1\n2. \u6709\u5e8f\u5217\u8868\u9879 2\n</code></pre>"},{"location":"tutorials/first-document/#_6","title":"\u94fe\u63a5","text":"<pre><code>[\u94fe\u63a5\u6587\u672c](URL)\n[\u5185\u90e8\u94fe\u63a5](../reference/markdown.md)\n</code></pre>"},{"location":"tutorials/first-document/#_7","title":"\u4ee3\u7801\u5757","text":"<pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n</code></pre>"},{"location":"tutorials/first-document/#_8","title":"\u5b9e\u8df5\u7ec3\u4e60","text":"<p>\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9\u7684\u65b0\u9875\u9762\uff1a</p> <ul> <li>[ ] \u6807\u9898\u548c\u5b50\u6807\u9898</li> <li>[ ] \u4e00\u4e9b\u683c\u5f0f\u5316\u6587\u672c</li> <li>[ ] \u4e00\u4e2a\u5217\u8868</li> <li>[ ] \u4e00\u4e2a\u4ee3\u7801\u793a\u4f8b</li> </ul>"},{"location":"tutorials/first-document/#_9","title":"\u63d0\u793a","text":"<p>!!! tip \"\u4e13\u4e1a\u63d0\u793a\"     \u4f7f\u7528 MkDocs Material \u7684\u544a\u793a\u6846\u529f\u80fd\u6765\u7a81\u51fa\u91cd\u8981\u4fe1\u606f\uff01</p> <p>!!! warning \"\u6ce8\u610f\"     \u8bb0\u5f97\u4fdd\u5b58\u60a8\u7684\u66f4\u6539\uff0c\u5f00\u53d1\u670d\u52a1\u5668\u4f1a\u81ea\u52a8\u91cd\u65b0\u52a0\u8f7d\u3002</p>"}]}