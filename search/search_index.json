{"config":{"lang":["zh","en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"LLM\u81ea\u5b66\u6307\u5357","text":"<p>\ud83c\udfaf 15\u5929\u638c\u63e1\u5927\u8bed\u8a00\u6a21\u578b\u6838\u5fc3\u6280\u672f\uff0c\u52a9\u529b\u6280\u672f\u9762\u8bd5\u6210\u529f\uff01</p> <p>\ud83d\udea7 \u9879\u76ee\u72b6\u6001\uff1a\u79ef\u6781\u5f00\u53d1\u4e2d </p> <p>\u5f53\u524d\u7248\u672c\u4e3b\u8981\u8986\u76d6\u6a21\u578b\u67b6\u6784\u4e0e\u5e94\u7528\u9886\u57df\u3002\u6211\u4eec\u6b63\u5728\u6301\u7eed\u6269\u5c55\u5185\u5bb9\uff0c\u8ba1\u5212\u6dfb\u52a0\u6570\u636e\u5904\u7406\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u63a8\u7406\u4f18\u5316\u7b49\u66f4\u591a\u6838\u5fc3\u9886\u57df\u3002</p> <p>\ud83d\udcc5 \u66f4\u65b0\u9891\u7387\uff1a\u6bcf\u5468\u65b0\u589e\u5185\u5bb9 | \u9884\u8ba1\u5b8c\u6574\u7248\u672c\uff1a2025\u5e74Q2</p> <p>\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u6280\u672f\u9762\u8bd5\u8bbe\u8ba1\u7684LLM\u5b66\u4e60\u8d44\u6e90\u5e93\uff0c\u6db5\u76d6\u4ece\u57fa\u7840\u539f\u7406\u5230\u524d\u6cbf\u6280\u672f\u7684\u5b8c\u6574\u77e5\u8bc6\u4f53\u7cfb\u3002</p>"},{"location":"#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<ul> <li>\ud83d\udcdd \u9762\u8bd5\u5bfc\u5411: \u91cd\u70b9\u8986\u76d6\u6280\u672f\u9762\u8bd5\u9ad8\u9891\u8003\u70b9</li> <li>\ud83d\udd2c \u6280\u672f\u6df1\u5ea6: \u65e2\u6709\u539f\u7406\u7406\u89e3\u53c8\u6709\u4ee3\u7801\u5b9e\u8df5</li> <li>\u26a1 \u9ad8\u6548\u5b66\u4e60: 15\u5929\u7cfb\u7edf\u5316\u5b66\u4e60\u8def\u5f84</li> <li>\ud83c\udf1f \u524d\u6cbf\u6280\u672f: \u5305\u542bDeepSeek\u7b49\u6700\u65b0\u6280\u672f\u521b\u65b0</li> <li>\ud83d\udcaa \u5b9e\u6218\u80fd\u529b: \u5b8c\u6574\u7684\u4ee3\u7801\u5b9e\u73b0\u548c\u9879\u76ee\u7ec3\u4e60</li> </ul>"},{"location":"#15","title":"\ud83d\udcc5 15\u5929\u5b66\u4e60\u8ba1\u5212","text":"\u9636\u6bb5 \u7ae0\u8282 \u4e3b\u8981\u5185\u5bb9 \u5b66\u4e60\u65f6\u95f4 \u91cd\u70b9\u6280\u80fd \u57fa\u7840\u7406\u8bba \u7b2c1\u8282 Transformer\u57fa\u7840 Attention + FFN + \u7f16\u7801\u5668-\u89e3\u7801\u5668 + Tokenizer 3\u5929 \u67b6\u6784\u7406\u89e3 \u6280\u672f\u5347\u7ea7 \u7b2c2\u8282 Attention\u5347\u7ea7 MHA\u2192MQA\u2192GQA\u2192MLA + KV Cache + RoPE 3\u5929 \u4f18\u5316\u6280\u672f \u73b0\u4ee3\u67b6\u6784 \u7b2c3\u8282 LLM\u5347\u7ea7\u6280\u672f MOE\u67b6\u6784 + \u5206\u5e03\u5f0f\u8bad\u7ec3\u57fa\u7840 1.5\u5929 \u5de5\u7a0b\u6982\u5ff5 \u524d\u6cbf\u521b\u65b0 \u7b2c4\u8282 DeepSeek\u6280\u672f MLA + DeepSeek MoE + MTP 3\u5929 \u524d\u6cbf\u6280\u672f \u5e94\u7528\u5b9e\u6218 \u7b2c5\u8282 Context Engineering \u4e0a\u4e0b\u6587\u5de5\u7a0b + \u63d0\u793a\u8bcd\u8bbe\u8ba1 1.5\u5929 \u5e94\u7528\u6280\u80fd \u5e94\u7528\u5b9e\u6218 \u7b2c6\u8282 RAG\u4e0eAgent RAG\u6280\u672f + AI Agent\u6846\u67b6 1.5\u5929 \u7cfb\u7edf\u67b6\u6784 \u5e94\u7528\u5b9e\u6218 \u7b2c7\u8282 CoT\u4e0e\u8bc4\u6d4b \u601d\u7ef4\u94fe + LangChain + \u6a21\u578b\u8bc4\u6d4b 1.5\u5929 \u8bc4\u6d4b\u65b9\u6cd5 <p>\u603b\u8ba1\uff1a15\u5929 | \u6838\u5fc3\u6280\u80fd\uff1a\u7406\u8bba+\u5b9e\u8df5+\u9762\u8bd5</p>"},{"location":"#_2","title":"\ud83d\ude80 \u5feb\u901f\u5f00\u59cb","text":""},{"location":"#_3","title":"\u63a8\u8350\u5b66\u4e60\u8def\u5f84","text":"<ol> <li>\u6309\u987a\u5e8f\u5b66\u4e60: \u6bcf\u4e2a\u7ae0\u8282\u90fd\u6709\u524d\u7f6e\u4f9d\u8d56\u5173\u7cfb</li> <li>\u7406\u8bba+\u5b9e\u8df5: \u6bcf\u8282\u90fd\u5305\u542b\u4ee3\u7801\u5b9e\u73b0\u548c\u9762\u8bd5\u95ee\u7b54</li> <li>\u91cd\u70b9\u5173\u6ce8: \u6807\u8bb0\u4e3a\ud83c\udf1f\u7684DeepSeek\u521b\u65b0\u6280\u672f\u662f\u9762\u8bd5\u70ed\u70b9</li> </ol>"},{"location":"#_4","title":"\u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<ul> <li>\u2705 \u7406\u8bba\u7406\u89e3: \u80fd\u6e05\u6670\u89e3\u91ca\u6838\u5fc3\u539f\u7406</li> <li>\u2705 \u4ee3\u7801\u5b9e\u73b0: \u80fd\u5199\u51fa\u5173\u952e\u7b97\u6cd5\u4ee3\u7801</li> <li>\u2705 \u9762\u8bd5\u51c6\u5907: \u80fd\u56de\u7b54\u6280\u672f\u9762\u8bd5\u95ee\u9898</li> </ul>"},{"location":"#_5","title":"\ud83d\udd25 \u6838\u5fc3\u7279\u8272","text":""},{"location":"#_6","title":"\ud83d\udcca \u6280\u672f\u8986\u76d6","text":"<pre><code>\u6a21\u578b\u57fa\u7840 (10.5\u5929)\n\u251c\u2500\u2500 Transformer\u57fa\u7840 (3\u5929)\n\u251c\u2500\u2500 Attention\u5347\u7ea7 (3\u5929)\n\u251c\u2500\u2500 \u73b0\u4ee3\u67b6\u6784 (1.5\u5929)\n\u2514\u2500\u2500 DeepSeek\u524d\u6cbf (3\u5929)\n\n\u5e94\u7528\u5b9e\u6218 (4.5\u5929)\n\u251c\u2500\u2500 Context Engineering (1.5\u5929)\n\u251c\u2500\u2500 RAG\u4e0eAgent (1.5\u5929)\n\u2514\u2500\u2500 CoT\u4e0e\u8bc4\u6d4b (1.5\u5929)\n</code></pre>"},{"location":"#_7","title":"\ud83c\udf96\ufe0f \u6280\u672f\u6df1\u5ea6","text":"<ul> <li>\u57fa\u7840\u624e\u5b9e: \u4eceAttention\u5230Transformer\u5b8c\u6574\u7406\u89e3</li> <li>\u6280\u672f\u524d\u6cbf: DeepSeek MLA/MoE\u7b49\u6700\u65b0\u521b\u65b0</li> <li>\u5b9e\u6218\u5bfc\u5411: \u5b8c\u6574\u7684\u5de5\u7a0b\u5b9e\u73b0\u548c\u4f18\u5316\u7b56\u7565</li> </ul>"},{"location":"#_8","title":"\ud83d\ude80 \u672a\u6765\u89c4\u5212","text":""},{"location":"#_9","title":"\ud83d\udccb \u5373\u5c06\u6dfb\u52a0\u7684\u5185\u5bb9 (\u6309\u4f18\u5148\u7ea7\u6392\u5e8f)","text":"\u4f18\u5148\u7ea7 \u9886\u57df \u4e3b\u8981\u5185\u5bb9 \u9884\u8ba1\u65f6\u95f4 \ud83d\udd25 P0 \u6570\u636e\u4e0e\u9884\u5904\u7406 \u6570\u636e\u6536\u96c6\u3001\u6e05\u6d17\u3001\u9884\u5904\u7406\u6d41\u7a0b\u3001\u6570\u636e\u96c6\u6784\u5efa 3-4\u5929 \ud83d\udd25 P0 \u6a21\u578b\u8bad\u7ec3 \u9884\u8bad\u7ec3\u6280\u672f\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u8bad\u7ec3\u76d1\u63a7\u8c03\u8bd5 4-5\u5929 \ud83c\udfaf P1 \u5f3a\u5316\u5b66\u4e60\u4e0e\u5bf9\u9f50 RLHF\u3001DPO\u3001Constitutional AI\u3001\u5b89\u5168\u5bf9\u9f50 3-4\u5929 \u26a1 P1 \u63a8\u7406\u4f18\u5316\u4e0e\u90e8\u7f72 \u91cf\u5316\u526a\u679d\u3001\u63a8\u7406\u52a0\u901f\u3001\u751f\u4ea7\u90e8\u7f72\u7b56\u7565 3-4\u5929 \ud83d\udcca P2 \u8bc4\u6d4b\u4e0e\u57fa\u51c6 \u8bc4\u6d4b\u4f53\u7cfb\u3001\u57fa\u51c6\u6d4b\u8bd5\u3001\u6a21\u578b\u5206\u6790\u65b9\u6cd5 2-3\u5929 \ud83c\udf0d P2 \u591a\u6a21\u6001\u4e0e\u4e13\u4e1a\u9886\u57df Vision-Language\u3001\u4ee3\u7801\u751f\u6210\u3001\u79d1\u5b66\u8ba1\u7b97 3-4\u5929 \ud83c\udfe2 P3 \u5de5\u7a0b\u4e0e\u4ea7\u54c1\u5316 MLOps\u3001\u4ea7\u54c1\u5316\u3001\u5546\u4e1a\u5316\u8003\u91cf 2-3\u5929"},{"location":"#_10","title":"\ud83c\udfaf \u5b8c\u6574\u7248\u672c\u9884\u89c8","text":"<p>\u5b8c\u6574\u7248\u672c\u5c06\u5305\u542b 35-40\u5929 \u7684\u7cfb\u7edf\u5316\u5b66\u4e60\u5185\u5bb9\uff0c\u8986\u76d6LLM\u4ece\u7406\u8bba\u5230\u5b9e\u8df5\u7684\u5168\u6808\u6280\u672f\u4f53\u7cfb\u3002</p>"},{"location":"#_11","title":"\ud83c\udf89 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u611f\u5174\u8da3\u7684\u7ae0\u8282\u5f00\u59cb\u5b66\u4e60\u5427\uff01\u5f53\u524d\u5185\u5bb9\u5df2\u8db3\u591f\u652f\u6491\u5927\u90e8\u5206LLM\u6280\u672f\u9762\u8bd5\u9700\u6c42\u3002</p> <p>\u70b9\u51fb\u5de6\u4fa7\u5bfc\u822a\u680f\u9009\u62e9\u7ae0\u8282\uff0c\u5f00\u59cb\u4f60\u7684LLM\u5b66\u4e60\u4e4b\u65c5\uff01</p>"},{"location":"applications/context-engineering/","title":"\u7b2c4\u8282\uff1aContext Engineering","text":""},{"location":"applications/context-engineering/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6838\u5fc3\u6280\u5de7\uff0c\u5b66\u4f1a\u8bbe\u8ba1\u9ad8\u6548\u7684\u63d0\u793a\u8bcd\u6765\u63d0\u5347\u6a21\u578b\u8868\u73b0\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - \u4ec0\u4e48\u662f\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff1f - \u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\uff1f - \u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u7684\u5e94\u5bf9\u7b56\u7565</p>"},{"location":"applications/context-engineering/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a1.5\u5929</p> <ul> <li>Day 1: \u4e0a\u4e0b\u6587\u5de5\u7a0b\u57fa\u7840\u7406\u8bba\u548c\u8bbe\u8ba1\u539f\u5219</li> <li>\u534a\u5929: \u5b9e\u8df5\u6280\u5de7\u548c\u9ad8\u7ea7\u6848\u4f8b\u5206\u6790</li> </ul>"},{"location":"applications/context-engineering/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"applications/context-engineering/#1","title":"1. \u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5","text":"<ul> <li>\u63d0\u793a\u8bcd\u8bbe\u8ba1\u539f\u5219</li> <li>Few-shot\u5b66\u4e60\u6280\u5de7</li> <li>\u4e0a\u4e0b\u6587\u4f18\u5316\u7b56\u7565</li> </ul>"},{"location":"applications/context-engineering/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u9879\u76ee\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u7406\u8bba\u638c\u63e1: \u7406\u89e3\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6838\u5fc3\u539f\u7406</li> <li>\u5b9e\u8df5\u5e94\u7528: \u80fd\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\u6a21\u677f</li> </ol>"},{"location":"applications/context-engineering/#_5","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u8fd9\u662f\u5927\u6a21\u578b\u5e94\u7528\u7684\u6838\u5fc3\u6280\u80fd\uff0c\u5b9e\u7528\u6027\u5f88\u5f3a\uff01</p>"},{"location":"applications/context-engineering/practices/","title":"\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5b9e\u8df5","text":""},{"location":"applications/context-engineering/practices/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u638c\u63e1\u5b9e\u7528\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u6280\u5de7\u548c\u6700\u4f73\u5b9e\u8df5\u3002</p>"},{"location":"applications/context-engineering/practices/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"applications/context-engineering/practices/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Manus \u5185\u90e8\u7684 Context \u5de5\u7a0b\u7ecf\u9a8c - \u77e5\u4e4e</li> <li>Context Engineering GitHub\u9879\u76ee - \u5b9e\u8df5\u6848\u4f8b</li> </ol>"},{"location":"applications/context-engineering/practices/#manus-context","title":"\ud83d\udcdd Manus Context\u5de5\u7a0b\u5b9e\u6218\u7ecf\u9a8c","text":""},{"location":"applications/context-engineering/practices/#_5","title":"\u80cc\u666f\u4e0e\u9009\u62e9","text":"<p>Manus\u56e2\u961f\u5728\u6784\u5efaAgent\u65f6\u9009\u62e9\u4e86\u4e0a\u4e0b\u6587\u5de5\u7a0b\u800c\u975e\u7aef\u5230\u7aef\u6a21\u578b\u8bad\u7ec3\u7684\u8def\u5f84\uff0c\u539f\u56e0\uff1a - \u5feb\u901f\u8fed\u4ee3\uff1a\u6539\u8fdb\u53d1\u5e03\u5468\u671f\u4ece\u6570\u5468\u7f29\u77ed\u5230\u51e0\u5c0f\u65f6 - \u4e0e\u5e95\u5c42\u6a21\u578b\u53d1\u5c55\u4fdd\u6301\"\u6b63\u4ea4\"\u5173\u7cfb\uff1a\u6a21\u578b\u8fdb\u6b65\u5982\u6c34\u6da8\u8239\u9ad8\uff0cManus\u6210\u4e3a\u6f6e\u5934\u4e0a\u7684\u8239</p>"},{"location":"applications/context-engineering/practices/#_6","title":"\u516d\u5927\u6838\u5fc3\u5b9e\u8df5\u7ecf\u9a8c","text":""},{"location":"applications/context-engineering/practices/#1-kv","title":"1. \u56f4\u7ed5KV\u7f13\u5b58\u8fdb\u884c\u8bbe\u8ba1","text":"<p>KV\u7f13\u5b58\u547d\u4e2d\u7387\u662f\u751f\u4ea7\u73af\u5883\u4e2dAI\u667a\u80fd\u4f53\u6700\u5173\u952e\u7684\u5355\u4e00\u6307\u6807\uff0c\u76f4\u63a5\u5f71\u54cd\u5ef6\u8fdf\u548c\u6210\u672c\u3002</p> <p>Agent\u5de5\u4f5c\u539f\u7406\uff1a - \u6bcf\u6b21\u8fed\u4ee3\u4e2d\uff0c\u6a21\u578b\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u9009\u62e9\u52a8\u4f5c - \u52a8\u4f5c\u6267\u884c\u540e\u4ea7\u751f\u89c2\u6d4b\u7ed3\u679c\uff0c\u8ffd\u52a0\u5230\u4e0a\u4e0b\u6587\u4e2d - \u4e0a\u4e0b\u6587\u6301\u7eed\u589e\u957f\uff0c\u8f93\u5165\u4e0e\u8f93\u51fatoken\u6bd4\u4f8b\u901a\u5e38\u8fbe\u5230100:1</p> <p>\u63d0\u5347\u7f13\u5b58\u547d\u4e2d\u7387\u7684\u5173\u952e\u5b9e\u8df5\uff1a - \u4fdd\u6301\u63d0\u793a\u8bcd\u524d\u7f00\u7a33\u5b9a\u6027\uff1a\u907f\u514d\u5728\u7cfb\u7edf\u63d0\u793a\u8bcd\u5f00\u5934\u5305\u542b\u65f6\u95f4\u6233 - \u53ea\u5bf9\u4e0a\u4e0b\u6587\u8fdb\u884c\u8ffd\u52a0\uff08append-only\uff09\uff1a\u907f\u514d\u4fee\u6539\u4e4b\u524d\u7684\u52a8\u4f5c\u6216\u89c2\u6d4b\u7ed3\u679c - \u786e\u4fdd\u5e8f\u5217\u5316\u8fc7\u7a0b\u662f\u786e\u5b9a\u6027\u7684\uff1aJSON\u5bf9\u8c61\u5e8f\u5217\u5316\u65f6\u4fdd\u8bc1\u952e\u7684\u987a\u5e8f\u56fa\u5b9a - \u660e\u786e\u6807\u8bb0\u7f13\u5b58\u65ad\u70b9\uff1a\u8003\u8651\u7f13\u5b58\u8fc7\u671f\u65f6\u95f4\uff0c\u81f3\u5c11\u5728\u7cfb\u7edf\u63d0\u793a\u8bcd\u672b\u5c3e\u8bbe\u7f6e\u65ad\u70b9</p> <p>\u6210\u672c\u5f71\u54cd\uff1a\u4ee5Claude Sonnet\u4e3a\u4f8b\uff0c\u547d\u4e2d\u7f13\u5b58\u7684\u8f93\u5165token\u6210\u672c\u4e3a0.30\u7f8e\u5143/\u767e\u4e07token\uff0c\u672a\u547d\u4e2d\u7f13\u5b58\u6210\u672c\u4e3a3\u7f8e\u5143/\u767e\u4e07token\u2014\u2014\u76f8\u5dee\u5341\u500d\u3002</p>"},{"location":"applications/context-engineering/practices/#2-mask","title":"2. \u906e\u853d\uff08Mask\uff09\uff0c\u800c\u975e\u79fb\u9664","text":"<p>\u95ee\u9898\uff1a\u968f\u7740Agent\u80fd\u529b\u589e\u52a0\uff0c\u5de5\u5177\u6570\u91cf\u7206\u70b8\u5f0f\u589e\u957f\uff0c\u6a21\u578b\u5bb9\u6613\u9009\u9519\u884c\u52a8\u6216\u91c7\u53d6\u4f4e\u6548\u8def\u5f84\u3002</p> <p>\u9519\u8bef\u65b9\u6848\uff1a\u52a8\u6001\u589e\u5220\u5de5\u5177\u4f1a\u5bfc\u81f4\uff1a 1. \u5de5\u5177\u5b9a\u4e49\u4f4d\u4e8e\u4e0a\u4e0b\u6587\u524d\u90e8\uff0c\u4efb\u4f55\u66f4\u6539\u90fd\u4f1a\u5bfc\u81f4\u540e\u7eed\u6240\u6709KV\u7f13\u5b58\u5931\u6548 2. \u5f53\u52a8\u4f5c\u5f15\u7528\u4e0d\u518d\u5b58\u5728\u7684\u5de5\u5177\u65f6\uff0c\u6a21\u578b\u4f1a\u56f0\u60d1\u5e76\u4ea7\u751f\u5e7b\u89c9</p> <p>Manus\u89e3\u51b3\u65b9\u6848\uff1a\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u72b6\u6001\u673a\u6765\u7ba1\u7406\u5de5\u5177\u53ef\u7528\u6027\uff1a - \u4e0d\u79fb\u9664\u5de5\u5177\uff0c\u800c\u662f\u5728\u89e3\u7801\u9636\u6bb5\u906e\u853dtoken logits - \u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u963b\u6b62\u6216\u5f3a\u5236\u6a21\u578b\u9009\u62e9\u67d0\u4e9b\u52a8\u4f5c - \u8bbe\u8ba1\u5177\u6709\u4e00\u81f4\u6027\u524d\u7f00\u7684\u52a8\u4f5c\u540d\u79f0\uff08\u5982browser_\u3001shell_\u5f00\u5934\uff09</p> <p>\u51fd\u6570\u8c03\u7528\u4e09\u79cd\u6a21\u5f0f\uff08\u4ee5Hermes format\u4e3a\u4f8b\uff09\uff1a - \u81ea\u52a8\uff08Auto\uff09\uff1a<code>&lt;|im_start|&gt;assistant</code> - \u5fc5\u9700\uff08Required\uff09\uff1a<code>&lt;|im_start|&gt;assistant&lt;tool_call&gt;</code> - \u6307\u5b9a\uff08Specified\uff09\uff1a<code>&lt;|im_start|&gt;assistant&lt;tool_call&gt;{\"name\": \"browser_\"</code></p>"},{"location":"applications/context-engineering/practices/#3","title":"3. \u5c06\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u4e0a\u4e0b\u6587","text":"<p>\u6311\u6218\uff1a\u5373\u4f7f\u6709128K+\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5728\u771f\u5b9e\u667a\u80fd\u4f53\u573a\u666f\u4e2d\u4ecd\u9762\u4e34\uff1a 1. \u89c2\u6d4b\u7ed3\u679c\u53ef\u80fd\u6781\u5176\u5e9e\u5927\uff08\u7f51\u9875\u3001PDF\u7b49\uff09 2. \u6a21\u578b\u6027\u80fd\u5728\u8d85\u8fc7\u4e00\u5b9a\u4e0a\u4e0b\u6587\u957f\u5ea6\u540e\u4e0b\u964d 3. \u957f\u8f93\u5165\u975e\u5e38\u6602\u8d35</p> <p>Manus\u65b9\u6848\uff1a\u5c06\u6587\u4ef6\u7cfb\u7edf\u89c6\u4e3a\u7ec8\u6781\u4e0a\u4e0b\u6587 - \u5bb9\u91cf\u65e0\u9650\u3001\u5929\u7136\u6301\u4e45\u3001\u667a\u80fd\u4f53\u53ef\u76f4\u63a5\u64cd\u4f5c - \u6a21\u578b\u5b66\u4e60\u6309\u9700\u8bfb\u5199\u6587\u4ef6\uff0c\u5c06\u6587\u4ef6\u7cfb\u7edf\u5f53\u4f5c\u7ed3\u6784\u5316\u7684\u5916\u90e8\u8bb0\u5fc6\u4f53</p> <p>\u53ef\u6062\u590d\u7684\u538b\u7f29\u7b56\u7565\uff1a - \u4fdd\u7559\u7f51\u9875URL\uff0c\u5185\u5bb9\u53ef\u4ece\u4e0a\u4e0b\u6587\u4e2d\u4e22\u5f03 - \u4fdd\u7559\u6587\u6863\u8def\u5f84\uff0c\u5185\u5bb9\u53ef\u88ab\u7701\u7565 - \u5728\u4e0d\u6c38\u4e45\u4e22\u5931\u4fe1\u606f\u7684\u524d\u63d0\u4e0b\u7f29\u51cf\u4e0a\u4e0b\u6587\u957f\u5ea6</p> <p>\u672a\u6765\u5c55\u671b\uff1a\u5177\u5907\u667a\u80fd\u4f53\u80fd\u529b\u7684\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u6216\u8bb8\u662fNeural Turing Machines\u771f\u6b63\u7684\u7ee7\u627f\u8005\u3002</p>"},{"location":"applications/context-engineering/practices/#4","title":"4. \u901a\u8fc7\"\u590d\u8ff0\"\u6765\u64cd\u63a7\u6ce8\u610f\u529b","text":"<p>\u73b0\u8c61\uff1aManus\u5904\u7406\u590d\u6742\u4efb\u52a1\u65f6\u4f1a\u521b\u5efatodo.md\u6587\u4ef6\uff0c\u968f\u7740\u4efb\u52a1\u8fdb\u5c55\u9010\u6b65\u66f4\u65b0\u3002</p> <p>\u8bbe\u8ba1\u76ee\u7684\uff1a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6ce8\u610f\u529b\u64cd\u63a7\u673a\u5236 - \u5178\u578b\u4efb\u52a1\u5e73\u5747\u9700\u898150\u6b21\u5de5\u5177\u8c03\u7528 - \u901a\u8fc7\u4e0d\u65ad\u91cd\u5199\u5f85\u529e\u4e8b\u9879\u5217\u8868\uff0c\u5c06\u4efb\u52a1\u76ee\u6807\"\u590d\u8ff0\"\u5230\u4e0a\u4e0b\u6587\u672b\u5c3e - \u5c06\u5168\u5c40\u8ba1\u5212\u6ce8\u5165\u5230\u6a21\u578b\u7684\u8fd1\u671f\u6ce8\u610f\u529b\u8303\u56f4 - \u907f\u514d\"\u4e2d\u95f4\u9057\u5fd8\"\uff08lost-in-the-middle\uff09\u95ee\u9898\uff0c\u51cf\u5c11\u76ee\u6807\u504f\u79bb</p> <p>\u672c\u8d28\uff1a\u7528\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u81ea\u8eab\u6ce8\u610f\u529b\uff0c\u805a\u7126\u4e8e\u4efb\u52a1\u76ee\u6807\u3002</p>"},{"location":"applications/context-engineering/practices/#5","title":"5. \u4fdd\u7559\u51fa\u9519\u8bb0\u5f55","text":"<p>\u6838\u5fc3\u89c2\u70b9\uff1aAgent\u4f1a\u72af\u9519\uff0c\u8fd9\u4e0d\u662fbug\uff0c\u800c\u662f\u73b0\u5b9e\u3002</p> <p>\u5e38\u89c1\u9519\u8bef\u505a\u6cd5\uff1a\u9690\u85cf\u9519\u8bef\u3001\u6e05\u7406\u75d5\u8ff9\u3001\u91cd\u8bd5\u52a8\u4f5c\u6216\u91cd\u7f6e\u6a21\u578b\u72b6\u6001 \u4ee3\u4ef7\uff1a\u6d88\u9664\u5931\u8d25\u8bb0\u5f55\uff0c\u4e5f\u5c31\u6d88\u9664\u4e86\u8fc7\u5f80\u7684\u884c\u52a8\u8bc1\u636e\uff0c\u6a21\u578b\u65e0\u6cd5\u9002\u5e94</p> <p>\u6700\u4f73\u5b9e\u8df5\uff1a\u5c06\u5931\u8d25\u7684\u5c1d\u8bd5\u4fdd\u7559\u5728\u4e0a\u4e0b\u6587\u4e2d - \u5f53\u6a21\u578b\u770b\u5230\u5931\u8d25\u7684\u52a8\u4f5c\u548c\u5806\u6808\u8ddf\u8e2a\u65f6\uff0c\u4f1a\u9690\u5f0f\u66f4\u65b0\u5185\u90e8\u8ba4\u77e5 - \u6539\u53d8\u5bf9\u76f8\u4f3c\u52a8\u4f5c\u7684\u5148\u9a8c\u5224\u65ad\uff0c\u51cf\u5c11\u91cd\u590d\u72af\u9519 - \u9519\u8bef\u6062\u590d\u80fd\u529b\u662f\u771f\u6b63\u667a\u80fd\u4f53\u884c\u4e3a\u6700\u660e\u786e\u7684\u6807\u5fd7</p>"},{"location":"applications/context-engineering/practices/#6-few-shot","title":"6. \u4e0d\u8981\u9677\u5165Few-Shot\u9677\u9631","text":"<p>\u95ee\u9898\uff1aFew-shot Prompting\u5728Agent\u7cfb\u7edf\u4e2d\u53ef\u80fd\u9002\u5f97\u5176\u53cd - \u8bed\u8a00\u6a21\u578b\u662f\u51fa\u8272\u7684\u6a21\u4eff\u8005\uff0c\u4f1a\u6a21\u4eff\u4e0a\u4e0b\u6587\u4e2d\u7684\u884c\u4e3a\u6a21\u5f0f - \u5982\u679c\u4e0a\u4e0b\u6587\u4e2d\u5145\u6ee1\u76f8\u4f3c\u7684\"\u52a8\u4f5c-\u89c2\u6d4b\u7ed3\u679c\"\u5bf9\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u9075\u5faa\u8fd9\u79cd\u6a21\u5f0f - \u5728\u91cd\u590d\u6027\u51b3\u7b56\u4efb\u52a1\u4e2d\u53ef\u80fd\u5bfc\u81f4\u884c\u4e3a\u6f02\u79fb\u3001\u8fc7\u5ea6\u6cdb\u5316\u751a\u81f3\u5e7b\u89c9</p> <p>\u89e3\u51b3\u65b9\u6cd5\uff1a\u589e\u52a0\u591a\u6837\u6027 - \u5728\u52a8\u4f5c\u548c\u89c2\u6d4b\u7ed3\u679c\u4e2d\u5f15\u5165\u5c11\u91cf\u7ed3\u6784\u5316\u7684\u53d8\u52a8 - \u4f7f\u7528\u4e0d\u540c\u7684\u5e8f\u5217\u5316\u6a21\u677f\u3001\u53d8\u6362\u63aa\u8f9e - \u5728\u987a\u5e8f\u6216\u683c\u5f0f\u4e0a\u5f15\u5165\u5fae\u5c0f\u7684\u566a\u97f3 - \u8fd9\u79cd\u53d7\u63a7\u7684\u968f\u673a\u6027\u6709\u52a9\u4e8e\u6253\u7834\u6a21\u5f0f\uff0c\u8c03\u6574\u6a21\u578b\u6ce8\u610f\u529b</p> <p>\u6838\u5fc3\u539f\u5219\uff1a\u4e0a\u4e0b\u6587\u7684\u6a21\u5f0f\u8d8a\u5355\u4e00\uff0c\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u5c31\u8d8a\u8106\u5f31\u3002</p>"},{"location":"applications/context-engineering/practices/#_7","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/context-engineering/practices/#context","title":"Context\u5de5\u7a0b\u6838\u5fc3\u539f\u5219","text":"<ol> <li>\u6027\u80fd\u4f18\u5148: \u56f4\u7ed5KV\u7f13\u5b58\u8bbe\u8ba1\uff0c\u6700\u5927\u5316\u7f13\u5b58\u547d\u4e2d\u7387</li> <li>\u72b6\u6001\u7ba1\u7406: \u4f7f\u7528\u906e\u853d\u800c\u975e\u79fb\u9664\uff0c\u4fdd\u6301\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027</li> <li>\u8bb0\u5fc6\u5916\u5316: \u5c06\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u6269\u5c55\u4e0a\u4e0b\u6587\uff0c\u5b9e\u73b0\u53ef\u6062\u590d\u7684\u538b\u7f29</li> <li>\u6ce8\u610f\u529b\u5f15\u5bfc: \u901a\u8fc7\u590d\u8ff0\u673a\u5236\u5c06\u5173\u952e\u4fe1\u606f\u7f6e\u4e8e\u6ce8\u610f\u529b\u7126\u70b9</li> <li>\u9519\u8bef\u5b66\u4e60: \u4fdd\u7559\u5931\u8d25\u8bb0\u5f55\uff0c\u8ba9Agent\u4ece\u9519\u8bef\u4e2d\u5b66\u4e60</li> <li>\u591a\u6837\u6027\u4fdd\u6301: \u907f\u514d\u8fc7\u5ea6\u6a21\u5f0f\u5316\uff0c\u589e\u52a0\u8f93\u51fa\u591a\u6837\u6027</li> </ol>"},{"location":"applications/context-engineering/practices/#_8","title":"\u4f20\u7edf\u63d0\u793a\u8bcd\u8bbe\u8ba1\u539f\u5219","text":"<ol> <li>\u660e\u786e\u6027: \u6e05\u6670\u8868\u8fbe\u4efb\u52a1\u8981\u6c42</li> <li>\u7ed3\u6784\u5316: \u4f7f\u7528\u4e00\u81f4\u7684\u683c\u5f0f</li> <li>\u793a\u4f8b\u5f15\u5bfc: \u63d0\u4f9bFew-shot\u793a\u4f8b\uff08\u6ce8\u610f\u907f\u514d\u9677\u9631\uff09</li> <li>\u89d2\u8272\u8bbe\u5b9a: \u660e\u786eAI\u7684\u89d2\u8272\u5b9a\u4f4d</li> </ol>"},{"location":"applications/context-engineering/practices/#_9","title":"\u5e38\u7528\u6280\u5de7","text":"<ul> <li>\u94fe\u5f0f\u601d\u7ef4(CoT): \u5f15\u5bfc\u9010\u6b65\u63a8\u7406</li> <li>\u6a21\u677f\u5316: \u6807\u51c6\u5316\u63d0\u793a\u8bcd\u7ed3\u6784</li> <li>\u4e0a\u4e0b\u6587\u538b\u7f29: \u4f18\u5316\u4fe1\u606f\u5bc6\u5ea6</li> <li>\u72b6\u6001\u673a\u63a7\u5236: \u52a8\u6001\u7ba1\u7406\u5de5\u5177\u53ef\u7528\u6027</li> </ul>"},{"location":"applications/context-engineering/practices/#_10","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/context-engineering/practices/#q1","title":"Q1: \u4ec0\u4e48\u662f\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff1f","text":"<p>\u6838\u5fc3\u5b9a\u4e49: \u4e0a\u4e0b\u6587\u5de5\u7a0b\u662f\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u8f93\u5165\u63d0\u793a\u8bcd\u6765\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u671f\u671b\u8f93\u51fa\u7684\u6280\u672f\u3002</p> <p>\u5173\u952e\u8981\u7d20: - \u4efb\u52a1\u63cf\u8ff0\u6e05\u6670 - \u63d0\u4f9b\u76f8\u5173\u793a\u4f8b - \u8bbe\u7f6e\u5408\u9002\u7684\u7ea6\u675f\u6761\u4ef6</p>"},{"location":"applications/context-engineering/practices/#_11","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u63d0\u793a\u8bcd\u8bbe\u8ba1\u7684\u6838\u5fc3\u539f\u5219</li> <li>[ ] \u80fd\u8bbe\u8ba1\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u63d0\u793a\u8bcd\u6a21\u677f</li> </ul>"},{"location":"applications/context-engineering/practices/#_12","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aRAG\u4e0eAgent</li> <li>\u8fd4\u56de\uff1aContext Engineering\u6982\u89c8</li> </ul>"},{"location":"applications/cot-evaluation/","title":"\u7b2c7\u8282\uff1aCoT\u4e0e\u8bc4\u6d4b","text":""},{"location":"applications/cot-evaluation/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u601d\u7ef4\u94fe(Chain of Thought)\u6280\u672f\u548c\u5927\u6a21\u578b\u8bc4\u6d4b\u65b9\u6cd5\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - CoT\u7684\u5de5\u4f5c\u539f\u7406\u548c\u5e94\u7528\u573a\u666f - LangChain\u7684\u6838\u5fc3\u7ec4\u4ef6 - \u5927\u6a21\u578b\u8bc4\u6d4b\u7684\u4e3b\u8981\u6307\u6807</p>"},{"location":"applications/cot-evaluation/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a1.5\u5929</p> <ul> <li>Day 1: CoT\u6280\u672f\u548cLangChain\u6846\u67b6</li> <li>\u534a\u5929: \u6a21\u578b\u8bc4\u6d4b\u65b9\u6cd5\u548c\u57fa\u51c6</li> </ul>"},{"location":"applications/cot-evaluation/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"applications/cot-evaluation/#1","title":"1. \u601d\u7ef4\u94fe\u6280\u672f","text":"<ul> <li>CoT\u7684\u539f\u7406\u548c\u5e94\u7528</li> <li>\u63d0\u5347\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5</li> </ul>"},{"location":"applications/cot-evaluation/#2-langchain","title":"2. LangChain\u6846\u67b6","text":"<ul> <li>LangChain\u7684\u6838\u5fc3\u6982\u5ff5</li> <li>\u94fe\u5f0f\u8c03\u7528\u548c\u5de5\u5177\u96c6\u6210</li> </ul>"},{"location":"applications/cot-evaluation/#3","title":"3. \u6a21\u578b\u8bc4\u6d4b","text":"<ul> <li>\u8bc4\u6d4b\u6307\u6807\u548c\u65b9\u6cd5</li> <li>\u57fa\u51c6\u6d4b\u8bd5\u4ecb\u7ecd</li> </ul>"},{"location":"applications/cot-evaluation/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<ul> <li>[ ] \u7406\u89e3CoT\u7684\u5de5\u4f5c\u539f\u7406</li> <li>[ ] \u4e86\u89e3\u6a21\u578b\u8bc4\u6d4b\u7684\u57fa\u672c\u65b9\u6cd5</li> </ul>"},{"location":"applications/cot-evaluation/cot/","title":"\u601d\u7ef4\u94fe\u6280\u672f","text":""},{"location":"applications/cot-evaluation/cot/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u601d\u7ef4\u94fe(Chain of Thought)\u6280\u672f\u5982\u4f55\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002</p>"},{"location":"applications/cot-evaluation/cot/#_3","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/cot-evaluation/cot/#cot","title":"CoT\u662f\u4ec0\u4e48\uff1f","text":"<p>Chain of Thought (CoT) \u662f\u4e00\u79cd\u63d0\u793a\u6280\u672f\uff0c\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u5c55\u793a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u6765\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u8868\u73b0\u3002</p>"},{"location":"applications/cot-evaluation/cot/#_4","title":"\u6838\u5fc3\u539f\u7406","text":"<ol> <li>\u5206\u6b65\u63a8\u7406: \u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u6b65\u9aa4</li> <li>\u663e\u5f0f\u601d\u8003: \u8ba9\u6a21\u578b\u5c55\u793a\u601d\u8003\u8fc7\u7a0b</li> <li>\u9010\u6b65\u5f15\u5bfc: \u901a\u8fc7\u793a\u4f8b\u6559\u4f1a\u6a21\u578b\u63a8\u7406\u6a21\u5f0f</li> </ol>"},{"location":"applications/cot-evaluation/cot/#_5","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/cot-evaluation/cot/#q1-cot","title":"Q1: \u4ec0\u4e48\u662f\u601d\u7ef4\u94fe(CoT)\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54: CoT\u662f\u901a\u8fc7\"\u8ba9\u6211\u60f3\u60f3...\"\u7684\u65b9\u5f0f\u5f15\u5bfc\u5927\u6a21\u578b\u8fdb\u884c\u5206\u6b65\u63a8\u7406\uff0c\u4ece\u800c\u63d0\u5347\u590d\u6742\u4efb\u52a1\u8868\u73b0\u7684\u6280\u672f\u3002</p> <p>\u6838\u5fc3\u673a\u5236: - \u4e0d\u76f4\u63a5\u7ed9\u7b54\u6848\uff0c\u800c\u662f\u5c55\u793a\u63a8\u7406\u8fc7\u7a0b - \u901a\u8fc7\u4e2d\u95f4\u6b65\u9aa4\u63d0\u5347\u6700\u7ec8\u7b54\u6848\u8d28\u91cf - \u5bf9\u6570\u5b66\u3001\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u7279\u522b\u6709\u6548</p>"},{"location":"applications/cot-evaluation/cot/#_6","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3CoT\u7684\u57fa\u672c\u539f\u7406</li> <li>[ ] \u77e5\u9053CoT\u9002\u7528\u7684\u573a\u666f</li> </ul>"},{"location":"applications/cot-evaluation/cot/#_7","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aLangChain\u6846\u67b6</li> <li>\u8fd4\u56de\uff1aCoT\u4e0e\u8bc4\u6d4b\u6982\u89c8</li> </ul>"},{"location":"applications/cot-evaluation/evaluation/","title":"\u6a21\u578b\u8bc4\u6d4b","text":""},{"location":"applications/cot-evaluation/evaluation/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u4e86\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u6d4b\u65b9\u6cd5\u548c\u4e3b\u8981\u57fa\u51c6\u3002</p>"},{"location":"applications/cot-evaluation/evaluation/#_3","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/cot-evaluation/evaluation/#_4","title":"\u8bc4\u6d4b\u7684\u91cd\u8981\u6027","text":"<p>\u6a21\u578b\u8bc4\u6d4b\u662f\u8861\u91cfLLM\u6027\u80fd\u3001\u6307\u5bfc\u6a21\u578b\u6539\u8fdb\u7684\u91cd\u8981\u624b\u6bb5\u3002</p>"},{"location":"applications/cot-evaluation/evaluation/#_5","title":"\u4e3b\u8981\u8bc4\u6d4b\u7ef4\u5ea6","text":"<ol> <li>\u80fd\u529b\u8bc4\u6d4b: \u63a8\u7406\u3001\u77e5\u8bc6\u3001\u8bed\u8a00\u7406\u89e3</li> <li>\u5b89\u5168\u8bc4\u6d4b: \u6709\u5bb3\u8f93\u51fa\u3001\u504f\u89c1\u68c0\u6d4b</li> <li>\u6548\u7387\u8bc4\u6d4b: \u901f\u5ea6\u3001\u8d44\u6e90\u6d88\u8017</li> <li>\u53ef\u9760\u6027: \u4e00\u81f4\u6027\u3001\u7a33\u5b9a\u6027</li> </ol>"},{"location":"applications/cot-evaluation/evaluation/#_6","title":"\u5e38\u89c1\u57fa\u51c6","text":"<ul> <li>MMLU: \u591a\u5b66\u79d1\u77e5\u8bc6\u7406\u89e3</li> <li>HellaSwag: \u5e38\u8bc6\u63a8\u7406</li> <li>HumanEval: \u4ee3\u7801\u751f\u6210\u80fd\u529b</li> <li>GSM8K: \u6570\u5b66\u63a8\u7406</li> </ul>"},{"location":"applications/cot-evaluation/evaluation/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/cot-evaluation/evaluation/#q1","title":"Q1: \u5927\u6a21\u578b\u8bc4\u6d4b\u6709\u54ea\u4e9b\u4e3b\u8981\u65b9\u6cd5\uff1f","text":"<p>\u4e3b\u8981\u65b9\u6cd5: - \u81ea\u52a8\u8bc4\u6d4b: \u57fa\u4e8e\u6807\u51c6\u7b54\u6848\u7684\u5ba2\u89c2\u6307\u6807 - \u4eba\u5de5\u8bc4\u6d4b: \u4e3b\u89c2\u8d28\u91cf\u8bc4\u4f30 - \u5bf9\u6bd4\u8bc4\u6d4b: \u6a21\u578b\u95f4\u76f8\u5bf9\u8868\u73b0 - \u5728\u7ebf\u8bc4\u6d4b: \u5b9e\u9645\u5e94\u7528\u573a\u666f\u6d4b\u8bd5</p>"},{"location":"applications/cot-evaluation/evaluation/#_8","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u4e86\u89e3\u6a21\u578b\u8bc4\u6d4b\u7684\u57fa\u672c\u6982\u5ff5</li> <li>[ ] \u77e5\u9053\u4e3b\u8981\u7684\u8bc4\u6d4b\u57fa\u51c6</li> </ul>"},{"location":"applications/cot-evaluation/evaluation/#_9","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aLangChain\u6846\u67b6</li> <li>\u9762\u8bd5\u9898\u5e93</li> <li>\u8fd4\u56de\uff1aCoT\u4e0e\u8bc4\u6d4b\u6982\u89c8</li> </ul>"},{"location":"applications/cot-evaluation/langchain/","title":"LangChain\u6846\u67b6","text":""},{"location":"applications/cot-evaluation/langchain/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u4e86\u89e3LangChain\u6846\u67b6\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u5e94\u7528\u573a\u666f\u3002</p>"},{"location":"applications/cot-evaluation/langchain/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/cot-evaluation/langchain/#langchain_1","title":"LangChain\u662f\u4ec0\u4e48\uff1f","text":"<p>LangChain \u662f\u4e00\u4e2a\u7528\u4e8e\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u94fe\u5f0f\u8c03\u7528\u3001\u5de5\u5177\u96c6\u6210\u7b49\u529f\u80fd\u3002</p>"},{"location":"applications/cot-evaluation/langchain/#_3","title":"\u6838\u5fc3\u7ec4\u4ef6","text":"<ol> <li>Chains: \u94fe\u5f0f\u8c03\u7528\u591a\u4e2a\u7ec4\u4ef6</li> <li>Agents: \u667a\u80fd\u4f53\u548c\u5de5\u5177\u4f7f\u7528</li> <li>Memory: \u5bf9\u8bdd\u8bb0\u5fc6\u7ba1\u7406</li> <li>Tools: \u5916\u90e8\u5de5\u5177\u96c6\u6210</li> </ol>"},{"location":"applications/cot-evaluation/langchain/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/cot-evaluation/langchain/#q1-langchain","title":"Q1: LangChain\u7684\u4e3b\u8981\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u4f5c\u7528: - \u7b80\u5316LLM\u5e94\u7528\u5f00\u53d1 - \u63d0\u4f9b\u6807\u51c6\u5316\u7684\u7ec4\u4ef6\u548c\u63a5\u53e3 - \u652f\u6301\u590d\u6742\u7684\u5de5\u4f5c\u6d41\u7f16\u6392</p>"},{"location":"applications/cot-evaluation/langchain/#_5","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u4e86\u89e3LangChain\u7684\u57fa\u672c\u6982\u5ff5</li> <li>[ ] \u7406\u89e3\u94fe\u5f0f\u8c03\u7528\u7684\u4f18\u52bf</li> </ul>"},{"location":"applications/cot-evaluation/langchain/#_6","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u601d\u7ef4\u94fe\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1a\u6a21\u578b\u8bc4\u6d4b</li> <li>\u8fd4\u56de\uff1aCoT\u4e0e\u8bc4\u6d4b\u6982\u89c8</li> </ul>"},{"location":"applications/rag-agent/","title":"\u7b2c6\u8282\uff1aRAG\u4e0eAgent","text":""},{"location":"applications/rag-agent/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u7406\u89e3\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548cAI Agent\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u638c\u63e1\u8fd9\u4e24\u4e2a\u91cd\u8981\u7684LLM\u5e94\u7528\u8303\u5f0f\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - RAG\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u6838\u5fc3\u7ec4\u4ef6 - AI Agent\u4e0e\u4f20\u7edf\u7a0b\u5e8f\u7684\u533a\u522b - \u5411\u91cf\u6570\u636e\u5e93\u7684\u9009\u62e9\u548c\u4f18\u5316</p>"},{"location":"applications/rag-agent/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a1.5\u5929</p> <ul> <li>Day 1: RAG\u6280\u672f\u539f\u7406\u548c\u5b9e\u73b0</li> <li>\u534a\u5929: AI Agent\u6982\u5ff5\u548c\u6846\u67b6</li> </ul>"},{"location":"applications/rag-agent/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"applications/rag-agent/#1-rag","title":"1. RAG\u6280\u672f","text":"<ul> <li>\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u539f\u7406</li> <li>RAG\u7cfb\u7edf\u67b6\u6784</li> <li>\u5411\u91cf\u6570\u636e\u5e93\u5e94\u7528</li> </ul>"},{"location":"applications/rag-agent/#2-ai-agent","title":"2. AI Agent","text":"<ul> <li>Agent\u7684\u5b9a\u4e49\u548c\u7279\u70b9</li> <li>\u591a\u6b65\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528</li> <li>Agent\u6846\u67b6\u548c\u5b9e\u73b0</li> </ul>"},{"location":"applications/rag-agent/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<ul> <li>[ ] \u7406\u89e3RAG\u7684\u5de5\u4f5c\u6d41\u7a0b</li> <li>[ ] \u638c\u63e1Agent\u7684\u57fa\u672c\u6982\u5ff5</li> </ul>"},{"location":"applications/rag-agent/agent/","title":"AI Agent","text":""},{"location":"applications/rag-agent/agent/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3AI Agent\u7684\u6838\u5fc3\u6982\u5ff5\u548c\u5b9e\u73b0\u65b9\u5f0f\u3002</p>"},{"location":"applications/rag-agent/agent/#_2","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"applications/rag-agent/agent/#_3","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>AI-Agent\u7cfb\u5217(\u4e00)\uff1a\u667a\u80fd\u4f53\u8d77\u6e90\u63a2\u7a76 - \u7406\u8bba\u57fa\u7840</li> <li>\u5927\u6a21\u578b-Agent \u9762\u8bd5\u516b\u80a1\u6587 - \u77e5\u4e4e</li> </ol>"},{"location":"applications/rag-agent/agent/#_4","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/rag-agent/agent/#agent","title":"Agent\u662f\u4ec0\u4e48\uff1f","text":"<p>AI Agent \u662f\u80fd\u591f\u611f\u77e5\u73af\u5883\u3001\u505a\u51fa\u51b3\u7b56\u5e76\u6267\u884c\u884c\u52a8\u4ee5\u5b9e\u73b0\u76ee\u6807\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\u3002</p>"},{"location":"applications/rag-agent/agent/#_5","title":"\u6838\u5fc3\u7279\u5f81","text":"<ol> <li>\u81ea\u4e3b\u6027: \u80fd\u591f\u72ec\u7acb\u505a\u51b3\u7b56</li> <li>\u53cd\u5e94\u6027: \u5bf9\u73af\u5883\u53d8\u5316\u505a\u51fa\u54cd\u5e94  </li> <li>\u4e3b\u52a8\u6027: \u4e3b\u52a8\u91c7\u53d6\u884c\u52a8\u5b9e\u73b0\u76ee\u6807</li> <li>\u5b66\u4e60\u6027: \u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u6539\u8fdb</li> </ol>"},{"location":"applications/rag-agent/agent/#_6","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/rag-agent/agent/#q1-ai-agent","title":"Q1: AI Agent\u6709\u4ec0\u4e48\u7279\u70b9\uff1f","text":"<p>\u6838\u5fc3\u7279\u70b9: - \u591a\u6b65\u63a8\u7406: \u80fd\u591f\u5206\u89e3\u590d\u6742\u4efb\u52a1 - \u5de5\u5177\u4f7f\u7528: \u53ef\u4ee5\u8c03\u7528\u5916\u90e8\u5de5\u5177\u548cAPI - \u8bb0\u5fc6\u673a\u5236: \u7ef4\u62a4\u5bf9\u8bdd\u5386\u53f2\u548c\u72b6\u6001 - \u76ee\u6807\u5bfc\u5411: \u671d\u7740\u7279\u5b9a\u76ee\u6807\u6267\u884c\u8ba1\u5212</p>"},{"location":"applications/rag-agent/agent/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3Agent\u7684\u57fa\u672c\u6982\u5ff5\u548c\u7279\u5f81</li> <li>[ ] \u4e86\u89e3Agent\u4e0e\u666e\u901aLLM\u7684\u533a\u522b</li> </ul>"},{"location":"applications/rag-agent/agent/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aRAG\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1aCoT\u4e0e\u8bc4\u6d4b</li> <li>\u8fd4\u56de\uff1aRAG\u4e0eAgent\u6982\u89c8</li> </ul>"},{"location":"applications/rag-agent/rag/","title":"RAG\u6280\u672f","text":""},{"location":"applications/rag-agent/rag/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u5de5\u4f5c\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\u3002</p>"},{"location":"applications/rag-agent/rag/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"applications/rag-agent/rag/#rag_1","title":"RAG\u662f\u4ec0\u4e48\uff1f","text":"<p>Retrieval-Augmented Generation (RAG) \u662f\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u548c\u751f\u6210\u7684\u6280\u672f\uff0c\u901a\u8fc7\u68c0\u7d22\u5916\u90e8\u77e5\u8bc6\u5e93\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56de\u7b54\u80fd\u529b\u3002</p>"},{"location":"applications/rag-agent/rag/#_3","title":"\u6838\u5fc3\u4f18\u52bf","text":"<ol> <li>\u77e5\u8bc6\u66f4\u65b0: \u53ef\u4ee5\u8bbf\u95ee\u6700\u65b0\u4fe1\u606f</li> <li>\u51cf\u5c11\u5e7b\u89c9: \u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u56de\u7b54</li> <li>\u53ef\u8ffd\u6eaf\u6027: \u53ef\u4ee5\u63d0\u4f9b\u4fe1\u606f\u6765\u6e90</li> </ol>"},{"location":"applications/rag-agent/rag/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"applications/rag-agent/rag/#q1-rag","title":"Q1: RAG\u7684\u5de5\u4f5c\u6d41\u7a0b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u6d41\u7a0b: 1. \u7d22\u5f15\u6784\u5efa: \u5c06\u6587\u6863\u5411\u91cf\u5316\u5b58\u50a8 2. \u67e5\u8be2\u68c0\u7d22: \u6839\u636e\u95ee\u9898\u68c0\u7d22\u76f8\u5173\u6587\u6863 3. \u589e\u5f3a\u751f\u6210: \u7ed3\u5408\u68c0\u7d22\u7ed3\u679c\u751f\u6210\u7b54\u6848</p>"},{"location":"applications/rag-agent/rag/#_5","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3RAG\u7684\u57fa\u672c\u5de5\u4f5c\u6d41\u7a0b</li> <li>[ ] \u77e5\u9053RAG\u76f8\u6bd4\u7eafLLM\u7684\u4f18\u52bf</li> </ul>"},{"location":"applications/rag-agent/rag/#_6","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aAI Agent</li> <li>\u8fd4\u56de\uff1aRAG\u4e0eAgent\u6982\u89c8</li> </ul>"},{"location":"code-examples/","title":"\u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"code-examples/#_2","title":"\ud83c\udfaf \u76ee\u6807","text":"<p>\u63d0\u4f9bLLM\u76f8\u5173\u6280\u672f\u7684\u53ef\u6267\u884c\u4ee3\u7801\u793a\u4f8b\uff0c\u5e2e\u52a9\u52a0\u6df1\u7406\u89e3\u3002</p>"},{"location":"code-examples/#_3","title":"\ud83d\udcbb \u4ee3\u7801\u5206\u7c7b","text":""},{"location":"code-examples/#_4","title":"\u57fa\u7840\u5b9e\u73b0","text":"<ul> <li>Self-Attention\u673a\u5236</li> <li>Multi-Head Attention</li> <li>Softmax\u51fd\u6570</li> </ul>"},{"location":"code-examples/#_5","title":"\u4f18\u5316\u6280\u672f","text":"<ul> <li>GQA (Grouped-Query Attention) \u5b9e\u73b0</li> <li>KV Cache\u6f14\u793a\u4ee3\u7801</li> <li>RoPE\u4f4d\u7f6e\u7f16\u7801\u5b8c\u6574\u5b9e\u73b0</li> <li>\u4e09\u79cd\u5f52\u4e00\u5316\u6280\u672f\u5bf9\u6bd4</li> </ul>"},{"location":"code-examples/#_6","title":"\u5e94\u7528\u793a\u4f8b","text":"<ul> <li>RAG\u7cfb\u7edf\u7b80\u5316\u7248</li> <li>\u7b80\u5355Agent\u6846\u67b6</li> <li>\u63d0\u793a\u8bcd\u5de5\u7a0b\u6a21\u677f</li> </ul>"},{"location":"code-examples/#_7","title":"\ud83d\udee0\ufe0f \u5b9e\u8df5\u5efa\u8bae","text":"<ol> <li>\u52a8\u624b\u7f16\u5199: \u4e0d\u8981\u53ea\u770b\u4ee3\u7801\uff0c\u8981\u81ea\u5df1\u5b9e\u73b0</li> <li>\u7406\u89e3\u539f\u7406: \u6bcf\u884c\u4ee3\u7801\u90fd\u8981\u77e5\u9053\u4e3a\u4ec0\u4e48\u8fd9\u6837\u5199</li> <li>\u8c03\u8bd5\u8fd0\u884c: \u786e\u4fdd\u4ee3\u7801\u80fd\u6b63\u786e\u6267\u884c</li> <li>\u6027\u80fd\u5bf9\u6bd4: \u6d4b\u8bd5\u4e0d\u540c\u5b9e\u73b0\u7684\u6548\u679c\u5dee\u5f02</li> </ol>"},{"location":"code-examples/#_8","title":"\ud83d\udccb \u68c0\u9a8c\u6e05\u5355","text":"<ul> <li>[ ] \u5b8c\u6210Self-Attention\u7f16\u7a0b\u7ec3\u4e60</li> <li>[ ] \u5b9e\u73b0KV Cache\u6f14\u793a</li> <li>[ ] \u7f16\u5199RoPE\u4f4d\u7f6e\u7f16\u7801</li> <li>[ ] \u5bf9\u6bd4\u4e0d\u540c\u5f52\u4e00\u5316\u6280\u672f</li> <li>[ ] \u8bbe\u8ba1\u63d0\u793a\u8bcd\u6a21\u677f</li> </ul> <p>\u6240\u6709\u4ee3\u7801\u90fd\u5728\u5bf9\u5e94\u6280\u672f\u7ae0\u8282\u4e2d\u63d0\u4f9b\uff0c\u8fd9\u91cc\u4f5c\u4e3a\u603b\u5165\u53e3\uff01</p>"},{"location":"fundamentals/attention-advanced/","title":"\u7b2c2\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f","text":""},{"location":"fundamentals/attention-advanced/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u73b0\u4ee3\u5927\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u6280\u672f\uff0c\u7406\u89e3\u63a8\u7406\u52a0\u901f\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u5de5\u7a0b\u89e3\u51b3\u65b9\u6848\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - MHA/MQA/GQA/MLA\u7684\u533a\u522b\u548c\u4f18\u52bf - KV Cache\u7684\u5de5\u4f5c\u539f\u7406\u548c\u52a0\u901f\u6548\u679c - LayerNorm vs RMSNorm\u7684\u9009\u62e9 - RoPE\u4f4d\u7f6e\u7f16\u7801\u7684\u6570\u5b66\u539f\u7406</p>"},{"location":"fundamentals/attention-advanced/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a3\u5929</p> <ul> <li>Day 1: \u6ce8\u610f\u529b\u53d8\u4f53\u6df1\u5ea6\u5b66\u4e60 (MHA\u2192MQA\u2192GQA\u2192MLA)</li> <li>Day 2: KV Cache\u6280\u672f + \u5f52\u4e00\u5316\u6280\u672f\u8be6\u89e3</li> <li>Day 3: \u4f4d\u7f6e\u7f16\u7801\u5347\u7ea7 + \u7efc\u5408\u6280\u672f\u5bf9\u6bd4\u5206\u6790</li> </ul>"},{"location":"fundamentals/attention-advanced/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/attention-advanced/#1","title":"1. \u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53","text":"<ul> <li>MHA \u2192 MQA \u2192 GQA \u2192 MLA\u6f14\u8fdb</li> <li>\u6ce8\u610f\u529b\u5934\u6570\u4f18\u5316\u7b56\u7565</li> <li>\u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790</li> </ul>"},{"location":"fundamentals/attention-advanced/#2-kv-cache","title":"2. KV Cache\u6280\u672f","text":"<ul> <li>\u63a8\u7406\u52a0\u901f\u539f\u7406</li> <li>\u5185\u5b58\u4f18\u5316\u7b56\u7565</li> <li>\u5b9e\u73b0\u7ec6\u8282\u548c\u4ee3\u7801\u793a\u4f8b</li> </ul>"},{"location":"fundamentals/attention-advanced/#3","title":"3. \u5f52\u4e00\u5316\u6280\u672f","text":"<ul> <li>BatchNorm vs LayerNorm vs RMSNorm</li> <li>Pre-Norm vs Post-Norm</li> <li>\u8bad\u7ec3\u7a33\u5b9a\u6027\u5206\u6790</li> </ul>"},{"location":"fundamentals/attention-advanced/#4","title":"4. \u4f4d\u7f6e\u7f16\u7801","text":"<ul> <li>\u7edd\u5bf9\u4f4d\u7f6e vs \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801</li> <li>RoPE\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u63a8\u5bfc</li> <li>\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b</li> </ul>"},{"location":"fundamentals/attention-advanced/#_4","title":"\ud83d\udcd6 \u6838\u5fc3\u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/#_5","title":"\u5fc5\u8bfb\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Transformer\u7684Attention\u53ca\u5176\u5404\u79cd\u53d8\u4f53 - \u51b7\u7738\u535a\u5ba2</li> <li>\u7f13\u5b58\u4e0e\u6548\u679c\u7684\u6781\u9650\u62c9\u626f\uff1a\u4eceMHA\u3001MQA\u3001GQA\u5230MLA - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u5927\u6a21\u578b\u4e2d\u5e38\u89c1\u76843\u79cdNorm - \u77e5\u4e4e</li> <li>\u4e3a\u4ec0\u4e48\u5f53\u524d\u4e3b\u6d41\u7684\u5927\u6a21\u578b\u90fd\u4f7f\u7528RMS-Norm\uff1f - \u77e5\u4e4e</li> <li>\u4e3a\u4ec0\u4e48Pre Norm\u7684\u6548\u679c\u4e0d\u5982Post Norm\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> <li>Sinusoidal\u4f4d\u7f6e\u7f16\u7801\u8ffd\u6839\u6eaf\u6e90 - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u535a\u91c7\u4f17\u957f\u7684\u65cb\u8f6c\u5f0f\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4</li> </ol>"},{"location":"fundamentals/attention-advanced/#_6","title":"\u9009\u8bfb\u6df1\u5165\u6750\u6599","text":"<ul> <li>BN\u7a76\u7adf\u8d77\u4e86\u4ec0\u4e48\u4f5c\u7528\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> </ul>"},{"location":"fundamentals/attention-advanced/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u9879\u76ee\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u6280\u672f\u5bf9\u6bd4: \u6e05\u6670\u533a\u5206MHA/MQA/GQA/MLA\u7684\u4f18\u7f3a\u70b9\u548c\u9002\u7528\u573a\u666f</li> <li>\u4ee3\u7801\u5b9e\u73b0: \u5b8c\u6210GQA\u9002\u914d\u548cKV Cache\u6f14\u793a\u4ee3\u7801</li> <li>\u539f\u7406\u7406\u89e3: \u80fd\u4ece\u6570\u5b66\u89d2\u5ea6\u89e3\u91caRoPE\u548cRMSNorm\u7684\u5de5\u4f5c\u539f\u7406</li> <li>\u9762\u8bd5\u51c6\u5907: \u80fd\u89e3\u91ca\u6bcf\u79cd\u6280\u672f\u9009\u62e9\u80cc\u540e\u7684\u5de5\u7a0btrade-off</li> </ol>"},{"location":"fundamentals/attention-advanced/#_8","title":"\ud83d\udca1 \u5b66\u4e60\u63d0\u793a","text":"<p>\u8fd9\u4e00\u8282\u6280\u672f\u542b\u91cf\u5f88\u9ad8\uff0c\u662f\u73b0\u4ee3\u5927\u6a21\u578b\u7684\u6838\u5fc3\u4f18\u5316\u6280\u672f\uff0c\u5efa\u8bae\uff1a - \u5faa\u5e8f\u6e10\u8fdb: \u5148\u7406\u89e3\u57fa\u7840\u6982\u5ff5\uff0c\u518d\u6df1\u5165\u6570\u5b66\u63a8\u5bfc - \u91cd\u70b9\u5173\u6ce8: \u6bcf\u79cd\u6280\u672f\u7684motivation\u548c\u89e3\u51b3\u7684\u5177\u4f53\u95ee\u9898 - \u5bf9\u6bd4\u5b66\u4e60: \u901a\u8fc7\u6280\u672f\u5bf9\u6bd4\u52a0\u6df1\u7406\u89e3\u5404\u81ea\u7684trade-off - \u5b9e\u8df5\u9a8c\u8bc1: \u901a\u8fc7\u4ee3\u7801\u5b9e\u73b0\u52a0\u6df1\u5bf9\u539f\u7406\u7684\u7406\u89e3 - \u9762\u8bd5\u5bfc\u5411: \u91cd\u70b9\u638c\u63e1\u6280\u672f\u9009\u62e9\u7684\u5de5\u7a0b\u8003\u91cf</p>"},{"location":"fundamentals/attention-advanced/#_9","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u611f\u5174\u8da3\u7684\u6280\u672f\u6a21\u5757\u6df1\u5165\u5b66\u4e60\uff0c\u6bcf\u4e2a\u90fd\u662f\u73b0\u4ee3\u5927\u6a21\u578b\u7684\u6838\u5fc3\u6280\u672f\uff01</p>"},{"location":"fundamentals/attention-advanced/kv-cache/","title":"KV Cache\u6280\u672f","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3KV Cache\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u638c\u63e1\u8fd9\u4e2a\u8ba9\u5927\u6a21\u578b\u63a8\u7406\u63d0\u901f\u6570\u500d\u7684\u5173\u952e\u6280\u672f\u3002</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_1","title":"KV Cache\u662f\u4ec0\u4e48\uff1f","text":"<p>\u5b9a\u4e49: KV Cache\u662f\u4e00\u79cd\u63a8\u7406\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u7f13\u5b58\u4e4b\u524d\u8ba1\u7b97\u8fc7\u7684Key\u548cValue\u77e9\u9635\uff0c\u907f\u514d\u91cd\u590d\u8ba1\u7b97\uff0c\u5927\u5e45\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_2","title":"\u4e3a\u4ec0\u4e48\u9700\u8981KV Cache\uff1f","text":"<p>\u95ee\u9898\u80cc\u666f: \u81ea\u56de\u5f52\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u91cd\u590d\u8ba1\u7b97</p> <pre><code># \u751f\u6210\"\u6211\u7231\u5317\u4eac\u5929\u5b89\u95e8\"\u7684\u8fc7\u7a0b\nStep 1: \u8f93\u5165[\"\u6211\"]        \u2192 \u9884\u6d4b\"\u7231\" \nStep 2: \u8f93\u5165[\"\u6211\",\"\u7231\"]    \u2192 \u9884\u6d4b\"\u5317\"\nStep 3: \u8f93\u5165[\"\u6211\",\"\u7231\",\"\u5317\"] \u2192 \u9884\u6d4b\"\u4eac\"\n...\n</code></pre> <p>\u91cd\u590d\u8ba1\u7b97\u95ee\u9898: - \u6bcf\u4e00\u6b65\u90fd\u8981\u91cd\u65b0\u8ba1\u7b97\u6240\u6709previous tokens\u7684K,V\u77e9\u9635 - \u8ba1\u7b97\u590d\u6742\u5ea6: O(n\u00b2)\uff0c\u5176\u4e2dn\u662f\u5e8f\u5217\u957f\u5ea6 - \u5927\u91cf\u91cd\u590d\u8ba1\u7b97\u5bfc\u81f4\u63a8\u7406\u901f\u5ea6\u6162</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_3","title":"KV Cache\u5de5\u4f5c\u539f\u7406","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#1","title":"1. \u4f20\u7edf\u65b9\u5f0f (\u65e0\u7f13\u5b58)","text":"<pre><code># \u6bcf\u6b21\u90fd\u91cd\u65b0\u8ba1\u7b97\u5168\u90e8K,V\ndef generate_token_naive(tokens):\n    # \u91cd\u65b0\u8ba1\u7b97\u6240\u6709token\u7684K,V - \u975e\u5e38\u4f4e\u6548\uff01\n    K = compute_K(tokens)  # \u5305\u542b\u6240\u6709\u5386\u53f2token\n    V = compute_V(tokens)  # \u5305\u542b\u6240\u6709\u5386\u53f2token\n    Q = compute_Q(tokens[-1])  # \u53ea\u9700\u8981\u6700\u540e\u4e00\u4e2atoken\u7684Q\n\n    attention_output = attention(Q, K, V)\n    return next_token\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#2-kv-cache","title":"2. KV Cache\u4f18\u5316\u65b9\u5f0f","text":"<pre><code># \u53ea\u8ba1\u7b97\u65b0token\u7684K,V\uff0c\u590d\u7528\u5386\u53f2\u7f13\u5b58\ndef generate_token_with_cache(new_token, kv_cache):\n    # \u53ea\u8ba1\u7b97\u65b0token\u7684K,V\n    new_K = compute_K(new_token)  \n    new_V = compute_V(new_token)\n\n    # \u66f4\u65b0\u7f13\u5b58\n    kv_cache.append(new_K, new_V)\n\n    # \u4f7f\u7528\u5b8c\u6574\u7684K,V (\u5386\u53f2+\u65b0\u589e)\n    Q = compute_Q(new_token)\n    attention_output = attention(Q, kv_cache.K, kv_cache.V)\n\n    return next_token\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#_3","title":"\u52a0\u901f\u6548\u679c\u5206\u6790","text":"<p>\u65f6\u95f4\u590d\u6742\u5ea6\u5bf9\u6bd4:</p> \u751f\u6210\u6b65\u9aa4 \u65e0Cache \u6709Cache \u52a0\u901f\u6bd4 \u7b2c1\u6b65 O(1) O(1) 1x \u7b2c2\u6b65 O(4) O(1) 4x \u7b2c3\u6b65 O(9) O(1) 9x \u7b2cn\u6b65 O(n\u00b2) O(1) n\u00b2x <p>\u5185\u5b58\u4f7f\u7528: - \u7a7a\u95f4\u6362\u65f6\u95f4\u7684\u7b56\u7565 - \u9700\u8981\u5b58\u50a8: <code>seq_len \u00d7 num_heads \u00d7 head_dim \u00d7 2</code> (K\u548cV) - \u957f\u5e8f\u5217\u65f6\u5185\u5b58\u9700\u6c42\u663e\u8457\u589e\u52a0</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#_4","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#q1-kv-cachekv-cache","title":"Q1: KV Cache\u662f\u4ec0\u4e48\uff0c\u4e3a\u4ec0\u4e48KV Cache\u80fd\u52a0\u901f\u6a21\u578b\u63a8\u7406\uff1f","text":"<p>\u6838\u5fc3\u7b54\u6848:  KV Cache\u662f\u7f13\u5b58\u6ce8\u610f\u529b\u673a\u5236\u4e2dKey\u548cValue\u77e9\u9635\u7684\u6280\u672f\uff0c\u901a\u8fc7\u907f\u514d\u91cd\u590d\u8ba1\u7b97\u5386\u53f2token\u7684K,V\u6765\u52a0\u901f\u63a8\u7406\u3002</p> <p>\u8be6\u7ec6\u89e3\u91ca:</p> <ol> <li>\u95ee\u9898\u6839\u6e90: </li> <li>\u81ea\u56de\u5f52\u751f\u6210\u6bcf\u6b65\u90fd\u9700\u8981\u5b8c\u6574\u7684attention\u8ba1\u7b97</li> <li>\u5386\u53f2token\u7684K,V\u77e9\u9635\u5728\u6bcf\u6b65\u4e2d\u4fdd\u6301\u4e0d\u53d8</li> <li> <p>\u91cd\u590d\u8ba1\u7b97\u9020\u6210O(n\u00b2)\u7684\u65f6\u95f4\u590d\u6742\u5ea6</p> </li> <li> <p>\u89e3\u51b3\u65b9\u6848:</p> </li> <li>\u7f13\u5b58\u5df2\u8ba1\u7b97\u7684K,V\u77e9\u9635</li> <li>\u65b0token\u53ea\u9700\u8ba1\u7b97\u81ea\u5df1\u7684K,V\u5e76\u8ffd\u52a0\u5230\u7f13\u5b58</li> <li> <p>\u5c06\u65f6\u95f4\u590d\u6742\u5ea6\u4eceO(n\u00b2)\u964d\u4f4e\u5230O(1)</p> </li> <li> <p>\u52a0\u901f\u539f\u7406:    <pre><code>\u4f20\u7edf\u65b9\u5f0f: \u6bcf\u6b65\u8ba1\u7b97\u5b8c\u6574\u5e8f\u5217\u7684K,V\n\u7f13\u5b58\u65b9\u5f0f: \u53ea\u8ba1\u7b97\u65b0\u589etoken\u7684K,V\n</code></pre></p> </li> </ol>"},{"location":"fundamentals/attention-advanced/kv-cache/#q2-kv-cache","title":"Q2: KV Cache\u7684\u5185\u5b58\u5f00\u9500\u5982\u4f55\uff1f","text":"<p>\u5185\u5b58\u9700\u6c42\u8ba1\u7b97: <pre><code>memory_per_token = num_layers \u00d7 num_heads \u00d7 head_dim \u00d7 2 \u00d7 dtype_size\ntotal_memory = memory_per_token \u00d7 max_seq_length\n</code></pre></p> <p>\u5177\u4f53\u4f8b\u5b50 (LLaMA-7B): <pre><code>\u53c2\u6570: 32\u5c42, 32\u5934, 128\u7ef4\u5ea6, FP16\n\u6bcf\u4e2atoken: 32 \u00d7 32 \u00d7 128 \u00d7 2 \u00d7 2 bytes = 524KB\n2048\u957f\u5ea6: 524KB \u00d7 2048 \u2248 1GB\n</code></pre></p> <p>\u5185\u5b58\u4f18\u5316\u7b56\u7565: - \u4f7f\u7528\u66f4\u4f4e\u7cbe\u5ea6(FP16/INT8) - \u5206\u5c42\u7f13\u5b58\uff0c\u53ea\u4fdd\u7559\u6700\u8fd1\u7684token - \u6ed1\u52a8\u7a97\u53e3\uff0c\u4e22\u5f03\u8fc7\u65e7\u7684\u7f13\u5b58</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#q3-kv-cache","title":"Q3: KV Cache\u5728\u4e0d\u540c\u6ce8\u610f\u529b\u53d8\u4f53\u4e2d\u7684\u8868\u73b0\uff1f","text":"<p>\u5404\u53d8\u4f53\u7684KV Cache\u9700\u6c42:</p> \u6ce8\u610f\u529b\u7c7b\u578b KV Cache\u5927\u5c0f \u8bf4\u660e MHA <code>h \u00d7 d \u00d7 L</code> \u6bcf\u4e2a\u5934\u72ec\u7acb\u5b58\u50a8K,V MQA <code>d \u00d7 L</code> \u6240\u6709\u5934\u5171\u4eabK,V\uff0c\u5185\u5b58\u51cf\u5c11h\u500d GQA <code>g \u00d7 d \u00d7 L</code> \u5206\u7ec4\u5171\u4eab\uff0c\u5185\u5b58\u9700\u6c42\u5728MHA\u548cMQA\u4e4b\u95f4 MLA \u6700\u5c0f \u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u8fdb\u4e00\u6b65\u538b\u7f29 <p>h=\u5934\u6570, d=\u7ef4\u5ea6, L=\u5e8f\u5217\u957f\u5ea6, g=\u7ec4\u6570</p>"},{"location":"fundamentals/attention-advanced/kv-cache/#_5","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/kv-cache/#kv-cache_4","title":"\u5b8c\u6574KV Cache\u6f14\u793a","text":"<pre><code>import torch\nimport torch.nn as nn\n\nclass KVCache:\n    \"\"\"\u7b80\u5316\u7684KV Cache\u5b9e\u73b0\"\"\"\n\n    def __init__(self, max_seq_len, num_heads, head_dim, device='cpu'):\n        self.max_seq_len = max_seq_len\n        self.num_heads = num_heads\n        self.head_dim = head_dim\n        self.device = device\n\n        # \u9884\u5206\u914d\u7f13\u5b58\u7a7a\u95f4\n        self.cache_k = torch.zeros(\n            max_seq_len, num_heads, head_dim, \n            device=device, dtype=torch.float16\n        )\n        self.cache_v = torch.zeros(\n            max_seq_len, num_heads, head_dim,\n            device=device, dtype=torch.float16\n        )\n\n        self.current_length = 0\n\n    def update_cache(self, new_k, new_v):\n        \"\"\"\u66f4\u65b0\u7f13\u5b58\u5e76\u8fd4\u56de\u5b8c\u6574\u7684K,V\"\"\"\n        batch_size, seq_len, num_heads, head_dim = new_k.shape\n\n        # \u68c0\u67e5\u662f\u5426\u8d85\u51fa\u7f13\u5b58\u5bb9\u91cf\n        if self.current_length + seq_len &gt; self.max_seq_len:\n            raise ValueError(\"Sequence length exceeds cache capacity\")\n\n        # \u66f4\u65b0\u7f13\u5b58\n        end_pos = self.current_length + seq_len\n        self.cache_k[self.current_length:end_pos] = new_k[0]  # \u5047\u8bbebatch_size=1\n        self.cache_v[self.current_length:end_pos] = new_v[0]\n\n        self.current_length = end_pos\n\n        # \u8fd4\u56de\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u5b8c\u6574K,V\n        return (\n            self.cache_k[:self.current_length].unsqueeze(0),  # \u6dfb\u52a0batch\u7ef4\u5ea6\n            self.cache_v[:self.current_length].unsqueeze(0)\n        )\n\n    def clear(self):\n        \"\"\"\u6e05\u7a7a\u7f13\u5b58\"\"\"\n        self.current_length = 0\n\n    def get_cache_info(self):\n        \"\"\"\u83b7\u53d6\u7f13\u5b58\u72b6\u6001\u4fe1\u606f\"\"\"\n        return {\n            'current_length': self.current_length,\n            'capacity': self.max_seq_len,\n            'usage_ratio': self.current_length / self.max_seq_len,\n            'memory_mb': self.cache_k.numel() * 2 * 2 / 1024 / 1024  # FP16\n        }\n\n\nclass AttentionWithKVCache(nn.Module):\n    \"\"\"\u5e26KV Cache\u7684\u6ce8\u610f\u529b\u5c42\"\"\"\n\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        self.kv_cache = None\n\n    def setup_cache(self, max_seq_len, device):\n        \"\"\"\u521d\u59cb\u5316KV Cache\"\"\"\n        self.kv_cache = KVCache(max_seq_len, self.num_heads, self.head_dim, device)\n\n    def forward(self, x, use_cache=False):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u8ba1\u7b97Q, K, V\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        if use_cache and self.kv_cache is not None:\n            # \u4f7f\u7528\u7f13\u5b58\u6a21\u5f0f\uff1a\u66f4\u65b0\u7f13\u5b58\u5e76\u83b7\u53d6\u5b8c\u6574\u7684K,V\n            K, V = self.kv_cache.update_cache(K, V)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attn_weights = torch.softmax(scores, dim=-1)\n        out = torch.matmul(attn_weights, V)\n\n        # \u5408\u5e76\u591a\u5934\u8f93\u51fa\n        out = out.view(batch_size, seq_len, d_model)\n        return self.W_o(out)\n\n\n# \u4f7f\u7528\u793a\u4f8b\ndef demo_kv_cache():\n    \"\"\"KV Cache\u4f7f\u7528\u6f14\u793a\"\"\"\n\n    # \u521d\u59cb\u5316\u6a21\u578b\n    attention = AttentionWithKVCache(d_model=512, num_heads=8)\n    attention.setup_cache(max_seq_len=1024, device='cpu')\n\n    print(\"=== KV Cache\u6f14\u793a ===\")\n\n    # \u6a21\u62df\u751f\u6210\u8fc7\u7a0b\n    vocab_size = 1000\n    sequence = []\n\n    for step in range(5):\n        if step == 0:\n            # \u7b2c\u4e00\u6b65\uff1a\u8f93\u5165\u5b8c\u6574\u7684prompt\n            current_input = torch.randint(0, vocab_size, (1, 3, 512))  # 3\u4e2atoken\u7684prompt\n            print(f\"Step {step}: \u8f93\u5165prompt (3 tokens)\")\n        else:\n            # \u540e\u7eed\u6b65\u9aa4\uff1a\u53ea\u8f93\u5165\u65b0\u751f\u6210\u7684token\n            current_input = torch.randint(0, vocab_size, (1, 1, 512))  # 1\u4e2a\u65b0token\n            print(f\"Step {step}: \u8f93\u5165\u65b0token (1 token)\")\n\n        # \u524d\u5411\u4f20\u64ad\uff08\u4f7f\u7528\u7f13\u5b58\uff09\n        output = attention(current_input, use_cache=True)\n\n        # \u663e\u793a\u7f13\u5b58\u72b6\u6001\n        cache_info = attention.kv_cache.get_cache_info()\n        print(f\"  \u7f13\u5b58\u957f\u5ea6: {cache_info['current_length']}\")\n        print(f\"  \u5185\u5b58\u4f7f\u7528: {cache_info['memory_mb']:.2f} MB\")\n        print()\n\nif __name__ == \"__main__\":\n    demo_kv_cache()\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#_6","title":"\u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5","text":"<pre><code>import time\n\ndef benchmark_with_without_cache():\n    \"\"\"\u5bf9\u6bd4\u6709\u65e0KV Cache\u7684\u6027\u80fd\"\"\"\n\n    d_model, num_heads = 768, 12\n    max_seq_len = 512\n\n    # \u521d\u59cb\u5316\u6a21\u578b\n    attention_with_cache = AttentionWithKVCache(d_model, num_heads)\n    attention_with_cache.setup_cache(max_seq_len, 'cpu')\n\n    attention_without_cache = AttentionWithKVCache(d_model, num_heads)\n\n    # \u6a21\u62df\u5e8f\u5217\u751f\u6210\n    prompt_len = 50\n    generate_len = 100\n\n    print(\"=== \u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5 ===\")\n\n    # \u6d4b\u8bd5\u65e0\u7f13\u5b58\u7248\u672c\n    start_time = time.time()\n    sequence_input = torch.randn(1, prompt_len, d_model)\n\n    for i in range(generate_len):\n        # \u6bcf\u6b21\u90fd\u8f93\u5165\u5b8c\u6574\u5e8f\u5217\uff08\u65e0\u7f13\u5b58\uff09\n        full_input = torch.randn(1, prompt_len + i + 1, d_model)\n        _ = attention_without_cache(full_input, use_cache=False)\n\n    no_cache_time = time.time() - start_time\n    print(f\"\u65e0\u7f13\u5b58\u751f\u6210\u65f6\u95f4: {no_cache_time:.3f}\u79d2\")\n\n    # \u6d4b\u8bd5\u6709\u7f13\u5b58\u7248\u672c  \n    start_time = time.time()\n\n    # \u5904\u7406prompt\n    _ = attention_with_cache(sequence_input, use_cache=True)\n\n    # \u9010\u6b65\u751f\u6210\n    for i in range(generate_len):\n        # \u6bcf\u6b21\u53ea\u8f93\u5165\u65b0token\uff08\u6709\u7f13\u5b58\uff09\n        new_token = torch.randn(1, 1, d_model)\n        _ = attention_with_cache(new_token, use_cache=True)\n\n    with_cache_time = time.time() - start_time\n    print(f\"\u6709\u7f13\u5b58\u751f\u6210\u65f6\u95f4: {with_cache_time:.3f}\u79d2\")\n\n    speedup = no_cache_time / with_cache_time\n    print(f\"\u52a0\u901f\u500d\u6570: {speedup:.1f}x\")\n\nif __name__ == \"__main__\":\n    benchmark_with_without_cache()\n</code></pre>"},{"location":"fundamentals/attention-advanced/kv-cache/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3KV Cache\u7684\u5de5\u4f5c\u539f\u7406\u548c\u52a0\u901f\u673a\u5236</li> <li>[ ] \u80fd\u8ba1\u7b97KV Cache\u7684\u5185\u5b58\u9700\u6c42</li> <li>[ ] \u5b8c\u6210KV Cache\u6f14\u793a\u4ee3\u7801\u7684\u7f16\u5199\u548c\u6d4b\u8bd5</li> <li>[ ] \u7406\u89e3\u4e0d\u540c\u6ce8\u610f\u529b\u53d8\u4f53\u5bf9KV Cache\u7684\u5f71\u54cd</li> </ul>"},{"location":"fundamentals/attention-advanced/kv-cache/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53</li> <li>\u4e0b\u4e00\u8282\uff1a\u5f52\u4e00\u5316\u6280\u672f</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/","title":"\u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4eceMHA\u5230MLA\u7684\u6280\u672f\u6f14\u8fdb\uff0c\u638c\u63e1\u4e0d\u540c\u6ce8\u610f\u529b\u673a\u5236\u7684\u4f18\u5316\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\u3002</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Transformer\u7684Attention\u53ca\u5176\u5404\u79cd\u53d8\u4f53 - \u8be6\u7ec6\u5bf9\u6bd4\u5206\u6790</li> <li>\u7f13\u5b58\u4e0e\u6548\u679c\u7684\u6781\u9650\u62c9\u626f\uff1a\u4eceMHA\u3001MQA\u3001GQA\u5230MLA - \u79d1\u5b66\u7a7a\u95f4\u6df1\u5ea6\u89e3\u6790</li> </ol>"},{"location":"fundamentals/attention-advanced/mha-variants/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#_6","title":"\u6280\u672f\u6f14\u8fdb\u8def\u5f84","text":"<pre><code>MHA (\u6807\u51c6\u591a\u5934) \u2192 MQA (\u5171\u4eabKV) \u2192 GQA (\u5206\u7ec4\u5171\u4eab) \u2192 MLA (\u6f5c\u5728\u7a7a\u95f4)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_7","title":"\u5404\u53d8\u4f53\u8be6\u7ec6\u5bf9\u6bd4","text":"\u53d8\u4f53 KV Cache\u9700\u6c42 \u8ba1\u7b97\u590d\u6742\u5ea6 \u6027\u80fd\u8868\u73b0 \u4e3b\u8981\u5e94\u7528 MHA O(h\u00d7d\u00d7L) O(H x N^2 x D) \u57fa\u51c6\u6027\u80fd \u6807\u51c6Transformer MQA O(d\u00d7L) O(N^2\u00d7D) \u8f7b\u5fae\u4e0b\u964d \u8d44\u6e90\u53d7\u9650\u573a\u666f GQA O(g\u00d7d\u00d7L) O(G\u00d7N^2\u00d7D) \u5e73\u8861\u4f18\u79c0 \u4e3b\u6d41\u5927\u6a21\u578b MLA \u6700\u4f18\u5316 O(R\u00d7N^2\u00d7D) \u63a5\u8fd1MHA \u957f\u4e0a\u4e0b\u6587 <p>\u5176\u4e2d\uff1ah=\u5934\u6570\uff0cd=\u7ef4\u5ea6\uff0cL=\u5e8f\u5217\u957f\u5ea6\uff0cg=\u7ec4\u6570</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_8","title":"\u6838\u5fc3\u6280\u672f\u7ec6\u8282","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#1-mha-multi-head-attention","title":"1. MHA (Multi-Head Attention)","text":"<pre><code># \u6bcf\u4e2a\u5934\u90fd\u6709\u72ec\u7acb\u7684Q\u3001K\u3001V\nfor i in range(num_heads):\n    Q_i = input @ W_Q_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u67e5\u8be2\u77e9\u9635\n    K_i = input @ W_K_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u952e\u77e9\u9635  \n    V_i = input @ W_V_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u503c\u77e9\u9635\n    head_i = attention(Q_i, K_i, V_i)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#2-mqa-multi-query-attention","title":"2. MQA (Multi-Query Attention)","text":"<pre><code># \u6240\u6709\u5934\u5171\u4eabK\u3001V\uff0c\u53ea\u6709Q\u72ec\u7acb\nK_shared = input @ W_K  # \u5171\u4eab\u7684\u952e\u77e9\u9635\nV_shared = input @ W_V  # \u5171\u4eab\u7684\u503c\u77e9\u9635\n\nfor i in range(num_heads):\n    Q_i = input @ W_Q_i  # \u6bcf\u4e2a\u5934\u72ec\u7acb\u7684\u67e5\u8be2\u77e9\u9635\n    head_i = attention(Q_i, K_shared, V_shared)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3-gqa-grouped-query-attention","title":"3. GQA (Grouped-Query Attention)","text":"<pre><code># \u5206\u7ec4\u5171\u4eab\uff1a\u6bcf\u7ec4\u5185\u5171\u4eabK\u3001V\nnum_groups = num_heads // group_size\n\nfor g in range(num_groups):\n    K_g = input @ W_K_g  # \u7ec4\u5171\u4eab\u7684\u952e\u77e9\u9635\n    V_g = input @ W_V_g  # \u7ec4\u5171\u4eab\u7684\u503c\u77e9\u9635\n\n    for i in range(group_size):\n        Q_i = input @ W_Q_i\n        head_i = attention(Q_i, K_g, V_g)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_9","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#q1-mhamqagqamla","title":"Q1: MHA\u3001MQA\u3001GQA\u3001MLA\u90fd\u662f\u4ec0\u4e48\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54\uff1a \u8fd9\u662fTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684\u56db\u4e2a\u6f14\u8fdb\u9636\u6bb5\uff0c\u4e3b\u8981\u4f18\u5316KV Cache\u7684\u5b58\u50a8\u9700\u6c42\uff1a</p> <ul> <li>MHA: \u6807\u51c6\u591a\u5934\u6ce8\u610f\u529b\uff0c\u6bcf\u4e2a\u5934\u72ec\u7acbQKV</li> <li>MQA: \u591a\u67e5\u8be2\u6ce8\u610f\u529b\uff0c\u6240\u6709\u5934\u5171\u4eabKV</li> <li>GQA: \u5206\u7ec4\u67e5\u8be2\u6ce8\u610f\u529b\uff0c\u5206\u7ec4\u5185\u5171\u4eabKV  </li> <li>MLA: \u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff0c\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u4f18\u5316</li> </ul> <p>\u6280\u672f\u7ec6\u8282\uff1a</p> <p>MHA\u95ee\u9898: KV Cache\u968f\u5934\u6570\u7ebf\u6027\u589e\u957f\uff0c\u5185\u5b58\u5f00\u9500\u5927 <pre><code>\u5185\u5b58\u9700\u6c42 = \u5934\u6570 \u00d7 \u7ef4\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6\n</code></pre></p> <p>MQA\u89e3\u51b3\u65b9\u6848: \u5171\u4eabKV\u77e9\u9635\uff0c\u5185\u5b58\u9700\u6c42\u964d\u4f4eh\u500d <pre><code># \u4ece h\u00d7(d_k + d_v) \u964d\u4f4e\u5230 (d_k + d_v)\n</code></pre></p> <p>GQA\u5e73\u8861\u65b9\u6848: \u5206\u7ec4\u5171\u4eab\uff0c\u517c\u987e\u6027\u80fd\u548c\u6548\u7387 <pre><code># \u5185\u5b58\u9700\u6c42 = \u7ec4\u6570 \u00d7 \u7ef4\u5ea6 \u00d7 \u5e8f\u5217\u957f\u5ea6  \n# \u5176\u4e2d\uff1a\u7ec4\u6570 = \u5934\u6570 / \u6bcf\u7ec4\u5934\u6570\n</code></pre></p> <p>MLA\u7ec8\u6781\u4f18\u5316: \u6f5c\u5728\u7a7a\u95f4\u6295\u5f71\uff0c\u6700\u5c0f\u5316KV Cache</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#mla","title":"\ud83d\udd2c MLA\u6280\u672f\u6df1\u5ea6\u89e3\u6790","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#mla_1","title":"MLA\u6838\u5fc3\u521b\u65b0","text":"<p>Multi-head Latent Attention (MLA) \u662fDeepSeek\u56e2\u961f\u63d0\u51fa\u7684\u9769\u547d\u6027\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u4e09\u5927\u521b\u65b0\u663e\u8457\u964d\u4f4eKV Cache\u5185\u5b58\u9700\u6c42\uff1a</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#1-kv","title":"1. \u4f4e\u79e9KV\u8054\u5408\u538b\u7f29","text":"<p>\u6838\u5fc3\u601d\u60f3: \u5c06\u9ad8\u7ef4\u7684Key\u548cValue\u77e9\u9635\u8054\u5408\u538b\u7f29\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4</p> <pre><code># \u4f20\u7edf\u65b9\u5f0f\uff1a\u6bcf\u4e2a\u5934\u72ec\u7acb\u5b58\u50a8KV\ntraditional_kv_cache = num_heads \u00d7 head_dim \u00d7 seq_len \u00d7 2  # K\u548cV\n\n# MLA\u65b9\u5f0f\uff1a\u538b\u7f29\u540e\u7684\u6f5c\u5728\u5411\u91cf\nmla_kv_cache = compressed_dim \u00d7 seq_len + rope_dim \u00d7 seq_len\n</code></pre> <p>\u538b\u7f29\u8fc7\u7a0b: \\(\\(c_t^{KV} = x_t W^{DKV}\\)\\)</p> <p>\u5176\u4e2d \\(W^{DKV} \\in \\mathbb{R}^{d \\times d_c}\\)\uff0c\\(d_c \\ll h \\cdot d_h\\)</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#2-rope","title":"2. RoPE\u89e3\u8026\u673a\u5236","text":"<p>\u95ee\u9898: \u4f4d\u7f6e\u7f16\u7801\u4e0e\u538b\u7f29\u673a\u5236\u7684\u51b2\u7a81 - \u4f20\u7edfRoPE\u9700\u8981\u5728\u539f\u59cbQK\u7a7a\u95f4\u4e2d\u5e94\u7528 - \u538b\u7f29\u7834\u574f\u4e86\u4f4d\u7f6e\u4fe1\u606f\u7684\u6b63\u786e\u4f20\u9012</p> <p>\u89e3\u51b3\u65b9\u6848: \u5c06Query\u548cKey\u5206\u4e3a\u4e24\u90e8\u5206 - \u8bed\u4e49\u90e8\u5206 (\\(q^C, k^C\\)): \u643a\u5e26\u4e3b\u8981\u8bed\u4e49\u4fe1\u606f\uff0c\u53ef\u4ee5\u538b\u7f29 - \u4f4d\u7f6e\u90e8\u5206 (\\(q^R, k^R\\)): \u643a\u5e26\u4f4d\u7f6e\u4fe1\u606f\uff0c\u4fdd\u6301\u539f\u7ef4\u5ea6</p> <pre><code>def mla_with_rope_decoupling(x, position):\n    # 1. \u751f\u6210\u6f5c\u5728\u5411\u91cf\n    c_kv = x @ W_down_kv  # \u538b\u7f29\n\n    # 2. Query\u5206\u79bb\n    q = x @ W_q\n    q_c, q_r = q[:, :d_c], q[:, d_c:]  # \u8bed\u4e49 + \u4f4d\u7f6e\n\n    # 3. Key\u5206\u79bb\u548c\u6062\u590d\n    k_c = c_kv @ W_up_k   # \u4ece\u6f5c\u5728\u7a7a\u95f4\u6062\u590d\u8bed\u4e49Key\n    k_r = x @ W_k_r       # \u76f4\u63a5\u751f\u6210\u4f4d\u7f6eKey\n\n    # 4. \u5206\u522b\u5e94\u7528RoPE\n    q_r = apply_rope(q_r, position)\n    k_r = apply_rope(k_r, position)\n\n    # 5. \u7ec4\u5408\u8ba1\u7b97\n    q_combined = concat([q_c, q_r])\n    k_combined = concat([k_c, k_r])\n    v = c_kv @ W_up_v\n\n    return attention(q_combined, k_combined, v)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3","title":"3. \u6743\u91cd\u5438\u6536\u4f18\u5316","text":"<p>\u76ee\u6807: \u51cf\u5c11\u63a8\u7406\u65f6\u7684\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c</p> <p>\u6280\u672f: \u5229\u7528\u77e9\u9635\u4e58\u6cd5\u7ed3\u5408\u5f8b\uff0c\u9884\u5148\u5408\u5e76\u6743\u91cd\u77e9\u9635</p> <pre><code># \u539f\u59cb\u8ba1\u7b97\uff1a\u4e24\u6b21\u77e9\u9635\u4e58\u6cd5\nc_kv = x @ W_down_kv\nk = c_kv @ W_up_k\n\n# \u6743\u91cd\u5438\u6536\uff1a\u5408\u5e76\u4e3a\u4e00\u6b21\u4e58\u6cd5\nW_combined = W_down_kv @ W_up_k\nk = x @ W_combined\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_10","title":"\u5185\u5b58\u6548\u7387\u5bf9\u6bd4","text":"\u65b9\u6cd5 KV Cache\u5927\u5c0f \u538b\u7f29\u6bd4 MHA \\(2 h \\cdot d_h \\cdot L\\) 1.0\u00d7 (\u57fa\u51c6) MQA \\(2 d_h \\cdot L\\) \\(h\\)\u00d7 GQA \\(2 g \\cdot d_h \\cdot L\\) \\(h/g\\)\u00d7 MLA \\((d_c + d_h^R) \\cdot L\\) ~10-20\u00d7 <p>\u5177\u4f53\u4f8b\u5b50 (LLaMA-7B\u89c4\u6a21): - \u539f\u59cbMHA: 32\u5934 \u00d7 128\u7ef4 \u00d7 2 = 8192\u7ef4/token - MLA\u538b\u7f29: 512\u7ef4 + 128\u7ef4 = 640\u7ef4/token - \u538b\u7f29\u6bd4: 12.8\u00d7</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_11","title":"\u6027\u80fd\u4fdd\u6301\u673a\u5236","text":"<p>\u5c3d\u7ba1\u5927\u5e45\u538b\u7f29\uff0cMLA\u901a\u8fc7\u5de7\u5999\u8bbe\u8ba1\u4fdd\u6301\u4e86\u63a5\u8fd1MHA\u7684\u6027\u80fd\uff1a</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#1","title":"1. \u8868\u8fbe\u80fd\u529b\u4fdd\u6301","text":"<ul> <li>\u4f4e\u79e9\u5047\u8bbe\uff1a\u5927\u90e8\u5206\u6ce8\u610f\u529b\u6a21\u5f0f\u53ef\u4ee5\u7528\u4f4e\u79e9\u77e9\u9635\u8fd1\u4f3c</li> <li>\u5173\u952e\u4fe1\u606f\u4fdd\u7559\uff1a\u4f4d\u7f6e\u4fe1\u606f\u901a\u8fc7\u89e3\u8026\u673a\u5236\u5b8c\u6574\u4fdd\u7559</li> <li>\u6e10\u8fdb\u6062\u590d\uff1a\u591a\u5c42\u5806\u53e0\u9010\u6b65\u6062\u590d\u5b8c\u6574\u4fe1\u606f</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/#2","title":"2. \u8bad\u7ec3\u7a33\u5b9a\u6027","text":"<pre><code># \u6b8b\u5dee\u8fde\u63a5\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\ndef mla_block(x):\n    # MLA\u6ce8\u610f\u529b\n    attn_out = mla_attention(x)\n    x = x + attn_out  # \u6b8b\u5dee\u8fde\u63a5\n\n    # FFN\n    ffn_out = feed_forward(x)\n    x = x + ffn_out   # \u6b8b\u5dee\u8fde\u63a5\n\n    return x\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3_1","title":"3. \u4f4d\u7f6e\u654f\u611f\u6027","text":"<ul> <li>RoPE\u89e3\u8026\u786e\u4fdd\u4f4d\u7f6e\u4fe1\u606f\u4e0d\u4e22\u5931</li> <li>\u4f4d\u7f6e\u7f16\u7801\u7ef4\u5ea6\u53ef\u4ee5\u6839\u636e\u4efb\u52a1\u9700\u6c42\u8c03\u6574</li> <li>\u957f\u5e8f\u5217\u5916\u63a8\u80fd\u529b\u5f97\u5230\u4fdd\u6301</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/#q2","title":"Q2: \u4e3a\u4ec0\u4e48\u9700\u8981\u8fd9\u4e9b\u4f18\u5316\uff1f","text":"<p>\u6838\u5fc3\u52a8\u673a\uff1a</p> <ol> <li>\u5185\u5b58\u74f6\u9888</li> <li>\u957f\u5e8f\u5217\u63a8\u7406\u65f6KV Cache\u5360\u7528\u5927\u91cf\u663e\u5b58</li> <li> <p>\u9650\u5236\u4e86\u6a21\u578b\u7684\u90e8\u7f72\u548c\u6269\u5c55\u80fd\u529b</p> </li> <li> <p>\u63a8\u7406\u901f\u5ea6</p> </li> <li>\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\uff0c\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387</li> <li> <p>\u652f\u6301\u66f4\u5927\u7684batch size</p> </li> <li> <p>\u6210\u672c\u8003\u8651</p> </li> <li>\u964d\u4f4e\u786c\u4ef6\u8981\u6c42</li> <li>\u63d0\u9ad8\u670d\u52a1\u5e76\u53d1\u80fd\u529b</li> </ol>"},{"location":"fundamentals/attention-advanced/mha-variants/#q3","title":"Q3: \u5404\u53d8\u4f53\u7684\u4f18\u7f3a\u70b9\u5bf9\u6bd4\uff1f","text":"\u7ef4\u5ea6 MHA MQA GQA MLA \u6027\u80fd \ud83d\udfe2 \u57fa\u51c6\u6700\u597d \ud83d\udfe1 \u8f7b\u5fae\u4e0b\u964d \ud83d\udfe2 \u63a5\u8fd1MHA \ud83d\udfe2 \u8d85\u8d8aMHA \u5185\u5b58 \ud83d\udd34 \u9700\u6c42\u6700\u9ad8 \ud83d\udfe2 \u663e\u8457\u964d\u4f4e \ud83d\udfe1 \u9002\u4e2d \ud83d\udfe2 \u6700\u4f18 \u901f\u5ea6 \ud83d\udfe1 \u6807\u51c6 \ud83d\udfe2 \u6700\u5feb \ud83d\udfe2 \u8f83\u5feb \ud83d\udfe2 \u4f18\u79c0 \u5b9e\u73b0 \ud83d\udfe2 \u7b80\u5355 \ud83d\udfe2 \u7b80\u5355 \ud83d\udfe1 \u4e2d\u7b49 \ud83d\udd34 \u590d\u6742"},{"location":"fundamentals/attention-advanced/mha-variants/#q4","title":"Q4: \u5982\u4f55\u9009\u62e9\u5408\u9002\u7684\u6ce8\u610f\u529b\u673a\u5236\uff1f","text":"<p>\u9009\u62e9\u7b56\u7565\uff1a</p> <pre><code>if \u8d44\u6e90\u5145\u8db3 and \u8ffd\u6c42\u6700\u4f73\u6027\u80fd:\n    \u9009\u62e9 MHA\nelif \u8d44\u6e90\u4e25\u91cd\u53d7\u9650 and \u53ef\u63a5\u53d7\u6027\u80fd\u635f\u5931:\n    \u9009\u62e9 MQA  \nelif \u9700\u8981\u5e73\u8861\u6027\u80fd\u548c\u6548\u7387:\n    \u9009\u62e9 GQA  # \u4e3b\u6d41\u9009\u62e9\nelif \u957f\u4e0a\u4e0b\u6587 and \u5185\u5b58\u654f\u611f:\n    \u9009\u62e9 MLA\n</code></pre> <p>\u5b9e\u9645\u8003\u8651\u56e0\u7d20\uff1a - \u786c\u4ef6\u5185\u5b58\u9650\u5236 - \u5e8f\u5217\u957f\u5ea6\u9700\u6c42 - \u5ef6\u8fdf\u8981\u6c42 - \u5f00\u53d1\u590d\u6742\u5ea6</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#_12","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/mha-variants/#1-softmax","title":"\u7ec3\u4e601: \u5b9e\u73b0Softmax\u51fd\u6570","text":"<p>\u5e73\u53f0: Deep-ML Softmax</p>"},{"location":"fundamentals/attention-advanced/mha-variants/#2-mhagqa","title":"\u7ec3\u4e602: MHA\u5230GQA\u7684\u9002\u914d","text":"<pre><code>class GroupedQueryAttention(nn.Module):\n    def __init__(self, d_model, num_heads, num_groups):\n        super().__init__()\n        assert num_heads % num_groups == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.num_groups = num_groups\n        self.heads_per_group = num_heads // num_groups\n        self.d_k = d_model // num_heads\n\n        # Q\u77e9\u9635\uff1a\u6bcf\u4e2a\u5934\u72ec\u7acb\n        self.W_q = nn.Linear(d_model, d_model)\n\n        # K,V\u77e9\u9635\uff1a\u6309\u7ec4\u5171\u4eab\n        self.W_k = nn.Linear(d_model, num_groups * self.d_k)\n        self.W_v = nn.Linear(d_model, num_groups * self.d_k)\n\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u751f\u6210Q\uff1a\u6bcf\u4e2a\u5934\u72ec\u7acb\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k)\n\n        # \u751f\u6210K,V\uff1a\u6309\u7ec4\u5171\u4eab\n        K = self.W_k(x).view(batch_size, seq_len, self.num_groups, self.d_k)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_groups, self.d_k)\n\n        # \u91cd\u590dK,V\u4ee5\u5339\u914dQ\u7684\u5934\u6570\n        K = K.repeat_interleave(self.heads_per_group, dim=2)\n        V = V.repeat_interleave(self.heads_per_group, dim=2)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        attn_weights = F.softmax(scores, dim=-1)\n        out = torch.matmul(attn_weights, V)\n\n        # \u5408\u5e76\u591a\u5934\u8f93\u51fa\n        out = out.view(batch_size, seq_len, d_model)\n        return self.W_o(out)\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#3-kv-cache","title":"\u7ec3\u4e603: KV Cache\u5b9e\u73b0\u9884\u89c8","text":"<pre><code>class KVCache:\n    def __init__(self, max_seq_len, num_heads, d_k):\n        self.max_seq_len = max_seq_len\n        self.cache_k = torch.zeros(max_seq_len, num_heads, d_k)\n        self.cache_v = torch.zeros(max_seq_len, num_heads, d_k) \n        self.current_len = 0\n\n    def update(self, new_k, new_v):\n        \"\"\"\u66f4\u65b0\u7f13\u5b58\u5e76\u8fd4\u56de\u5b8c\u6574\u7684K,V\"\"\"\n        seq_len = new_k.size(0)\n\n        # \u5b58\u50a8\u65b0\u7684K,V\n        self.cache_k[self.current_len:self.current_len+seq_len] = new_k\n        self.cache_v[self.current_len:self.current_len+seq_len] = new_v\n\n        self.current_len += seq_len\n\n        # \u8fd4\u56de\u5230\u76ee\u524d\u4e3a\u6b62\u7684\u5b8c\u6574K,V\n        return (self.cache_k[:self.current_len], \n                self.cache_v[:self.current_len])\n</code></pre>"},{"location":"fundamentals/attention-advanced/mha-variants/#_13","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u80fd\u89e3\u91ca\u5404\u53d8\u4f53\u7684\u6838\u5fc3\u533a\u522b</li> <li>[ ] \u7406\u89e3KV Cache\u4f18\u5316\u7684\u539f\u7406</li> <li>[ ] \u5b8c\u6210GQA\u4ee3\u7801\u5b9e\u73b0</li> <li>[ ] \u80fd\u6839\u636e\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u6ce8\u610f\u529b\u673a\u5236</li> </ul>"},{"location":"fundamentals/attention-advanced/mha-variants/#_14","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aKV Cache\u6280\u672f</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/attention-advanced/normalization/","title":"\u5f52\u4e00\u5316\u6280\u672f","text":""},{"location":"fundamentals/attention-advanced/normalization/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u638c\u63e1\u6df1\u5ea6\u5b66\u4e60\u4e2d\u4e09\u5927\u5f52\u4e00\u5316\u6280\u672f\u7684\u539f\u7406\u548c\u5e94\u7528\u573a\u666f\uff0c\u7406\u89e3Pre-Norm\u4e0ePost-Norm\u5728Transformer\u4e2d\u7684\u5f71\u54cd\u3002</p>"},{"location":"fundamentals/attention-advanced/normalization/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/normalization/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>\u5927\u6a21\u578b\u4e2d\u5e38\u89c1\u76843\u79cdNorm - \u77e5\u4e4e</li> <li>\u4e3a\u4ec0\u4e48\u5f53\u524d\u4e3b\u6d41\u7684\u5927\u6a21\u578b\u90fd\u4f7f\u7528RMS-Norm\uff1f - \u77e5\u4e4e  </li> <li>\u4e3a\u4ec0\u4e48Pre Norm\u7684\u6548\u679c\u4e0d\u5982Post Norm\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> </ol>"},{"location":"fundamentals/attention-advanced/normalization/#_5","title":"\u9009\u8bfb\u6df1\u5165\u6750\u6599","text":"<ul> <li>BN\u7a76\u7adf\u8d77\u4e86\u4ec0\u4e48\u4f5c\u7528\uff1f - \u79d1\u5b66\u7a7a\u95f4</li> </ul>"},{"location":"fundamentals/attention-advanced/normalization/#_6","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/normalization/#_7","title":"\u4e09\u5927\u5f52\u4e00\u5316\u6280\u672f\u5bf9\u6bd4","text":"\u6280\u672f \u5f52\u4e00\u5316\u7ef4\u5ea6 \u9002\u7528\u573a\u666f \u4e3b\u8981\u4f18\u52bf \u8ba1\u7b97\u6210\u672c BatchNorm \u8de8\u6837\u672c\u7279\u5f81\u7ef4\u5ea6 CNN\u3001\u5927batch \u8bad\u7ec3\u52a0\u901f\u3001\u9632\u8fc7\u62df\u5408 \u4e2d\u7b49 LayerNorm \u5355\u6837\u672c\u6240\u6709\u7279\u5f81 RNN\u3001Transformer \u4e0d\u4f9d\u8d56batch\u5927\u5c0f \u8f83\u9ad8 RMSNorm \u5355\u6837\u672cRMS\u5f52\u4e00\u5316 \u5927\u578b\u8bed\u8a00\u6a21\u578b \u8ba1\u7b97\u9ad8\u6548\u3001\u6548\u679c\u76f8\u5f53 \u6700\u4f4e"},{"location":"fundamentals/attention-advanced/normalization/#_8","title":"\u6570\u5b66\u516c\u5f0f\u8be6\u89e3","text":""},{"location":"fundamentals/attention-advanced/normalization/#1-batch-normalization","title":"1. Batch Normalization","text":"\\[BN(x) = \u03b3 \u00d7 \\frac{x - \u03bc_B}{\\sqrt{\u03c3_B^2 + \u03b5}} + \u03b2\\] <p>\u6838\u5fc3\u7279\u70b9: - \u03bc_B, \u03c3_B: \u5728batch\u7ef4\u5ea6\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee - \u8bad\u7ec3\u65f6\u4f7f\u7528\u5f53\u524dbatch\u7edf\u8ba1\u91cf\uff0c\u63a8\u7406\u65f6\u4f7f\u7528\u79fb\u52a8\u5e73\u5747 - \u9700\u8981\u03b3(\u7f29\u653e)\u548c\u03b2(\u504f\u79fb)\u53ef\u5b66\u4e60\u53c2\u6570</p> <p>\u95ee\u9898: - \u4f9d\u8d56batch\u5927\u5c0f\uff0c\u5c0fbatch\u6548\u679c\u5dee - \u8bad\u7ec3\u548c\u63a8\u7406\u4e0d\u4e00\u81f4 - \u5728\u5e8f\u5217\u6a21\u578b\u4e2d\u6548\u679c\u4e0d\u4f73</p>"},{"location":"fundamentals/attention-advanced/normalization/#2-layer-normalization","title":"2. Layer Normalization","text":"\\[LN(x) = \u03b3 \u00d7 \\frac{x - \u03bc_L}{\\sqrt{\u03c3_L^2 + \u03b5}} + \u03b2\\] <p>\u6838\u5fc3\u7279\u70b9: - \u03bc_L, \u03c3_L: \u5728\u7279\u5f81\u7ef4\u5ea6\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee - \u6bcf\u4e2a\u6837\u672c\u72ec\u7acb\u5f52\u4e00\u5316\uff0c\u4e0d\u4f9d\u8d56\u5176\u4ed6\u6837\u672c - \u8bad\u7ec3\u548c\u63a8\u7406\u4e00\u81f4</p> <p>\u4f18\u52bf: - \u9002\u5408\u53d8\u957f\u5e8f\u5217 - \u4e0d\u53d7batch\u5927\u5c0f\u5f71\u54cd - Transformer\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/attention-advanced/normalization/#3-rms-normalization","title":"3. RMS Normalization","text":"\\[RMSNorm(x) = \u03b3 \u00d7 \\frac{x}{\\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2 + \u03b5}}\\] <p>\u6838\u5fc3\u7279\u70b9: - \u53ea\u8ba1\u7b97RMS\uff0c\u4e0d\u51cf\u53bb\u5747\u503c - \u7b80\u5316\u4e86LayerNorm\u7684\u8ba1\u7b97 - \u53ea\u9700\u8981\u03b3\u53c2\u6570\uff0c\u65e0\u9700\u03b2</p> <p>\u4f18\u52bf: - \u8ba1\u7b97\u6210\u672c\u66f4\u4f4e - \u5728\u5927\u6a21\u578b\u4e2d\u6548\u679c\u4e0d\u8f93LayerNorm - \u5185\u5b58\u53cb\u597d</p>"},{"location":"fundamentals/attention-advanced/normalization/#pre-norm-vs-post-norm","title":"Pre-Norm vs Post-Norm","text":""},{"location":"fundamentals/attention-advanced/normalization/#post-norm-transformer","title":"Post-Norm (\u539f\u59cbTransformer)","text":"<pre><code>Input \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 LayerNorm \u2192 Output\n</code></pre> <p>\u7279\u70b9: - \u5f52\u4e00\u5316\u5728\u6b8b\u5dee\u8fde\u63a5\u4e4b\u540e - \u9700\u8981\u5b66\u4e60\u7387warmup\u624d\u80fd\u7a33\u5b9a\u8bad\u7ec3 - \u6d45\u5c42\u6a21\u578b(\u22646\u5c42)\u6548\u679c\u66f4\u597d - \u68af\u5ea6\u4f20\u64ad\u53ef\u80fd\u4e0d\u7a33\u5b9a</p>"},{"location":"fundamentals/attention-advanced/normalization/#pre-norm","title":"Pre-Norm (\u73b0\u4ee3\u4e3b\u6d41)","text":"<pre><code>Input \u2192 LayerNorm \u2192 Attention \u2192 Add \u2192 LayerNorm \u2192 FFN \u2192 Add \u2192 Output\n</code></pre> <p>\u7279\u70b9: - \u5f52\u4e00\u5316\u5728\u6b8b\u5dee\u8fde\u63a5\u4e4b\u524d - \u8bad\u7ec3\u66f4\u7a33\u5b9a\uff0c\u65e0\u9700warmup - \u6df1\u5c42\u6a21\u578b\u8bad\u7ec3\u66f4\u5bb9\u6613 - \u73b0\u4ee3\u5927\u6a21\u578b\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/attention-advanced/normalization/#_9","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/normalization/#q1-batch-normlayer-norm","title":"Q1: Batch Norm\u548cLayer Norm\u7684\u533a\u522b\uff1f","text":"<p>\u6838\u5fc3\u533a\u522b:</p> <ol> <li>\u5f52\u4e00\u5316\u7ef4\u5ea6\u4e0d\u540c:</li> <li>BatchNorm: \u5728batch\u7ef4\u5ea6\u5f52\u4e00\u5316\uff0c\u6bcf\u4e2a\u7279\u5f81\u72ec\u7acb</li> <li> <p>LayerNorm: \u5728\u7279\u5f81\u7ef4\u5ea6\u5f52\u4e00\u5316\uff0c\u6bcf\u4e2a\u6837\u672c\u72ec\u7acb</p> </li> <li> <p>\u5e94\u7528\u573a\u666f:</p> </li> <li>BatchNorm: CNN\u3001\u89c6\u89c9\u4efb\u52a1\u3001\u5927batch\u8bad\u7ec3</li> <li> <p>LayerNorm: NLP\u3001\u5e8f\u5217\u6a21\u578b\u3001\u5c0fbatch\u6216\u53d8\u957f\u5e8f\u5217</p> </li> <li> <p>\u4f9d\u8d56\u6027:</p> </li> <li>BatchNorm: \u4f9d\u8d56batch\u5927\u5c0f\u548c\u5176\u4ed6\u6837\u672c</li> <li>LayerNorm: \u53ea\u4f9d\u8d56\u5f53\u524d\u6837\u672c\uff0c\u66f4\u7a33\u5b9a</li> </ol> <p>\u6280\u672f\u7ec6\u8282: <pre><code># BatchNorm: \u5728batch\u7ef4\u5ea6\u8ba1\u7b97\u7edf\u8ba1\u91cf\nbatch_mean = x.mean(dim=0)  # [features]\nbatch_var = x.var(dim=0)    # [features]\n\n# LayerNorm: \u5728\u7279\u5f81\u7ef4\u5ea6\u8ba1\u7b97\u7edf\u8ba1\u91cf  \nlayer_mean = x.mean(dim=-1, keepdim=True)  # [batch, 1]\nlayer_var = x.var(dim=-1, keepdim=True)    # [batch, 1]\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/normalization/#q2-rmsnorm","title":"Q2: \u4e3a\u4ec0\u4e48\u73b0\u5728\u7528RMSNorm\uff1f","text":"<p>\u4e3b\u8981\u539f\u56e0:</p> <ol> <li>\u8ba1\u7b97\u6548\u7387:</li> <li>\u7701\u7565\u4e86\u5747\u503c\u8ba1\u7b97\uff0c\u51cf\u5c11\u4e86\u7ea615%\u7684\u8ba1\u7b97\u91cf</li> <li> <p>\u5185\u5b58\u8bbf\u95ee\u66f4\u5c11\uff0c\u5bf9GPU\u66f4\u53cb\u597d</p> </li> <li> <p>\u6548\u679c\u76f8\u5f53:</p> </li> <li>\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eRMSNorm\u6548\u679c\u4e0d\u8f93LayerNorm</li> <li> <p>\u5728\u5927\u6a21\u578b\u4e2d\u8868\u73b0\u751a\u81f3\u66f4\u597d</p> </li> <li> <p>\u7b80\u5316\u5b9e\u73b0:</p> </li> <li>\u4e0d\u9700\u8981\u03b2\u53c2\u6570\uff0c\u51cf\u5c11\u4e86\u53c2\u6570\u91cf</li> <li>\u6570\u503c\u7a33\u5b9a\u6027\u66f4\u597d</li> </ol> <p>\u6280\u672f\u539f\u7406: <pre><code># LayerNorm\u9700\u8981\u8ba1\u7b97\u5747\u503c\u548c\u65b9\u5dee\nmean = x.mean(dim=-1, keepdim=True)\nvar = x.var(dim=-1, keepdim=True)  \nln_out = gamma * (x - mean) / sqrt(var + eps) + beta\n\n# RMSNorm\u53ea\u9700\u8981\u8ba1\u7b97RMS\nrms = sqrt(x.pow(2).mean(dim=-1, keepdim=True) + eps)\nrms_out = gamma * x / rms\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/normalization/#q3-pre-normpost-norm","title":"Q3: Pre-Norm\u548cPost-Norm\u7684\u4f4d\u7f6e\u533a\u522b\uff1f","text":"<p>\u67b6\u6784\u5bf9\u6bd4:</p> <p>Post-Norm (\u539f\u59cb): <pre><code>x \u2192 Attention \u2192 (+) \u2192 LayerNorm \u2192 FFN \u2192 (+) \u2192 LayerNorm\n    \u2191_______________|              \u2191_______|\n</code></pre></p> <p>Pre-Norm (\u73b0\u4ee3): <pre><code>x \u2192 LayerNorm \u2192 Attention \u2192 (+) \u2192 LayerNorm \u2192 FFN \u2192 (+)\n    \u2191________________________|              \u2191______|\n</code></pre></p> <p>\u8bad\u7ec3\u7a33\u5b9a\u6027\u5dee\u5f02:</p> \u65b9\u9762 Post-Norm Pre-Norm \u5b66\u4e60\u7387warmup \u5fc5\u9700 \u53ef\u9009 \u6df1\u5c42\u8bad\u7ec3 \u5bb9\u6613\u5931\u8d25 \u7a33\u5b9a \u68af\u5ea6\u4f20\u64ad \u53ef\u80fd\u4e0d\u7a33\u5b9a \u66f4\u5e73\u6ed1 \u6536\u655b\u901f\u5ea6 \u8f83\u6162 \u8f83\u5feb \u6700\u7ec8\u6027\u80fd \u6d45\u5c42\u66f4\u597d \u6df1\u5c42\u66f4\u4f18"},{"location":"fundamentals/attention-advanced/normalization/#q4-pre-norm","title":"Q4: \u4e3a\u4ec0\u4e48Pre-Norm\u8bad\u7ec3\u66f4\u7a33\u5b9a\uff1f","text":"<p>\u68af\u5ea6\u4f20\u64ad\u5206\u6790:</p> <ol> <li>Post-Norm\u95ee\u9898:</li> <li>\u68af\u5ea6\u9700\u8981\u7ecf\u8fc7LayerNorm\u7684\u53cd\u5411\u4f20\u64ad</li> <li>LayerNorm\u7684\u5bfc\u6570\u53ef\u80fd\u653e\u5927\u6216\u7f29\u5c0f\u68af\u5ea6</li> <li> <p>\u6df1\u5c42\u7f51\u7edc\u5bb9\u6613\u51fa\u73b0\u68af\u5ea6\u7206\u70b8/\u6d88\u5931</p> </li> <li> <p>Pre-Norm\u4f18\u52bf:</p> </li> <li>\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u6052\u7b49\u8def\u5f84(identity path)</li> <li>\u68af\u5ea6\u53ef\u4ee5\u66f4\u76f4\u63a5\u5730\u53cd\u5411\u4f20\u64ad</li> <li>\u5728\u6df1\u5c42\u7f51\u7edc\u4e2d\u68af\u5ea6\u7684\u6570\u91cf\u7ea7\u4e3a\u221aL (L\u4e3a\u5c42\u6570)</li> </ol> <p>\u6570\u5b66\u76f4\u89c9: <pre><code>Post-Norm: \u68af\u5ea6\u9700\u8981\u7a7f\u8fc7LayerNorm\n\u2207L/\u2202x = \u2207L/\u2202norm \u00d7 \u2202norm/\u2202x  (\u4e0d\u7a33\u5b9a)\n\nPre-Norm: \u6052\u7b49\u8def\u5f84\u66f4\u5f3a  \n\u2207L/\u2202x = \u2207L/\u2202residual + \u2207L/\u2202processed  (\u66f4\u7a33\u5b9a)\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/normalization/#_10","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/normalization/#normpytorch","title":"\u4e09\u79cdNorm\u7684PyTorch\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass BatchNorm1d(nn.Module):\n    \"\"\"\u81ea\u5b9e\u73b0BatchNorm\"\"\"\n    def __init__(self, num_features, eps=1e-5, momentum=0.9):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n\n        # \u53ef\u5b66\u4e60\u53c2\u6570\n        self.gamma = nn.Parameter(torch.ones(num_features))\n        self.beta = nn.Parameter(torch.zeros(num_features))\n\n        # \u79fb\u52a8\u5e73\u5747\u7edf\u8ba1\u91cf\n        self.register_buffer('running_mean', torch.zeros(num_features))\n        self.register_buffer('running_var', torch.ones(num_features))\n\n    def forward(self, x):\n        # x shape: [batch_size, num_features]\n        if self.training:\n            # \u8ba1\u7b97\u5f53\u524dbatch\u7684\u7edf\u8ba1\u91cf\n            batch_mean = x.mean(dim=0)\n            batch_var = x.var(dim=0, unbiased=False)\n\n            # \u66f4\u65b0\u79fb\u52a8\u5e73\u5747\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * batch_var\n\n            # \u4f7f\u7528\u5f53\u524dbatch\u7edf\u8ba1\u91cf\u5f52\u4e00\u5316\n            mean, var = batch_mean, batch_var\n        else:\n            # \u63a8\u7406\u65f6\u4f7f\u7528\u79fb\u52a8\u5e73\u5747\n            mean, var = self.running_mean, self.running_var\n\n        # \u5f52\u4e00\u5316\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        return self.gamma * x_norm + self.beta\n\nclass LayerNorm(nn.Module):\n    \"\"\"\u81ea\u5b9e\u73b0LayerNorm\"\"\"\n    def __init__(self, normalized_shape, eps=1e-5):\n        super().__init__()\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n\n    def forward(self, x):\n        # \u5728\u6700\u540e\u4e00\u4e2a\u7ef4\u5ea6\u8ba1\u7b97\u7edf\u8ba1\u91cf\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n\n        # \u5f52\u4e00\u5316\n        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n        return self.gamma * x_norm + self.beta\n\nclass RMSNorm(nn.Module):\n    \"\"\"\u81ea\u5b9e\u73b0RMSNorm\"\"\"\n    def __init__(self, normalized_shape, eps=1e-5):\n        super().__init__()\n        self.normalized_shape = normalized_shape\n        self.eps = eps\n\n        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n\n    def forward(self, x):\n        # \u8ba1\u7b97RMS\n        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n\n        # RMS\u5f52\u4e00\u5316\n        x_norm = x / rms\n        return self.gamma * x_norm\n\n# \u4f7f\u7528\u793a\u4f8b\u548c\u6027\u80fd\u5bf9\u6bd4\ndef compare_normalization():\n    \"\"\"\u5bf9\u6bd4\u4e09\u79cd\u5f52\u4e00\u5316\u7684\u8ba1\u7b97\u6210\u672c\"\"\"\n\n    batch_size, seq_len, d_model = 32, 512, 768\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # \u521d\u59cb\u5316\u4e09\u79cdnorm\n    bn = BatchNorm1d(d_model)\n    ln = LayerNorm(d_model)\n    rms = RMSNorm(d_model)\n\n    print(\"=== \u5f52\u4e00\u5316\u6280\u672f\u5bf9\u6bd4 ===\")\n\n    # \u6d4b\u8bd5LayerNorm\n    import time\n    start_time = time.time()\n    for _ in range(1000):\n        _ = ln(x)\n    ln_time = time.time() - start_time\n    print(f\"LayerNorm\u8017\u65f6: {ln_time:.4f}\u79d2\")\n\n    # \u6d4b\u8bd5RMSNorm\n    start_time = time.time()\n    for _ in range(1000):\n        _ = rms(x)\n    rms_time = time.time() - start_time\n    print(f\"RMSNorm\u8017\u65f6: {rms_time:.4f}\u79d2\")\n\n    speedup = ln_time / rms_time\n    print(f\"RMSNorm\u52a0\u901f\u500d\u6570: {speedup:.2f}x\")\n\n    # \u53c2\u6570\u91cf\u5bf9\u6bd4\n    ln_params = sum(p.numel() for p in ln.parameters())\n    rms_params = sum(p.numel() for p in rms.parameters())\n    print(f\"LayerNorm\u53c2\u6570\u91cf: {ln_params}\")\n    print(f\"RMSNorm\u53c2\u6570\u91cf: {rms_params}\")\n    print(f\"\u53c2\u6570\u51cf\u5c11: {(ln_params - rms_params) / ln_params * 100:.1f}%\")\n\n# Pre-Norm vs Post-Norm\u5b9e\u73b0\nclass PostNormBlock(nn.Module):\n    \"\"\"Post-Norm Transformer Block\"\"\"\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model)\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        # Post-Norm: Attention \u2192 Add \u2192 Norm\n        attn_out, _ = self.attention(x, x, x)\n        x = self.norm1(x + attn_out)\n\n        # FFN \u2192 Add \u2192 Norm\n        ffn_out = self.ffn(x)\n        x = self.norm2(x + ffn_out)\n        return x\n\nclass PreNormBlock(nn.Module):\n    \"\"\"Pre-Norm Transformer Block\"\"\"\n    def __init__(self, d_model, num_heads):\n        super().__init__()\n        self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n        self.ffn = nn.Sequential(\n            nn.Linear(d_model, 4 * d_model),\n            nn.ReLU(),\n            nn.Linear(4 * d_model, d_model)\n        )\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, x):\n        # Pre-Norm: Norm \u2192 Attention \u2192 Add\n        norm_x = self.norm1(x)\n        attn_out, _ = self.attention(norm_x, norm_x, norm_x)\n        x = x + attn_out\n\n        # Norm \u2192 FFN \u2192 Add\n        norm_x = self.norm2(x)\n        ffn_out = self.ffn(norm_x)\n        x = x + ffn_out\n        return x\n\nif __name__ == \"__main__\":\n    compare_normalization()\n</code></pre>"},{"location":"fundamentals/attention-advanced/normalization/#_11","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u4e09\u79cd\u5f52\u4e00\u5316\u7684\u6570\u5b66\u539f\u7406\u548c\u8ba1\u7b97\u65b9\u5f0f</li> <li>[ ] \u80fd\u89e3\u91ca\u4e3a\u4ec0\u4e48Transformer\u9009\u62e9LayerNorm\u800c\u975eBatchNorm</li> <li>[ ] \u638c\u63e1Pre-Norm\u76f8\u6bd4Post-Norm\u7684\u8bad\u7ec3\u4f18\u52bf</li> <li>[ ] \u5b8c\u6210\u5f52\u4e00\u5316\u6280\u672f\u7684\u4ee3\u7801\u5b9e\u73b0\u548c\u6027\u80fd\u5bf9\u6bd4</li> </ul>"},{"location":"fundamentals/attention-advanced/normalization/#_12","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aKV Cache\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1a\u4f4d\u7f6e\u7f16\u7801</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/attention-advanced/positional-encoding/","title":"\u4f4d\u7f6e\u7f16\u7801","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4f4d\u7f6e\u7f16\u7801\u5728Transformer\u4e2d\u7684\u4f5c\u7528\uff0c\u638c\u63e1\u4ece\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u5230RoPE\u7684\u6280\u672f\u6f14\u8fdb\uff0c\u80fd\u591f\u63a8\u5bfcRoPE\u7684\u6570\u5b66\u539f\u7406\u3002</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#_4","title":"\u6838\u5fc3\u6280\u672f\u6587\u7ae0","text":"<ol> <li>Sinusoidal\u4f4d\u7f6e\u7f16\u7801\u8ffd\u6839\u6eaf\u6e90 - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u535a\u91c7\u4f17\u957f\u7684\u65cb\u8f6c\u5f0f\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4</li> <li>\u8ba9\u7814\u7a76\u4eba\u5458\u7ede\u5c3d\u8111\u6c41\u7684Transformer\u4f4d\u7f6e\u7f16\u7801 - \u79d1\u5b66\u7a7a\u95f4</li> </ol>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#_6","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u4f4d\u7f6e\u7f16\u7801\uff1f","text":"<p>\u6838\u5fc3\u95ee\u9898: Transformer\u7684Self-Attention\u673a\u5236\u662f\u7f6e\u6362\u4e0d\u53d8\u7684\uff08permutation invariant\uff09\uff0c\u65e0\u6cd5\u533a\u5206token\u7684\u987a\u5e8f\u3002</p> <pre><code># \u6ca1\u6709\u4f4d\u7f6e\u4fe1\u606f\u65f6\uff0c\u8fd9\u4e24\u4e2a\u5e8f\u5217\u662f\u7b49\u4ef7\u7684\nsequence1 = [\"\u6211\", \"\u7231\", \"\u5317\u4eac\"]\nsequence2 = [\"\u7231\", \"\u5317\u4eac\", \"\u6211\"]\n# Self-Attention\u4f1a\u7ed9\u51fa\u76f8\u540c\u7684\u7ed3\u679c\uff01\n</code></pre> <p>\u89e3\u51b3\u65b9\u6848: \u5728\u8f93\u5165\u4e2d\u6ce8\u5165\u4f4d\u7f6e\u4fe1\u606f\uff0c\u8ba9\u6a21\u578b\u80fd\u591f\u7406\u89e3token\u4e4b\u95f4\u7684\u76f8\u5bf9\u6216\u7edd\u5bf9\u4f4d\u7f6e\u5173\u7cfb\u3002</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_7","title":"\u4f4d\u7f6e\u7f16\u7801\u5206\u7c7b","text":"<pre><code>\u4f4d\u7f6e\u7f16\u7801\n\u251c\u2500\u2500 \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 (APE)\n\u2502   \u251c\u2500\u2500 \u53ef\u8bad\u7ec3\u4f4d\u7f6e\u7f16\u7801 (Learned PE)\n\u2502   \u2514\u2500\u2500 \u56fa\u5b9a\u4f4d\u7f6e\u7f16\u7801 (Sinusoidal PE)\n\u2514\u2500\u2500 \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 (RPE)\n    \u251c\u2500\u2500 \u7ecf\u5178\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\n    \u251c\u2500\u2500 \u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801 (RoPE)\n    \u2514\u2500\u2500 \u5176\u4ed6\u53d8\u4f53 (ALiBi\u7b49)\n</code></pre>"},{"location":"fundamentals/attention-advanced/positional-encoding/#vs","title":"\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 vs \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801","text":"\u7ef4\u5ea6 \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 \u7f16\u7801\u5bf9\u8c61 token\u7684\u7edd\u5bf9\u4f4d\u7f6e token\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb \u64cd\u4f5c\u4f4d\u7f6e \u8f93\u5165\u5c42\u6dfb\u52a0\u4f4d\u7f6e\u5411\u91cf \u6ce8\u610f\u529b\u5c42\u4fee\u6539\u8ba1\u7b97\u65b9\u5f0f \u5b9e\u73b0\u590d\u6742\u5ea6 \u7b80\u5355 \u76f8\u5bf9\u590d\u6742 \u957f\u5ea6\u5916\u63a8 \u8f83\u5dee \u8f83\u597d \u6027\u80fd\u8868\u73b0 \u77ed\u5e8f\u5217\u8db3\u591f \u957f\u5e8f\u5217\u66f4\u4f18"},{"location":"fundamentals/attention-advanced/positional-encoding/#_8","title":"\u6838\u5fc3\u6280\u672f\u8be6\u89e3","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#1-sinusoidal-transformer","title":"1. Sinusoidal\u4f4d\u7f6e\u7f16\u7801 (\u539f\u59cbTransformer)","text":"<p>\u6570\u5b66\u516c\u5f0f: \\(\\(PE(pos, 2i) = sin(pos / 10000^{2i/d_{model}})\\)\\) \\(\\(PE(pos, 2i+1) = cos(pos / 10000^{2i/d_{model}})\\)\\)</p> <p>\u6838\u5fc3\u7279\u70b9: - \u4f7f\u7528\u6b63\u5f26\u4f59\u5f26\u51fd\u6570\u751f\u6210\u4f4d\u7f6e\u7f16\u7801 - \u4e0d\u540c\u7ef4\u5ea6\u4f7f\u7528\u4e0d\u540c\u7684\u9891\u7387 - \u56fa\u5b9a\u7f16\u7801\uff0c\u4e0d\u9700\u8981\u8bad\u7ec3 - \u7406\u8bba\u4e0a\u652f\u6301\u4efb\u610f\u957f\u5ea6\u5e8f\u5217</p> <p>\u4f18\u52bf: - \u8ba1\u7b97\u7b80\u5355\uff0c\u4e0d\u5360\u7528\u53c2\u6570 - \u5177\u6709\u4e00\u5b9a\u7684\u5916\u63a8\u80fd\u529b - \u76f8\u5bf9\u4f4d\u7f6e\u6709\u4e00\u5b9a\u7684\u89c4\u5f8b\u6027</p> <p>\u7f3a\u70b9: - \u4f4d\u7f6e\u4fe1\u606f\u5728\u6df1\u5c42\u53ef\u80fd\u8870\u51cf - \u5bf9\u76f8\u5bf9\u4f4d\u7f6e\u7684\u5efa\u6a21\u4e0d\u591f\u76f4\u63a5</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#2","title":"2. \u53ef\u8bad\u7ec3\u4f4d\u7f6e\u7f16\u7801","text":"<p>\u5b9e\u73b0\u65b9\u5f0f: <pre><code># \u4e3a\u6bcf\u4e2a\u4f4d\u7f6e\u5b66\u4e60\u4e00\u4e2a\u5411\u91cf\nposition_embeddings = nn.Embedding(max_seq_len, d_model)\npos_emb = position_embeddings(position_ids)\ninput_emb = token_emb + pos_emb\n</code></pre></p> <p>\u7279\u70b9: - \u6bcf\u4e2a\u4f4d\u7f6e\u5bf9\u5e94\u4e00\u4e2a\u53ef\u5b66\u4e60\u7684\u5411\u91cf - \u901a\u8fc7\u8bad\u7ec3\u4f18\u5316\u4f4d\u7f6e\u8868\u793a - \u5728\u8bad\u7ec3\u957f\u5ea6\u8303\u56f4\u5185\u6548\u679c\u901a\u5e38\u66f4\u597d</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#3-rope","title":"3. RoPE (\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801)","text":"<p>\u6838\u5fc3\u601d\u60f3: \u901a\u8fc7\u65cb\u8f6c\u53d8\u6362\u5c06\u4f4d\u7f6e\u4fe1\u606f\u7f16\u7801\u5230\u67e5\u8be2\u548c\u952e\u5411\u91cf\u4e2d\uff0c\u4f7f\u5f97\u6ce8\u610f\u529b\u5206\u6570\u81ea\u7136\u5730\u4f9d\u8d56\u4e8e\u76f8\u5bf9\u4f4d\u7f6e\u3002</p> <p>\u6570\u5b66\u63a8\u5bfc:</p> <p>\u6b65\u9aa41: \u5c06\u7279\u5f81\u5206\u4e3apairs\uff0c\u6bcf\u5bf9\u7279\u5f81\u770b\u4f5c2D\u5e73\u9762\u7684\u5750\u6807 \\(\\(x = [x_1, x_2, x_3, x_4, ...] \u2192 [(x_1, x_2), (x_3, x_4), ...]\\)\\)</p> <p>\u6b65\u9aa42: \u5bf9\u6bcf\u4e00\u5bf9\u7279\u5f81\u5e94\u7528\u65cb\u8f6c\u77e9\u9635 \\(\\(\\begin{pmatrix} x_{m}^{(1)} \\\\ x_{m}^{(2)} \\end{pmatrix} \u2192 \\begin{pmatrix} \\cos(m\\theta) &amp; -\\sin(m\\theta) \\\\ \\sin(m\\theta) &amp; \\cos(m\\theta) \\end{pmatrix} \\begin{pmatrix} x_{m}^{(1)} \\\\ x_{m}^{(2)} \\end{pmatrix}\\)\\)</p> <p>\u6b65\u9aa43: \u65cb\u8f6c\u540e\u7684\u5411\u91cf \\(\\(\\begin{pmatrix} x_{m}^{(1)} \\cos(m\\theta) - x_{m}^{(2)} \\sin(m\\theta) \\\\ x_{m}^{(2)} \\cos(m\\theta) + x_{m}^{(1)} \\sin(m\\theta) \\end{pmatrix}\\)\\)</p> <p>\u6838\u5fc3\u6027\u8d28: \u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56 \\(\\(\\langle RoPE(q_m), RoPE(k_n) \\rangle = \\langle q_m, k_n \\rangle \\cos((m-n)\\theta) + \\text{\u5176\u4ed6\u9879}\\)\\)</p> <p>\u6ce8\u610f\u529b\u5206\u6570\u53ea\u4f9d\u8d56\u4e8e\u76f8\u5bf9\u8ddd\u79bb (m-n)\uff01</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_9","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#q1","title":"Q1: \u4ec0\u4e48\u662f\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff1f","text":"<p>\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801 (APE): - \u5b9a\u4e49: \u4e3a\u6bcf\u4e2atoken\u7684\u7edd\u5bf9\u4f4d\u7f6e\u5206\u914d\u4e00\u4e2a\u4f4d\u7f6e\u5411\u91cf - \u5b9e\u73b0: \u5728\u8f93\u5165\u5c42\u5c06\u4f4d\u7f6e\u5411\u91cf\u52a0\u5230token embedding\u4e0a - \u7279\u70b9: \u7b80\u5355\u76f4\u63a5\uff0c\u6bcf\u4e2a\u4f4d\u7f6e\u6709\u56fa\u5b9a\u7684\u7f16\u7801</p> <p>\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801 (RPE): - \u5b9a\u4e49: \u5728\u8ba1\u7b97\u6ce8\u610f\u529b\u65f6\u8003\u8651token\u4e4b\u95f4\u7684\u76f8\u5bf9\u8ddd\u79bb - \u5b9e\u73b0: \u4fee\u6539\u6ce8\u610f\u529b\u8ba1\u7b97\u516c\u5f0f\uff0c\u52a0\u5165\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e - \u7279\u70b9: \u66f4\u7b26\u5408\u76f4\u89c9\uff0c\u5916\u63a8\u80fd\u529b\u66f4\u5f3a</p> <p>\u6280\u672f\u7ec6\u8282\u5bf9\u6bd4: <pre><code># \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\ninput_emb = token_emb + position_emb[pos]\n\n# \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801  \nattention_score = QK^T + relative_position_bias[i-j]\n</code></pre></p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#q2-rope","title":"Q2: \u63a8\u5bfcRoPE\u7684\u6570\u5b66\u539f\u7406","text":"<p>\u63a8\u5bfc\u6b65\u9aa4:</p> <p>\u76ee\u6807: \u8bbe\u8ba1\u4e00\u4e2a\u51fd\u6570f\uff0c\u4f7f\u5f97\uff1a \\(\\(\\langle f(q, m), f(k, n) \\rangle = g(q, k, m-n)\\)\\) \u5373\u6ce8\u610f\u529b\u5206\u6570\u53ea\u4f9d\u8d56\u76f8\u5bf9\u4f4d\u7f6e m-n\u3002</p> <p>\u89e3\u51b3\u65b9\u6848: \u590d\u6570\u57df\u7684\u65cb\u8f6c\u53d8\u6362</p> <p>\u6b65\u9aa41: \u5c06\u5b9e\u6570\u5411\u91cf\u6620\u5c04\u5230\u590d\u6570 \\(\\(q_{1} + i q_{2} \u2192 q_{complex}\\)\\)</p> <p>\u6b65\u9aa42: \u5e94\u7528\u590d\u6570\u65cb\u8f6c \\(\\(f(q, m) = q_{complex} \\cdot e^{im\\theta} = q_{complex} \\cdot (\\cos(m\\theta) + i\\sin(m\\theta))\\)\\)</p> <p>\u6b65\u9aa43: \u9a8c\u8bc1\u76f8\u5bf9\u4f4d\u7f6e\u6027\u8d28 \\(\\(\\langle f(q,m), f(k,n) \\rangle^* = \\langle q \\cdot e^{im\\theta}, k \\cdot e^{in\\theta} \\rangle = \\langle q, k \\rangle \\cdot e^{i(m-n)\\theta}\\)\\)</p> <p>\u53ea\u4f9d\u8d56\u4e8e (m-n)\uff01</p> <p>\u6b65\u9aa44: \u8f6c\u6362\u56de\u5b9e\u6570\u57df \\(\\(\\begin{pmatrix} q_1 \\cos(m\\theta) - q_2 \\sin(m\\theta) \\\\ q_1 \\sin(m\\theta) + q_2 \\cos(m\\theta) \\end{pmatrix}\\)\\)</p> <p>\u5173\u952e\u6d1e\u5bdf: \u901a\u8fc7\u65cb\u8f6c\u53d8\u6362\uff0c\u76f8\u5bf9\u4f4d\u7f6e\u4fe1\u606f\u81ea\u7136\u5730\u7f16\u7801\u5728\u4e86\u5411\u91cf\u7684\u51e0\u4f55\u5173\u7cfb\u4e2d\u3002</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#q3-rope","title":"Q3: RoPE\u76f8\u6bd4\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801\u7684\u4f18\u52bf\uff1f","text":"<p>\u6838\u5fc3\u4f18\u52bf:</p> <ol> <li>\u81ea\u7136\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56</li> <li>\u6ce8\u610f\u529b\u5206\u6570\u76f4\u63a5\u4f9d\u8d56\u76f8\u5bf9\u8ddd\u79bb</li> <li> <p>\u65e0\u9700\u989d\u5916\u7684\u76f8\u5bf9\u4f4d\u7f6e\u504f\u7f6e\u9879</p> </li> <li> <p>\u4f18\u79c0\u7684\u5916\u63a8\u80fd\u529b</p> </li> <li>\u8bad\u7ec3\u65f6\u7684\u76f8\u5bf9\u4f4d\u7f6e\u6a21\u5f0f\u53ef\u4ee5\u6cdb\u5316\u5230\u66f4\u957f\u5e8f\u5217</li> <li> <p>\u7406\u8bba\u4e0a\u652f\u6301\u65e0\u9650\u957f\u5ea6\u5916\u63a8</p> </li> <li> <p>\u8ba1\u7b97\u9ad8\u6548</p> </li> <li>\u65e0\u9700\u5b58\u50a8\u4f4d\u7f6e\u5d4c\u5165\u8868</li> <li> <p>\u65cb\u8f6c\u64cd\u4f5c\u53ef\u4ee5\u9ad8\u6548\u5b9e\u73b0</p> </li> <li> <p>\u7406\u8bba\u4f18\u96c5</p> </li> <li>\u6570\u5b66\u57fa\u7840\u624e\u5b9e</li> <li>\u57fa\u4e8e\u590d\u6570\u65cb\u8f6c\u7684\u51e0\u4f55\u76f4\u89c9</li> </ol> <p>\u5b9e\u9a8c\u9a8c\u8bc1: - \u5728\u591a\u4e2aNLP\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4f20\u7edf\u4f4d\u7f6e\u7f16\u7801 - \u957f\u5e8f\u5217\u4efb\u52a1\u4e0a\u8868\u73b0\u7279\u522b\u7a81\u51fa - \u5df2\u88ab\u591a\u4e2a\u5927\u6a21\u578b\u91c7\u7528(LLaMA\u3001PaLM\u7b49)</p>"},{"location":"fundamentals/attention-advanced/positional-encoding/#q4-rope","title":"Q4: RoPE\u5728\u5b9e\u9645\u5b9e\u73b0\u4e2d\u6709\u4ec0\u4e48\u6280\u5de7\uff1f","text":"<p>\u5b9e\u73b0\u4f18\u5316:</p> <ol> <li> <p>\u9891\u7387\u9009\u62e9 <pre><code># \u4e0d\u540c\u7ef4\u5ea6\u4f7f\u7528\u4e0d\u540c\u9891\u7387\ntheta = 10000 ** (-2 * torch.arange(0, dim, 2) / dim)\n</code></pre></p> </li> <li> <p>\u9884\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635 <pre><code># \u907f\u514d\u91cd\u590d\u8ba1\u7b97sin/cos\ncos_cached = torch.cos(position * theta)\nsin_cached = torch.sin(position * theta)\n</code></pre></p> </li> <li> <p>\u5411\u91cf\u5316\u5b9e\u73b0 <pre><code># \u540c\u65f6\u5904\u7406\u6240\u6709\u4f4d\u7f6e\u548c\u7ef4\u5ea6\nq_rot = q * cos_cached - q_shifted * sin_cached\n</code></pre></p> </li> </ol>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_10","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/attention-advanced/positional-encoding/#rope","title":"RoPE\u5b8c\u6574\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass RotaryPositionalEmbedding(nn.Module):\n    \"\"\"RoPE (Rotary Position Embedding) \u5b9e\u73b0\"\"\"\n\n    def __init__(self, dim, max_seq_len=2048, base=10000):\n        super().__init__()\n        self.dim = dim\n        self.max_seq_len = max_seq_len\n        self.base = base\n\n        # \u8ba1\u7b97\u65cb\u8f6c\u9891\u7387\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n        # \u9884\u8ba1\u7b97\u4f4d\u7f6e\u7f16\u7801\n        self._build_cache(max_seq_len)\n\n    def _build_cache(self, seq_len):\n        \"\"\"\u9884\u8ba1\u7b97\u5e76\u7f13\u5b58\u65cb\u8f6c\u77e9\u9635\"\"\"\n        # \u751f\u6210\u4f4d\u7f6e\u5e8f\u5217\n        position = torch.arange(seq_len).float()\n\n        # \u8ba1\u7b97\u89d2\u5ea6: position * inv_freq\n        freqs = torch.outer(position, self.inv_freq)  # [seq_len, dim//2]\n\n        # \u62fc\u63a5\uff0c\u5f62\u6210\u5b8c\u6574\u7684\u9891\u7387\u77e9\u9635\n        emb = torch.cat([freqs, freqs], dim=-1)  # [seq_len, dim]\n\n        # \u8ba1\u7b97cos\u548csin\n        cos_cached = emb.cos()\n        sin_cached = emb.sin()\n\n        self.register_buffer('cos_cached', cos_cached)\n        self.register_buffer('sin_cached', sin_cached)\n\n    def rotate_half(self, x):\n        \"\"\"\u5c06\u8f93\u5165\u7684\u540e\u534a\u90e8\u5206\u53d6\u8d1f\u53f7\u5e76\u79fb\u5230\u524d\u9762\"\"\"\n        x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n        return torch.cat([-x2, x1], dim=-1)\n\n    def forward(self, q, k, seq_len=None):\n        \"\"\"\n        \u5bf9\u67e5\u8be2\u548c\u952e\u5411\u91cf\u5e94\u7528RoPE\n\n        Args:\n            q: \u67e5\u8be2\u77e9\u9635 [batch, heads, seq_len, dim]\n            k: \u952e\u77e9\u9635 [batch, heads, seq_len, dim]\n            seq_len: \u5e8f\u5217\u957f\u5ea6\n        \"\"\"\n        if seq_len is None:\n            seq_len = q.shape[-2]\n\n        # \u5982\u679c\u5e8f\u5217\u957f\u5ea6\u8d85\u51fa\u7f13\u5b58\uff0c\u91cd\u65b0\u6784\u5efa\n        if seq_len &gt; self.max_seq_len:\n            self._build_cache(seq_len)\n\n        # \u83b7\u53d6\u5bf9\u5e94\u957f\u5ea6\u7684cos\u548csin\n        cos = self.cos_cached[:seq_len]  # [seq_len, dim]\n        sin = self.sin_cached[:seq_len]  # [seq_len, dim]\n\n        # \u5e94\u7528\u65cb\u8f6c\u53d8\u6362\n        q_rot = q * cos + self.rotate_half(q) * sin\n        k_rot = k * cos + self.rotate_half(k) * sin\n\n        return q_rot, k_rot\n\nclass MultiHeadAttentionWithRoPE(nn.Module):\n    \"\"\"\u5e26RoPE\u7684\u591a\u5934\u6ce8\u610f\u529b\"\"\"\n\n    def __init__(self, d_model, num_heads, max_seq_len=2048):\n        super().__init__()\n        assert d_model % num_heads == 0\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n        # RoPE\u53ea\u5e94\u7528\u5230\u90e8\u5206\u7ef4\u5ea6\uff08\u901a\u5e38\u662f\u524d\u534a\u90e8\u5206\uff09\n        self.rope = RotaryPositionalEmbedding(\n            dim=self.head_dim, \n            max_seq_len=max_seq_len\n        )\n\n    def forward(self, x, mask=None):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u8ba1\u7b97Q, K, V\n        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n        V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # \u8f6c\u7f6e\u4ee5\u7b26\u5408\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u7ef4\u5ea6\u8981\u6c42\n        Q = Q.transpose(1, 2)  # [batch, heads, seq_len, head_dim]\n        K = K.transpose(1, 2)\n        V = V.transpose(1, 2)\n\n        # \u5e94\u7528RoPE\n        Q, K = self.rope(Q, K, seq_len)\n\n        # \u8ba1\u7b97\u6ce8\u610f\u529b\u5206\u6570\n        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n\n        # \u5e94\u7528\u63a9\u7801\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n\n        # Softmax\u5f52\u4e00\u5316\n        attn_weights = torch.softmax(scores, dim=-1)\n\n        # \u8ba1\u7b97\u8f93\u51fa\n        out = torch.matmul(attn_weights, V)\n\n        # \u91cd\u5851\u5e76\u5408\u5e76\u591a\u5934\n        out = out.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, d_model\n        )\n\n        return self.W_o(out)\n\n# \u4e0d\u540c\u4f4d\u7f6e\u7f16\u7801\u7684\u5bf9\u6bd4\u6d4b\u8bd5\ndef compare_position_encodings():\n    \"\"\"\u5bf9\u6bd4\u4e0d\u540c\u4f4d\u7f6e\u7f16\u7801\u7684\u6548\u679c\"\"\"\n\n    d_model, seq_len = 512, 64\n    batch_size, num_heads = 2, 8\n\n    print(\"=== \u4f4d\u7f6e\u7f16\u7801\u5bf9\u6bd4\u6d4b\u8bd5 ===\")\n\n    # \u6d4b\u8bd5\u6570\u636e\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # 1. \u65e0\u4f4d\u7f6e\u7f16\u7801\u7684\u6ce8\u610f\u529b\n    attn_no_pos = MultiHeadAttentionWithRoPE(d_model, num_heads)\n    # \u4e34\u65f6\u79fb\u9664RoPE\n    attn_no_pos.rope = lambda q, k, seq_len: (q, k)\n    out_no_pos = attn_no_pos(x)\n\n    # 2. \u5e26RoPE\u7684\u6ce8\u610f\u529b\n    attn_with_rope = MultiHeadAttentionWithRoPE(d_model, num_heads)\n    out_with_rope = attn_with_rope(x)\n\n    print(f\"\u65e0\u4f4d\u7f6e\u7f16\u7801\u8f93\u51fa\u6807\u51c6\u5dee: {out_no_pos.std():.4f}\")\n    print(f\"RoPE\u4f4d\u7f6e\u7f16\u7801\u8f93\u51fa\u6807\u51c6\u5dee: {out_with_rope.std():.4f}\")\n\n    # 3. \u6d4b\u8bd5\u5916\u63a8\u80fd\u529b\n    print(\"\\n=== \u5916\u63a8\u80fd\u529b\u6d4b\u8bd5 ===\")\n\n    # \u77ed\u5e8f\u5217\u8bad\u7ec3\n    short_len = 32\n    x_short = torch.randn(1, short_len, d_model)\n\n    # \u957f\u5e8f\u5217\u63a8\u7406\n    long_len = 128\n    x_long = torch.randn(1, long_len, d_model)\n\n    try:\n        out_short = attn_with_rope(x_short)\n        out_long = attn_with_rope(x_long)\n        print(f\"\u77ed\u5e8f\u5217({short_len})\u5904\u7406\u6210\u529f\")\n        print(f\"\u957f\u5e8f\u5217({long_len})\u5904\u7406\u6210\u529f - RoPE\u652f\u6301\u5916\u63a8\")\n    except Exception as e:\n        print(f\"\u5916\u63a8\u5931\u8d25: {e}\")\n\n# \u624b\u52a8\u9a8c\u8bc1RoPE\u7684\u76f8\u5bf9\u4f4d\u7f6e\u6027\u8d28\ndef verify_rope_property():\n    \"\"\"\u9a8c\u8bc1RoPE\u7684\u76f8\u5bf9\u4f4d\u7f6e\u4f9d\u8d56\u6027\u8d28\"\"\"\n\n    print(\"=== \u9a8c\u8bc1RoPE\u76f8\u5bf9\u4f4d\u7f6e\u6027\u8d28 ===\")\n\n    dim = 64\n    rope = RotaryPositionalEmbedding(dim, max_seq_len=10)\n\n    # \u521b\u5efa\u4e24\u4e2a\u4f4d\u7f6e\u7684\u67e5\u8be2\u548c\u952e\n    q = torch.randn(1, 1, 1, dim)  # \u4f4d\u7f6e0\u7684\u67e5\u8be2\n    k = torch.randn(1, 1, 1, dim)  # \u4f4d\u7f6e0\u7684\u952e\n\n    # \u5728\u4e0d\u540c\u76f8\u5bf9\u8ddd\u79bb\u4e0b\u6d4b\u8bd5\n    distances = [1, 2, 3]\n\n    for dist in distances:\n        # \u8ba1\u7b97\u4f4d\u7f6e(0, dist)\u7684\u76f8\u5bf9\u6ce8\u610f\u529b\n        q_pos0, k_pos_dist = rope(q, k, seq_len=dist+1)\n        score1 = torch.matmul(q_pos0[:,:,0:1], k_pos_dist[:,:,dist:dist+1].transpose(-2,-1))\n\n        # \u8ba1\u7b97\u4f4d\u7f6e(1, 1+dist)\u7684\u76f8\u5bf9\u6ce8\u610f\u529b  \n        q_pos1, k_pos1_dist = rope(q, k, seq_len=dist+2)\n        score2 = torch.matmul(q_pos1[:,:,1:2], k_pos1_dist[:,:,1+dist:2+dist].transpose(-2,-1))\n\n        print(f\"\u76f8\u5bf9\u8ddd\u79bb{dist}: \u5206\u6570\u5dee\u5f02 = {abs(score1.item() - score2.item()):.6f}\")\n\nif __name__ == \"__main__\":\n    compare_position_encodings()\n    print()\n    verify_rope_property()\n</code></pre>"},{"location":"fundamentals/attention-advanced/positional-encoding/#sinusoidal","title":"Sinusoidal\u4f4d\u7f6e\u7f16\u7801\u5b9e\u73b0","text":"<pre><code>class SinusoidalPositionalEncoding(nn.Module):\n    \"\"\"\u539f\u59cbTransformer\u7684\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\"\"\"\n\n    def __init__(self, d_model, max_seq_len=5000):\n        super().__init__()\n\n        pe = torch.zeros(max_seq_len, d_model)\n        position = torch.arange(0, max_seq_len).unsqueeze(1).float()\n\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           -(math.log(10000.0) / d_model))\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        self.register_buffer('pe', pe.unsqueeze(0))\n\n    def forward(self, x):\n        seq_len = x.size(1)\n        return x + self.pe[:, :seq_len]\n</code></pre>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_11","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\u548c\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u7684\u533a\u522b</li> <li>[ ] \u80fd\u63a8\u5bfcRoPE\u7684\u6570\u5b66\u539f\u7406</li> <li>[ ] \u638c\u63e1RoPE\u7684\u5b9e\u73b0\u7ec6\u8282\u548c\u4f18\u5316\u6280\u5de7</li> <li>[ ] \u5b8c\u6210\u4f4d\u7f6e\u7f16\u7801\u7684\u4ee3\u7801\u5b9e\u73b0\u548c\u6548\u679c\u9a8c\u8bc1</li> </ul>"},{"location":"fundamentals/attention-advanced/positional-encoding/#_12","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u5f52\u4e00\u5316\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1aLLM\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aAttention\u5347\u7ea7\u6982\u89c8</li> </ul>"},{"location":"fundamentals/deepseek-innovations/","title":"\u7b2c4\u8282\uff1aDeepSeek\u6838\u5fc3\u4f18\u5316\u6280\u672f","text":""},{"location":"fundamentals/deepseek-innovations/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3DeepSeek\u56e2\u961f\u63d0\u51fa\u7684\u4e09\u5927\u6838\u5fc3\u6280\u672f\u521b\u65b0\uff0c\u638c\u63e1\u8fd9\u4e9b\u524d\u6cbf\u4f18\u5316\u6280\u672f\u5728\u5927\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - MLA\u5982\u4f55\u5b9e\u73b010\u500d\u4ee5\u4e0a\u7684KV Cache\u538b\u7f29\uff1f - DeepSeek MoE\u7684\u521b\u65b0\u8def\u7531\u673a\u5236 - MTP\u5982\u4f55\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\uff1f</p>"},{"location":"fundamentals/deepseek-innovations/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a3\u5929</p> <ul> <li>Day 1: MLA\u6838\u5fc3\u6280\u672f\u6df1\u5ea6\u89e3\u6790 (\u4f4e\u79e9\u538b\u7f29+RoPE\u89e3\u8026+\u6743\u91cd\u5438\u6536)</li> <li>Day 2: DeepSeek MoE\u521b\u65b0\u6280\u672f (\u7ec6\u7c92\u5ea6\u8def\u7531+\u5171\u4eab\u4e13\u5bb6\u673a\u5236)</li> <li>Day 3: MTP\u591atoken\u9884\u6d4b\u6280\u672f + \u4e09\u5927\u6280\u672f\u7efc\u5408\u5bf9\u6bd4\u5206\u6790</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#deepseek","title":"\ud83c\udfc6 DeepSeek\u6280\u672f\u521b\u65b0\u6982\u89c8","text":"<p>DeepSeek\u4f5c\u4e3a\u56fd\u5185\u9886\u5148\u7684AI\u516c\u53f8\uff0c\u5728\u5927\u6a21\u578b\u4f18\u5316\u65b9\u9762\u63d0\u51fa\u4e86\u4e09\u9879\u9769\u547d\u6027\u6280\u672f\uff1a</p>"},{"location":"fundamentals/deepseek-innovations/#_3","title":"\u6838\u5fc3\u6280\u672f\u6808","text":"<pre><code>DeepSeek\u521b\u65b0\u6280\u672f\u4f53\u7cfb\n\u251c\u2500\u2500 MLA (Multi-head Latent Attention)\n\u2502   \u251c\u2500\u2500 \u4f4e\u79e9KV\u8054\u5408\u538b\u7f29\n\u2502   \u251c\u2500\u2500 RoPE\u89e3\u8026\u673a\u5236  \n\u2502   \u2514\u2500\u2500 \u6743\u91cd\u5438\u6536\u4f18\u5316\n\u251c\u2500\u2500 DeepSeek MoE\n\u2502   \u251c\u2500\u2500 \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1\n\u2502   \u251c\u2500\u2500 \u5171\u4eab\u4e13\u5bb6\u673a\u5236\n\u2502   \u2514\u2500\u2500 \u591a\u7ea7\u8d1f\u8f7d\u5747\u8861\n\u2514\u2500\u2500 MTP (Multi-Token Prediction)\n    \u251c\u2500\u2500 \u5e76\u884c\u591atoken\u9884\u6d4b\n    \u251c\u2500\u2500 \u72ec\u7acb\u9884\u6d4b\u5934\u8bbe\u8ba1\n    \u2514\u2500\u2500 \u5bc6\u96c6\u76d1\u7763\u4fe1\u53f7\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/#_4","title":"\u6280\u672f\u5f71\u54cd\u529b","text":"\u6280\u672f \u6838\u5fc3\u521b\u65b0 \u6027\u80fd\u63d0\u5347 \u5e94\u7528\u6a21\u578b MLA KV Cache\u538b\u7f29 \u5185\u5b58\u51cf\u5c1110-20\u00d7 DeepSeek-V2/V3 DeepSeek MoE \u7ec6\u7c92\u5ea6\u8def\u7531 \u53c2\u6570\u6548\u7387\u63d0\u5347 DeepSeek-V2/V3 MTP \u591atoken\u9884\u6d4b \u8bad\u7ec3\u6548\u7387\u63d0\u5347 DeepSeek-V3"},{"location":"fundamentals/deepseek-innovations/#_5","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/deepseek-innovations/#1-mla","title":"1. MLA\u6838\u5fc3\u6280\u672f","text":"<ul> <li>\u4f4e\u79e9\u538b\u7f29\u7684\u6570\u5b66\u539f\u7406</li> <li>RoPE\u89e3\u8026\u7684\u6280\u672f\u7ec6\u8282</li> <li>\u6743\u91cd\u5438\u6536\u7684\u5de5\u7a0b\u4f18\u5316</li> <li>\u4e0e\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u6bd4</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#2-deepseek-moe","title":"2. DeepSeek MoE\u521b\u65b0","text":"<ul> <li>\u7ec6\u7c92\u5ea6\u4e13\u5bb6vs\u7c97\u7c92\u5ea6\u4e13\u5bb6</li> <li>\u5171\u4eab\u4e13\u5bb6\u7684\u8bbe\u8ba1\u7406\u5ff5</li> <li>\u591a\u7ea7\u8d1f\u8f7d\u5747\u8861\u7b56\u7565</li> <li>\u8def\u7531\u673a\u5236\u7684\u6f14\u8fdb\u5386\u7a0b</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#3-mtptoken","title":"3. MTP\u591atoken\u9884\u6d4b","text":"<ul> <li>\u5e76\u884c\u9884\u6d4b\u7684\u67b6\u6784\u8bbe\u8ba1</li> <li>\u591a\u5934\u9884\u6d4b\u7684\u8bad\u7ec3\u7b56\u7565</li> <li>\u5bc6\u96c6\u76d1\u7763\u4fe1\u53f7\u7684\u4f5c\u7528</li> <li>\u4e0e\u4f20\u7edf\u8bad\u7ec3\u65b9\u6cd5\u5bf9\u6bd4</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#_6","title":"\ud83d\udd2c \u6280\u672f\u6df1\u5ea6\u5206\u6790","text":""},{"location":"fundamentals/deepseek-innovations/#_7","title":"\u521b\u65b0\u52a8\u673a","text":"<ol> <li>\u5185\u5b58\u74f6\u9888: \u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u5e8f\u5217\u4e0a\u7684\u5185\u5b58\u5f00\u9500\u8fc7\u5927</li> <li>\u8ba1\u7b97\u6548\u7387: MOE\u6a21\u578b\u7684\u8d1f\u8f7d\u5747\u8861\u548c\u4e13\u5bb6\u5229\u7528\u7387\u95ee\u9898</li> <li>\u8bad\u7ec3\u6548\u7387: \u4f20\u7edfnext-token\u9884\u6d4b\u7684\u4fe1\u606f\u5bc6\u5ea6\u4e0d\u8db3</li> </ol>"},{"location":"fundamentals/deepseek-innovations/#_8","title":"\u89e3\u51b3\u65b9\u6848","text":"<ol> <li>MLA: \u901a\u8fc7\u4f4e\u79e9\u538b\u7f29\u548c\u89e3\u8026\u8bbe\u8ba1\u5b9e\u73b0\u5185\u5b58\u4f18\u5316</li> <li>DeepSeek MoE: \u901a\u8fc7\u7ec6\u7c92\u5ea6\u8def\u7531\u548c\u5171\u4eab\u4e13\u5bb6\u63d0\u5347\u6548\u7387</li> <li>MTP: \u901a\u8fc7\u591atoken\u9884\u6d4b\u589e\u52a0\u8bad\u7ec3\u4fe1\u53f7\u5bc6\u5ea6</li> </ol>"},{"location":"fundamentals/deepseek-innovations/#_9","title":"\u534f\u540c\u6548\u5e94","text":"<p>\u8fd9\u4e09\u9879\u6280\u672f\u5728DeepSeek\u6a21\u578b\u4e2d\u534f\u540c\u5de5\u4f5c\uff1a - MLA\u964d\u4f4e\u63a8\u7406\u5185\u5b58\u9700\u6c42 - DeepSeek MoE\u63d0\u4f9b\u53c2\u6570\u89c4\u6a21\u6269\u5c55\u80fd\u529b - MTP\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u8d28\u91cf</p>"},{"location":"fundamentals/deepseek-innovations/#_10","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u9879\u76ee\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u6280\u672f\u7406\u89e3: \u80fd\u6e05\u6670\u89e3\u91ca\u6bcf\u9879\u6280\u672f\u7684\u6838\u5fc3\u539f\u7406</li> <li>\u5bf9\u6bd4\u5206\u6790: \u80fd\u8bf4\u660e\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u4f18\u52bf\u5bf9\u6bd4</li> <li>\u5e94\u7528\u573a\u666f: \u7406\u89e3\u8fd9\u4e9b\u6280\u672f\u7684\u9002\u7528\u573a\u666f\u548c\u9650\u5236</li> <li>\u5de5\u7a0b\u5b9e\u73b0: \u4e86\u89e3\u5173\u952e\u7684\u5b9e\u73b0\u7ec6\u8282\u548c\u5de5\u7a0b\u6311\u6218</li> </ol>"},{"location":"fundamentals/deepseek-innovations/#_11","title":"\ud83c\udf1f \u4e3a\u4ec0\u4e48\u8fd9\u4e9b\u6280\u672f\u91cd\u8981\uff1f","text":""},{"location":"fundamentals/deepseek-innovations/#1","title":"1. \u6280\u672f\u9886\u5148\u6027","text":"<ul> <li>\u4ee3\u8868\u4e86\u5f53\u524d\u5927\u6a21\u578b\u4f18\u5316\u7684\u6700\u524d\u6cbf\u6280\u672f</li> <li>\u5f88\u591a\u6982\u5ff5\u548c\u65b9\u6cd5\u88ab\u540e\u7eed\u7814\u7a76\u5e7f\u6cdb\u91c7\u7528</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#2","title":"2. \u5b9e\u7528\u4ef7\u503c","text":"<ul> <li>\u5df2\u5728\u771f\u5b9e\u7684\u5927\u89c4\u6a21\u6a21\u578b\u4e2d\u5f97\u5230\u9a8c\u8bc1</li> <li>\u4e3a\u5de5\u4e1a\u7ea7\u5927\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#3","title":"3. \u9762\u8bd5\u70ed\u70b9","text":"<ul> <li>\u56fd\u5185AI\u516c\u53f8\u9762\u8bd5\u7684\u9ad8\u9891\u8003\u70b9</li> <li>\u4f53\u73b0\u5bf9\u524d\u6cbf\u6280\u672f\u7684\u7406\u89e3\u548c\u5173\u6ce8</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#4","title":"4. \u53d1\u5c55\u8d8b\u52bf","text":"<ul> <li>\u6307\u5f15\u4e86\u5927\u6a21\u578b\u4f18\u5316\u7684\u91cd\u8981\u65b9\u5411</li> <li>\u4e3a\u540e\u7eed\u6280\u672f\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840</li> </ul>"},{"location":"fundamentals/deepseek-innovations/#_12","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u611f\u5174\u8da3\u7684\u6280\u672f\u6a21\u5757\u6df1\u5165\u5b66\u4e60\u3002\u5efa\u8bae\u6309\u987a\u5e8f\u5b66\u4e60\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6280\u672f\u4e4b\u95f4\u5b58\u5728\u4e00\u5b9a\u7684\u5173\u8054\u6027\u3002\u6bcf\u4e2a\u6a21\u5757\u90fd\u5305\u542b\u8be6\u7ec6\u7684\u6280\u672f\u539f\u7406\u3001\u4ee3\u7801\u5b9e\u73b0\u548c\u9762\u8bd5\u95ee\u7b54\u3002</p>"},{"location":"fundamentals/deepseek-innovations/#_13","title":"\ud83c\udf93 \u5b66\u4e60\u5efa\u8bae","text":"<ol> <li>\u91cd\u70b9\u7406\u89e3\u539f\u7406: \u4e0d\u8981\u4ec5\u4ec5\u8bb0\u4f4f\u7ed3\u8bba\uff0c\u8981\u7406\u89e3\u4e3a\u4ec0\u4e48\u8fd9\u6837\u8bbe\u8ba1</li> <li>\u5bf9\u6bd4\u4f20\u7edf\u65b9\u6cd5: \u901a\u8fc7\u5bf9\u6bd4\u52a0\u6df1\u5bf9\u521b\u65b0\u70b9\u7684\u7406\u89e3</li> <li>\u5173\u6ce8\u5de5\u7a0b\u7ec6\u8282: \u8fd9\u4e9b\u6280\u672f\u7684\u6210\u529f\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5de5\u7a0b\u5b9e\u73b0</li> <li>\u601d\u8003\u5e94\u7528\u573a\u666f: \u8003\u8651\u8fd9\u4e9b\u6280\u672f\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027</li> </ol> <p>\u51c6\u5907\u597d\u6df1\u5165\u63a2\u7d22\u8fd9\u4e9b\u9769\u547d\u6027\u7684\u6280\u672f\u521b\u65b0\u4e86\u5417\uff1f</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/","title":"DeepSeek MoE\u521b\u65b0","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3DeepSeek\u5728\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u4e0a\u7684\u521b\u65b0\u8bbe\u8ba1\uff0c\u638c\u63e1\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u548c\u5171\u4eab\u4e13\u5bb6\u7684\u6838\u5fc3\u7406\u5ff5\u3002</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_2","title":"\ud83d\udcdd \u6280\u672f\u521b\u65b0\u89e3\u6790","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#deepseek-moe_1","title":"DeepSeek MoE\u6f14\u8fdb\u5386\u7a0b","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_3","title":"\u7248\u672c\u6f14\u8fdb","text":"<pre><code>DeepSeek MoE\u6280\u672f\u6f14\u8fdb\n\u251c\u2500\u2500 DeepSeek V1 (2023)\n\u2502   \u251c\u2500\u2500 \u57fa\u7840MoE\u67b6\u6784\n\u2502   \u2514\u2500\u2500 \u6807\u51c6Token-Choice\u8def\u7531\n\u251c\u2500\u2500 DeepSeek V2 (2024)\n\u2502   \u251c\u2500\u2500 \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1\n\u2502   \u251c\u2500\u2500 \u5171\u4eab\u4e13\u5bb6\u673a\u5236\n\u2502   \u2514\u2500\u2500 \u591a\u7ea7\u8d1f\u8f7d\u5747\u8861\n\u2514\u2500\u2500 DeepSeek V3 (2024)\n    \u251c\u2500\u2500 \u4f18\u5316\u8def\u7531\u7b56\u7565\n    \u251c\u2500\u2500 \u52a8\u6001\u4e13\u5bb6\u5bb9\u91cf\n    \u2514\u2500\u2500 \u66f4\u7cbe\u7ec6\u7684\u8d1f\u8f7d\u63a7\u5236\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_4","title":"\u6838\u5fc3\u521b\u65b0\u6280\u672f","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#1","title":"1. \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1","text":"<p>\u4f20\u7edf\u95ee\u9898: \u7c97\u7c92\u5ea6\u4e13\u5bb6\u5bfc\u81f4\u7684\u4e13\u4e1a\u5316\u4e0d\u8db3</p> <p>DeepSeek\u89e3\u51b3\u65b9\u6848: \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u5206\u5de5</p> <pre><code># \u4f20\u7edf\u7c97\u7c92\u5ea6\u4e13\u5bb6\nclass CoarseGrainedExpert(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.ffn = FeedForward(d_model, d_ff)  # \u5b8c\u6574FFN\n\n    def forward(self, x):\n        return self.ffn(x)\n\n# DeepSeek\u7ec6\u7c92\u5ea6\u4e13\u5bb6\nclass FineGrainedExpert(nn.Module):\n    def __init__(self, d_model, d_ff, expert_type='gate', shared_gate=None):\n        super().__init__()\n        self.expert_type = expert_type\n\n        if expert_type == 'gate':\n            # \u95e8\u63a7\u4e13\u5bb6\uff1a\u53ea\u8d1f\u8d23\u95e8\u63a7\u6fc0\u6d3b\n            self.gate_proj = nn.Linear(d_model, d_ff, bias=False)\n        elif expert_type == 'up':\n            # \u4e0a\u6295\u5f71\u4e13\u5bb6\uff1a\u8d1f\u8d23\u7279\u5f81\u63d0\u53d6\n            self.up_proj = nn.Linear(d_model, d_ff, bias=False)\n        elif expert_type == 'down':\n            # \u4e0b\u6295\u5f71\u4e13\u5bb6\uff1a\u8d1f\u8d23\u8f93\u51fa\u6295\u5f71\n            self.down_proj = nn.Linear(d_ff, d_model, bias=False)\n\n        self.shared_gate = shared_gate\n\n    def forward(self, x):\n        if self.expert_type == 'gate':\n            return self.gate_proj(x)\n        elif self.expert_type == 'up':\n            return self.up_proj(x)\n        elif self.expert_type == 'down':\n            return self.down_proj(x)\n</code></pre> <p>\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u7684\u4f18\u52bf: - \u66f4\u7cbe\u7ec6\u7684\u4e13\u4e1a\u5316: \u6bcf\u79cd\u64cd\u4f5c\u7c7b\u578b\u90fd\u6709\u4e13\u95e8\u7684\u4e13\u5bb6 - \u66f4\u597d\u7684\u53c2\u6570\u5229\u7528: \u907f\u514d\u4e86\u4e13\u5bb6\u5185\u90e8\u7684\u5197\u4f59 - \u7075\u6d3b\u7684\u7ec4\u5408: \u53ef\u4ee5\u52a8\u6001\u7ec4\u5408\u4e0d\u540c\u7c7b\u578b\u7684\u4e13\u5bb6</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#2","title":"2. \u5171\u4eab\u4e13\u5bb6\u673a\u5236","text":"<p>\u8bbe\u8ba1\u7406\u5ff5: \u90e8\u5206\u77e5\u8bc6\u5bf9\u6240\u6709\u8f93\u5165\u90fd\u6709\u7528\uff0c\u5e94\u8be5\u88ab\u5171\u4eab</p> <pre><code>class DeepSeekMoELayer(nn.Module):\n    \"\"\"DeepSeek MoE\u5c42\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, num_experts, num_shared_experts, \n                 expert_capacity, d_ff):\n        super().__init__()\n\n        # \u5171\u4eab\u4e13\u5bb6\uff1a\u59cb\u7ec8\u6fc0\u6d3b\n        self.shared_experts = nn.ModuleList([\n            FeedForward(d_model, d_ff) \n            for _ in range(num_shared_experts)\n        ])\n\n        # \u8def\u7531\u4e13\u5bb6\uff1a\u52a8\u6001\u9009\u62e9\n        self.routed_experts = nn.ModuleList([\n            FeedForward(d_model, d_ff)\n            for _ in range(num_experts)\n        ])\n\n        # \u8def\u7531\u7f51\u7edc\n        self.router = Router(d_model, num_experts)\n\n        # \u4e13\u5bb6\u914d\u7f6e\n        self.num_experts = num_experts\n        self.num_shared_experts = num_shared_experts\n        self.expert_capacity = expert_capacity\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n\n        # 1. \u5171\u4eab\u4e13\u5bb6\u5904\u7406\uff08\u59cb\u7ec8\u6fc0\u6d3b\uff09\n        shared_output = torch.zeros_like(x)\n        for shared_expert in self.shared_experts:\n            shared_output += shared_expert(x) / self.num_shared_experts\n\n        # 2. \u8def\u7531\u4e13\u5bb6\u5904\u7406\uff08\u52a8\u6001\u9009\u62e9\uff09\n        router_probs, expert_indices = self.router(x)\n        routed_output = self.route_to_experts(x, router_probs, expert_indices)\n\n        # 3. \u7ec4\u5408\u5171\u4eab\u548c\u8def\u7531\u8f93\u51fa\n        final_output = shared_output + routed_output\n\n        return final_output\n\n    def route_to_experts(self, x, probs, indices):\n        \"\"\"\u8def\u7531\u5230\u4e13\u5bb6\u7684\u8be6\u7ec6\u5b9e\u73b0\"\"\"\n        output = torch.zeros_like(x)\n\n        # Token-choice\u8def\u7531\u7b56\u7565\n        for i in range(2):  # Top-2\u8def\u7531\n            expert_idx = indices[:, :, i]\n            expert_prob = probs[:, :, i]\n\n            # \u4e13\u5bb6\u5bb9\u91cf\u9650\u5236\n            expert_tokens = self.apply_capacity_limit(x, expert_idx, expert_prob)\n\n            # \u4e13\u5bb6\u5904\u7406\n            for expert_id in range(self.num_experts):\n                mask = (expert_idx == expert_id)\n                if mask.any():\n                    expert_input = x[mask]\n                    expert_output = self.routed_experts[expert_id](expert_input)\n                    output[mask] += expert_prob[mask].unsqueeze(-1) * expert_output\n\n        return output\n</code></pre> <p>\u5171\u4eab\u4e13\u5bb6\u7684\u4f5c\u7528: - \u901a\u7528\u77e5\u8bc6: \u5b58\u50a8\u5bf9\u6240\u6709\u8f93\u5165\u90fd\u6709\u7528\u7684\u57fa\u7840\u77e5\u8bc6 - \u7a33\u5b9a\u57fa\u7ebf: \u4e3a\u6a21\u578b\u63d0\u4f9b\u7a33\u5b9a\u7684\u57fa\u7840\u8f93\u51fa - \u8d1f\u8f7d\u5206\u62c5: \u51cf\u8f7b\u8def\u7531\u4e13\u5bb6\u7684\u8d1f\u62c5</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#3","title":"3. \u591a\u7ea7\u8d1f\u8f7d\u5747\u8861","text":"<p>\u6311\u6218: \u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a</p> <p>DeepSeek\u7684\u591a\u7ea7\u89e3\u51b3\u65b9\u6848:</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_5","title":"\u8bbe\u5907\u7ea7\u8d1f\u8f7d\u5747\u8861","text":"<pre><code>def device_level_load_balancing(expert_assignments, world_size):\n    \"\"\"\u8bbe\u5907\u7ea7\u522b\u7684\u8d1f\u8f7d\u5747\u8861\"\"\"\n\n    # \u7edf\u8ba1\u6bcf\u4e2a\u8bbe\u5907\u4e0a\u7684token\u6570\u91cf\n    device_loads = torch.zeros(world_size)\n\n    for expert_id, tokens in expert_assignments.items():\n        device_id = expert_id % world_size\n        device_loads[device_id] += len(tokens)\n\n    # \u8ba1\u7b97\u8d1f\u8f7d\u5747\u8861\u635f\u5931\n    ideal_load = device_loads.sum() / world_size\n    load_variance = torch.var(device_loads)\n\n    device_balance_loss = load_variance / (ideal_load ** 2)\n\n    return device_balance_loss\n\ndef expert_level_load_balancing(router_probs, expert_indices):\n    \"\"\"\u4e13\u5bb6\u7ea7\u522b\u7684\u8d1f\u8f7d\u5747\u8861\"\"\"\n\n    num_experts = router_probs.size(-1)\n\n    # \u8ba1\u7b97\u6bcf\u4e2a\u4e13\u5bb6\u7684\u9009\u62e9\u9891\u7387\n    expert_frequencies = torch.zeros(num_experts)\n    for expert_id in range(num_experts):\n        expert_frequencies[expert_id] = (expert_indices == expert_id).float().sum()\n\n    # \u7406\u60f3\u9891\u7387\n    total_selections = expert_indices.numel()\n    ideal_frequency = total_selections / num_experts\n\n    # \u8ba1\u7b97\u5747\u8861\u635f\u5931\n    expert_balance_loss = torch.var(expert_frequencies) / (ideal_frequency ** 2)\n\n    return expert_balance_loss\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_6","title":"\u52a8\u6001\u5bb9\u91cf\u8c03\u6574","text":"<pre><code>class DynamicCapacityRouter(nn.Module):\n    \"\"\"\u52a8\u6001\u5bb9\u91cf\u8c03\u6574\u8def\u7531\u5668\"\"\"\n\n    def __init__(self, d_model, num_experts, base_capacity_factor=1.25):\n        super().__init__()\n        self.gate = nn.Linear(d_model, num_experts)\n        self.base_capacity_factor = base_capacity_factor\n        self.expert_utilization = torch.ones(num_experts)  # \u4e13\u5bb6\u5229\u7528\u7387\u8ddf\u8e2a\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n        num_tokens = batch_size * seq_len\n\n        # \u8ba1\u7b97\u57fa\u7840\u5bb9\u91cf\n        base_capacity = int(self.base_capacity_factor * num_tokens / self.num_experts)\n\n        # \u6839\u636e\u5386\u53f2\u5229\u7528\u7387\u52a8\u6001\u8c03\u6574\u5bb9\u91cf\n        adjusted_capacities = []\n        for expert_id in range(self.num_experts):\n            utilization = self.expert_utilization[expert_id]\n\n            if utilization &lt; 0.5:  # \u5229\u7528\u7387\u4f4e\uff0c\u51cf\u5c11\u5bb9\u91cf\n                adjusted_capacity = int(base_capacity * 0.8)\n            elif utilization &gt; 1.5:  # \u5229\u7528\u7387\u9ad8\uff0c\u589e\u52a0\u5bb9\u91cf\n                adjusted_capacity = int(base_capacity * 1.2)\n            else:\n                adjusted_capacity = base_capacity\n\n            adjusted_capacities.append(adjusted_capacity)\n\n        # \u8def\u7531\u8ba1\u7b97\n        logits = self.gate(x)\n        return self.route_with_dynamic_capacity(x, logits, adjusted_capacities)\n\n    def update_utilization(self, expert_assignments):\n        \"\"\"\u66f4\u65b0\u4e13\u5bb6\u5229\u7528\u7387\u7edf\u8ba1\"\"\"\n        current_utilization = torch.zeros(self.num_experts)\n\n        for expert_id, tokens in expert_assignments.items():\n            current_utilization[expert_id] = len(tokens)\n\n        # \u6307\u6570\u79fb\u52a8\u5e73\u5747\u66f4\u65b0\n        alpha = 0.1\n        self.expert_utilization = (1 - alpha) * self.expert_utilization + alpha * current_utilization\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#4","title":"4. \u8def\u7531\u7b56\u7565\u4f18\u5316","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#expert-choice-vs-token-choice","title":"Expert-Choice vs Token-Choice\u6df7\u5408\u8def\u7531","text":"<pre><code>class HybridRouter(nn.Module):\n    \"\"\"\u6df7\u5408\u8def\u7531\u7b56\u7565\"\"\"\n\n    def __init__(self, d_model, num_experts):\n        super().__init__()\n        self.gate = nn.Linear(d_model, num_experts)\n        self.routing_strategy = 'adaptive'  # adaptive, token_choice, expert_choice\n\n    def forward(self, x):\n        logits = self.gate(x)\n\n        if self.routing_strategy == 'token_choice':\n            return self.token_choice_routing(x, logits)\n        elif self.routing_strategy == 'expert_choice':\n            return self.expert_choice_routing(x, logits)\n        else:  # adaptive\n            return self.adaptive_routing(x, logits)\n\n    def adaptive_routing(self, x, logits):\n        \"\"\"\u81ea\u9002\u5e94\u8def\u7531\u7b56\u7565\"\"\"\n        batch_size, seq_len, _ = x.shape\n\n        # \u6839\u636e\u8d1f\u8f7d\u60c5\u51b5\u52a8\u6001\u9009\u62e9\u8def\u7531\u7b56\u7565\n        current_load = self.estimate_current_load()\n\n        if current_load &gt; 0.8:  # \u9ad8\u8d1f\u8f7d\u65f6\u4f7f\u7528expert-choice\n            return self.expert_choice_routing(x, logits)\n        else:  # \u4f4e\u8d1f\u8f7d\u65f6\u4f7f\u7528token-choice\n            return self.token_choice_routing(x, logits)\n\n    def estimate_current_load(self):\n        \"\"\"\u4f30\u8ba1\u5f53\u524d\u7cfb\u7edf\u8d1f\u8f7d\"\"\"\n        # \u7b80\u5316\u5b9e\u73b0\uff1a\u57fa\u4e8e\u5386\u53f2\u7edf\u8ba1\n        return 0.6  # \u5360\u4f4d\u7b26\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#deepseek-moe_2","title":"DeepSeek MoE\u67b6\u6784\u56fe","text":"<pre><code>                 \u8f93\u5165Token\n                     \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502                 \u2502\n       \u5171\u4eab\u4e13\u5bb6           \u8def\u7531\u7f51\u7edc\n      (\u59cb\u7ec8\u6fc0\u6d3b)           \u2502\n            \u2502               \u2502\n            \u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502        \u2502           \u2502\n            \u2502     Top-K        \u5bb9\u91cf\n            \u2502     \u9009\u62e9         \u9650\u5236\n            \u2502        \u2502           \u2502\n            \u2502     \u8def\u7531\u4e13\u5bb6     \u8d1f\u8f7d\n            \u2502    (\u52a8\u6001\u9009\u62e9)    \u5747\u8861\n            \u2502        \u2502           \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                  \u6700\u7ec8\u8f93\u51fa\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#q1-deepseek-moemoe","title":"Q1: DeepSeek MoE\u76f8\u6bd4\u4f20\u7edfMoE\u6709\u4ec0\u4e48\u521b\u65b0\uff1f","text":"<p>\u6838\u5fc3\u521b\u65b0:</p> <ol> <li>\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8bbe\u8ba1: \u5c06FFN\u5206\u89e3\u4e3a\u66f4\u4e13\u4e1a\u5316\u7684\u7ec4\u4ef6</li> <li>\u5171\u4eab\u4e13\u5bb6\u673a\u5236: \u90e8\u5206\u4e13\u5bb6\u59cb\u7ec8\u6fc0\u6d3b\uff0c\u63d0\u4f9b\u7a33\u5b9a\u57fa\u7ebf</li> <li>\u591a\u7ea7\u8d1f\u8f7d\u5747\u8861: \u8bbe\u5907\u7ea7\u548c\u4e13\u5bb6\u7ea7\u7684\u53cc\u91cd\u5747\u8861\u7b56\u7565</li> <li>\u52a8\u6001\u5bb9\u91cf\u8c03\u6574: \u6839\u636e\u4e13\u5bb6\u5229\u7528\u7387\u52a8\u6001\u8c03\u6574\u5bb9\u91cf</li> </ol>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#q2","title":"Q2: \u5171\u4eab\u4e13\u5bb6\u673a\u5236\u6709\u4ec0\u4e48\u597d\u5904\uff1f","text":"<p>\u4e3b\u8981\u4f18\u52bf: - \u77e5\u8bc6\u5171\u4eab: \u901a\u7528\u77e5\u8bc6\u4e0d\u9700\u8981\u5728\u6bcf\u4e2a\u4e13\u5bb6\u4e2d\u91cd\u590d - \u8bad\u7ec3\u7a33\u5b9a: \u63d0\u4f9b\u7a33\u5b9a\u7684\u68af\u5ea6\u4fe1\u53f7 - \u8d1f\u8f7d\u5206\u62c5: \u51cf\u5c11\u8def\u7531\u4e13\u5bb6\u7684\u538b\u529b - \u6027\u80fd\u4fdd\u8bc1: \u5373\u4f7f\u8def\u7531\u5931\u8d25\u4e5f\u6709\u57fa\u7840\u8f93\u51fa</p>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#q3-moe","title":"Q3: \u5982\u4f55\u89e3\u51b3MoE\u7684\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\uff1f","text":"<p>DeepSeek\u7684\u591a\u5c42\u6b21\u65b9\u6848:</p> <ol> <li> <p>\u8f85\u52a9\u635f\u5931\u51fd\u6570:     <pre><code>balance_loss = device_balance_loss + expert_balance_loss\ntotal_loss = task_loss + \u03bb * balance_loss\n</code></pre></p> </li> <li> <p>\u52a8\u6001\u5bb9\u91cf\u8c03\u6574: \u6839\u636e\u4e13\u5bb6\u5386\u53f2\u5229\u7528\u7387\u8c03\u6574\u5bb9\u91cf</p> </li> <li> <p>\u6df7\u5408\u8def\u7531\u7b56\u7565: \u5728token-choice\u548cexpert-choice\u95f4\u81ea\u9002\u5e94\u5207\u6362</p> </li> <li> <p>\u4e13\u5bb6\u5206\u7ec4: \u901a\u8fc7\u5c42\u6b21\u5316\u7ed3\u6784\u63d0\u9ad8\u8d1f\u8f7d\u5206\u5e03</p> </li> </ol>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_8","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u7ec6\u7c92\u5ea6\u4e13\u5bb6vs\u7c97\u7c92\u5ea6\u4e13\u5bb6\u7684\u533a\u522b</li> <li>[ ] \u638c\u63e1\u5171\u4eab\u4e13\u5bb6\u673a\u5236\u7684\u8bbe\u8ba1\u7406\u5ff5</li> <li>[ ] \u4e86\u89e3\u591a\u7ea7\u8d1f\u8f7d\u5747\u8861\u7684\u5b9e\u73b0\u7b56\u7565</li> <li>[ ] \u80fd\u89e3\u91caDeepSeek MoE\u7684\u521b\u65b0\u4ef7\u503c</li> </ul>"},{"location":"fundamentals/deepseek-innovations/deepseek-moe/#_9","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aMLA\u6838\u5fc3\u6280\u672f</li> <li>\u4e0b\u4e00\u8282\uff1aMTP\u591atoken\u9884\u6d4b</li> <li>\u8fd4\u56de\uff1aDeepSeek\u4f18\u5316\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mla/","title":"MLA\u6838\u5fc3\u6280\u672f","text":""},{"location":"fundamentals/deepseek-innovations/mla/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u638c\u63e1Multi-head Latent Attention (MLA)\u7684\u5b8c\u6574\u6280\u672f\u539f\u7406\uff0c\u7406\u89e3\u5176\u5982\u4f55\u5b9e\u73b0\u9769\u547d\u6027\u7684\u5185\u5b58\u4f18\u5316\u3002</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_2","title":"\ud83d\udcdd \u6280\u672f\u6df1\u5ea6\u89e3\u6790","text":""},{"location":"fundamentals/deepseek-innovations/mla/#mla_1","title":"MLA\u8bbe\u8ba1\u80cc\u666f","text":"<p>\u4f20\u7edf\u591a\u5934\u6ce8\u610f\u529b(MHA)\u9762\u4e34\u7684\u6838\u5fc3\u95ee\u9898\uff1a</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_3","title":"\u5185\u5b58\u7206\u70b8\u95ee\u9898","text":"<pre><code># \u4f20\u7edfMHA\u7684KV Cache\u9700\u6c42\nkv_cache_size = num_layers \u00d7 num_heads \u00d7 head_dim \u00d7 seq_len \u00d7 2  # K\u548cV\n\n# \u5177\u4f53\u4f8b\u5b50\uff1aLLaMA-7B\u6a21\u578b\n# 32\u5c42 \u00d7 32\u5934 \u00d7 128\u7ef4 \u00d7 2048\u5e8f\u5217\u957f\u5ea6 \u00d7 2 = 1GB+ \u5185\u5b58\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_4","title":"\u63a8\u7406\u74f6\u9888","text":"<ul> <li>\u957f\u5e8f\u5217\u63a8\u7406\u65f6KV Cache\u5360\u7528\u5927\u91cf\u663e\u5b58</li> <li>\u9650\u5236\u4e86batch size\u548c\u5e8f\u5217\u957f\u5ea6</li> <li>\u63a8\u7406\u6210\u672c\u5c45\u9ad8\u4e0d\u4e0b</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mla/#mla_2","title":"MLA\u6838\u5fc3\u521b\u65b0","text":""},{"location":"fundamentals/deepseek-innovations/mla/#1-kv","title":"1. \u4f4e\u79e9KV\u8054\u5408\u538b\u7f29","text":"<p>\u6838\u5fc3\u601d\u60f3: \u5c06\u9ad8\u7ef4\u7684Key\u548cValue\u77e9\u9635\u8054\u5408\u538b\u7f29\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_5","title":"\u6570\u5b66\u539f\u7406","text":"\\[c_t^{KV} = x_t W^{DKV}\\] <p>\u5176\u4e2d\uff1a - \\(x_t \\in \\mathbb{R}^{d}\\): \u8f93\u5165\u5411\u91cf - \\(W^{DKV} \\in \\mathbb{R}^{d \\times d_c}\\): \u4e0b\u6295\u5f71\u77e9\u9635 - \\(c_t^{KV} \\in \\mathbb{R}^{d_c}\\): \u538b\u7f29\u540e\u7684\u6f5c\u5728\u5411\u91cf - \\(d_c \\ll h \\cdot d_h\\) (\u538b\u7f29\u7ef4\u5ea6\u8fdc\u5c0f\u4e8e\u539f\u59cb\u7ef4\u5ea6)</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_6","title":"\u6062\u590d\u8fc7\u7a0b","text":"<pre><code>def kv_compression_recovery():\n    # 1. \u538b\u7f29\uff1a\u5c06\u8f93\u5165\u538b\u7f29\u5230\u4f4e\u7ef4\u7a7a\u95f4\n    c_kv = x @ W_down_kv  # [batch, seq, d_model] -&gt; [batch, seq, d_c]\n\n    # 2. \u6062\u590d\uff1a\u4ece\u4f4e\u7ef4\u7a7a\u95f4\u6062\u590d\u9ad8\u7ef4K,V\n    k_compressed = c_kv @ W_up_k  # [batch, seq, d_c] -&gt; [batch, seq, d_k]\n    v_compressed = c_kv @ W_up_v  # [batch, seq, d_c] -&gt; [batch, seq, d_v]\n\n    return k_compressed, v_compressed\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_7","title":"\u538b\u7f29\u6548\u679c\u5bf9\u6bd4","text":"\u6a21\u578b\u89c4\u6a21 \u539f\u59cbKV Cache MLA\u538b\u7f29\u540e \u538b\u7f29\u6bd4 7B\u6a21\u578b 8192\u7ef4/token 640\u7ef4/token 12.8\u00d7 67B\u6a21\u578b 16384\u7ef4/token 1024\u7ef4/token 16\u00d7 236B\u6a21\u578b 32768\u7ef4/token 1536\u7ef4/token 21.3\u00d7"},{"location":"fundamentals/deepseek-innovations/mla/#2-rope","title":"2. RoPE\u89e3\u8026\u673a\u5236","text":"<p>\u95ee\u9898: \u4f20\u7edfRoPE\u5728\u538b\u7f29\u7a7a\u95f4\u4e2d\u65e0\u6cd5\u6b63\u786e\u5de5\u4f5c</p> <p>\u89e3\u51b3\u65b9\u6848: \u5c06Query\u548cKey\u5206\u4e3a\u4e24\u4e2a\u72ec\u7acb\u90e8\u5206</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_8","title":"\u5206\u79bb\u7b56\u7565","text":"<pre><code>def rope_decoupling(x, position):\n    # 1. \u751f\u6210\u539f\u59cbQuery\n    q_full = x @ W_q  # [batch, seq, d_model]\n\n    # 2. \u5206\u79bb\u4e3a\u4e24\u90e8\u5206\n    q_compressed = q_full[:, :, :d_c]      # \u8bed\u4e49\u90e8\u5206\uff0c\u53ef\u538b\u7f29\n    q_rope = q_full[:, :, d_c:d_c+d_r]     # \u4f4d\u7f6e\u90e8\u5206\uff0c\u4fdd\u6301\u539f\u7ef4\u5ea6\n\n    # 3. \u5206\u522b\u5904\u7406\n    # \u8bed\u4e49\u90e8\u5206\uff1a\u901a\u8fc7\u538b\u7f29\u7a7a\u95f4\u5904\u7406\n    c_kv = x @ W_down_kv\n    k_compressed = c_kv @ W_up_k\n\n    # \u4f4d\u7f6e\u90e8\u5206\uff1a\u76f4\u63a5\u751f\u6210\u5e76\u5e94\u7528RoPE\n    k_rope = x @ W_k_rope\n    q_rope_rotated = apply_rope(q_rope, position)\n    k_rope_rotated = apply_rope(k_rope, position)\n\n    # 4. \u7ec4\u5408\u6700\u7ec8\u7ed3\u679c\n    q_final = torch.cat([q_compressed, q_rope_rotated], dim=-1)\n    k_final = torch.cat([k_compressed, k_rope_rotated], dim=-1)\n\n    return q_final, k_final\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#rope","title":"RoPE\u89e3\u8026\u7684\u6570\u5b66\u8868\u793a","text":"\\[q_t = [q_t^C; q_t^R], \\quad k_s = [k_s^C; k_s^R]\\] <p>\u5176\u4e2d\uff1a - \\(q_t^C, k_s^C\\): \u8bed\u4e49\u7ec4\u4ef6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u751f\u6210 - \\(q_t^R, k_s^R\\): \u4f4d\u7f6e\u7ec4\u4ef6\uff0c\u5e94\u7528RoPE\u65cb\u8f6c\u7f16\u7801</p> <p>\u6ce8\u610f\u529b\u8ba1\u7b97\uff1a \\(\\(\\text{Attention} = \\text{softmax}\\left(\\frac{q_t^C (k_s^C)^T + q_t^R (k_s^R)^T}{\\sqrt{d_h}}\\right)\\)\\)</p>"},{"location":"fundamentals/deepseek-innovations/mla/#3","title":"3. \u6743\u91cd\u5438\u6536\u4f18\u5316","text":"<p>\u76ee\u6807: \u51cf\u5c11\u63a8\u7406\u65f6\u7684\u77e9\u9635\u4e58\u6cd5\u64cd\u4f5c</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_9","title":"\u4f20\u7edf\u8ba1\u7b97\u8def\u5f84","text":"<pre><code># \u9700\u8981\u4e24\u6b21\u77e9\u9635\u4e58\u6cd5\nc_kv = x @ W_down_kv     # \u7b2c\u4e00\u6b21\uff1a\u964d\u7ef4\nk = c_kv @ W_up_k        # \u7b2c\u4e8c\u6b21\uff1a\u5347\u7ef4\u6062\u590d\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_10","title":"\u6743\u91cd\u5438\u6536\u540e","text":"<pre><code># \u9884\u8ba1\u7b97\u5408\u5e76\u6743\u91cd\nW_combined_k = W_down_kv @ W_up_k  # \u79bb\u7ebf\u8ba1\u7b97\nW_combined_v = W_down_kv @ W_up_v\n\n# \u63a8\u7406\u65f6\u53ea\u9700\u4e00\u6b21\u77e9\u9635\u4e58\u6cd5\nk_absorbed = x @ W_combined_k      # \u76f4\u63a5\u5f97\u5230\u7ed3\u679c\nv_absorbed = x @ W_combined_v\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_11","title":"\u8ba1\u7b97\u590d\u6742\u5ea6\u5206\u6790","text":"\u64cd\u4f5c \u4f20\u7edfMLA \u6743\u91cd\u5438\u6536MLA \u51cf\u5c11\u91cf \u77e9\u9635\u4e58\u6cd5\u6b21\u6570 2\u6b21 1\u6b21 50% \u5185\u5b58\u8bbf\u95ee \u9ad8 \u4f4e ~30% \u63a8\u7406\u5ef6\u8fdf \u57fa\u51c6 \u51cf\u5c1115-20% \u663e\u8457"},{"location":"fundamentals/deepseek-innovations/mla/#mla_3","title":"\u5b8c\u6574MLA\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass MLAAttention(nn.Module):\n    \"\"\"Multi-head Latent Attention\u5b8c\u6574\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, num_heads, d_compressed=None, d_rope=None):\n        super().__init__()\n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.head_dim = d_model // num_heads\n\n        # \u538b\u7f29\u7ef4\u5ea6\u8bbe\u7f6e\n        self.d_compressed = d_compressed or d_model // 8  # \u9ed8\u8ba48\u500d\u538b\u7f29\n        self.d_rope = d_rope or self.head_dim // 2        # RoPE\u7ef4\u5ea6\n\n        # Query\u6295\u5f71\n        self.W_q = nn.Linear(d_model, d_model, bias=False)\n\n        # KV\u8054\u5408\u538b\u7f29\u6295\u5f71\n        self.W_down_kv = nn.Linear(d_model, self.d_compressed, bias=False)\n        self.W_up_k = nn.Linear(self.d_compressed, d_model - self.d_rope * num_heads, bias=False)\n        self.W_up_v = nn.Linear(self.d_compressed, d_model, bias=False)\n\n        # RoPE\u90e8\u5206\u7684Key\u6295\u5f71\n        self.W_k_rope = nn.Linear(d_model, self.d_rope * num_heads, bias=False)\n\n        # \u8f93\u51fa\u6295\u5f71\n        self.W_o = nn.Linear(d_model, d_model, bias=False)\n\n        # RoPE\u521d\u59cb\u5316\n        self.rope = RoPEEmbedding(self.d_rope)\n\n        # \u6743\u91cd\u5438\u6536\u4f18\u5316(\u53ef\u9009)\n        self.enable_weight_absorption = True\n        if self.enable_weight_absorption:\n            self._setup_absorbed_weights()\n\n    def _setup_absorbed_weights(self):\n        \"\"\"\u8bbe\u7f6e\u6743\u91cd\u5438\u6536\u7684\u5408\u5e76\u77e9\u9635\"\"\"\n        # \u9884\u8ba1\u7b97\u5408\u5e76\u6743\u91cd\u77e9\u9635\n        with torch.no_grad():\n            self.W_absorbed_k = nn.Parameter(\n                self.W_down_kv.weight.T @ self.W_up_k.weight.T\n            )\n            self.W_absorbed_v = nn.Parameter(\n                self.W_down_kv.weight.T @ self.W_up_v.weight.T\n            )\n\n    def forward(self, x, position_ids=None, kv_cache=None):\n        batch_size, seq_len, d_model = x.shape\n\n        # 1. \u8ba1\u7b97Query\n        q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # 2. \u5206\u79bbQuery\u7684\u538b\u7f29\u90e8\u5206\u548cRoPE\u90e8\u5206\n        q_compressed = q[:, :, :, :-self.d_rope]  # \u8bed\u4e49\u90e8\u5206\n        q_rope = q[:, :, :, -self.d_rope:]        # \u4f4d\u7f6e\u90e8\u5206\n\n        # 3. \u8ba1\u7b97\u538b\u7f29\u7684Key\u548cValue\n        if self.enable_weight_absorption:\n            # \u4f7f\u7528\u6743\u91cd\u5438\u6536\u4f18\u5316\n            k_compressed_flat = x @ self.W_absorbed_k\n            v_flat = x @ self.W_absorbed_v\n        else:\n            # \u6807\u51c6\u4e24\u6b65\u8ba1\u7b97\n            c_kv = x @ self.W_down_kv.weight.T\n            k_compressed_flat = c_kv @ self.W_up_k.weight.T\n            v_flat = c_kv @ self.W_up_v.weight.T\n\n        # \u91cd\u5851\u4e3a\u591a\u5934\u683c\u5f0f\n        k_compressed = k_compressed_flat.view(\n            batch_size, seq_len, self.num_heads, -1\n        )\n        v = v_flat.view(batch_size, seq_len, self.num_heads, self.head_dim)\n\n        # 4. \u8ba1\u7b97RoPE\u90e8\u5206\u7684Key\n        k_rope_flat = x @ self.W_k_rope.weight.T\n        k_rope = k_rope_flat.view(batch_size, seq_len, self.num_heads, self.d_rope)\n\n        # 5. \u5e94\u7528RoPE\u65cb\u8f6c\u7f16\u7801\n        if position_ids is not None:\n            q_rope = self.rope(q_rope, position_ids)\n            k_rope = self.rope(k_rope, position_ids)\n\n        # 6. \u7ec4\u5408\u5b8c\u6574\u7684Key\n        k = torch.cat([k_compressed, k_rope], dim=-1)\n        q = torch.cat([q_compressed, q_rope], dim=-1)\n\n        # 7. KV Cache\u5904\u7406\n        if kv_cache is not None:\n            # \u66f4\u65b0\u7f13\u5b58 - \u53ea\u7f13\u5b58\u538b\u7f29\u540e\u7684\u8868\u793a\n            compressed_cache = torch.cat([k_compressed, k_rope], dim=-1)\n            k, v = kv_cache.update(compressed_cache, v)\n\n        # 8. \u8ba1\u7b97\u6ce8\u610f\u529b\n        # \u8f6c\u7f6e\u7ef4\u5ea6: [batch, seq, heads, head_dim] -&gt; [batch, heads, seq, head_dim]\n        q = q.transpose(1, 2)\n        k = k.transpose(1, 2)\n        v = v.transpose(1, 2)\n\n        # \u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\n        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, v)\n\n        # 9. \u91cd\u5851\u548c\u8f93\u51fa\u6295\u5f71\n        attn_output = attn_output.transpose(1, 2).contiguous().view(\n            batch_size, seq_len, d_model\n        )\n\n        return self.W_o(attn_output)\n\nclass MLAKVCache:\n    \"\"\"MLA\u4e13\u7528\u7684KV Cache\"\"\"\n\n    def __init__(self, max_seq_len, num_heads, compressed_dim, rope_dim):\n        self.max_seq_len = max_seq_len\n        self.num_heads = num_heads\n        self.compressed_dim = compressed_dim\n        self.rope_dim = rope_dim\n\n        # \u53ea\u7f13\u5b58\u538b\u7f29\u540e\u7684\u8868\u793a\n        self.cache_dim = compressed_dim + rope_dim\n        self.cache_k = torch.zeros(max_seq_len, num_heads, self.cache_dim)\n        self.cache_v = torch.zeros(max_seq_len, num_heads, compressed_dim)\n        self.current_len = 0\n\n    def update(self, new_k, new_v):\n        \"\"\"\u66f4\u65b0\u7f13\u5b58\u5e76\u8fd4\u56de\u5b8c\u6574\u7684K,V\"\"\"\n        seq_len = new_k.size(1)\n\n        # \u66f4\u65b0\u7f13\u5b58\n        end_pos = self.current_len + seq_len\n        self.cache_k[:, self.current_len:end_pos] = new_k[0].transpose(0, 1)\n        self.cache_v[:, self.current_len:end_pos] = new_v[0].transpose(0, 1)\n\n        self.current_len = end_pos\n\n        # \u8fd4\u56de\u5b8c\u6574\u7684K,V\n        return (\n            self.cache_k[:self.current_len].transpose(0, 1).unsqueeze(0),\n            self.cache_v[:self.current_len].transpose(0, 1).unsqueeze(0)\n        )\n\n    def get_memory_usage(self):\n        \"\"\"\u83b7\u53d6\u5185\u5b58\u4f7f\u7528\u60c5\u51b5\"\"\"\n        total_elements = self.current_len * self.num_heads * (self.cache_dim + self.compressed_dim)\n        memory_mb = total_elements * 4 / 1024 / 1024  # \u5047\u8bbefloat32\n        return memory_mb\n\nclass RoPEEmbedding(nn.Module):\n    \"\"\"\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\"\"\"\n\n    def __init__(self, dim, base=10000):\n        super().__init__()\n        self.dim = dim\n        self.base = base\n\n        # \u9884\u8ba1\u7b97\u9891\u7387\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer('inv_freq', inv_freq)\n\n    def forward(self, x, position_ids):\n        # x: [batch, seq, heads, dim]\n        # position_ids: [batch, seq]\n\n        seq_len = x.size(1)\n        position = position_ids.float()\n\n        # \u8ba1\u7b97\u89d2\u5ea6\n        freqs = torch.outer(position.flatten(), self.inv_freq)\n\n        # \u751f\u6210cos\u548csin\n        cos = freqs.cos().view(*position.shape, -1)\n        sin = freqs.sin().view(*position.shape, -1)\n\n        # \u5e94\u7528\u65cb\u8f6c\n        x1, x2 = x[..., ::2], x[..., 1::2]\n\n        # \u65cb\u8f6c\u53d8\u6362\n        rotated_x1 = x1 * cos.unsqueeze(2) - x2 * sin.unsqueeze(2)\n        rotated_x2 = x1 * sin.unsqueeze(2) + x2 * cos.unsqueeze(2)\n\n        # \u91cd\u65b0\u7ec4\u5408\n        rotated_x = torch.stack([rotated_x1, rotated_x2], dim=-1).flatten(-2)\n\n        return rotated_x\n\n# \u6027\u80fd\u5bf9\u6bd4\u6d4b\u8bd5\ndef benchmark_mla_vs_mha():\n    \"\"\"\u5bf9\u6bd4MLA\u548cMHA\u7684\u6027\u80fd\"\"\"\n\n    d_model, num_heads = 4096, 32\n    seq_len, batch_size = 2048, 4\n\n    # \u521b\u5efa\u6d4b\u8bd5\u6570\u636e\n    x = torch.randn(batch_size, seq_len, d_model)\n    position_ids = torch.arange(seq_len).unsqueeze(0).repeat(batch_size, 1)\n\n    # MLA\u6a21\u578b\n    mla = MLAAttention(d_model, num_heads, d_compressed=512)\n\n    # \u4f20\u7edfMHA\u6a21\u578b (\u7b80\u5316\u7248\u672c)\n    mha = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n\n    print(\"=== MLA vs MHA \u6027\u80fd\u5bf9\u6bd4 ===\")\n\n    # \u53c2\u6570\u91cf\u5bf9\u6bd4\n    mla_params = sum(p.numel() for p in mla.parameters())\n    mha_params = sum(p.numel() for p in mha.parameters())\n\n    print(f\"MLA\u53c2\u6570\u91cf: {mla_params:,}\")\n    print(f\"MHA\u53c2\u6570\u91cf: {mha_params:,}\")\n    print(f\"\u53c2\u6570\u51cf\u5c11: {(mha_params - mla_params) / mha_params * 100:.1f}%\")\n\n    # KV Cache\u5185\u5b58\u5bf9\u6bd4\n    traditional_kv_cache = num_heads * (d_model // num_heads) * seq_len * 2\n    mla_kv_cache = (512 + 64) * seq_len  # \u538b\u7f29\u7ef4\u5ea6 + RoPE\u7ef4\u5ea6\n\n    print(f\"\u4f20\u7edfKV Cache: {traditional_kv_cache:,} \u5143\u7d20\")\n    print(f\"MLA KV Cache: {mla_kv_cache:,} \u5143\u7d20\")\n    print(f\"\u5185\u5b58\u51cf\u5c11: {traditional_kv_cache / mla_kv_cache:.1f}\u00d7\")\n\n    # \u63a8\u7406\u901f\u5ea6\u6d4b\u8bd5\n    import time\n\n    with torch.no_grad():\n        # MLA\u63a8\u7406\u65f6\u95f4\n        start = time.time()\n        for _ in range(100):\n            _ = mla(x, position_ids)\n        mla_time = time.time() - start\n\n        # MHA\u63a8\u7406\u65f6\u95f4\n        start = time.time()\n        for _ in range(100):\n            _, _ = mha(x, x, x)\n        mha_time = time.time() - start\n\n    print(f\"MLA\u63a8\u7406\u65f6\u95f4: {mla_time:.4f}\u79d2\")\n    print(f\"MHA\u63a8\u7406\u65f6\u95f4: {mha_time:.4f}\u79d2\")\n    print(f\"\u901f\u5ea6\u63d0\u5347: {mha_time / mla_time:.2f}\u00d7\")\n\nif __name__ == \"__main__\":\n    benchmark_mla_vs_mha()\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mla/#_12","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/deepseek-innovations/mla/#q1-mla10kv-cache","title":"Q1: MLA\u5982\u4f55\u5b9e\u73b010\u500d\u4ee5\u4e0a\u7684KV Cache\u538b\u7f29\uff1f","text":"<p>\u6838\u5fc3\u673a\u5236:</p> <ol> <li>\u4f4e\u79e9\u8054\u5408\u538b\u7f29: \u5c06\u539f\u672c\u9700\u8981\u5b58\u50a8\u7684 <code>num_heads \u00d7 head_dim \u00d7 2</code> \u7ef4\u5ea6\u538b\u7f29\u5230 <code>compressed_dim</code></li> <li>RoPE\u89e3\u8026: \u53ea\u5bf9\u5c11\u91cf\u7ef4\u5ea6\u4fdd\u6301\u539f\u59cb\u7cbe\u5ea6\uff0c\u5927\u90e8\u5206\u7ef4\u5ea6\u53ef\u4ee5\u538b\u7f29</li> <li>\u667a\u80fd\u8bbe\u8ba1: \u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u4f4e\u79e9\u7279\u6027\u8fdb\u884c\u6709\u635f\u4f46\u5408\u7406\u7684\u538b\u7f29</li> </ol> <p>\u5177\u4f53\u6570\u5b57: <pre><code>\u4f20\u7edfMHA: 32\u5934 \u00d7 128\u7ef4 \u00d7 2 = 8192\u7ef4/token\nMLA\u538b\u7f29: 512\u7ef4(\u538b\u7f29) + 128\u7ef4(RoPE) = 640\u7ef4/token\n\u538b\u7f29\u6bd4: 8192 \u00f7 640 = 12.8\u00d7\n</code></pre></p>"},{"location":"fundamentals/deepseek-innovations/mla/#q2-rope","title":"Q2: RoPE\u89e3\u8026\u4e3a\u4ec0\u4e48\u662f\u5fc5\u8981\u7684\uff1f","text":"<p>\u6838\u5fc3\u95ee\u9898: \u4f4d\u7f6e\u7f16\u7801\u4e0e\u538b\u7f29\u7684\u51b2\u7a81</p> <p>\u8be6\u7ec6\u89e3\u91ca: 1. RoPE\u4f9d\u8d56: \u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u9700\u8981\u5728\u7279\u5b9a\u7ef4\u5ea6\u7a7a\u95f4\u4e2d\u5de5\u4f5c 2. \u538b\u7f29\u7834\u574f: \u4f4e\u79e9\u538b\u7f29\u4f1a\u7834\u574fRoPE\u7684\u6570\u5b66\u7ed3\u6784 3. \u89e3\u8026\u65b9\u6848: \u5c06\u5411\u91cf\u5206\u4e3a\u8bed\u4e49\u90e8\u5206(\u53ef\u538b\u7f29)\u548c\u4f4d\u7f6e\u90e8\u5206(\u4e0d\u538b\u7f29) 4. \u6548\u679c\u4fdd\u8bc1: \u65e2\u83b7\u5f97\u538b\u7f29\u6536\u76ca\uff0c\u53c8\u4fdd\u6301\u4f4d\u7f6e\u654f\u611f\u6027</p>"},{"location":"fundamentals/deepseek-innovations/mla/#q3","title":"Q3: \u6743\u91cd\u5438\u6536\u4f18\u5316\u7684\u5b9e\u9645\u6548\u679c\u5982\u4f55\uff1f","text":"<p>\u6027\u80fd\u63d0\u5347: - \u8ba1\u7b97\u6b21\u6570: \u4ece2\u6b21\u77e9\u9635\u4e58\u6cd5\u51cf\u5c11\u52301\u6b21 - \u5185\u5b58\u8bbf\u95ee: \u51cf\u5c11\u4e2d\u95f4\u7ed3\u679c\u7684\u5b58\u50a8\u548c\u8bfb\u53d6 - \u63a8\u7406\u5ef6\u8fdf: \u901a\u5e38\u51cf\u5c1115-20%</p> <p>\u5de5\u7a0b\u8003\u8651: - \u9700\u8981\u989d\u5916\u7684\u53c2\u6570\u5b58\u50a8\u7a7a\u95f4 - \u9884\u8ba1\u7b97\u5f00\u9500\uff08\u4e00\u6b21\u6027\uff09 - \u6570\u503c\u7cbe\u5ea6\u53ef\u80fd\u6709\u5fae\u5c0f\u635f\u5931</p>"},{"location":"fundamentals/deepseek-innovations/mla/#_13","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3MLA\u7684\u4e09\u5927\u6838\u5fc3\u6280\u672f\u539f\u7406</li> <li>[ ] \u80fd\u8ba1\u7b97MLA\u76f8\u6bd4MHA\u7684\u5185\u5b58\u538b\u7f29\u6bd4</li> <li>[ ] \u638c\u63e1RoPE\u89e3\u8026\u7684\u5fc5\u8981\u6027\u548c\u5b9e\u73b0\u65b9\u6cd5</li> <li>[ ] \u7406\u89e3\u6743\u91cd\u5438\u6536\u4f18\u5316\u7684\u5de5\u7a0b\u4ef7\u503c</li> <li>[ ] \u80fd\u5b9e\u73b0\u7b80\u5316\u7248\u7684MLA\u6ce8\u610f\u529b\u673a\u5236</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mla/#_14","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1aDeepSeek MoE\u521b\u65b0</li> <li>\u4e0a\u4e00\u7ae0\uff1a\u591a\u5934\u6ce8\u610f\u529b\u53d8\u4f53</li> <li>\u8fd4\u56de\uff1aDeepSeek\u4f18\u5316\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mtp/","title":"MTP\u591atoken\u9884\u6d4b","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3Multi-Token Prediction (MTP)\u6280\u672f\uff0c\u638c\u63e1\u5176\u5982\u4f55\u901a\u8fc7\u5e76\u884c\u9884\u6d4b\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002</p>"},{"location":"fundamentals/deepseek-innovations/mtp/#_2","title":"\ud83d\udcdd \u6280\u672f\u539f\u7406\u89e3\u6790","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#mtp","title":"MTP\u8bbe\u8ba1\u80cc\u666f","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#_3","title":"\u4f20\u7edf\u8bad\u7ec3\u7684\u5c40\u9650\u6027","text":"<p>\u5355token\u9884\u6d4b\u95ee\u9898: <pre><code># \u4f20\u7edfnext-token\u9884\u6d4b\nfor position in sequence:\n    prediction = model(input[:position])\n    loss = cross_entropy(prediction, target[position])\n    # \u6bcf\u6b65\u53ea\u6709\u4e00\u4e2a\u76d1\u7763\u4fe1\u53f7\n</code></pre></p> <p>\u95ee\u9898\u5206\u6790: 1. \u4fe1\u606f\u5bc6\u5ea6\u4f4e: \u6bcf\u4e2a\u524d\u5411\u4f20\u64ad\u53ea\u4ea7\u751f\u4e00\u4e2a\u9884\u6d4b 2. \u957f\u671f\u4f9d\u8d56\u5f31: \u96be\u4ee5\u5efa\u7acb\u8fdc\u8ddd\u79bb\u7684\u4f9d\u8d56\u5173\u7cfb 3. \u8bad\u7ec3\u6548\u7387\u4f4e: \u5e8f\u5217\u8d8a\u957f\uff0c\u6709\u6548\u4fe1\u53f7\u8d8a\u7a00\u758f</p>"},{"location":"fundamentals/deepseek-innovations/mtp/#mtp_1","title":"MTP\u89e3\u51b3\u65b9\u6848","text":"<p>\u6838\u5fc3\u601d\u60f3: \u5728\u6bcf\u4e2a\u4f4d\u7f6e\u540c\u65f6\u9884\u6d4b\u672a\u6765\u591a\u4e2atoken</p> <pre><code># MTP\u591atoken\u9884\u6d4b\nfor position in sequence:\n    predictions = model.multi_head_predict(input[:position])\n    # predictions[0] = \u9884\u6d4bposition+1\u7684token\n    # predictions[1] = \u9884\u6d4bposition+2\u7684token  \n    # predictions[n] = \u9884\u6d4bposition+n+1\u7684token\n\n    multi_loss = sum([\n        cross_entropy(predictions[i], target[position+i+1])\n        for i in range(n_predictions)\n    ])\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#mtp_2","title":"MTP\u67b6\u6784\u8bbe\u8ba1","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#1","title":"1. \u591a\u9884\u6d4b\u5934\u67b6\u6784","text":"<pre><code>class MultiTokenPredictionHead(nn.Module):\n    \"\"\"\u591atoken\u9884\u6d4b\u5934\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, vocab_size, num_predictions, \n                 share_embeddings=True):\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.num_predictions = num_predictions\n        self.share_embeddings = share_embeddings\n\n        # \u5171\u4eab\u7684Transformer\u9aa8\u5e72\u7f51\u7edc\n        self.backbone = TransformerBackbone(d_model)\n\n        # \u591a\u4e2a\u72ec\u7acb\u7684\u9884\u6d4b\u5934\n        if share_embeddings:\n            # \u5171\u4eab\u8f93\u51fa\u5d4c\u5165\u5c42\n            self.output_embedding = nn.Linear(d_model, vocab_size)\n            self.prediction_heads = nn.ModuleList([\n                PredictionHead(d_model, self.output_embedding)\n                for _ in range(num_predictions)\n            ])\n        else:\n            # \u72ec\u7acb\u7684\u9884\u6d4b\u5934\n            self.prediction_heads = nn.ModuleList([\n                nn.Linear(d_model, vocab_size)\n                for _ in range(num_predictions)\n            ])\n\n    def forward(self, x):\n        # \u5171\u4eab\u9aa8\u5e72\u7f51\u7edc\u63d0\u53d6\u7279\u5f81\n        hidden_states = self.backbone(x)\n\n        # \u591a\u4e2a\u9884\u6d4b\u5934\u5e76\u884c\u9884\u6d4b\n        predictions = []\n        for i, head in enumerate(self.prediction_heads):\n            if self.share_embeddings:\n                # \u6dfb\u52a0\u4f4d\u7f6e\u7279\u5b9a\u7684\u8c03\u5236\n                modulated_hidden = self.position_modulation(hidden_states, i)\n                pred = head(modulated_hidden)\n            else:\n                pred = head(hidden_states)\n\n            predictions.append(pred)\n\n        return predictions\n\n    def position_modulation(self, hidden, prediction_step):\n        \"\"\"\u4f4d\u7f6e\u7279\u5b9a\u7684\u7279\u5f81\u8c03\u5236\"\"\"\n        # \u4e3a\u4e0d\u540c\u9884\u6d4b\u6b65\u9aa4\u6dfb\u52a0\u4f4d\u7f6e\u7279\u5b9a\u7684\u53d8\u6362\n        step_embedding = self.step_embeddings[prediction_step]\n        return hidden + step_embedding\n\nclass PredictionHead(nn.Module):\n    \"\"\"\u5355\u4e2a\u9884\u6d4b\u5934\"\"\"\n\n    def __init__(self, d_model, shared_output_layer=None):\n        super().__init__()\n\n        if shared_output_layer is not None:\n            self.output_proj = shared_output_layer\n        else:\n            self.output_proj = nn.Linear(d_model, vocab_size)\n\n        # \u9884\u6d4b\u6b65\u9aa4\u7279\u5b9a\u7684\u53d8\u6362\n        self.step_transform = nn.Sequential(\n            nn.Linear(d_model, d_model),\n            nn.GELU(),\n            nn.Linear(d_model, d_model)\n        )\n\n    def forward(self, hidden_states):\n        # \u6b65\u9aa4\u7279\u5b9a\u53d8\u6362\n        transformed = self.step_transform(hidden_states)\n\n        # \u6b8b\u5dee\u8fde\u63a5\n        output_hidden = hidden_states + transformed\n\n        # \u8f93\u51fa\u6295\u5f71\n        logits = self.output_proj(output_hidden)\n\n        return logits\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#2","title":"2. \u635f\u5931\u51fd\u6570\u8bbe\u8ba1","text":"<pre><code>class MTPLoss(nn.Module):\n    \"\"\"\u591atoken\u9884\u6d4b\u635f\u5931\u51fd\u6570\"\"\"\n\n    def __init__(self, num_predictions, loss_weights=None, \n                 auxiliary_loss_weight=0.1):\n        super().__init__()\n        self.num_predictions = num_predictions\n        self.auxiliary_loss_weight = auxiliary_loss_weight\n\n        if loss_weights is None:\n            # \u9ed8\u8ba4\u6743\u91cd\uff1a\u8ddd\u79bb\u8d8a\u8fdc\u6743\u91cd\u8d8a\u5c0f\n            self.loss_weights = [1.0 / (i + 1) for i in range(num_predictions)]\n        else:\n            self.loss_weights = loss_weights\n\n    def forward(self, predictions, targets, primary_targets):\n        \"\"\"\n        predictions: List[Tensor] - \u591a\u4e2a\u9884\u6d4b\u5934\u7684\u8f93\u51fa\n        targets: Tensor - \u5bf9\u5e94\u7684\u76ee\u6807\u5e8f\u5217\n        primary_targets: Tensor - \u4e3b\u8981\u4efb\u52a1\u76ee\u6807\uff08next-token\u9884\u6d4b\uff09\n        \"\"\"\n\n        # \u4e3b\u8981\u635f\u5931\uff1a\u4f20\u7edfnext-token\u9884\u6d4b\n        primary_loss = F.cross_entropy(predictions[0], primary_targets)\n\n        # \u8f85\u52a9\u635f\u5931\uff1a\u591atoken\u9884\u6d4b\n        auxiliary_losses = []\n        for i, (pred, weight) in enumerate(zip(predictions, self.loss_weights)):\n            if i &lt; targets.size(1):\n                target_slice = targets[:, i]\n                aux_loss = F.cross_entropy(pred, target_slice)\n                auxiliary_losses.append(weight * aux_loss)\n\n        total_auxiliary_loss = sum(auxiliary_losses) / len(auxiliary_losses)\n\n        # \u7ec4\u5408\u635f\u5931\n        total_loss = primary_loss + self.auxiliary_loss_weight * total_auxiliary_loss\n\n        return {\n            'total_loss': total_loss,\n            'primary_loss': primary_loss,\n            'auxiliary_loss': total_auxiliary_loss\n        }\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#3","title":"3. \u8bad\u7ec3\u7b56\u7565","text":"<pre><code>class MTPTrainer:\n    \"\"\"MTP\u8bad\u7ec3\u5668\"\"\"\n\n    def __init__(self, model, num_predictions=4, \n                 teacher_forcing=True):\n        self.model = model\n        self.num_predictions = num_predictions\n        self.teacher_forcing = teacher_forcing\n        self.loss_fn = MTPLoss(num_predictions)\n\n    def train_step(self, batch):\n        input_ids = batch['input_ids']\n        batch_size, seq_len = input_ids.shape\n\n        # \u751f\u6210\u591a\u4e2a\u9884\u6d4b\u76ee\u6807\n        targets = self.prepare_multi_targets(input_ids)\n\n        # \u524d\u5411\u4f20\u64ad\n        predictions = self.model(input_ids)\n\n        # \u8ba1\u7b97\u635f\u5931\n        loss_dict = self.loss_fn(\n            predictions, \n            targets['multi_targets'],\n            targets['primary_target']\n        )\n\n        return loss_dict\n\n    def prepare_multi_targets(self, input_ids):\n        \"\"\"\u51c6\u5907\u591atoken\u9884\u6d4b\u7684\u76ee\u6807\"\"\"\n        batch_size, seq_len = input_ids.shape\n\n        # \u4e3b\u8981\u76ee\u6807\uff1a\u4e0b\u4e00\u4e2atoken\n        primary_target = input_ids[:, 1:]\n\n        # \u591atoken\u76ee\u6807\uff1a\u672a\u6765n\u4e2atoken\n        multi_targets = []\n        for i in range(self.num_predictions):\n            if i + 1 &lt; seq_len:\n                target = input_ids[:, i+1:]\n                # \u586b\u5145\u5230\u76f8\u540c\u957f\u5ea6\n                if target.size(1) &lt; seq_len - 1:\n                    padding = torch.zeros(\n                        batch_size, \n                        seq_len - 1 - target.size(1),\n                        dtype=input_ids.dtype,\n                        device=input_ids.device\n                    )\n                    target = torch.cat([target, padding], dim=1)\n\n                multi_targets.append(target)\n\n        return {\n            'primary_target': primary_target,\n            'multi_targets': multi_targets\n        }\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#mtp_3","title":"MTP\u7684\u4f18\u52bf\u673a\u5236","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#1_1","title":"1. \u5bc6\u96c6\u76d1\u7763\u4fe1\u53f7","text":"<p>\u4f20\u7edf\u8bad\u7ec3: <pre><code># \u6bcf\u4e2a\u4f4d\u7f6e\u53ea\u6709\u4e00\u4e2a\u76d1\u7763\u4fe1\u53f7\nsupervision_density = 1 / sequence_length\n</code></pre></p> <p>MTP\u8bad\u7ec3: <pre><code># \u6bcf\u4e2a\u4f4d\u7f6e\u6709\u591a\u4e2a\u76d1\u7763\u4fe1\u53f7\nsupervision_density = num_predictions / sequence_length\n# \u901a\u5e38\u63d0\u53472-4\u500d\u7684\u4fe1\u53f7\u5bc6\u5ea6\n</code></pre></p>"},{"location":"fundamentals/deepseek-innovations/mtp/#2_1","title":"2. \u957f\u671f\u4f9d\u8d56\u5efa\u6a21","text":"<pre><code>def analyze_dependency_modeling():\n    \"\"\"\u5206\u6790MTP\u5982\u4f55\u6539\u5584\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\"\"\"\n\n    # \u4f20\u7edf\u65b9\u5f0f\uff1a\u53ea\u80fd\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u5efa\u7acb\u4f9d\u8d56\n    traditional_dependency_range = max_gradient_flow_length\n\n    # MTP\u65b9\u5f0f\uff1a\u76f4\u63a5\u5efa\u7acb\u8fdc\u8ddd\u79bb\u76d1\u7763\n    mtp_dependency_range = num_predictions * traditional_dependency_range\n\n    print(f\"\u4f9d\u8d56\u5efa\u6a21\u8303\u56f4\u63d0\u5347: {mtp_dependency_range / traditional_dependency_range}\u00d7\")\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#3_1","title":"3. \u6837\u672c\u6548\u7387\u63d0\u5347","text":"<pre><code>class SampleEfficiencyAnalyzer:\n    \"\"\"\u6837\u672c\u6548\u7387\u5206\u6790\u5668\"\"\"\n\n    def __init__(self, sequence_length, num_predictions):\n        self.seq_len = sequence_length\n        self.num_pred = num_predictions\n\n    def calculate_effective_samples(self, batch_size):\n        \"\"\"\u8ba1\u7b97\u6709\u6548\u6837\u672c\u6570\u91cf\"\"\"\n\n        # \u4f20\u7edf\u65b9\u5f0f\n        traditional_samples = batch_size * (self.seq_len - 1)\n\n        # MTP\u65b9\u5f0f\n        mtp_samples = batch_size * (self.seq_len - 1) * self.num_pred\n\n        efficiency_gain = mtp_samples / traditional_samples\n\n        return {\n            'traditional_samples': traditional_samples,\n            'mtp_samples': mtp_samples,\n            'efficiency_gain': efficiency_gain\n        }\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#_4","title":"\u63a8\u7406\u65f6\u7684\u5e94\u7528","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#1_2","title":"1. \u6295\u673a\u89e3\u7801\u52a0\u901f","text":"<pre><code>class SpeculativeDecoding:\n    \"\"\"\u57fa\u4e8eMTP\u7684\u6295\u673a\u89e3\u7801\"\"\"\n\n    def __init__(self, model_with_mtp, draft_model):\n        self.main_model = model_with_mtp\n        self.draft_model = draft_model\n\n    def generate(self, input_ids, max_length):\n        \"\"\"\u6295\u673a\u89e3\u7801\u751f\u6210\"\"\"\n        current_ids = input_ids\n\n        while current_ids.size(1) &lt; max_length:\n            # 1. \u4f7f\u7528draft model\u5feb\u901f\u751f\u6210\u5019\u9009\n            draft_predictions = self.draft_model.multi_predict(\n                current_ids, num_tokens=4\n            )\n\n            # 2. \u4f7f\u7528\u4e3b\u6a21\u578b\u9a8c\u8bc1\u5019\u9009\n            main_predictions = self.main_model.multi_predict(\n                current_ids, num_tokens=4\n            )\n\n            # 3. \u627e\u5230\u7b2c\u4e00\u4e2a\u4e0d\u5339\u914d\u7684\u4f4d\u7f6e\n            accepted_length = self.find_acceptance_length(\n                draft_predictions, main_predictions\n            )\n\n            # 4. \u63a5\u53d7\u9a8c\u8bc1\u901a\u8fc7\u7684token\n            if accepted_length &gt; 0:\n                new_tokens = draft_predictions[:accepted_length]\n                current_ids = torch.cat([current_ids, new_tokens], dim=1)\n            else:\n                # \u5982\u679c\u90fd\u4e0d\u5339\u914d\uff0c\u4f7f\u7528\u4e3b\u6a21\u578b\u751f\u6210\u4e00\u4e2atoken\n                next_token = self.main_model.generate_next(current_ids)\n                current_ids = torch.cat([current_ids, next_token], dim=1)\n\n        return current_ids\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#2_2","title":"2. \u5e76\u884c\u89e3\u7801","text":"<pre><code>def parallel_decoding_with_mtp(model, input_ids, beam_width=4):\n    \"\"\"\u57fa\u4e8eMTP\u7684\u5e76\u884c\u89e3\u7801\"\"\"\n\n    batch_size, seq_len = input_ids.shape\n\n    # 1. \u4f7f\u7528MTP\u540c\u65f6\u9884\u6d4b\u591a\u4e2a\u4f4d\u7f6e\n    multi_predictions = model.multi_predict(input_ids, num_tokens=beam_width)\n\n    # 2. \u4e3a\u6bcf\u4e2a\u9884\u6d4b\u4f4d\u7f6e\u751f\u6210\u5019\u9009\n    candidates = []\n    for i, predictions in enumerate(multi_predictions):\n        top_k_tokens = torch.topk(predictions, k=beam_width, dim=-1)\n        candidates.append(top_k_tokens.indices)\n\n    # 3. \u6784\u5efa\u5019\u9009\u5e8f\u5217\n    candidate_sequences = []\n    for seq_candidate in itertools.product(*candidates):\n        candidate_seq = torch.tensor(seq_candidate).unsqueeze(0)\n        candidate_sequences.append(\n            torch.cat([input_ids, candidate_seq], dim=1)\n        )\n\n    # 4. \u8bc4\u4f30\u6240\u6709\u5019\u9009\u5e8f\u5217\n    scores = []\n    for candidate in candidate_sequences:\n        score = model.score_sequence(candidate)\n        scores.append(score)\n\n    # 5. \u9009\u62e9\u6700\u4f73\u5019\u9009\n    best_idx = torch.argmax(torch.tensor(scores))\n    return candidate_sequences[best_idx]\n</code></pre>"},{"location":"fundamentals/deepseek-innovations/mtp/#_5","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/deepseek-innovations/mtp/#q1-mtp","title":"Q1: MTP\u5982\u4f55\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff1f","text":"<p>\u6838\u5fc3\u673a\u5236:</p> <ol> <li>\u76d1\u7763\u4fe1\u53f7\u5bc6\u5ea6: \u4ece\u6bcf\u4f4d\u7f6e1\u4e2a\u4fe1\u53f7\u63d0\u5347\u5230n\u4e2a\u4fe1\u53f7</li> <li>\u6837\u672c\u6548\u7387: \u76f8\u540c\u6570\u636e\u4ea7\u751f\u66f4\u591a\u8bad\u7ec3\u4fe1\u53f7</li> <li>\u957f\u671f\u4f9d\u8d56: \u76f4\u63a5\u5efa\u7acb\u8fdc\u8ddd\u79bb\u76d1\u7763\u8fde\u63a5</li> <li>\u5e76\u884c\u8bad\u7ec3: \u591a\u4e2a\u9884\u6d4b\u5934\u53ef\u4ee5\u5e76\u884c\u8ba1\u7b97</li> </ol> <p>\u5177\u4f53\u6570\u636e: <pre><code>\u4f20\u7edf\u8bad\u7ec3\uff1a1\u4e2a\u9884\u6d4b/\u4f4d\u7f6e\nMTP\u8bad\u7ec3\uff1a4\u4e2a\u9884\u6d4b/\u4f4d\u7f6e \u2192 4\u00d7\u4fe1\u53f7\u5bc6\u5ea6\n</code></pre></p>"},{"location":"fundamentals/deepseek-innovations/mtp/#q2-mtp","title":"Q2: MTP\u5728\u63a8\u7406\u65f6\u6709\u4ec0\u4e48\u7528\u9014\uff1f","text":"<p>\u4e3b\u8981\u5e94\u7528:</p> <ol> <li>\u6295\u673a\u89e3\u7801: \u4e00\u6b21\u751f\u6210\u591a\u4e2a\u5019\u9009token\uff0c\u901a\u8fc7\u9a8c\u8bc1\u52a0\u901f</li> <li>\u5e76\u884c\u89e3\u7801: \u540c\u65f6\u8003\u8651\u591a\u4e2a\u672a\u6765\u4f4d\u7f6e\u7684\u9884\u6d4b</li> <li>\u8d28\u91cf\u63d0\u5347: \u66f4\u597d\u7684\u957f\u671f\u89c4\u5212\u80fd\u529b</li> <li>beam search\u4f18\u5316: \u66f4\u51c6\u786e\u7684\u5019\u9009\u8bc4\u4f30</li> </ol>"},{"location":"fundamentals/deepseek-innovations/mtp/#q3-mtp","title":"Q3: MTP\u7684\u8bad\u7ec3\u6210\u672c\u5982\u4f55\uff1f","text":"<p>\u6210\u672c\u5206\u6790:</p> <p>\u589e\u52a0\u7684\u6210\u672c: - \u591a\u4e2a\u9884\u6d4b\u5934\u7684\u53c2\u6570\uff08\u901a\u5e38\u589e\u52a010-20%\uff09 - \u989d\u5916\u7684\u524d\u5411\u8ba1\u7b97\uff08\u589e\u52a0\u9884\u6d4b\u5934\u90e8\u5206\uff09 - \u66f4\u590d\u6742\u7684\u635f\u5931\u8ba1\u7b97</p> <p>\u6536\u76ca: - \u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6 - \u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd - \u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b</p> <p>\u603b\u4f53\u8bc4\u4f30: \u867d\u7136\u5355\u6b65\u6210\u672c\u589e\u52a0\uff0c\u4f46\u6536\u655b\u66f4\u5feb\uff0c\u603b\u4f53\u8bad\u7ec3\u6548\u7387\u63d0\u5347</p>"},{"location":"fundamentals/deepseek-innovations/mtp/#_6","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3MTP\u76f8\u6bd4\u4f20\u7edf\u8bad\u7ec3\u7684\u4f18\u52bf</li> <li>[ ] \u638c\u63e1\u591a\u9884\u6d4b\u5934\u7684\u67b6\u6784\u8bbe\u8ba1</li> <li>[ ] \u4e86\u89e3MTP\u5728\u63a8\u7406\u52a0\u901f\u4e2d\u7684\u5e94\u7528</li> <li>[ ] \u80fd\u5206\u6790MTP\u7684\u6210\u672c\u6548\u76ca\u6743\u8861</li> </ul>"},{"location":"fundamentals/deepseek-innovations/mtp/#_7","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aDeepSeek MoE\u521b\u65b0</li> <li>\u76f8\u5173\u6280\u672f\uff1a\u601d\u7ef4\u94fe\u6280\u672f</li> <li>\u8fd4\u56de\uff1aDeepSeek\u4f18\u5316\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/llm-advanced/","title":"\u7b2c3\u8282\uff1aLLM\u5347\u7ea7\u6280\u672f","text":""},{"location":"fundamentals/llm-advanced/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u4e86\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u524d\u6cbf\u4f18\u5316\u6280\u672f\uff0c\u638c\u63e1MOE\u67b6\u6784\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u57fa\u672c\u6982\u5ff5\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - MOE\u662f\u4ec0\u4e48\uff0c\u6709\u4ec0\u4e48\u597d\u5904\uff1f - \u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u57fa\u672c\u7b56\u7565 - \u5927\u6a21\u578b\u8bad\u7ec3\u7684\u5de5\u7a0b\u6311\u6218</p>"},{"location":"fundamentals/llm-advanced/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a1.5\u5929</p> <ul> <li>Day 1: MOE\u67b6\u6784\u539f\u7406\u6df1\u5165\u7406\u89e3</li> <li>\u534a\u5929: \u5206\u5e03\u5f0f\u8bad\u7ec3\u57fa\u7840\u6982\u5ff5</li> </ul>"},{"location":"fundamentals/llm-advanced/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/llm-advanced/#1-moe","title":"1. MOE\u67b6\u6784","text":"<ul> <li>\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u539f\u7406</li> <li>\u7a00\u758f\u6fc0\u6d3b\u7684\u4f18\u52bf</li> <li>\u5de5\u7a0b\u5b9e\u73b0\u6311\u6218</li> </ul>"},{"location":"fundamentals/llm-advanced/#2","title":"2. \u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<ul> <li>\u6570\u636e\u5e76\u884c vs \u6a21\u578b\u5e76\u884c</li> <li>\u6d41\u6c34\u7ebf\u5e76\u884c</li> <li>\u901a\u4fe1\u4f18\u5316\u7b56\u7565</li> </ul>"},{"location":"fundamentals/llm-advanced/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u4e24\u9879\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u95ee\u9898\u89e3\u7b54: \u80fd\u89e3\u91caMOE\u7684\u5de5\u4f5c\u539f\u7406\u548c\u4f18\u52bf</li> <li>\u6982\u5ff5\u7406\u89e3: \u7406\u89e3\u5927\u6a21\u578b\u8bad\u7ec3\u7684\u5206\u5e03\u5f0f\u7b56\u7565</li> </ol>"},{"location":"fundamentals/llm-advanced/#_5","title":"\ud83d\udca1 \u5b66\u4e60\u5efa\u8bae","text":"<p>\u8fd9\u4e00\u8282\u5185\u5bb9\u76f8\u5bf9\u57fa\u7840\uff0c\u4e3b\u8981\u4ee5\u6982\u5ff5\u7406\u89e3\u4e3a\u4e3b\uff1a - MOE\u90e8\u5206: \u91cd\u70b9\u7406\u89e3\u7a00\u758f\u6fc0\u6d3b\u7684\u4f18\u52bf\u548c\u6311\u6218 - \u5206\u5e03\u5f0f\u8bad\u7ec3: \u4e86\u89e3\u57fa\u672c\u6982\u5ff5\u5373\u53ef\uff0c\u65e0\u9700\u6df1\u5165\u5de5\u7a0b\u7ec6\u8282 - \u5b66\u4e60\u91cd\u70b9: \u4e3a\u540e\u7eedDeepSeek MoE\u7b49\u9ad8\u7ea7\u6280\u672f\u6253\u4e0b\u57fa\u7840</p>"},{"location":"fundamentals/llm-advanced/#_6","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u6a21\u5757\u5f00\u59cb\u5b66\u4e60\uff0c\u91cd\u70b9\u638c\u63e1\u6838\u5fc3\u6982\u5ff5\u548c\u539f\u7406\u3002</p>"},{"location":"fundamentals/llm-advanced/distributed/","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3","text":""},{"location":"fundamentals/llm-advanced/distributed/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u4e86\u89e3\u5927\u6a21\u578b\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u57fa\u672c\u7b56\u7565\u548c\u6982\u5ff5\u3002</p>"},{"location":"fundamentals/llm-advanced/distributed/#_3","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/llm-advanced/distributed/#_4","title":"\u4e3b\u8981\u5e76\u884c\u7b56\u7565","text":"<ol> <li>\u6570\u636e\u5e76\u884c: \u4e0d\u540c\u8bbe\u5907\u5904\u7406\u4e0d\u540c\u7684\u6570\u636e\u6279\u6b21</li> <li>\u6a21\u578b\u5e76\u884c: \u5c06\u6a21\u578b\u53c2\u6570\u5206\u5e03\u5230\u4e0d\u540c\u8bbe\u5907</li> <li>\u6d41\u6c34\u7ebf\u5e76\u884c: \u5c06\u6a21\u578b\u5c42\u5206\u5e03\u5230\u4e0d\u540c\u8bbe\u5907\uff0c\u5f62\u6210\u6d41\u6c34\u7ebf</li> </ol>"},{"location":"fundamentals/llm-advanced/distributed/#_5","title":"\u5de5\u7a0b\u6311\u6218","text":"<ul> <li>\u901a\u4fe1\u5f00\u9500\u4f18\u5316</li> <li>\u5185\u5b58\u7ba1\u7406</li> <li>\u8d1f\u8f7d\u5747\u8861</li> </ul>"},{"location":"fundamentals/llm-advanced/distributed/#_6","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/llm-advanced/distributed/#q1","title":"Q1: \u5206\u5e03\u5f0f\u8bad\u7ec3\u6709\u54ea\u4e9b\u4e3b\u8981\u7b56\u7565\uff1f","text":"<p>\u6838\u5fc3\u7b56\u7565: - \u6570\u636e\u5e76\u884c: \u7b80\u5355\u4f46\u901a\u4fe1\u5f00\u9500\u5927 - \u6a21\u578b\u5e76\u884c: \u9002\u5408\u8d85\u5927\u6a21\u578b - \u6df7\u5408\u5e76\u884c: \u7ed3\u5408\u591a\u79cd\u7b56\u7565\u7684\u73b0\u4ee3\u65b9\u6848</p>"},{"location":"fundamentals/llm-advanced/distributed/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u4e86\u89e3\u57fa\u672c\u7684\u5e76\u884c\u8bad\u7ec3\u6982\u5ff5</li> <li>[ ] \u7406\u89e3\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u4e3b\u8981\u6311\u6218</li> </ul>"},{"location":"fundamentals/llm-advanced/distributed/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aMOE\u67b6\u6784</li> <li>\u4e0b\u4e00\u8282\uff1aContext Engineering</li> <li>\u8fd4\u56de\uff1aLLM\u5347\u7ea7\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/llm-advanced/moe/","title":"MOE\u67b6\u6784","text":""},{"location":"fundamentals/llm-advanced/moe/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3\u4e13\u5bb6\u6df7\u5408\u6a21\u578b(Mixture of Experts)\u7684\u6838\u5fc3\u539f\u7406\u3001\u8def\u7531\u673a\u5236\u548c\u5de5\u7a0b\u5b9e\u73b0\u6311\u6218\u3002</p>"},{"location":"fundamentals/llm-advanced/moe/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/llm-advanced/moe/#moe_1","title":"MOE\u57fa\u672c\u6982\u5ff5","text":"<p>Mixture of Experts (MOE) \u662f\u4e00\u79cd\u7a00\u758f\u6fc0\u6d3b\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u673a\u5236\u5c06\u4e0d\u540c\u8f93\u5165\u5206\u914d\u7ed9\u4e13\u95e8\u7684\"\u4e13\u5bb6\"\u5b50\u7f51\u7edc\u5904\u7406\u3002</p>"},{"location":"fundamentals/llm-advanced/moe/#_3","title":"\u6838\u5fc3\u601d\u60f3","text":"<ul> <li>\u6761\u4ef6\u8ba1\u7b97: \u6839\u636e\u8f93\u5165\u5185\u5bb9\u52a8\u6001\u9009\u62e9\u8ba1\u7b97\u8def\u5f84</li> <li>\u4e13\u5bb6\u5206\u5de5: \u4e0d\u540c\u4e13\u5bb6\u5b66\u4e60\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u7684\u6a21\u5f0f</li> <li>\u7a00\u758f\u6fc0\u6d3b: \u6bcf\u6b21\u53ea\u6fc0\u6d3b\u5c11\u6570\u4e13\u5bb6\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6</li> </ul>"},{"location":"fundamentals/llm-advanced/moe/#moe_2","title":"MOE\u6838\u5fc3\u7ec4\u4ef6","text":""},{"location":"fundamentals/llm-advanced/moe/#1-experts","title":"1. \u4e13\u5bb6\u7f51\u7edc (Experts)","text":"<pre><code>class Expert(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super().__init__()\n        self.w1 = nn.Linear(d_model, d_ff)\n        self.w2 = nn.Linear(d_ff, d_model)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        return self.w2(self.activation(self.w1(x)))\n\n# \u591a\u4e2a\u4e13\u5bb6\u7ec4\u6210\u4e13\u5bb6\u6c60\nexperts = nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_experts)])\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#2-routergate","title":"2. \u8def\u7531\u7f51\u7edc (Router/Gate)","text":"<pre><code>class Router(nn.Module):\n    def __init__(self, d_model, num_experts):\n        super().__init__()\n        self.gate = nn.Linear(d_model, num_experts)\n\n    def forward(self, x):\n        # \u8ba1\u7b97\u6bcf\u4e2a\u4e13\u5bb6\u7684\u95e8\u63a7\u5206\u6570\n        logits = self.gate(x)  # [batch, seq_len, num_experts]\n\n        # \u9009\u62e9Top-K\u4e13\u5bb6\n        top_k_logits, top_k_indices = torch.topk(logits, k=2, dim=-1)\n        top_k_probs = F.softmax(top_k_logits, dim=-1)\n\n        return top_k_probs, top_k_indices\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#3","title":"3. \u805a\u5408\u673a\u5236","text":"<pre><code>def moe_forward(x, experts, router):\n    # \u83b7\u53d6\u8def\u7531\u4fe1\u606f\n    probs, indices = router(x)  # [batch, seq_len, k], [batch, seq_len, k]\n\n    # \u521d\u59cb\u5316\u8f93\u51fa\n    output = torch.zeros_like(x)\n\n    # \u5bf9\u6bcf\u4e2a\u9009\u4e2d\u7684\u4e13\u5bb6\u8ba1\u7b97\u8f93\u51fa\n    for i in range(k):\n        expert_idx = indices[:, :, i]\n        expert_prob = probs[:, :, i]\n\n        # \u83b7\u53d6\u5bf9\u5e94\u4e13\u5bb6\u7684\u8f93\u51fa\n        expert_output = experts[expert_idx](x)\n\n        # \u6309\u6982\u7387\u52a0\u6743\n        output += expert_prob.unsqueeze(-1) * expert_output\n\n    return output\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#_4","title":"\u8def\u7531\u7b56\u7565\u8be6\u89e3","text":""},{"location":"fundamentals/llm-advanced/moe/#1-token-choice","title":"1. Token-Choice\u8def\u7531","text":"<p>\u673a\u5236: \u6bcf\u4e2atoken\u9009\u62e9top-k\u4e2a\u4e13\u5bb6 <pre><code>def token_choice_routing(x, num_experts, k=2):\n    \"\"\"\u6bcf\u4e2atoken\u9009\u62e9k\u4e2a\u4e13\u5bb6\"\"\"\n    batch_size, seq_len, d_model = x.shape\n\n    # \u8def\u7531\u6253\u5206\n    router_logits = router(x)  # [batch, seq_len, num_experts]\n\n    # \u9009\u62e9top-k\u4e13\u5bb6\n    top_k_probs, top_k_indices = torch.topk(router_logits, k, dim=-1)\n    top_k_probs = F.softmax(top_k_probs, dim=-1)\n\n    return top_k_probs, top_k_indices\n</code></pre></p> <p>\u4f18\u52bf:  - \u4fdd\u8bc1\u6bcf\u4e2atoken\u90fd\u88ab\u5904\u7406 - \u63a7\u5236\u8ba1\u7b97\u590d\u6742\u5ea6\u7a33\u5b9a</p> <p>\u52a3\u52bf: - \u53ef\u80fd\u5bfc\u81f4\u4e13\u5bb6\u8d1f\u8f7d\u4e0d\u5747\u8861 - \u90e8\u5206\u4e13\u5bb6\u53ef\u80fd\u5f97\u4e0d\u5230\u8bad\u7ec3</p>"},{"location":"fundamentals/llm-advanced/moe/#2-expert-choice","title":"2. Expert-Choice\u8def\u7531","text":"<p>\u673a\u5236: \u6bcf\u4e2a\u4e13\u5bb6\u9009\u62e9top-k\u4e2atoken <pre><code>def expert_choice_routing(x, num_experts, capacity):\n    \"\"\"\u6bcf\u4e2a\u4e13\u5bb6\u9009\u62e9\u56fa\u5b9a\u6570\u91cf\u7684token\"\"\"\n    batch_size, seq_len, d_model = x.shape\n\n    # \u8def\u7531\u6253\u5206\n    router_logits = router(x)  # [batch, seq_len, num_experts]\n\n    # \u4e3a\u6bcf\u4e2a\u4e13\u5bb6\u9009\u62e9top tokens\n    expert_assignments = {}\n    for expert_id in range(num_experts):\n        expert_scores = router_logits[:, :, expert_id]\n        top_tokens = torch.topk(expert_scores.flatten(), capacity).indices\n        expert_assignments[expert_id] = top_tokens\n\n    return expert_assignments\n</code></pre></p> <p>\u4f18\u52bf: - \u81ea\u7136\u7684\u8d1f\u8f7d\u5747\u8861 - \u4e13\u5bb6\u80fd\u591f\u9009\u62e9\u6700\u76f8\u5173\u7684\u8f93\u5165</p> <p>\u52a3\u52bf: - \u53ef\u80fd\u6709token\u88ab\u4e22\u5f03 - \u5b9e\u73b0\u66f4\u590d\u6742</p>"},{"location":"fundamentals/llm-advanced/moe/#_5","title":"\u8d1f\u8f7d\u5747\u8861\u6280\u672f","text":""},{"location":"fundamentals/llm-advanced/moe/#1","title":"1. \u8f85\u52a9\u635f\u5931\u51fd\u6570","text":"<pre><code>def load_balancing_loss(router_probs, expert_indices, num_experts):\n    \"\"\"\u8ba1\u7b97\u8d1f\u8f7d\u5747\u8861\u635f\u5931\"\"\"\n    # \u8ba1\u7b97\u6bcf\u4e2a\u4e13\u5bb6\u88ab\u9009\u62e9\u7684\u9891\u7387\n    expert_counts = torch.zeros(num_experts)\n    for expert_id in range(num_experts):\n        expert_counts[expert_id] = (expert_indices == expert_id).float().sum()\n\n    # \u7406\u60f3\u60c5\u51b5\u4e0b\u6bcf\u4e2a\u4e13\u5bb6\u5904\u7406\u76f8\u540c\u6570\u91cf\u7684token\n    ideal_count = expert_indices.numel() / num_experts\n\n    # \u8ba1\u7b97\u8d1f\u8f7d\u4e0d\u5747\u8861\u635f\u5931\n    load_loss = torch.var(expert_counts) / (ideal_count ** 2)\n\n    return load_loss\n\n# \u603b\u635f\u5931 = \u4e3b\u4efb\u52a1\u635f\u5931 + \u03bb * \u8d1f\u8f7d\u5747\u8861\u635f\u5931\ntotal_loss = task_loss + lambda_balance * load_balancing_loss(probs, indices, num_experts)\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#2","title":"2. \u4e13\u5bb6\u5bb9\u91cf\u9650\u5236","text":"<pre><code>def capacity_limited_routing(router_logits, capacity_factor=1.25):\n    \"\"\"\u9650\u5236\u6bcf\u4e2a\u4e13\u5bb6\u7684\u5904\u7406\u5bb9\u91cf\"\"\"\n    num_tokens = router_logits.shape[0] * router_logits.shape[1]\n    expert_capacity = int(capacity_factor * num_tokens / num_experts)\n\n    # \u4e3a\u6bcf\u4e2a\u4e13\u5bb6\u5206\u914d\u56fa\u5b9a\u5bb9\u91cf\n    expert_assignments = []\n    expert_counts = torch.zeros(num_experts)\n\n    for token_idx in range(num_tokens):\n        # \u83b7\u53d6\u5f53\u524dtoken\u7684\u4e13\u5bb6\u504f\u597d\n        token_probs = F.softmax(router_logits.flatten()[token_idx], dim=-1)\n\n        # \u9009\u62e9\u5bb9\u91cf\u672a\u6ee1\u7684\u6700\u4f18\u4e13\u5bb6\n        for expert_id in torch.argsort(token_probs, descending=True):\n            if expert_counts[expert_id] &lt; expert_capacity:\n                expert_assignments.append((token_idx, expert_id))\n                expert_counts[expert_id] += 1\n                break\n\n    return expert_assignments\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#moe_3","title":"MOE\u53d8\u4f53\u548c\u4f18\u5316","text":""},{"location":"fundamentals/llm-advanced/moe/#1-moe-vs-moe","title":"1. \u7a00\u758fMOE vs \u5bc6\u96c6MOE","text":"\u7279\u6027 \u7a00\u758fMOE \u5bc6\u96c6MOE \u6fc0\u6d3b\u4e13\u5bb6\u6570 Top-K (K&lt;&lt;N) \u5168\u90e8\u4e13\u5bb6 \u8ba1\u7b97\u590d\u6742\u5ea6 O(K) O(N) \u53c2\u6570\u5229\u7528\u7387 \u4f4e \u9ad8 \u6269\u5c55\u6027 \u597d \u5dee"},{"location":"fundamentals/llm-advanced/moe/#2-moe","title":"2. \u5c42\u7ea7MOE","text":"<pre><code>class HierarchicalMoE(nn.Module):\n    \"\"\"\u5c42\u7ea7\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\"\"\"\n\n    def __init__(self, d_model, num_coarse_experts, num_fine_experts):\n        super().__init__()\n        # \u7c97\u7c92\u5ea6\u4e13\u5bb6\u9009\u62e9\n        self.coarse_router = Router(d_model, num_coarse_experts)\n\n        # \u7ec6\u7c92\u5ea6\u4e13\u5bb6\u7ec4\n        self.fine_routers = nn.ModuleList([\n            Router(d_model, num_fine_experts) \n            for _ in range(num_coarse_experts)\n        ])\n\n        # \u4e13\u5bb6\u7f51\u7edc\n        self.experts = nn.ModuleList([\n            nn.ModuleList([Expert(d_model, d_ff) for _ in range(num_fine_experts)])\n            for _ in range(num_coarse_experts)\n        ])\n\n    def forward(self, x):\n        # \u7b2c\u4e00\u5c42\uff1a\u9009\u62e9\u7c97\u7c92\u5ea6\u4e13\u5bb6\n        coarse_probs, coarse_indices = self.coarse_router(x)\n\n        output = torch.zeros_like(x)\n\n        # \u7b2c\u4e8c\u5c42\uff1a\u5728\u9009\u4e2d\u7684\u7c97\u7c92\u5ea6\u4e13\u5bb6\u5185\u9009\u62e9\u7ec6\u7c92\u5ea6\u4e13\u5bb6\n        for i, coarse_idx in enumerate(coarse_indices[0, 0]):  # \u7b80\u5316\u5904\u7406\n            fine_probs, fine_indices = self.fine_routers[coarse_idx](x)\n\n            # \u8ba1\u7b97\u7ec6\u7c92\u5ea6\u4e13\u5bb6\u8f93\u51fa\n            for j, fine_idx in enumerate(fine_indices[0, 0]):\n                expert_output = self.experts[coarse_idx][fine_idx](x)\n                weight = coarse_probs[0, 0, i] * fine_probs[0, 0, j]\n                output += weight * expert_output\n\n        return output\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#_6","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3\u6311\u6218","text":""},{"location":"fundamentals/llm-advanced/moe/#1_1","title":"1. \u901a\u4fe1\u6a21\u5f0f","text":"<pre><code># All-to-All\u901a\u4fe1\u6a21\u5f0f\ndef all_to_all_communication(tokens, expert_assignments):\n    \"\"\"\n    \u5c06tokens\u5206\u53d1\u5230\u4e0d\u540c\u8bbe\u5907\u4e0a\u7684\u4e13\u5bb6\n    \"\"\"\n    # Token dispatch: \u6839\u636e\u8def\u7531\u7ed3\u679c\u91cd\u65b0\u5206\u5e03token\n    expert_inputs = {}\n    for expert_id, token_list in expert_assignments.items():\n        device_id = expert_id % world_size\n        expert_inputs[device_id] = expert_inputs.get(device_id, []) + token_list\n\n    # \u8de8\u8bbe\u5907\u901a\u4fe1\n    for device_id, tokens in expert_inputs.items():\n        send_to_device(tokens, device_id)\n\n    # Expert processing\n    expert_outputs = process_on_experts(expert_inputs)\n\n    # Token combine: \u6536\u96c6\u4e13\u5bb6\u8f93\u51fa\n    final_outputs = all_gather(expert_outputs)\n\n    return final_outputs\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#2_1","title":"2. \u5185\u5b58\u4f18\u5316","text":"<pre><code>class MemoryEfficientMoE(nn.Module):\n    \"\"\"\u5185\u5b58\u9ad8\u6548\u7684MOE\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, num_experts, expert_capacity):\n        super().__init__()\n        self.num_experts = num_experts\n        self.expert_capacity = expert_capacity\n\n        # \u5171\u4eab\u4e13\u5bb6\u53c2\u6570\u5b58\u50a8\n        self.expert_weights = nn.Parameter(torch.randn(num_experts, d_model, d_ff))\n        self.router = Router(d_model, num_experts)\n\n    def forward(self, x):\n        batch_size, seq_len, d_model = x.shape\n\n        # \u83b7\u53d6\u8def\u7531\u51b3\u7b56\n        probs, indices = self.router(x)\n\n        # \u91cd\u5851\u4e3a\u4e13\u5bb6\u6279\u5904\u7406\u683c\u5f0f\n        flat_x = x.view(-1, d_model)\n        flat_probs = probs.view(-1, 2)\n        flat_indices = indices.view(-1, 2)\n\n        # \u6279\u91cf\u5904\u7406\u51cf\u5c11\u5185\u5b58\u5360\u7528\n        outputs = []\n        for batch_start in range(0, flat_x.shape[0], self.expert_capacity):\n            batch_end = min(batch_start + self.expert_capacity, flat_x.shape[0])\n            batch_output = self._process_batch(\n                flat_x[batch_start:batch_end],\n                flat_probs[batch_start:batch_end],\n                flat_indices[batch_start:batch_end]\n            )\n            outputs.append(batch_output)\n\n        # \u91cd\u7ec4\u8f93\u51fa\n        final_output = torch.cat(outputs, dim=0)\n        return final_output.view(batch_size, seq_len, d_model)\n</code></pre>"},{"location":"fundamentals/llm-advanced/moe/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/llm-advanced/moe/#q1-moe","title":"Q1: MOE\u662f\u4ec0\u4e48\uff0c\u5b83\u6709\u4ec0\u4e48\u597d\u5904\u5462\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54: MOE\u662f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u673a\u5236\u8ba9\u4e0d\u540c\u7684\u5b50\u7f51\u7edc(\u4e13\u5bb6)\u5904\u7406\u4e0d\u540c\u7684\u8f93\u5165\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u91cf\u76f8\u5bf9\u7a33\u5b9a\u7684\u60c5\u51b5\u4e0b\u5927\u5e45\u589e\u52a0\u6a21\u578b\u5bb9\u91cf\u3002</p> <p>\u8be6\u7ec6\u89e3\u91ca: - \u5de5\u4f5c\u539f\u7406: \u8f93\u5165\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u9009\u62e9\u6fc0\u6d3b\u5c11\u6570\u51e0\u4e2a\u4e13\u5bb6 - \u6838\u5fc3\u4f18\u52bf: \u53c2\u6570\u91cf\u5927\u4f46\u8ba1\u7b97\u91cf\u53ef\u63a7\uff0c\u5b9e\u73b0\u6761\u4ef6\u8ba1\u7b97 - \u5b9e\u9645\u5e94\u7528: Google\u7684Switch Transformer\u3001GLaM\u3001PaLM\u7b49\u5927\u6a21\u578b</p>"},{"location":"fundamentals/llm-advanced/moe/#q2-moe","title":"Q2: MOE\u7684\u4e3b\u8981\u6311\u6218\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u6311\u6218:</p> <ol> <li>\u8d1f\u8f7d\u5747\u8861: \u9632\u6b62\u6240\u6709\u8f93\u5165\u90fd\u8def\u7531\u5230\u5c11\u6570\u4e13\u5bb6</li> <li>\u901a\u4fe1\u5f00\u9500: \u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684All-to-All\u901a\u4fe1\u6210\u672c\u9ad8</li> <li>\u8bad\u7ec3\u4e0d\u7a33\u5b9a: \u8def\u7531\u7f51\u7edc\u7684\u8bad\u7ec3\u53ef\u80fd\u4e0d\u6536\u655b</li> <li>\u63a8\u7406\u590d\u6742\u5ea6: \u52a8\u6001\u8def\u7531\u589e\u52a0\u63a8\u7406\u65f6\u7684\u8c03\u5ea6\u590d\u6742\u6027</li> </ol>"},{"location":"fundamentals/llm-advanced/moe/#q3-moe","title":"Q3: \u5982\u4f55\u89e3\u51b3MOE\u7684\u8d1f\u8f7d\u5747\u8861\u95ee\u9898\uff1f","text":"<p>\u4e3b\u8981\u65b9\u6cd5:</p> <ol> <li>\u8f85\u52a9\u635f\u5931: \u6dfb\u52a0\u9f13\u52b1\u5747\u5300\u5206\u5e03\u7684\u6b63\u5219\u5316\u9879</li> <li>\u4e13\u5bb6\u5bb9\u91cf\u9650\u5236: \u9650\u5236\u6bcf\u4e2a\u4e13\u5bb6\u5904\u7406\u7684token\u6570\u91cf</li> <li>Expert-Choice\u8def\u7531: \u8ba9\u4e13\u5bb6\u4e3b\u52a8\u9009\u62e9\u8981\u5904\u7406\u7684token</li> <li>\u566a\u58f0\u6ce8\u5165: \u5728\u8def\u7531\u51b3\u7b56\u4e2d\u52a0\u5165\u968f\u673a\u6027</li> </ol>"},{"location":"fundamentals/llm-advanced/moe/#_8","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3MOE\u7684\u57fa\u672c\u67b6\u6784\u548c\u5de5\u4f5c\u539f\u7406</li> <li>[ ] \u638c\u63e1\u4e0d\u540c\u8def\u7531\u7b56\u7565\u7684\u4f18\u52a3</li> <li>[ ] \u4e86\u89e3\u8d1f\u8f7d\u5747\u8861\u7684\u91cd\u8981\u6027\u548c\u89e3\u51b3\u65b9\u6848</li> <li>[ ] \u7406\u89e3MOE\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u6311\u6218</li> </ul>"},{"location":"fundamentals/llm-advanced/moe/#_9","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1a\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u4e0b\u4e00\u7ae0\uff1aDeepSeek\u4f18\u5316\u6280\u672f</li> <li>\u8fd4\u56de\uff1aLLM\u5347\u7ea7\u6280\u672f\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/","title":"\u7b2c1\u8282\uff1aTransformer\u57fa\u7840","text":""},{"location":"fundamentals/transformer/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1Transformer\u67b6\u6784\u7684\u6838\u5fc3\u6982\u5ff5\uff0c\u7406\u89e3Attention\u673a\u5236\u539f\u7406\uff0c\u533a\u5206\u4e0d\u540c\u7684\u8bed\u8a00\u6a21\u578b\u67b6\u6784\uff0c\u4e3a\u9762\u8bd5\u505a\u597d\u57fa\u7840\u51c6\u5907\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - Attention\u8ba1\u7b97\u516c\u5f0f\u548c\u539f\u7406 - FFN\u5728Transformer\u4e2d\u7684\u4f5c\u7528 - Encoder-Only vs Decoder-Only\u67b6\u6784\u5dee\u5f02 - \u4e3a\u4ec0\u4e48\u4e3b\u6d41\u5927\u6a21\u578b\u9009\u62e9Decoder-Only - GPT\u548cBERT\u7684\u67b6\u6784\u533a\u522b - BPE\u548cWordPiece\u7684\u533a\u522b</p>"},{"location":"fundamentals/transformer/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a3\u5929</p> <ul> <li>Day 1: Attention\u673a\u5236 + FFN\u6280\u672f\u6df1\u5165\u7406\u89e3</li> <li>Day 2: \u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784 + \u8bed\u8a00\u6a21\u578b\u67b6\u6784\u5bf9\u6bd4</li> <li>Day 3: Tokenizer\u6280\u672f + \u7efc\u5408\u5b9e\u8df5\u4e0e\u590d\u4e60</li> </ul>"},{"location":"fundamentals/transformer/#_3","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"fundamentals/transformer/#1-attention","title":"1. Attention\u673a\u5236","text":"<ul> <li>\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u516c\u5f0f\u63a8\u5bfc</li> <li>Softmax\u548c\u7f29\u653e\u56e0\u5b50\u4f5c\u7528</li> <li>\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236</li> <li>\u4ee3\u7801\u5b9e\u73b0\u7ec3\u4e60</li> </ul>"},{"location":"fundamentals/transformer/#2","title":"2. \u524d\u9988\u795e\u7ecf\u7f51\u7edc","text":"<ul> <li>FFN\u7ed3\u6784\u548c\u529f\u80fd\u539f\u7406</li> <li>\u6fc0\u6d3b\u51fd\u6570\u6f14\u8fdb(ReLU\u2192GELU\u2192SwiGLU)</li> <li>\u77e5\u8bc6\u5b58\u50a8\u673a\u5236</li> <li>\u4e0e\u6ce8\u610f\u529b\u7684\u4e92\u8865\u5173\u7cfb</li> </ul>"},{"location":"fundamentals/transformer/#3-","title":"3. \u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784","text":"<ul> <li>\u539f\u59cbTransformer\u5b8c\u6574\u67b6\u6784</li> <li>\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236\u8be6\u89e3</li> <li>\u63a9\u7801\u673a\u5236\u548c\u4ea4\u53c9\u6ce8\u610f\u529b</li> <li>\u73b0\u4ee3\u67b6\u6784\u6f14\u8fdb\u8d8b\u52bf</li> </ul>"},{"location":"fundamentals/transformer/#4","title":"4. \u8bed\u8a00\u6a21\u578b\u67b6\u6784","text":"<ul> <li>Encoder-Only vs Decoder-Only</li> <li>GPT vs BERT\u67b6\u6784\u5bf9\u6bd4</li> <li>\u4e3b\u6d41\u6a21\u578b\u9009\u62e9\u5206\u6790</li> <li>\u6a21\u578b\u67b6\u6784\u56fe\u89e3</li> </ul>"},{"location":"fundamentals/transformer/#5-tokenizer","title":"5. Tokenizer\u6280\u672f","text":"<ul> <li>BPE/WordPiece/Unigram\u7b97\u6cd5\u5bf9\u6bd4</li> <li>Byte-level BPE\u73b0\u4ee3\u65b9\u6848</li> <li>\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u548c\u4f18\u5316</li> <li>\u591a\u8bed\u8a00\u5904\u7406\u7b56\u7565</li> </ul>"},{"location":"fundamentals/transformer/#_4","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u4e09\u9879\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ol> <li>\u95ee\u9898\u89e3\u7b54: \u80fd\u7528\u81ea\u5df1\u7684\u8bdd\u56de\u7b54\u6240\u67096\u4e2a\u6838\u5fc3\u9762\u8bd5\u95ee\u9898</li> <li>\u4ee3\u7801\u5b9e\u73b0: \u5b8c\u6210Self-Attention\u3001FFN\u3001Tokenizer\u7684\u7f16\u7a0b\u7ec3\u4e60</li> <li>\u67b6\u6784\u7406\u89e3: \u80fd\u753b\u51fa\u5e76\u89e3\u91ca\u5b8c\u6574\u7684Transformer\u67b6\u6784\u56fe</li> </ol>"},{"location":"fundamentals/transformer/#_5","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u4e00\u4e2a\u5b50\u6a21\u5757\u5f00\u59cb\u4f60\u7684\u5b66\u4e60\u4e4b\u65c5\uff01\u5efa\u8bae\u6309\u987a\u5e8f\u5b66\u4e60\uff0c\u6bcf\u4e2a\u6a21\u5757\u90fd\u5305\u542b\u7cbe\u9009\u7684\u9605\u8bfb\u6750\u6599\u3001\u89c6\u9891\u8d44\u6e90\u548c\u5b9e\u6218\u7ec3\u4e60\u3002</p>"},{"location":"fundamentals/transformer/attention/","title":"Attention\u673a\u5236","text":""},{"location":"fundamentals/transformer/attention/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3Self-Attention\u673a\u5236\u7684\u6570\u5b66\u539f\u7406\u548c\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u638c\u63e1\u9762\u8bd5\u4e2d\u7684\u9ad8\u9891\u95ee\u9898\u3002</p>"},{"location":"fundamentals/transformer/attention/#_2","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/transformer/attention/#_3","title":"\u5fc5\u8bfb\u6587\u7ae0","text":"<ol> <li>\u300aAttention is All You Need\u300b\u6d45\u8bfb\uff08\u7b80\u4ecb+\u4ee3\u7801\uff09 - \u79d1\u5b66\u7a7a\u95f4</li> <li>GPT\u4e0eBERT\u5dee\u522b\u6df1\u5165\u89e3\u6790 - \u77e5\u4e4e</li> <li>\u6df1\u5165\u6d45\u51fa\u7406\u89e3transformer - \u77e5\u4e4e  </li> <li>Transformer\u6a21\u578b\u8be6\u89e3\uff08\u56fe\u89e3\u6700\u5b8c\u6574\u7248\uff09 - \u77e5\u4e4e</li> <li>Transformer\u4f4d\u7f6e\u7f16\u7801\uff08\u57fa\u7840\uff09 - \u77e5\u4e4e</li> </ol>"},{"location":"fundamentals/transformer/attention/#_4","title":"\u539f\u8bba\u6587","text":"<ul> <li>Attention Is All You Need - \u7ecf\u5178\u5fc5\u8bfb</li> </ul>"},{"location":"fundamentals/transformer/attention/#_5","title":"\ud83c\udfac \u89c6\u9891\u6750\u6599","text":"<p>\u5b66\u4e60\u5efa\u8bae\uff1a \u500d\u901f\u89c2\u770b\uff0c\u91cd\u70b9\u7406\u89e3\u6982\u5ff5\u800c\u975e\u7ec6\u8282</p> <ol> <li>GPT\uff0cGPT-2\uff0cGPT-3 \u8bba\u6587\u7cbe\u8bfb - \u54d4\u54e9\u54d4\u54e9</li> <li>Llama 3.1\u8bba\u6587\u7cbe\u8bfb - \u54d4\u54e9\u54d4\u54e9</li> </ol>"},{"location":"fundamentals/transformer/attention/#_6","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/attention/#self-attention","title":"Self-Attention\u8ba1\u7b97\u516c\u5f0f","text":"\\[Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\\] <p>\u6838\u5fc3\u7ec4\u4ef6: - Query (Q): \u67e5\u8be2\u5411\u91cf\uff0c\u51b3\u5b9a\u5f53\u524d\u4f4d\u7f6e\u5173\u6ce8\u4ec0\u4e48 - Key (K): \u952e\u5411\u91cf\uff0c\u88ab\u67e5\u8be2\u7684\u5185\u5bb9 - Value (V): \u503c\u5411\u91cf\uff0c\u5b9e\u9645\u4f20\u9012\u7684\u4fe1\u606f - \u7f29\u653e\u56e0\u5b50: \\(\\sqrt{d_k}\\)\uff0c\u9632\u6b62softmax\u68af\u5ea6\u6d88\u5931</p>"},{"location":"fundamentals/transformer/attention/#multi-head-attention","title":"Multi-Head Attention","text":"<p>\u5c06\u8f93\u5165\u6295\u5f71\u5230\u591a\u4e2a\u4e0d\u540c\u7684\u5b50\u7a7a\u95f4\uff0c\u5e76\u884c\u8ba1\u7b97\u591a\u4e2a\u6ce8\u610f\u529b\u5934\uff1a</p> \\[MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\\] <p>\u5176\u4e2d \\(head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\\)</p>"},{"location":"fundamentals/transformer/attention/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/attention/#q1-attention","title":"Q1: Attention\u8ba1\u7b97\u516c\u5f0f\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6807\u51c6\u56de\u7b54\uff1a Self-Attention\u7684\u6838\u5fc3\u516c\u5f0f\u662f <code>Attention(Q,K,V) = softmax(QK^T/\u221ad_k)V</code>\u3002</p> <p>\u8be6\u7ec6\u89e3\u91ca\uff1a 1. \u5148\u8ba1\u7b97Query\u548cKey\u7684\u70b9\u79ef\u5f97\u5230\u6ce8\u610f\u529b\u5206\u6570 2. \u9664\u4ee5\u221ad_k\u8fdb\u884c\u7f29\u653e 3. \u901a\u8fc7softmax\u5f52\u4e00\u5316\u5f97\u5230\u6ce8\u610f\u529b\u6743\u91cd 4. \u6700\u540e\u4e0eValue\u76f8\u4e58\u5f97\u5230\u8f93\u51fa</p>"},{"location":"fundamentals/transformer/attention/#q11-d_k","title":"Q1.1: \u4e3a\u4ec0\u4e48\u8981\u9664\u4ee5\u6839\u53f7d_k\uff1f","text":"<p>\u6838\u5fc3\u539f\u56e0\uff1a \u9632\u6b62softmax\u51fd\u6570\u8fdb\u5165\u9971\u548c\u533a\u5bfc\u81f4\u68af\u5ea6\u6d88\u5931\u3002</p> <p>\u6280\u672f\u89e3\u91ca\uff1a - \u5f53d_k\u8f83\u5927\u65f6\uff0cQK^T\u7684\u65b9\u5dee\u4f1a\u5f88\u5927 - \u5927\u7684\u6570\u503c\u7ecf\u8fc7softmax\u540e\u68af\u5ea6\u63a5\u8fd10 - \u9664\u4ee5\u221ad_k\u53ef\u4ee5\u63a7\u5236\u65b9\u5dee\u4e3a1\uff0c\u4fdd\u6301\u68af\u5ea6\u7a33\u5b9a</p>"},{"location":"fundamentals/transformer/attention/#q12-softmax","title":"Q1.2: Softmax\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f","text":"<p>\u4e3b\u8981\u4f5c\u7528\uff1a 1. \u5f52\u4e00\u5316: \u786e\u4fdd\u6ce8\u610f\u529b\u6743\u91cd\u4e4b\u548c\u4e3a1 2. \u7a81\u51fa\u91cd\u70b9: \u901a\u8fc7\u6307\u6570\u51fd\u6570\u653e\u5927\u91cd\u8981\u7279\u5f81 3. \u53ef\u5fae\u5206: \u4fdd\u8bc1\u53cd\u5411\u4f20\u64ad\u53ef\u4ee5\u6b63\u5e38\u8fdb\u884c</p>"},{"location":"fundamentals/transformer/attention/#_8","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/attention/#1-self-attention","title":"\u7ec3\u4e601: \u5b9e\u73b0Self-Attention\u673a\u5236","text":"<p>\u5e73\u53f0: Deep-ML Self-Attention</p>"},{"location":"fundamentals/transformer/attention/#2-multi-head-attention","title":"\u7ec3\u4e602: \u5b9e\u73b0Multi-Head Attention","text":"<p>\u5e73\u53f0: Deep-ML Multi-Head Attention</p>"},{"location":"fundamentals/transformer/attention/#_9","title":"\u4ee3\u7801\u6a21\u677f","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass SelfAttention(nn.Module):\n    def __init__(self, d_model, n_heads):\n        super().__init__()\n        self.d_model = d_model\n        self.n_heads = n_heads\n        self.d_k = d_model // n_heads\n\n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n\n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        # TODO: \u5b9e\u73b0\u6ce8\u610f\u529b\u8ba1\u7b97\n        pass\n\n    def forward(self, x, mask=None):\n        # TODO: \u5b9e\u73b0\u5b8c\u6574\u7684\u524d\u5411\u4f20\u64ad\n        pass\n</code></pre>"},{"location":"fundamentals/transformer/attention/#_10","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u68c0\u9a8c\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p> <ul> <li>[ ] \u80fd\u753b\u51faSelf-Attention\u7684\u8ba1\u7b97\u6d41\u7a0b\u56fe</li> <li>[ ] \u5b8c\u6210Deep-ML\u5e73\u53f0\u7684\u4e24\u4e2a\u7f16\u7a0b\u7ec3\u4e60</li> <li>[ ] \u9762\u8bd5\u95ee\u9898\u80fd\u7528\u81ea\u5df1\u7684\u8bdd\u6d41\u5229\u56de\u7b54</li> </ul>"},{"location":"fundamentals/transformer/attention/#_11","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1a\u8bed\u8a00\u6a21\u578b\u67b6\u6784</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/encoder-decoder/","title":"\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3\u539f\u59cbTransformer\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff0c\u638c\u63e1\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236\u7684\u533a\u522b\u548c\u4f5c\u7528\u3002</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_3","title":"\u6574\u4f53\u67b6\u6784\u6982\u89c8","text":"<p>\u539f\u59cbTransformer\u91c7\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668(Encoder-Decoder)\u67b6\u6784\uff0c\u4e13\u95e8\u7528\u4e8e\u5e8f\u5217\u5230\u5e8f\u5217\u7684\u8f6c\u6362\u4efb\u52a1\u3002</p> <pre><code>\u8f93\u5165\u5e8f\u5217 \u2192 \u7f16\u7801\u5668 \u2192 \u7f16\u7801\u8868\u793a \u2192 \u89e3\u7801\u5668 \u2192 \u8f93\u51fa\u5e8f\u5217\n</code></pre>"},{"location":"fundamentals/transformer/encoder-decoder/#_4","title":"\u6838\u5fc3\u7ec4\u4ef6","text":"<ol> <li>\u7f16\u7801\u5668(Encoder): \u5c06\u8f93\u5165\u5e8f\u5217\u7f16\u7801\u4e3a\u9ad8\u5c42\u8bed\u4e49\u8868\u793a</li> <li>\u89e3\u7801\u5668(Decoder): \u57fa\u4e8e\u7f16\u7801\u8868\u793a\u81ea\u56de\u5f52\u751f\u6210\u8f93\u51fa\u5e8f\u5217</li> <li>\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236: \u81ea\u6ce8\u610f\u529b\u3001\u63a9\u7801\u81ea\u6ce8\u610f\u529b\u3001\u4ea4\u53c9\u6ce8\u610f\u529b</li> </ol>"},{"location":"fundamentals/transformer/encoder-decoder/#encoder","title":"\u7f16\u7801\u5668 (Encoder) \u8be6\u89e3","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_5","title":"\u7ed3\u6784\u7ec4\u6210","text":"<ul> <li>N\u5c42\u76f8\u540c\u7684\u5c42(\u539f\u8bba\u6587N=6)</li> <li>\u6bcf\u5c42\u5305\u542b\u4e24\u4e2a\u5b50\u5c42\uff1a</li> <li>\u591a\u5934\u81ea\u6ce8\u610f\u529b\u673a\u5236</li> <li>\u524d\u9988\u795e\u7ecf\u7f51\u7edc</li> <li>\u6bcf\u4e2a\u5b50\u5c42\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316</li> </ul>"},{"location":"fundamentals/transformer/encoder-decoder/#_6","title":"\u6570\u5b66\u8868\u793a","text":"<pre><code># \u7f16\u7801\u5668\u5c42\u7684\u8ba1\u7b97\u8fc7\u7a0b\ndef encoder_layer(x):\n    # \u591a\u5934\u81ea\u6ce8\u610f\u529b\n    attn_output = multi_head_attention(x, x, x)\n    x = layer_norm(x + attn_output)  # \u6b8b\u5dee\u8fde\u63a5 + \u5c42\u5f52\u4e00\u5316\n\n    # \u524d\u9988\u7f51\u7edc\n    ffn_output = feed_forward(x)\n    x = layer_norm(x + ffn_output)   # \u6b8b\u5dee\u8fde\u63a5 + \u5c42\u5f52\u4e00\u5316\n\n    return x\n</code></pre>"},{"location":"fundamentals/transformer/encoder-decoder/#_7","title":"\u6838\u5fc3\u7279\u5f81","text":"<p>1. \u5e76\u884c\u5904\u7406 - \u53ef\u4ee5\u540c\u65f6\u5904\u7406\u6574\u4e2a\u8f93\u5165\u5e8f\u5217 - \u6bcf\u4e2a\u4f4d\u7f6e\u90fd\u80fd\u770b\u5230\u6240\u6709\u5176\u4ed6\u4f4d\u7f6e - \u8bad\u7ec3\u6548\u7387\u9ad8\uff0c\u65e0\u5e8f\u5217\u8ba1\u7b97\u4f9d\u8d56</p> <p>2. \u53cc\u5411\u4e0a\u4e0b\u6587 - \u81ea\u6ce8\u610f\u529b\u673a\u5236\u5141\u8bb8\u6bcf\u4e2a\u4f4d\u7f6e\u5173\u6ce8\u6574\u4e2a\u5e8f\u5217 - \u80fd\u591f\u6355\u83b7\u5168\u5c40\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f - \u9002\u5408\u7406\u89e3\u7c7b\u4efb\u52a1</p> <p>3. \u8bed\u4e49\u63d0\u5347 - \u5c06\u4f4e\u9636\u8bcd\u5411\u91cf\u8f6c\u6362\u4e3a\u9ad8\u9636\u8bed\u4e49\u8868\u793a - \u591a\u5c42\u5806\u53e0\u9010\u6b65\u62bd\u8c61\u8bed\u4e49\u4fe1\u606f - \u6700\u7ec8\u8f93\u51fa\u5305\u542b\u4e30\u5bcc\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f</p>"},{"location":"fundamentals/transformer/encoder-decoder/#decoder","title":"\u89e3\u7801\u5668 (Decoder) \u8be6\u89e3","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_8","title":"\u7ed3\u6784\u7ec4\u6210","text":"<ul> <li>N\u5c42\u76f8\u540c\u7684\u5c42(\u539f\u8bba\u6587N=6)</li> <li>\u6bcf\u5c42\u5305\u542b\u4e09\u4e2a\u5b50\u5c42\uff1a</li> <li>\u63a9\u7801\u591a\u5934\u81ea\u6ce8\u610f\u529b</li> <li>\u7f16\u7801\u5668-\u89e3\u7801\u5668\u4ea4\u53c9\u6ce8\u610f\u529b</li> <li>\u524d\u9988\u795e\u7ecf\u7f51\u7edc</li> <li>\u6bcf\u4e2a\u5b50\u5c42\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u548c\u5c42\u5f52\u4e00\u5316</li> </ul>"},{"location":"fundamentals/transformer/encoder-decoder/#_9","title":"\u6570\u5b66\u8868\u793a","text":"<pre><code># \u89e3\u7801\u5668\u5c42\u7684\u8ba1\u7b97\u8fc7\u7a0b\ndef decoder_layer(x, encoder_output):\n    # 1. \u63a9\u7801\u81ea\u6ce8\u610f\u529b\n    masked_attn = masked_multi_head_attention(x, x, x)\n    x = layer_norm(x + masked_attn)\n\n    # 2. \u4ea4\u53c9\u6ce8\u610f\u529b\n    cross_attn = multi_head_attention(\n        query=x, \n        key=encoder_output, \n        value=encoder_output\n    )\n    x = layer_norm(x + cross_attn)\n\n    # 3. \u524d\u9988\u7f51\u7edc\n    ffn_output = feed_forward(x)\n    x = layer_norm(x + ffn_output)\n\n    return x\n</code></pre>"},{"location":"fundamentals/transformer/encoder-decoder/#_10","title":"\u6838\u5fc3\u7279\u5f81","text":"<p>1. \u81ea\u56de\u5f52\u751f\u6210 - \u9010\u6b65\u751f\u6210\u8f93\u51fa\u5e8f\u5217 - \u5f53\u524d\u4f4d\u7f6e\u53ea\u80fd\u770b\u5230\u4e4b\u524d\u7684\u4f4d\u7f6e - \u4f7f\u7528\u63a9\u7801\u673a\u5236\u9632\u6b62\u4fe1\u606f\u6cc4\u9732</p> <p>2. \u53cc\u8f93\u5165\u673a\u5236 - \u8f93\u51651\uff1a\u89e3\u7801\u5668\u4e4b\u524d\u7684\u8f93\u51fa(\u81ea\u56de\u5f52) - \u8f93\u51652\uff1a\u7f16\u7801\u5668\u7684\u8f93\u51fa\u8868\u793a(\u4ea4\u53c9\u6ce8\u610f\u529b) - \u7ed3\u5408\u81ea\u8eab\u5386\u53f2\u548c\u6e90\u5e8f\u5217\u4fe1\u606f</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_11","title":"\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236\u8be6\u89e3","text":""},{"location":"fundamentals/transformer/encoder-decoder/#1-encoder-self-attention","title":"1. \u7f16\u7801\u5668\u81ea\u6ce8\u610f\u529b (Encoder Self-Attention)","text":"<p>\u4f5c\u7528: \u8ba9\u7f16\u7801\u5668\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5173\u6ce8\u8f93\u5165\u5e8f\u5217\u7684\u6240\u6709\u4f4d\u7f6e</p> <pre><code># \u7f16\u7801\u5668\u81ea\u6ce8\u610f\u529b\nQ = K = V = encoder_input  # \u90fd\u6765\u81ea\u8f93\u5165\u5e8f\u5217\nattention_output = Attention(Q, K, V)\n</code></pre> <p>\u7279\u70b9: - \u65e0\u63a9\u7801\u9650\u5236\uff0c\u53ef\u4ee5\u770b\u5230\u5168\u5e8f\u5217 - \u5efa\u7acb\u8f93\u5165\u5e8f\u5217\u5185\u90e8\u7684\u4f9d\u8d56\u5173\u7cfb - \u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56</p>"},{"location":"fundamentals/transformer/encoder-decoder/#2-masked-self-attention","title":"2. \u89e3\u7801\u5668\u63a9\u7801\u81ea\u6ce8\u610f\u529b (Masked Self-Attention)","text":"<p>\u4f5c\u7528: \u8ba9\u89e3\u7801\u5668\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u53ea\u5173\u6ce8\u4e4b\u524d\u7684\u4f4d\u7f6e</p> <pre><code># \u63a9\u7801\u81ea\u6ce8\u610f\u529b\nQ = K = V = decoder_input  # \u90fd\u6765\u81ea\u89e3\u7801\u5668\u8f93\u5165\nmask = create_causal_mask(seq_len)  # \u4e0b\u4e09\u89d2\u63a9\u7801\nattention_output = Attention(Q, K, V, mask=mask)\n</code></pre> <p>\u63a9\u7801\u673a\u5236: <pre><code>\u4f4d\u7f6e:  0  1  2  3\n\u63a9\u7801: [1  0  0  0]  # \u4f4d\u7f6e0\u53ea\u80fd\u770b\u81ea\u5df1\n      [1  1  0  0]  # \u4f4d\u7f6e1\u80fd\u770b0,1\n      [1  1  1  0]  # \u4f4d\u7f6e2\u80fd\u770b0,1,2\n      [1  1  1  1]  # \u4f4d\u7f6e3\u80fd\u770b0,1,2,3\n</code></pre></p>"},{"location":"fundamentals/transformer/encoder-decoder/#3-cross-attention","title":"3. \u7f16\u7801\u5668-\u89e3\u7801\u5668\u4ea4\u53c9\u6ce8\u610f\u529b (Cross-Attention)","text":"<p>\u4f5c\u7528: \u8ba9\u89e3\u7801\u5668\u5173\u6ce8\u7f16\u7801\u5668\u7684\u8f93\u51fa\uff0c\u5b9e\u73b0\u5e8f\u5217\u5bf9\u9f50</p> <pre><code># \u4ea4\u53c9\u6ce8\u610f\u529b\nQ = decoder_hidden        # \u67e5\u8be2\u6765\u81ea\u89e3\u7801\u5668\nK = V = encoder_output    # \u952e\u503c\u6765\u81ea\u7f16\u7801\u5668\nattention_output = Attention(Q, K, V)\n</code></pre> <p>\u5de5\u4f5c\u539f\u7406: - Query\uff1a\u89e3\u7801\u5668\u60f3\u8981\u4ec0\u4e48\u4fe1\u606f - Key\uff1a\u7f16\u7801\u5668\u6709\u4ec0\u4e48\u4fe1\u606f - Value\uff1a\u7f16\u7801\u5668\u63d0\u4f9b\u7684\u5177\u4f53\u4fe1\u606f - \u5b9e\u73b0\u6e90\u5e8f\u5217\u548c\u76ee\u6807\u5e8f\u5217\u7684\u5bf9\u9f50</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_12","title":"\u67b6\u6784\u4f18\u52bf\u4e0e\u5e94\u7528","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_13","title":"\u4f18\u52bf\u7279\u70b9","text":"<p>1. \u5e76\u884c\u8bad\u7ec3 - \u7f16\u7801\u5668\u53ef\u4ee5\u5e76\u884c\u5904\u7406\u6574\u4e2a\u8f93\u5165 - \u89e3\u7801\u5668\u5728\u8bad\u7ec3\u65f6\u4e5f\u53ef\u4ee5\u5e76\u884c(Teacher Forcing) - \u76f8\u6bd4RNN\u8bad\u7ec3\u901f\u5ea6\u5927\u5e45\u63d0\u5347</p> <p>2. \u957f\u8ddd\u79bb\u4f9d\u8d56 - \u6ce8\u610f\u529b\u673a\u5236\u76f4\u63a5\u8fde\u63a5\u4efb\u610f\u4e24\u4e2a\u4f4d\u7f6e - \u907f\u514d\u4e86RNN\u7684\u68af\u5ea6\u4f20\u64ad\u95ee\u9898 - \u66f4\u597d\u5730\u6355\u83b7\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb</p> <p>3. \u53ef\u89e3\u91ca\u6027 - \u6ce8\u610f\u529b\u6743\u91cd\u63d0\u4f9b\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89c6\u5316 - \u53ef\u4ee5\u770b\u5230\u6a21\u578b\u5173\u6ce8\u7684\u8f93\u5165\u90e8\u5206 - \u4fbf\u4e8e\u5206\u6790\u548c\u8c03\u8bd5</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_14","title":"\u5178\u578b\u5e94\u7528","text":"<p>1. \u673a\u5668\u7ffb\u8bd1 - \u539f\u59cbTransformer\u7684\u8bbe\u8ba1\u76ee\u6807 - \u7f16\u7801\u5668\u7406\u89e3\u6e90\u8bed\u8a00\uff0c\u89e3\u7801\u5668\u751f\u6210\u76ee\u6807\u8bed\u8a00 - \u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u8bed\u8a00\u5bf9\u9f50</p> <p>2. \u6587\u672c\u6458\u8981 - \u7f16\u7801\u5668\u7406\u89e3\u539f\u6587\uff0c\u89e3\u7801\u5668\u751f\u6210\u6458\u8981 - \u4ea4\u53c9\u6ce8\u610f\u529b\u9009\u62e9\u91cd\u8981\u4fe1\u606f - \u63a7\u5236\u6458\u8981\u957f\u5ea6\u548c\u5185\u5bb9</p> <p>3. \u5bf9\u8bdd\u7cfb\u7edf - \u7f16\u7801\u5668\u7406\u89e3\u7528\u6237\u8f93\u5165 - \u89e3\u7801\u5668\u751f\u6210\u56de\u590d - \u7ef4\u6301\u5bf9\u8bdd\u4e0a\u4e0b\u6587</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_15","title":"\u73b0\u4ee3\u53d1\u5c55\u8d8b\u52bf","text":""},{"location":"fundamentals/transformer/encoder-decoder/#_16","title":"\u67b6\u6784\u6f14\u8fdb","text":"<p>Encoder-Only: - \u4ee3\u8868: BERT, RoBERTa - \u64c5\u957f: \u7406\u89e3\u4efb\u52a1(\u5206\u7c7b\u3001\u9605\u8bfb\u7406\u89e3) - \u7279\u70b9: \u53cc\u5411\u6ce8\u610f\u529b\uff0c\u5e76\u884c\u8bad\u7ec3</p> <p>Decoder-Only: - \u4ee3\u8868: GPT, LLaMA, ChatGPT - \u64c5\u957f: \u751f\u6210\u4efb\u52a1(\u5bf9\u8bdd\u3001\u5199\u4f5c) - \u7279\u70b9: \u56e0\u679c\u6ce8\u610f\u529b\uff0c\u7edf\u4e00\u8303\u5f0f</p> <p>\u4e3a\u4ec0\u4e48Decoder-Only\u6210\u4e3a\u4e3b\u6d41\uff1f 1. \u4efb\u52a1\u7edf\u4e00: \u6240\u6709\u4efb\u52a1\u90fd\u53ef\u4ee5\u8868\u8ff0\u4e3a\u751f\u6210\u95ee\u9898 2. \u6269\u5c55\u6027\u597d: \u66f4\u5bb9\u6613\u6269\u5c55\u5230\u5927\u89c4\u6a21 3. \u6d8c\u73b0\u80fd\u529b: \u5927\u89c4\u6a21\u540e\u5c55\u73b0\u5f3a\u5927\u7684few-shot\u80fd\u529b 4. \u5de5\u7a0b\u7b80\u5316: \u67b6\u6784\u66f4\u7b80\u5355\uff0c\u6613\u4e8e\u4f18\u5316</p>"},{"location":"fundamentals/transformer/encoder-decoder/#_17","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/encoder-decoder/#q1","title":"Q1: \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u4e3b\u8981\u533a\u522b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u533a\u522b:</p> \u7ef4\u5ea6 \u7f16\u7801\u5668 \u89e3\u7801\u5668 \u6ce8\u610f\u529b\u7c7b\u578b \u53cc\u5411\u81ea\u6ce8\u610f\u529b \u5355\u5411\u63a9\u7801\u81ea\u6ce8\u610f\u529b + \u4ea4\u53c9\u6ce8\u610f\u529b \u5904\u7406\u65b9\u5f0f \u5e76\u884c\u5904\u7406 \u81ea\u56de\u5f52\u751f\u6210 \u8f93\u5165\u6765\u6e90 \u539f\u59cb\u8f93\u5165\u5e8f\u5217 \u524d\u4e00\u6b65\u8f93\u51fa + \u7f16\u7801\u5668\u8f93\u51fa \u4e3b\u8981\u529f\u80fd \u7406\u89e3\u548c\u7f16\u7801 \u751f\u6210\u548c\u89e3\u7801"},{"location":"fundamentals/transformer/encoder-decoder/#q2","title":"Q2: \u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u5de5\u4f5c\u539f\u7406\u662f\u4ec0\u4e48\uff1f","text":"<p>\u5de5\u4f5c\u673a\u5236: 1. Query\u6765\u81ea\u89e3\u7801\u5668: \u8868\u793a\"\u6211\u60f3\u8981\u4ec0\u4e48\u4fe1\u606f\" 2. Key/Value\u6765\u81ea\u7f16\u7801\u5668: \u8868\u793a\"\u53ef\u4ee5\u63d0\u4f9b\u4ec0\u4e48\u4fe1\u606f\" 3. \u6ce8\u610f\u529b\u8ba1\u7b97: \u8ba1\u7b97\u89e3\u7801\u5668\u5bf9\u7f16\u7801\u5668\u6bcf\u4e2a\u4f4d\u7f6e\u7684\u5173\u6ce8\u5ea6 4. \u4fe1\u606f\u878d\u5408: \u6839\u636e\u6ce8\u610f\u529b\u6743\u91cd\u805a\u5408\u7f16\u7801\u5668\u4fe1\u606f</p> <p>\u6570\u5b66\u8fc7\u7a0b: \\(\\(CrossAttention = Attention(Q_{decoder}, K_{encoder}, V_{encoder})\\)\\)</p>"},{"location":"fundamentals/transformer/encoder-decoder/#q3","title":"Q3: \u4e3a\u4ec0\u4e48\u89e3\u7801\u5668\u9700\u8981\u63a9\u7801\u81ea\u6ce8\u610f\u529b\uff1f","text":"<p>\u6838\u5fc3\u539f\u56e0: \u9632\u6b62\u4fe1\u606f\u6cc4\u9732</p> <p>\u8be6\u7ec6\u89e3\u91ca: 1. \u8bad\u7ec3\u65f6: \u4f7f\u7528Teacher Forcing\uff0c\u6a21\u578b\u80fd\u770b\u5230\u5b8c\u6574\u76ee\u6807\u5e8f\u5217 2. \u63a8\u7406\u65f6: \u53ea\u80fd\u770b\u5230\u5df2\u751f\u6210\u7684\u90e8\u5206 3. \u4e00\u81f4\u6027\u8981\u6c42: \u8bad\u7ec3\u548c\u63a8\u7406\u7684\u53ef\u89c1\u4fe1\u606f\u5fc5\u987b\u4e00\u81f4 4. \u63a9\u7801\u4f5c\u7528: \u5728\u8bad\u7ec3\u65f6\u4eba\u4e3a\u9650\u5236\u53ef\u89c1\u8303\u56f4</p> <p>\u4ee3\u7801\u793a\u4f8b: <pre><code>def create_causal_mask(seq_len):\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask.masked_fill(mask == 0, float('-inf'))\n</code></pre></p>"},{"location":"fundamentals/transformer/encoder-decoder/#q4-decoder-only","title":"Q4: \u4e3a\u4ec0\u4e48\u73b0\u5728\u66f4\u6d41\u884cDecoder-Only\u67b6\u6784\uff1f","text":"<p>\u4e3b\u8981\u539f\u56e0:</p> <ol> <li>\u7edf\u4e00\u6027: </li> <li>\u6240\u6709\u4efb\u52a1\u90fd\u53ef\u4ee5\u8f6c\u5316\u4e3a\u751f\u6210\u4efb\u52a1</li> <li>\u5206\u7c7b \u2192 \u751f\u6210\u7c7b\u522b\u6807\u7b7e</li> <li> <p>\u95ee\u7b54 \u2192 \u751f\u6210\u7b54\u6848</p> </li> <li> <p>\u6269\u5c55\u6027:</p> </li> <li>\u67b6\u6784\u7b80\u5355\uff0c\u6613\u4e8e\u6269\u5927\u89c4\u6a21</li> <li> <p>\u8bad\u7ec3\u66f4\u7a33\u5b9a\uff0c\u53c2\u6570\u5229\u7528\u7387\u9ad8</p> </li> <li> <p>\u6d8c\u73b0\u80fd\u529b:</p> </li> <li>\u5927\u89c4\u6a21\u8bad\u7ec3\u540e\u5c55\u73b0\u5f3a\u5927\u7684zero/few-shot\u80fd\u529b</li> <li> <p>\u6307\u4ee4\u8ddf\u968f\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b49\u80fd\u529b</p> </li> <li> <p>\u5de5\u7a0b\u4f18\u52bf:</p> </li> <li>\u5b9e\u73b0\u7b80\u5355\uff0c\u4f18\u5316\u6210\u719f</li> <li>\u63a8\u7406\u6548\u7387\u9ad8(KV Cache\u7b49\u6280\u672f)</li> </ol>"},{"location":"fundamentals/transformer/encoder-decoder/#_18","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/encoder-decoder/#encoder-decoder","title":"\u5b8c\u6574Encoder-Decoder\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\n\nclass EncoderDecoderTransformer(nn.Module):\n    \"\"\"\u5b8c\u6574\u7684Encoder-Decoder Transformer\u5b9e\u73b0\"\"\"\n\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, \n                 num_heads=8, num_layers=6, d_ff=2048, max_seq_len=1000):\n        super().__init__()\n\n        self.d_model = d_model\n        self.max_seq_len = max_seq_len\n\n        # \u8bcd\u5d4c\u5165\u5c42\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n\n        # \u4f4d\u7f6e\u7f16\u7801\n        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n\n        # \u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\n        self.encoder = Encoder(d_model, num_heads, num_layers, d_ff)\n        self.decoder = Decoder(d_model, num_heads, num_layers, d_ff)\n\n        # \u8f93\u51fa\u6295\u5f71\u5c42\n        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n        # \u6e90\u5e8f\u5217\u7f16\u7801\n        src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n        src_emb = self.pos_encoding(src_emb)\n        src_emb = self.dropout(src_emb)\n\n        # \u76ee\u6807\u5e8f\u5217\u7f16\u7801\n        tgt_emb = self.tgt_embedding(tgt) * math.sqrt(self.d_model)\n        tgt_emb = self.pos_encoding(tgt_emb)\n        tgt_emb = self.dropout(tgt_emb)\n\n        # \u7f16\u7801\u5668\n        encoder_output = self.encoder(src_emb, src_mask)\n\n        # \u89e3\u7801\u5668\n        decoder_output = self.decoder(tgt_emb, encoder_output, tgt_mask, src_mask)\n\n        # \u8f93\u51fa\u6295\u5f71\n        output = self.output_projection(decoder_output)\n\n        return output\n\nclass Encoder(nn.Module):\n    \"\"\"Transformer\u7f16\u7801\u5668\"\"\"\n\n    def __init__(self, d_model, num_heads, num_layers, d_ff):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            EncoderLayer(d_model, num_heads, d_ff)\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, x, mask=None):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return x\n\nclass EncoderLayer(nn.Module):\n    \"\"\"\u7f16\u7801\u5668\u5c42\"\"\"\n\n    def __init__(self, d_model, num_heads, d_ff):\n        super().__init__()\n        self.self_attention = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x, mask=None):\n        # \u81ea\u6ce8\u610f\u529b\u5b50\u5c42\n        attn_output = self.self_attention(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n\n        # \u524d\u9988\u7f51\u7edc\u5b50\u5c42\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n\n        return x\n\nclass Decoder(nn.Module):\n    \"\"\"Transformer\u89e3\u7801\u5668\"\"\"\n\n    def __init__(self, d_model, num_heads, num_layers, d_ff):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            DecoderLayer(d_model, num_heads, d_ff)\n            for _ in range(num_layers)\n        ])\n\n    def forward(self, x, encoder_output, tgt_mask=None, src_mask=None):\n        for layer in self.layers:\n            x = layer(x, encoder_output, tgt_mask, src_mask)\n        return x\n\nclass DecoderLayer(nn.Module):\n    \"\"\"\u89e3\u7801\u5668\u5c42\"\"\"\n\n    def __init__(self, d_model, num_heads, d_ff):\n        super().__init__()\n        self.masked_self_attention = MultiHeadAttention(d_model, num_heads)\n        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = FeedForward(d_model, d_ff)\n\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x, encoder_output, tgt_mask=None, src_mask=None):\n        # \u63a9\u7801\u81ea\u6ce8\u610f\u529b\u5b50\u5c42\n        masked_attn = self.masked_self_attention(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(masked_attn))\n\n        # \u4ea4\u53c9\u6ce8\u610f\u529b\u5b50\u5c42\n        cross_attn = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n        x = self.norm2(x + self.dropout(cross_attn))\n\n        # \u524d\u9988\u7f51\u7edc\u5b50\u5c42\n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n\n        return x\n\ndef create_causal_mask(seq_len):\n    \"\"\"\u521b\u5efa\u56e0\u679c\u63a9\u7801(\u4e0b\u4e09\u89d2\u77e9\u9635)\"\"\"\n    mask = torch.tril(torch.ones(seq_len, seq_len))\n    return mask.masked_fill(mask == 0, float('-inf'))\n\ndef create_padding_mask(seq, pad_token=0):\n    \"\"\"\u521b\u5efa\u586b\u5145\u63a9\u7801\"\"\"\n    return (seq != pad_token).unsqueeze(1).unsqueeze(2)\n\n# \u4f7f\u7528\u793a\u4f8b\ndef demo_encoder_decoder():\n    \"\"\"\u6f14\u793a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7684\u4f7f\u7528\"\"\"\n\n    # \u6a21\u578b\u53c2\u6570\n    src_vocab_size = 1000\n    tgt_vocab_size = 1000\n    d_model = 512\n    seq_len = 20\n    batch_size = 2\n\n    # \u521b\u5efa\u6a21\u578b\n    model = EncoderDecoderTransformer(\n        src_vocab_size, tgt_vocab_size, d_model\n    )\n\n    # \u6a21\u62df\u6570\u636e\n    src = torch.randint(1, src_vocab_size, (batch_size, seq_len))\n    tgt = torch.randint(1, tgt_vocab_size, (batch_size, seq_len))\n\n    # \u521b\u5efa\u63a9\u7801\n    tgt_mask = create_causal_mask(seq_len)\n    src_mask = create_padding_mask(src)\n\n    # \u524d\u5411\u4f20\u64ad\n    output = model(src, tgt, src_mask, tgt_mask)\n\n    print(f\"\u8f93\u5165\u5f62\u72b6: {src.shape}\")\n    print(f\"\u8f93\u51fa\u5f62\u72b6: {output.shape}\")\n    print(f\"\u53c2\u6570\u91cf: {sum(p.numel() for p in model.parameters()):,}\")\n\nif __name__ == \"__main__\":\n    demo_encoder_decoder()\n</code></pre>"},{"location":"fundamentals/transformer/encoder-decoder/#_19","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u7684\u6574\u4f53\u8bbe\u8ba1</li> <li>[ ] \u638c\u63e1\u4e09\u79cd\u6ce8\u610f\u529b\u673a\u5236\u7684\u533a\u522b\u548c\u4f5c\u7528</li> <li>[ ] \u7406\u89e3\u63a9\u7801\u673a\u5236\u7684\u5fc5\u8981\u6027\u548c\u5b9e\u73b0</li> <li>[ ] \u80fd\u591f\u5b9e\u73b0\u5b8c\u6574\u7684Encoder-Decoder\u6a21\u578b</li> <li>[ ] \u7406\u89e3\u4e3a\u4ec0\u4e48\u73b0\u4ee3\u6a21\u578b\u504f\u5411Decoder-Only</li> </ul>"},{"location":"fundamentals/transformer/encoder-decoder/#_20","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u524d\u9988\u795e\u7ecf\u7f51\u7edc</li> <li>\u4e0b\u4e00\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/ffn/","title":"\u524d\u9988\u795e\u7ecf\u7f51\u7edc (FFN)","text":""},{"location":"fundamentals/transformer/ffn/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3Transformer\u4e2d\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u7684\u4f5c\u7528\u673a\u5236\uff0c\u638c\u63e1\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u7684\u6f14\u8fdb\u548c\u77e5\u8bc6\u5b58\u50a8\u539f\u7406\u3002</p>"},{"location":"fundamentals/transformer/ffn/#_2","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/ffn/#ffn_1","title":"FFN\u7684\u57fa\u672c\u7ed3\u6784","text":"<p>\u524d\u9988\u795e\u7ecf\u7f51\u7edc(Feed-Forward Network)\u662fTransformer\u4e2d\u9664\u6ce8\u610f\u529b\u673a\u5236\u5916\u7684\u53e6\u4e00\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u4f4d\u4e8e\u6bcf\u4e2aTransformer\u5c42\u4e2d\u3002</p>"},{"location":"fundamentals/transformer/ffn/#_3","title":"\u6570\u5b66\u8868\u793a","text":"\\[FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2\\] <p>\u5176\u4e2d\uff1a - \\(W_1\\): \u7b2c\u4e00\u5c42\u6743\u91cd\u77e9\u9635 (\u5347\u7ef4) - \\(W_2\\): \u7b2c\u4e8c\u5c42\u6743\u91cd\u77e9\u9635 (\u964d\u7ef4) - \\(b_1, b_2\\): \u504f\u7f6e\u5411\u91cf - \\(\\max(0, \u00b7)\\): ReLU\u6fc0\u6d3b\u51fd\u6570</p>"},{"location":"fundamentals/transformer/ffn/#_4","title":"\u7ef4\u5ea6\u53d8\u5316","text":"<pre><code>\u8f93\u5165: [batch_size, seq_len, d_model]\n  \u2193 W1 (\u7ebf\u6027\u5c421)\n\u4e2d\u95f4: [batch_size, seq_len, d_ff]    # d_ff = 4 * d_model\n  \u2193 \u6fc0\u6d3b\u51fd\u6570\n\u4e2d\u95f4: [batch_size, seq_len, d_ff]    \n  \u2193 W2 (\u7ebf\u6027\u5c422)  \n\u8f93\u51fa: [batch_size, seq_len, d_model]\n</code></pre>"},{"location":"fundamentals/transformer/ffn/#ffn_2","title":"FFN\u7684\u6838\u5fc3\u529f\u80fd","text":""},{"location":"fundamentals/transformer/ffn/#1","title":"1. \u8bed\u4e49\u4fe1\u606f\u63d0\u53d6","text":"<ul> <li>\u9010\u4f4d\u7f6e\u5904\u7406: \u5bf9\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u4f4d\u7f6e\u72ec\u7acb\u8fdb\u884c\u975e\u7ebf\u6027\u53d8\u6362</li> <li>\u7279\u5f81\u6620\u5c04: \u5c06\u6ce8\u610f\u529b\u8f93\u51fa\u6620\u5c04\u5230\u66f4\u9ad8\u7ef4\u7684\u7279\u5f81\u7a7a\u95f4</li> <li>\u6a21\u5f0f\u8bc6\u522b: \u6355\u83b7\u590d\u6742\u7684\u8bed\u4e49\u6a21\u5f0f\u548c\u7279\u5f81\u7ec4\u5408</li> </ul>"},{"location":"fundamentals/transformer/ffn/#2","title":"2. \u77e5\u8bc6\u5b58\u50a8\u673a\u5236","text":"<p>FFN\u88ab\u8ba4\u4e3a\u662fTransformer\u7684\"\u8bb0\u5fc6\u5e93\"\uff1a</p> <p>\u5206\u5e03\u5f0f\u5b58\u50a8: - \u4e0d\u540c\u7684\u795e\u7ecf\u5143\u4e13\u95e8\u5b58\u50a8\u4e0d\u540c\u7c7b\u578b\u7684\u77e5\u8bc6 - \u901a\u8fc7\u6743\u91cd\u77e9\u9635\u7f16\u7801\u8bed\u8a00\u6a21\u5f0f\u548c\u4e16\u754c\u77e5\u8bc6 - \u7c7b\u4f3c\u4e8e\u952e\u503c\u5b58\u50a8\uff0c\u8f93\u5165\u4f5c\u4e3a\"\u952e\"\uff0c\u6fc0\u6d3b\u6a21\u5f0f\u4f5c\u4e3a\"\u503c\"</p> <p>\u77e5\u8bc6\u7535\u8def: - FFN\u4e2d\u7684\u7279\u5b9a\u795e\u7ecf\u5143\u6fc0\u6d3b\u8def\u5f84\u5f62\u6210\"\u77e5\u8bc6\u7535\u8def\" - \u8fd9\u4e9b\u7535\u8def\u7f16\u7801\u7279\u5b9a\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u4e8b\u5b9e\u77e5\u8bc6 - \u591a\u5c42FFN\u534f\u540c\u5de5\u4f5c\uff0c\u6784\u5efa\u590d\u6742\u7684\u77e5\u8bc6\u8868\u793a</p>"},{"location":"fundamentals/transformer/ffn/#3","title":"3. \u8868\u8fbe\u80fd\u529b\u589e\u5f3a","text":"<ul> <li>\u975e\u7ebf\u6027\u53d8\u6362: \u6fc0\u6d3b\u51fd\u6570\u5f15\u5165\u975e\u7ebf\u6027\uff0c\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b</li> <li>\u7ef4\u5ea6\u6269\u5c55: \u4e2d\u95f4\u5c42\u7684\u9ad8\u7ef4\u5ea6\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u8868\u793a\u7a7a\u95f4</li> <li>\u7279\u5f81\u4ea4\u4e92: \u4fc3\u8fdb\u4e0d\u540c\u7279\u5f81\u7ef4\u5ea6\u4e4b\u95f4\u7684\u4ea4\u4e92</li> </ul>"},{"location":"fundamentals/transformer/ffn/#_5","title":"\u6fc0\u6d3b\u51fd\u6570\u7684\u6f14\u8fdb","text":""},{"location":"fundamentals/transformer/ffn/#1-relu-transformer","title":"1. ReLU (\u65e9\u671fTransformer)","text":"\\[\\text{ReLU}(x) = \\max(0, x)\\] <p>\u7279\u70b9: - \u7b80\u5355\u9ad8\u6548\uff0c\u8ba1\u7b97\u91cf\u5c0f - \u89e3\u51b3\u68af\u5ea6\u6d88\u5931\u95ee\u9898 - \u4f46\u5b58\u5728\"\u6b7b\u795e\u7ecf\u5143\"\u95ee\u9898</p>"},{"location":"fundamentals/transformer/ffn/#2-gelu-gpt","title":"2. GELU (GPT\u7b49\u6a21\u578b)","text":"\\[\\text{GELU}(x) = x \\cdot \\Phi(x) = x \\cdot \\frac{1}{2}[1 + \\text{erf}(\\frac{x}{\\sqrt{2}})]\\] <p>\u7279\u70b9: - \u66f4\u5e73\u6ed1\u7684\u6fc0\u6d3b\u51fd\u6570 - \u5728\u8d1f\u503c\u533a\u57df\u6709\u975e\u96f6\u68af\u5ea6 - \u6027\u80fd\u901a\u5e38\u4f18\u4e8eReLU</p>"},{"location":"fundamentals/transformer/ffn/#3-swiglu","title":"3. SwiGLU (\u73b0\u4ee3\u5927\u6a21\u578b)","text":"\\[\\text{SwiGLU}(x) = \\text{Swish}(W_1 x) \\odot (W_2 x)$$ $$\\text{Swish}(x) = x \\cdot \\sigma(x)\\] <p>\u7279\u70b9: - \u95e8\u63a7\u673a\u5236\uff0c\u66f4\u597d\u7684\u7279\u5f81\u9009\u62e9 - \u9700\u8981\u989d\u5916\u53c2\u6570\u4f46\u6027\u80fd\u63d0\u5347\u660e\u663e - LLaMA\u3001PaLM\u7b49\u73b0\u4ee3\u6a21\u578b\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/transformer/ffn/#ffn_3","title":"FFN\u7684\u72ec\u7279\u7279\u6027","text":""},{"location":"fundamentals/transformer/ffn/#1_1","title":"1. \u4f4d\u7f6e\u65e0\u5173\u5904\u7406","text":"<pre><code># FFN\u5bf9\u6bcf\u4e2a\u4f4d\u7f6e\u72ec\u7acb\u5904\u7406\nfor position in sequence:\n    hidden = ffn_layer1(input[position])\n    hidden = activation(hidden)\n    output[position] = ffn_layer2(hidden)\n</code></pre>"},{"location":"fundamentals/transformer/ffn/#2_1","title":"2. \u4e0e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e92\u8865","text":"\u673a\u5236 \u6ce8\u610f\u529b FFN \u529f\u80fd \u5e8f\u5217\u5185\u4fe1\u606f\u6574\u5408 \u4f4d\u7f6e\u5185\u7279\u5f81\u63d0\u53d6 \u4f9d\u8d56 \u5168\u5e8f\u5217 \u5355\u4e2a\u4f4d\u7f6e \u4f5c\u7528 \u5efa\u6a21\u5173\u7cfb \u5b58\u50a8\u77e5\u8bc6 \u8ba1\u7b97 \u5e8f\u5217\u957f\u5ea6\u76f8\u5173 \u5e8f\u5217\u957f\u5ea6\u65e0\u5173"},{"location":"fundamentals/transformer/ffn/#ffn_4","title":"\u73b0\u4ee3FFN\u4f18\u5316\u6280\u672f","text":""},{"location":"fundamentals/transformer/ffn/#1-mixture-of-experts-moe","title":"1. Mixture of Experts (MoE)","text":"<ul> <li>\u5c06FFN\u66ff\u6362\u4e3a\u591a\u4e2a\u4e13\u5bb6\u7f51\u7edc</li> <li>\u901a\u8fc7\u8def\u7531\u673a\u5236\u52a8\u6001\u9009\u62e9\u4e13\u5bb6</li> <li>\u5728\u4fdd\u6301\u8ba1\u7b97\u91cf\u7684\u540c\u65f6\u5927\u5e45\u589e\u52a0\u53c2\u6570</li> </ul>"},{"location":"fundamentals/transformer/ffn/#2-memory-layers","title":"2. Memory Layers","text":"<ul> <li>\u5f15\u5165\u5916\u90e8\u8bb0\u5fc6\u673a\u5236</li> <li>\u7f13\u5b58\u548c\u68c0\u7d22\u76f8\u5173\u77e5\u8bc6</li> <li>\u63d0\u9ad8\u957f\u5e8f\u5217\u5904\u7406\u80fd\u529b</li> </ul>"},{"location":"fundamentals/transformer/ffn/#3-kan-kolmogorov-arnold-networks","title":"3. KAN (Kolmogorov-Arnold Networks)","text":"<ul> <li>\u66ff\u4ee3\u4f20\u7edf\u7684\u7ebf\u6027\u5c42\u7ed3\u6784</li> <li>\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u6fc0\u6d3b\u51fd\u6570</li> <li>\u7406\u8bba\u4e0a\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b</li> </ul>"},{"location":"fundamentals/transformer/ffn/#_6","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/ffn/#q1-ffntransformer","title":"Q1: FFN\u5728Transformer\u4e2d\u8d77\u4ec0\u4e48\u4f5c\u7528\uff1f","text":"<p>\u6838\u5fc3\u4f5c\u7528: 1. \u77e5\u8bc6\u5b58\u50a8: \u4f5c\u4e3a\u6a21\u578b\u7684\"\u8bb0\u5fc6\u5e93\"\uff0c\u5b58\u50a8\u8bed\u8a00\u6a21\u5f0f\u548c\u4e16\u754c\u77e5\u8bc6 2. \u7279\u5f81\u63d0\u53d6: \u5bf9\u6bcf\u4e2a\u4f4d\u7f6e\u8fdb\u884c\u975e\u7ebf\u6027\u7279\u5f81\u53d8\u6362 3. \u8868\u8fbe\u589e\u5f3a: \u901a\u8fc7\u9ad8\u7ef4\u6620\u5c04\u589e\u5f3a\u6a21\u578b\u8868\u8fbe\u80fd\u529b 4. \u4e0e\u6ce8\u610f\u529b\u4e92\u8865: \u63d0\u4f9b\u4f4d\u7f6e\u5185\u7684\u6df1\u5ea6\u5904\u7406</p> <p>\u6280\u672f\u7ec6\u8282: - \u9010\u4f4d\u7f6e\u72ec\u7acb\u5904\u7406\uff0c\u4e0e\u6ce8\u610f\u529b\u7684\u5e8f\u5217\u5efa\u6a21\u5f62\u6210\u4e92\u8865 - \u901a\u8fc7\u5347\u7ef4-\u6fc0\u6d3b-\u964d\u7ef4\u7684\u8fc7\u7a0b\u589e\u5f3a\u7279\u5f81\u8868\u793a - \u53c2\u6570\u91cf\u901a\u5e38\u5360Transformer\u6a21\u578b\u603b\u53c2\u6570\u76842/3</p>"},{"location":"fundamentals/transformer/ffn/#q2-ffn","title":"Q2: \u4e3a\u4ec0\u4e48FFN\u8981\u5148\u5347\u7ef4\u518d\u964d\u7ef4\uff1f","text":"<p>\u8bbe\u8ba1\u539f\u7406: 1. \u8868\u793a\u7a7a\u95f4\u6269\u5c55: \u5347\u7ef4\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u7279\u5f81\u8868\u793a\u7a7a\u95f4 2. \u975e\u7ebf\u6027\u5efa\u6a21: \u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u66f4\u5bb9\u6613\u62df\u5408\u590d\u6742\u51fd\u6570 3. \u7279\u5f81\u4ea4\u4e92: \u66f4\u591a\u7ef4\u5ea6\u5141\u8bb8\u66f4\u590d\u6742\u7684\u7279\u5f81\u7ec4\u5408 4. \u4fe1\u606f\u74f6\u9888: \u6700\u7ec8\u964d\u7ef4\u8d77\u5230\u4fe1\u606f\u7b5b\u9009\u7684\u4f5c\u7528</p> <p>\u6570\u5b66\u76f4\u89c9: <pre><code>d_model \u2192 d_ff \u2192 d_model\n512 \u2192 2048 \u2192 512\n</code></pre> \u4e2d\u95f4\u7684\u9ad8\u7ef4\u7a7a\u95f4\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u975e\u7ebf\u6027\u5efa\u6a21\u80fd\u529b\u3002</p>"},{"location":"fundamentals/transformer/ffn/#q3","title":"Q3: \u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u4ec0\u4e48\u5f71\u54cd\uff1f","text":"<p>\u6027\u80fd\u5bf9\u6bd4:</p> \u6fc0\u6d3b\u51fd\u6570 \u4f18\u52bf \u52a3\u52bf \u9002\u7528\u573a\u666f ReLU \u8ba1\u7b97\u7b80\u5355\uff0c\u8bad\u7ec3\u5feb \u6b7b\u795e\u7ecf\u5143\u95ee\u9898 \u65e9\u671f\u6a21\u578b GELU \u5e73\u6ed1\uff0c\u6027\u80fd\u597d \u8ba1\u7b97\u7a0d\u590d\u6742 \u4e2d\u7b49\u89c4\u6a21\u6a21\u578b SwiGLU \u6027\u80fd\u6700\u4f73\uff0c\u95e8\u63a7\u673a\u5236 \u53c2\u6570\u91cf\u589e\u52a0 \u73b0\u4ee3\u5927\u6a21\u578b <p>\u9009\u62e9\u7b56\u7565: - \u8ba1\u7b97\u8d44\u6e90\u5145\u8db3\uff1a\u9009\u62e9SwiGLU - \u5e73\u8861\u6027\u80fd\u548c\u6548\u7387\uff1a\u9009\u62e9GELU - \u6781\u5ea6\u5173\u6ce8\u901f\u5ea6\uff1a\u9009\u62e9ReLU</p>"},{"location":"fundamentals/transformer/ffn/#q4-ffn","title":"Q4: FFN\u5982\u4f55\u5b58\u50a8\u548c\u68c0\u7d22\u77e5\u8bc6\uff1f","text":"<p>\u5b58\u50a8\u673a\u5236: 1. \u5206\u5e03\u5f0f\u8868\u793a: \u77e5\u8bc6\u5206\u5e03\u5728\u4e0d\u540c\u795e\u7ecf\u5143\u7684\u6743\u91cd\u4e2d 2. \u6fc0\u6d3b\u6a21\u5f0f: \u7279\u5b9a\u8f93\u5165\u89e6\u53d1\u7279\u5b9a\u7684\u795e\u7ecf\u5143\u7ec4\u5408 3. \u5c42\u6b21\u7ed3\u6784: \u4e0d\u540c\u5c42\u7684FFN\u5b58\u50a8\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u7684\u77e5\u8bc6</p> <p>\u68c0\u7d22\u8fc7\u7a0b: <pre><code># \u7b80\u5316\u7684\u77e5\u8bc6\u68c0\u7d22\u8fc7\u7a0b\ninput_features = attention_output  # \u67e5\u8be2\"\u952e\"\nactivated_neurons = ffn_layer1(input_features)  # \u6fc0\u6d3b\u76f8\u5173\u795e\u7ecf\u5143\nknowledge_pattern = activation_function(activated_neurons)  # \u77e5\u8bc6\u6a21\u5f0f\noutput_knowledge = ffn_layer2(knowledge_pattern)  # \u68c0\u7d22\"\u503c\"\n</code></pre></p>"},{"location":"fundamentals/transformer/ffn/#_7","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/ffn/#ffn_5","title":"\u6807\u51c6FFN\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FeedForward(nn.Module):\n    \"\"\"\u6807\u51c6Transformer FFN\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1, activation='relu'):\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_ff\n\n        self.linear1 = nn.Linear(d_model, d_ff)\n        self.linear2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n\n        # \u9009\u62e9\u6fc0\u6d3b\u51fd\u6570\n        if activation == 'relu':\n            self.activation = nn.ReLU()\n        elif activation == 'gelu':\n            self.activation = nn.GELU()\n        else:\n            raise ValueError(f\"Unsupported activation: {activation}\")\n\n    def forward(self, x):\n        # x shape: [batch_size, seq_len, d_model]\n\n        # \u7b2c\u4e00\u5c42\uff1a\u5347\u7ef4 + \u6fc0\u6d3b\n        hidden = self.activation(self.linear1(x))\n        hidden = self.dropout(hidden)\n\n        # \u7b2c\u4e8c\u5c42\uff1a\u964d\u7ef4\n        output = self.linear2(hidden)\n\n        return output\n\nclass SwiGLU(nn.Module):\n    \"\"\"SwiGLU\u6fc0\u6d3b\u51fd\u6570\u7684FFN\u5b9e\u73b0\"\"\"\n\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.d_ff = d_ff\n\n        # SwiGLU\u9700\u8981\u4e24\u4e2a\u7ebf\u6027\u5c42\u7528\u4e8e\u95e8\u63a7\n        self.w1 = nn.Linear(d_model, d_ff, bias=False)\n        self.w2 = nn.Linear(d_model, d_ff, bias=False) \n        self.w3 = nn.Linear(d_ff, d_model, bias=False)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # SwiGLU: swish(W1*x) \u2299 (W2*x)\n        swish_gate = F.silu(self.w1(x))  # Swish activation\n        linear_part = self.w2(x)\n\n        # \u95e8\u63a7\u673a\u5236\n        gated = swish_gate * linear_part\n        gated = self.dropout(gated)\n\n        # \u8f93\u51fa\u6295\u5f71\n        output = self.w3(gated)\n\n        return output\n\n# \u6027\u80fd\u5bf9\u6bd4\u793a\u4f8b\ndef compare_ffn_activations():\n    \"\"\"\u5bf9\u6bd4\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u7684FFN\u6027\u80fd\"\"\"\n\n    d_model, d_ff = 512, 2048\n    batch_size, seq_len = 32, 128\n\n    # \u6d4b\u8bd5\u6570\u636e\n    x = torch.randn(batch_size, seq_len, d_model)\n\n    # \u4e0d\u540cFFN\u5b9e\u73b0\n    ffn_relu = FeedForward(d_model, d_ff, activation='relu')\n    ffn_gelu = FeedForward(d_model, d_ff, activation='gelu')\n    ffn_swiglu = SwiGLU(d_model, d_ff)\n\n    print(\"=== FFN\u6fc0\u6d3b\u51fd\u6570\u5bf9\u6bd4 ===\")\n\n    # \u53c2\u6570\u91cf\u5bf9\u6bd4\n    relu_params = sum(p.numel() for p in ffn_relu.parameters())\n    gelu_params = sum(p.numel() for p in ffn_gelu.parameters())\n    swiglu_params = sum(p.numel() for p in ffn_swiglu.parameters())\n\n    print(f\"ReLU FFN\u53c2\u6570\u91cf: {relu_params:,}\")\n    print(f\"GELU FFN\u53c2\u6570\u91cf: {gelu_params:,}\")\n    print(f\"SwiGLU FFN\u53c2\u6570\u91cf: {swiglu_params:,}\")\n\n    # \u8ba1\u7b97\u65f6\u95f4\u5bf9\u6bd4\n    import time\n\n    with torch.no_grad():\n        # ReLU\n        start = time.time()\n        for _ in range(100):\n            _ = ffn_relu(x)\n        relu_time = time.time() - start\n\n        # GELU\n        start = time.time()\n        for _ in range(100):\n            _ = ffn_gelu(x)\n        gelu_time = time.time() - start\n\n        # SwiGLU\n        start = time.time()\n        for _ in range(100):\n            _ = ffn_swiglu(x)\n        swiglu_time = time.time() - start\n\n    print(f\"ReLU\u63a8\u7406\u65f6\u95f4: {relu_time:.4f}\u79d2\")\n    print(f\"GELU\u63a8\u7406\u65f6\u95f4: {gelu_time:.4f}\u79d2\")\n    print(f\"SwiGLU\u63a8\u7406\u65f6\u95f4: {swiglu_time:.4f}\u79d2\")\n\n# \u77e5\u8bc6\u5b58\u50a8\u53ef\u89c6\u5316\nclass KnowledgeAnalyzer:\n    \"\"\"\u5206\u6790FFN\u4e2d\u7684\u77e5\u8bc6\u5b58\u50a8\u6a21\u5f0f\"\"\"\n\n    def __init__(self, ffn_model):\n        self.ffn = ffn_model\n\n    def analyze_neuron_activation(self, inputs, texts):\n        \"\"\"\u5206\u6790\u4e0d\u540c\u8f93\u5165\u5bf9\u795e\u7ecf\u5143\u7684\u6fc0\u6d3b\u6a21\u5f0f\"\"\"\n\n        activations = []\n        with torch.no_grad():\n            for input_tensor in inputs:\n                # \u83b7\u53d6\u7b2c\u4e00\u5c42\u7684\u6fc0\u6d3b\n                hidden = torch.relu(self.ffn.linear1(input_tensor))\n                activations.append(hidden.mean(dim=1))  # \u5e73\u5747\u6c60\u5316\n\n        # \u5206\u6790\u6fc0\u6d3b\u6a21\u5f0f\n        activations = torch.stack(activations)\n\n        # \u627e\u51fa\u6700\u6d3b\u8dc3\u7684\u795e\u7ecf\u5143\n        neuron_activity = activations.mean(dim=0)\n        top_neurons = torch.topk(neuron_activity, k=10).indices\n\n        print(\"\u6700\u6d3b\u8dc3\u7684\u795e\u7ecf\u5143\u7d22\u5f15:\", top_neurons.tolist())\n\n        # \u5206\u6790\u4e0d\u540c\u8f93\u5165\u7684\u6fc0\u6d3b\u76f8\u4f3c\u6027\n        similarity_matrix = torch.cosine_similarity(\n            activations.unsqueeze(1), \n            activations.unsqueeze(0), \n            dim=2\n        )\n\n        return {\n            'activations': activations,\n            'top_neurons': top_neurons,\n            'similarity_matrix': similarity_matrix\n        }\n\nif __name__ == \"__main__\":\n    compare_ffn_activations()\n</code></pre>"},{"location":"fundamentals/transformer/ffn/#_8","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3FFN\u7684\u57fa\u672c\u7ed3\u6784\u548c\u6570\u5b66\u539f\u7406</li> <li>[ ] \u638c\u63e1\u4e0d\u540c\u6fc0\u6d3b\u51fd\u6570\u7684\u7279\u70b9\u548c\u9002\u7528\u573a\u666f</li> <li>[ ] \u7406\u89e3FFN\u7684\u77e5\u8bc6\u5b58\u50a8\u673a\u5236</li> <li>[ ] \u80fd\u5b9e\u73b0\u548c\u5bf9\u6bd4\u4e0d\u540c\u7684FFN\u53d8\u4f53</li> <li>[ ] \u7406\u89e3FFN\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e92\u8865\u5173\u7cfb</li> </ul>"},{"location":"fundamentals/transformer/ffn/#_9","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0b\u4e00\u8282\uff1a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/language-models/","title":"\u8bed\u8a00\u6a21\u578b\u67b6\u6784","text":""},{"location":"fundamentals/transformer/language-models/#_2","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u7406\u89e3\u4e0d\u540cTransformer\u67b6\u6784\u7684\u8bbe\u8ba1\u539f\u7406\uff0c\u638c\u63e1GPT\u4e0eBERT\u7684\u6838\u5fc3\u5dee\u5f02\uff0c\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5927\u6a21\u578b\u504f\u7231Decoder-Only\u67b6\u6784\u3002</p>"},{"location":"fundamentals/transformer/language-models/#_3","title":"\ud83d\udcd6 \u9605\u8bfb\u6750\u6599","text":""},{"location":"fundamentals/transformer/language-models/#_4","title":"\u6838\u5fc3\u6982\u5ff5\u6587\u7ae0","text":"<p>\u53c2\u8003Attention\u673a\u5236\u9875\u9762\u7684\u9605\u8bfb\u6750\u6599\uff0c\u91cd\u70b9\u5173\u6ce8\uff1a - Encoder-Only vs Decoder-Only\u67b6\u6784\u5bf9\u6bd4 - GPT\u7cfb\u5217\u6a21\u578b\u7684\u6f14\u8fdb - BERT\u7684\u53cc\u5411\u7f16\u7801\u7279\u6027</p>"},{"location":"fundamentals/transformer/language-models/#_5","title":"\ud83d\udcdd \u77e5\u8bc6\u603b\u7ed3","text":""},{"location":"fundamentals/transformer/language-models/#_6","title":"\u4e09\u5927\u67b6\u6784\u7c7b\u578b","text":"\u67b6\u6784\u7c7b\u578b \u4ee3\u8868\u6a21\u578b \u7279\u70b9 \u5e94\u7528\u573a\u666f Encoder-Only BERT, RoBERTa \u53cc\u5411\u6ce8\u610f\u529b\uff0c\u9002\u5408\u7406\u89e3\u4efb\u52a1 \u6587\u672c\u5206\u7c7b\u3001\u9605\u8bfb\u7406\u89e3\u3001\u60c5\u611f\u5206\u6790 Decoder-Only GPT, LLaMA, ChatGPT \u56e0\u679c\u6ce8\u610f\u529b\uff0c\u9002\u5408\u751f\u6210\u4efb\u52a1 \u6587\u672c\u751f\u6210\u3001\u5bf9\u8bdd\u3001\u4ee3\u7801\u751f\u6210 Encoder-Decoder T5, BART \u7f16\u7801-\u89e3\u7801\u7ed3\u6784 \u7ffb\u8bd1\u3001\u6458\u8981\u3001\u95ee\u7b54"},{"location":"fundamentals/transformer/language-models/#attention-mask","title":"Attention Mask\u5bf9\u6bd4","text":"<p>BERT (\u53cc\u5411\u6ce8\u610f\u529b): <pre><code>Token:  [CLS] I    love  AI   [SEP]\nMask:   \u2713     \u2713    \u2713     \u2713    \u2713\n\u6bcf\u4e2atoken\u90fd\u53ef\u4ee5\u770b\u5230\u6240\u6709\u5176\u4ed6token\n</code></pre></p> <p>GPT (\u56e0\u679c\u6ce8\u610f\u529b): <pre><code>Token:  I    love  AI    very  much\nMask:   \u2713    \n        \u2713    \u2713     \n        \u2713    \u2713     \u2713\n        \u2713    \u2713     \u2713     \u2713\n        \u2713    \u2713     \u2713     \u2713     \u2713\n\u6bcf\u4e2atoken\u53ea\u80fd\u770b\u5230\u5b83\u4e4b\u524d\u7684token\uff08\u5305\u62ec\u81ea\u5df1\uff09\n</code></pre></p>"},{"location":"fundamentals/transformer/language-models/#_7","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/language-models/#q1-encoder-onlydecoder-onlygptbert","title":"Q1: Encoder-Only\u548cDecoder-Only\u67b6\u6784\u662f\u4ec0\u4e48\uff1fGPT\u548cBERT\u5206\u522b\u662f\u4ec0\u4e48\u67b6\u6784\uff1f","text":"<p>\u7b80\u6d01\u56de\u7b54\uff1a - Encoder-Only: \u4f7f\u7528\u53cc\u5411\u6ce8\u610f\u529b\uff0c\u53ef\u4ee5\u540c\u65f6\u770b\u5230\u524d\u540e\u6587\u672c\uff0cBERT\u5c31\u662f\u8fd9\u79cd\u67b6\u6784 - Decoder-Only: \u4f7f\u7528\u56e0\u679c\u6ce8\u610f\u529b\uff0c\u53ea\u80fd\u770b\u5230\u5f53\u524d\u4f4d\u7f6e\u4e4b\u524d\u7684\u6587\u672c\uff0cGPT\u7cfb\u5217\u90fd\u662f\u8fd9\u79cd\u67b6\u6784</p>"},{"location":"fundamentals/transformer/language-models/#q11","title":"Q1.1: \u67b6\u6784\u533a\u522b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u6838\u5fc3\u533a\u522b\uff1a</p> <ol> <li>\u6ce8\u610f\u529b\u673a\u5236</li> <li>Encoder-Only: \u53cc\u5411\u6ce8\u610f\u529b\uff0c\u65e0\u63a9\u7801\u9650\u5236</li> <li> <p>Decoder-Only: \u56e0\u679c\u6ce8\u610f\u529b\uff0c\u4f7f\u7528\u4e0b\u4e09\u89d2\u63a9\u7801</p> </li> <li> <p>\u8bad\u7ec3\u76ee\u6807</p> </li> <li>BERT: \u63a9\u7801\u8bed\u8a00\u6a21\u578b(MLM) + \u4e0b\u4e00\u53e5\u9884\u6d4b(NSP)</li> <li> <p>GPT: \u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\uff0c\u9884\u6d4b\u4e0b\u4e00\u4e2atoken</p> </li> <li> <p>\u4f4d\u7f6e\u7f16\u7801</p> </li> <li>BERT: \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801</li> <li>GPT: \u7edd\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff08\u65e9\u671f\uff09\u2192 \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff08\u73b0\u4ee3\uff09</li> </ol>"},{"location":"fundamentals/transformer/language-models/#q12","title":"Q1.2: \u7528\u9014\u5dee\u5f02\uff1f","text":"<p>BERT\u64c5\u957f\u7406\u89e3\u4efb\u52a1\uff1a - \u6587\u672c\u5206\u7c7b - \u60c5\u611f\u5206\u6790 - \u9605\u8bfb\u7406\u89e3 - \u5b9e\u4f53\u8bc6\u522b</p> <p>GPT\u64c5\u957f\u751f\u6210\u4efb\u52a1\uff1a - \u6587\u672c\u7eed\u5199 - \u5bf9\u8bdd\u751f\u6210 - \u4ee3\u7801\u751f\u6210 - \u521b\u610f\u5199\u4f5c</p>"},{"location":"fundamentals/transformer/language-models/#q13","title":"Q1.3: \u4f18\u52a3\u6bd4\u8f83","text":"\u65b9\u9762 BERT\u4f18\u52bf GPT\u4f18\u52bf \u7406\u89e3\u80fd\u529b \u53cc\u5411\u4e0a\u4e0b\u6587\uff0c\u7406\u89e3\u66f4\u6df1\u5165 \u9002\u5408\u5e8f\u5217\u751f\u6210\uff0c\u903b\u8f91\u8fde\u8d2f \u8bad\u7ec3\u6548\u7387 \u5e76\u884c\u8bad\u7ec3\u6240\u6709\u4f4d\u7f6e \u81ea\u56de\u5f52\u8bad\u7ec3\uff0c\u7b80\u5355\u7a33\u5b9a \u63a8\u7406\u901f\u5ea6 \u5e76\u884c\u63a8\u7406 \u9700\u8981\u9010\u6b65\u751f\u6210 \u5e94\u7528\u7075\u6d3b\u6027 \u9700\u8981\u9488\u5bf9\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03 Zero-shot\u80fd\u529b\u5f3a"},{"location":"fundamentals/transformer/language-models/#q2-decoder-only","title":"Q2: \u4e3a\u4ec0\u4e48\u4e3b\u6d41\u5927\u6a21\u578b\u90fd\u7528Decoder-Only\u67b6\u6784\uff1f","text":"<p>\u4e3b\u8981\u539f\u56e0\uff1a</p> <ol> <li>\u7edf\u4e00\u7684\u751f\u6210\u8303\u5f0f</li> <li>\u6240\u6709\u4efb\u52a1\u90fd\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6587\u672c\u751f\u6210</li> <li> <p>\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u7279\u6b8a\u67b6\u6784</p> </li> <li> <p>\u66f4\u5f3a\u7684\u6d8c\u73b0\u80fd\u529b</p> </li> <li>\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u540e\u5c55\u73b0\u51fa\u5f3a\u5927\u7684few-shot\u5b66\u4e60\u80fd\u529b</li> <li> <p>\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u5929\u7136\u9002\u5408\u5bf9\u8bdd\u573a\u666f</p> </li> <li> <p>\u6269\u5c55\u6027\u66f4\u597d</p> </li> <li>\u67b6\u6784\u7b80\u5355\uff0c\u6613\u4e8e\u6269\u5927\u6a21\u578b\u89c4\u6a21</li> <li> <p>\u8bad\u7ec3\u7a33\u5b9a\u6027\u66f4\u597d</p> </li> <li> <p>\u5e94\u7528\u573a\u666f\u66f4\u5e7f</p> </li> <li>\u4ece\u5bf9\u8bdd\u5230\u4ee3\u7801\u751f\u6210\uff0c\u4e00\u4e2a\u6a21\u578b\u641e\u5b9a</li> <li>\u5546\u4e1a\u4ef7\u503c\u66f4\u9ad8</li> </ol>"},{"location":"fundamentals/transformer/language-models/#q3-qwenllama","title":"Q3: \u7b80\u5355\u8bb2\u89e3Qwen/LLaMA\u6a21\u578b\u67b6\u6784","text":"<p>LLaMA\u67b6\u6784\u7279\u70b9\uff1a</p> <pre><code>\u8f93\u5165 \u2192 Token Embedding + \u4f4d\u7f6e\u7f16\u7801\n     \u2193\n   N \u00d7 Decoder Layer:\n     \u251c\u2500\u2500 RMSNorm\n     \u251c\u2500\u2500 Multi-Head Attention (RoPE\u4f4d\u7f6e\u7f16\u7801)\n     \u251c\u2500\u2500 \u6b8b\u5dee\u8fde\u63a5\n     \u251c\u2500\u2500 RMSNorm  \n     \u251c\u2500\u2500 SwiGLU FFN\n     \u2514\u2500\u2500 \u6b8b\u5dee\u8fde\u63a5\n     \u2193\n   RMSNorm \u2192 \u8f93\u51fa\u5c42 \u2192 \u9884\u6d4b\u4e0b\u4e00\u4e2atoken\n</code></pre> <p>\u5173\u952e\u6280\u672f\u6539\u8fdb\uff1a - RMSNorm: \u66ff\u4ee3LayerNorm\uff0c\u8ba1\u7b97\u66f4\u9ad8\u6548 - RoPE\u4f4d\u7f6e\u7f16\u7801: \u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\uff0c\u652f\u6301\u66f4\u957f\u5e8f\u5217 - SwiGLU\u6fc0\u6d3b: \u66ff\u4ee3ReLU\uff0c\u6548\u679c\u66f4\u597d - Pre-Norm: \u5f52\u4e00\u5316\u524d\u7f6e\uff0c\u8bad\u7ec3\u66f4\u7a33\u5b9a</p>"},{"location":"fundamentals/transformer/language-models/#_8","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u73b0","text":""},{"location":"fundamentals/transformer/language-models/#_9","title":"\u67b6\u6784\u5bf9\u6bd4\u793a\u4f8b","text":"<pre><code># BERT\u98ce\u683c\u7684\u53cc\u5411\u6ce8\u610f\u529b\nclass BidirectionalAttention(nn.Module):\n    def forward(self, x):\n        # \u53ef\u4ee5\u770b\u5230\u5168\u90e8\u5e8f\u5217\n        attn_mask = torch.ones(seq_len, seq_len)  # \u51681\u77e9\u9635\n        return self.attention(x, mask=attn_mask)\n\n# GPT\u98ce\u683c\u7684\u56e0\u679c\u6ce8\u610f\u529b  \nclass CausalAttention(nn.Module):\n    def forward(self, x):\n        # \u53ea\u80fd\u770b\u5230\u5f53\u524d\u4f4d\u7f6e\u4e4b\u524d\n        seq_len = x.size(1)\n        attn_mask = torch.tril(torch.ones(seq_len, seq_len))  # \u4e0b\u4e09\u89d2\u77e9\u9635\n        return self.attention(x, mask=attn_mask)\n</code></pre>"},{"location":"fundamentals/transformer/language-models/#_10","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u80fd\u753b\u51faBERT\u548cGPT\u7684\u67b6\u6784\u5bf9\u6bd4\u56fe</li> <li>[ ] \u7406\u89e3\u4e3a\u4ec0\u4e48GPT\u9700\u8981\u56e0\u679c\u63a9\u7801</li> <li>[ ] \u80fd\u89e3\u91ca\u4e3b\u6d41\u5927\u6a21\u578b\u9009\u62e9Decoder-Only\u7684\u539f\u56e0</li> <li>[ ] \u638c\u63e1LLaMA/Qwen\u7684\u5173\u952e\u6280\u672f\u6539\u8fdb\u70b9</li> </ul>"},{"location":"fundamentals/transformer/language-models/#_11","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1aAttention\u673a\u5236</li> <li>\u4e0b\u4e00\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"fundamentals/transformer/tokenizer/","title":"Tokenizer\u6280\u672f","text":""},{"location":"fundamentals/transformer/tokenizer/#_1","title":"\ud83c\udfaf \u672c\u8282\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3Tokenizer\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u638c\u63e1\u4e0d\u540ctokenization\u7b97\u6cd5\u7684\u7279\u70b9\u548c\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u5927\u6a21\u578b\u7684\u6587\u672c\u5904\u7406\u5960\u5b9a\u57fa\u7840\u3002</p>"},{"location":"fundamentals/transformer/tokenizer/#_2","title":"\ud83d\udcdd \u6280\u672f\u539f\u7406\u89e3\u6790","text":""},{"location":"fundamentals/transformer/tokenizer/#tokenizer_1","title":"Tokenizer\u57fa\u672c\u6982\u5ff5","text":"<p>Tokenizer\u662f\u5c06\u539f\u59cb\u6587\u672c\u8f6c\u6362\u4e3a\u6a21\u578b\u53ef\u5904\u7406\u7684\u6570\u503c\u8868\u793a\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u662f\u8fde\u63a5\u4eba\u7c7b\u8bed\u8a00\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6865\u6881\u3002</p>"},{"location":"fundamentals/transformer/tokenizer/#_3","title":"\u6838\u5fc3\u529f\u80fd","text":"<pre><code># Tokenizer\u7684\u57fa\u672c\u6d41\u7a0b\ntext = \"Hello, world! This is a test.\"\n     \u2193 1. \u6587\u672c\u89c4\u8303\u5316(Normalization)\nnormalized = \"hello world this is a test\"\n     \u2193 2. \u9884\u5206\u8bcd(Pre-tokenization)  \npre_tokens = [\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\"]\n     \u2193 3. \u6a21\u578b\u5904\u7406(Model Processing)\ntokens = [\"hello\", \"wor\", \"##ld\", \"this\", \"is\", \"a\", \"test\"]\n     \u2193 4. \u540e\u5904\u7406(Post-processing)\nfinal_tokens = [\"[CLS]\", \"hello\", \"wor\", \"##ld\", \"this\", \"is\", \"a\", \"test\", \"[SEP]\"]\n     \u2193 5. \u6570\u503c\u6620\u5c04\ntoken_ids = [101, 7592, 24829, 2094, 2023, 2003, 1037, 3231, 102]\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#tokenization","title":"\u4e09\u5927Tokenization\u8303\u5f0f","text":""},{"location":"fundamentals/transformer/tokenizer/#1-word-level","title":"1. \u8bcd\u7ea7\u522b\u5206\u8bcd (Word-level)","text":"<p>\u539f\u7406: \u5c06\u6587\u672c\u6309\u5b8c\u6574\u5355\u8bcd\u5206\u5272</p> <pre><code>class WordLevelTokenizer:\n    def __init__(self, vocab_path):\n        self.word_to_id = self.load_vocab(vocab_path)\n        self.id_to_word = {v: k for k, v in self.word_to_id.items()}\n        self.unk_token = \"[UNK]\"\n\n    def tokenize(self, text):\n        words = text.lower().split()\n        tokens = []\n        for word in words:\n            if word in self.word_to_id:\n                tokens.append(word)\n            else:\n                tokens.append(self.unk_token)  # \u672a\u77e5\u8bcd\u5904\u7406\n        return tokens\n\n    def encode(self, text):\n        tokens = self.tokenize(text)\n        return [self.word_to_id.get(token, self.word_to_id[self.unk_token]) \n                for token in tokens]\n</code></pre> <p>\u4f18\u52bf: - \u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027 - \u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u4e60\u60ef - \u5b9e\u73b0\u7b80\u5355\u76f4\u89c2</p> <p>\u52a3\u52bf: - \u8bcd\u6c47\u8868\u5e9e\u5927(\u901a\u5e38&gt;100k) - \u672a\u767b\u5f55\u8bcd(OOV)\u95ee\u9898\u4e25\u91cd - \u65e0\u6cd5\u5904\u7406\u5f62\u6001\u53d8\u5316\u4e30\u5bcc\u7684\u8bed\u8a00</p>"},{"location":"fundamentals/transformer/tokenizer/#2-character-level","title":"2. \u5b57\u7b26\u7ea7\u522b\u5206\u8bcd (Character-level)","text":"<p>\u539f\u7406: \u5c06\u6587\u672c\u5206\u89e3\u4e3a\u5355\u4e2a\u5b57\u7b26</p> <pre><code>class CharacterLevelTokenizer:\n    def __init__(self):\n        # \u57fa\u672c\u5b57\u7b26\u96c6\n        self.chars = list(\"abcdefghijklmnopqrstuvwxyz0123456789 .,!?'\")\n        self.char_to_id = {char: i for i, char in enumerate(self.chars)}\n        self.id_to_char = {i: char for i, char in enumerate(self.chars)}\n\n    def tokenize(self, text):\n        return list(text.lower())\n\n    def encode(self, text):\n        chars = self.tokenize(text)\n        return [self.char_to_id.get(char, 0) for char in chars]  # 0 for unknown\n\n    def decode(self, token_ids):\n        chars = [self.id_to_char.get(id, \"\") for id in token_ids]\n        return \"\".join(chars)\n</code></pre> <p>\u4f18\u52bf: - \u8bcd\u6c47\u8868\u6781\u5c0f(\u901a\u5e38&lt;100) - \u65e0OOV\u95ee\u9898 - \u5bf9\u62fc\u5199\u9519\u8bef\u9c81\u68d2</p> <p>\u52a3\u52bf: - \u5e8f\u5217\u957f\u5ea6\u5927\u5e45\u589e\u52a0 - \u4e22\u5931\u8bcd\u8fb9\u754c\u4fe1\u606f - \u8bed\u4e49\u7406\u89e3\u56f0\u96be</p>"},{"location":"fundamentals/transformer/tokenizer/#3-subword-level","title":"3. \u5b50\u8bcd\u7ea7\u522b\u5206\u8bcd (Subword-level)","text":"<p>\u6838\u5fc3\u601d\u60f3: \u5728\u8bcd\u6c47\u8868\u5927\u5c0f\u548c\u8bed\u4e49\u4fdd\u6301\u4e4b\u95f4\u627e\u5230\u5e73\u8861</p>"},{"location":"fundamentals/transformer/tokenizer/#subword","title":"\ud83d\udd2c \u4e3b\u6d41Subword\u7b97\u6cd5\u8be6\u89e3","text":""},{"location":"fundamentals/transformer/tokenizer/#1-bpe-byte-pair-encoding","title":"1. BPE (Byte Pair Encoding)","text":"<p>\u7b97\u6cd5\u539f\u7406: \u8fed\u4ee3\u5408\u5e76\u6700\u9891\u7e41\u7684\u5b57\u7b26\u5bf9</p>"},{"location":"fundamentals/transformer/tokenizer/#_4","title":"\u8bad\u7ec3\u8fc7\u7a0b","text":"<pre><code>class BPETokenizer:\n    def __init__(self):\n        self.vocab = {}\n        self.merges = []\n\n    def train(self, corpus, vocab_size):\n        # 1. \u521d\u59cb\u5316\uff1a\u5c06\u6240\u6709\u5b57\u7b26\u4f5c\u4e3a\u57fa\u7840\u8bcd\u6c47\n        word_freqs = self.get_word_frequencies(corpus)\n\n        # \u5c06\u6bcf\u4e2a\u8bcd\u5206\u89e3\u4e3a\u5b57\u7b26\n        vocab = set()\n        for word in word_freqs:\n            for char in word:\n                vocab.add(char)\n\n        vocab = list(vocab)\n\n        # 2. \u8fed\u4ee3\u5408\u5e76\u6700\u9891\u7e41\u7684\u5b57\u7b26\u5bf9\n        while len(vocab) &lt; vocab_size:\n            # \u7edf\u8ba1\u6240\u6709\u5b57\u7b26\u5bf9\u7684\u9891\u7387\n            pairs = self.get_all_pairs(word_freqs)\n\n            if not pairs:\n                break\n\n            # \u627e\u5230\u6700\u9891\u7e41\u7684\u5b57\u7b26\u5bf9\n            best_pair = max(pairs, key=pairs.get)\n\n            # \u5408\u5e76\u8fd9\u4e2a\u5b57\u7b26\u5bf9\n            vocab.append(''.join(best_pair))\n            self.merges.append(best_pair)\n\n            # \u66f4\u65b0\u8bcd\u9891\u5b57\u5178\n            word_freqs = self.merge_vocab(best_pair, word_freqs)\n\n        self.vocab = {token: i for i, token in enumerate(vocab)}\n\n    def get_all_pairs(self, word_freqs):\n        \"\"\"\u83b7\u53d6\u6240\u6709\u76f8\u90bb\u5b57\u7b26\u5bf9\u53ca\u5176\u9891\u7387\"\"\"\n        pairs = {}\n        for word, freq in word_freqs.items():\n            symbols = word.split()\n            for i in range(len(symbols) - 1):\n                pair = (symbols[i], symbols[i + 1])\n                pairs[pair] = pairs.get(pair, 0) + freq\n        return pairs\n\n    def merge_vocab(self, pair, word_freqs):\n        \"\"\"\u5408\u5e76\u6307\u5b9a\u5b57\u7b26\u5bf9\"\"\"\n        new_word_freqs = {}\n        bigram = ' '.join(pair)\n        replacement = ''.join(pair)\n\n        for word in word_freqs:\n            new_word = word.replace(bigram, replacement)\n            new_word_freqs[new_word] = word_freqs[word]\n\n        return new_word_freqs\n\n    def tokenize(self, text):\n        \"\"\"\u4f7f\u7528\u8bad\u7ec3\u597d\u7684BPE\u8fdb\u884c\u5206\u8bcd\"\"\"\n        words = text.split()\n        result = []\n\n        for word in words:\n            # \u5c06\u8bcd\u5206\u89e3\u4e3a\u5b57\u7b26\n            word_tokens = list(word)\n\n            # \u5e94\u7528\u6240\u6709\u5b66\u5230\u7684\u5408\u5e76\u89c4\u5219\n            for merge in self.merges:\n                word_tokens = self.apply_merge(word_tokens, merge)\n\n            result.extend(word_tokens)\n\n        return result\n\n    def apply_merge(self, tokens, merge):\n        \"\"\"\u5e94\u7528\u5355\u4e2a\u5408\u5e76\u89c4\u5219\"\"\"\n        new_tokens = []\n        i = 0\n        while i &lt; len(tokens):\n            if (i &lt; len(tokens) - 1 and \n                tokens[i] == merge[0] and \n                tokens[i + 1] == merge[1]):\n                # \u627e\u5230\u5339\u914d\u7684\u5bf9\uff0c\u5408\u5e76\n                new_tokens.append(''.join(merge))\n                i += 2\n            else:\n                new_tokens.append(tokens[i])\n                i += 1\n        return new_tokens\n\n# \u4f7f\u7528\u793a\u4f8b\ndef demo_bpe():\n    corpus = [\"low lower newest widest\", \"low lower newest widest\"] * 1000\n\n    bpe = BPETokenizer()\n    bpe.train(corpus, vocab_size=1000)\n\n    # \u6d4b\u8bd5\u5206\u8bcd\n    text = \"lowest\"\n    tokens = bpe.tokenize(text)\n    print(f\"'{text}' -&gt; {tokens}\")\n    # \u53ef\u80fd\u8f93\u51fa: ['low', 'est'] \u6216\u7c7b\u4f3c\u7684\u5b50\u8bcd\u7ec4\u5408\n\ndemo_bpe()\n</code></pre> <p>BPE\u7279\u70b9: - \u6570\u636e\u9a71\u52a8\uff0c\u65e0\u9700\u8bed\u8a00\u5b66\u77e5\u8bc6 - \u80fd\u5904\u7406\u672a\u89c1\u8fc7\u7684\u8bcd - GPT\u7cfb\u5217\u6a21\u578b\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/transformer/tokenizer/#2-wordpiece","title":"2. WordPiece","text":"<p>\u6838\u5fc3\u6539\u8fdb: \u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u4f3c\u7136\u5ea6\u9009\u62e9\u5408\u5e76</p> <pre><code>class WordPieceTokenizer:\n    def __init__(self):\n        self.vocab = {}\n        self.unk_token = \"[UNK]\"\n\n    def train(self, corpus, vocab_size):\n        # 1. \u521d\u59cb\u5316\u57fa\u7840\u8bcd\u6c47(\u5b57\u7b26 + \u7279\u6b8atoken)\n        base_vocab = set()\n        for text in corpus:\n            for char in text:\n                base_vocab.add(char)\n\n        base_vocab.update([\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n        vocab = list(base_vocab)\n\n        # 2. \u8fed\u4ee3\u6dfb\u52a0\u6700\u4f18\u5b50\u8bcd\n        while len(vocab) &lt; vocab_size:\n            best_subword = self.find_best_subword(corpus, vocab)\n            if best_subword is None:\n                break\n            vocab.append(best_subword)\n\n        self.vocab = {token: i for i, token in enumerate(vocab)}\n\n    def find_best_subword(self, corpus, current_vocab):\n        \"\"\"\u627e\u5230\u80fd\u6700\u5927\u5316\u8bed\u8a00\u6a21\u578b\u4f3c\u7136\u5ea6\u7684\u5b50\u8bcd\"\"\"\n        candidates = self.generate_candidates(corpus, current_vocab)\n\n        best_score = float('-inf')\n        best_subword = None\n\n        for candidate in candidates:\n            # \u8ba1\u7b97\u6dfb\u52a0\u8fd9\u4e2a\u5b50\u8bcd\u540e\u7684\u8bed\u8a00\u6a21\u578b\u5f97\u5206\n            score = self.calculate_lm_score(corpus, current_vocab + [candidate])\n\n            if score &gt; best_score:\n                best_score = score\n                best_subword = candidate\n\n        return best_subword\n\n    def generate_candidates(self, corpus, vocab):\n        \"\"\"\u751f\u6210\u5019\u9009\u5b50\u8bcd\"\"\"\n        candidates = set()\n\n        # \u57fa\u4e8e\u73b0\u6709\u8bcd\u6c47\u751f\u6210\u5019\u9009\n        for text in corpus:\n            tokens = self.basic_tokenize(text)\n            for token in tokens:\n                for i in range(len(token)):\n                    for j in range(i + 1, len(token) + 1):\n                        subword = token[i:j]\n                        if len(subword) &gt; 1 and subword not in vocab:\n                            candidates.add(subword)\n\n        return list(candidates)\n\n    def tokenize(self, text):\n        \"\"\"WordPiece\u5206\u8bcd\u7b97\u6cd5\"\"\"\n        tokens = []\n\n        for word in text.split():\n            # \u8d2a\u5fc3\u6700\u957f\u5339\u914d\n            start = 0\n            sub_tokens = []\n\n            while start &lt; len(word):\n                end = len(word)\n                cur_substr = None\n\n                # \u4ece\u6700\u957f\u5b50\u4e32\u5f00\u59cb\u5c1d\u8bd5\n                while start &lt; end:\n                    substr = word[start:end]\n                    if start &gt; 0:\n                        substr = \"##\" + substr  # WordPiece\u524d\u7f00\n\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n\n                if cur_substr is None:\n                    # \u65e0\u6cd5\u5206\u8bcd\uff0c\u6807\u8bb0\u4e3a\u672a\u77e5\n                    sub_tokens.append(self.unk_token)\n                    break\n\n                sub_tokens.append(cur_substr)\n                start = end\n\n            tokens.extend(sub_tokens)\n\n        return tokens\n\n# BERT\u4f7f\u7528\u7684WordPiece\u793a\u4f8b\ndef demo_wordpiece():\n    text = \"unaffable\"\n    # WordPiece\u53ef\u80fd\u5206\u8bcd\u4e3a: [\"un\", \"##aff\", \"##able\"]\n\n    wp = WordPieceTokenizer()\n    # \u5047\u8bbe\u5df2\u8bad\u7ec3\n    wp.vocab = {\"un\": 1, \"##aff\": 2, \"##able\": 3, \"[UNK]\": 0}\n\n    tokens = wp.tokenize(text)\n    print(f\"'{text}' -&gt; {tokens}\")\n\ndemo_wordpiece()\n</code></pre> <p>WordPiece\u4f18\u52bf: - \u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u4f18\u5316\uff0c\u7406\u8bba\u66f4\u4e25\u683c - BERT\u7b49\u6a21\u578b\u7684\u6807\u51c6\u9009\u62e9 - \u80fd\u66f4\u597d\u5730\u4fdd\u6301\u8bed\u4e49\u76f8\u5173\u6027</p>"},{"location":"fundamentals/transformer/tokenizer/#3-unigram-language-model","title":"3. Unigram Language Model","text":"<p>\u6838\u5fc3\u601d\u60f3: \u4ece\u5927\u8bcd\u6c47\u8868\u5f00\u59cb\uff0c\u9010\u6b65\u79fb\u9664\u4e0d\u91cd\u8981\u7684token</p> <pre><code>class UnigramTokenizer:\n    def __init__(self):\n        self.vocab = {}\n        self.token_probs = {}\n\n    def train(self, corpus, vocab_size):\n        # 1. \u521d\u59cb\u5316\u5927\u8bcd\u6c47\u8868\uff08\u5305\u542b\u6240\u6709\u53ef\u80fd\u7684\u5b50\u4e32\uff09\n        initial_vocab = self.build_initial_vocab(corpus)\n\n        # 2. \u521d\u59cb\u5316token\u6982\u7387\n        self.token_probs = self.initialize_probabilities(initial_vocab, corpus)\n\n        # 3. \u8fed\u4ee3\u51cf\u5c11\u8bcd\u6c47\u8868\n        current_vocab = list(initial_vocab)\n\n        while len(current_vocab) &gt; vocab_size:\n            # \u8ba1\u7b97\u79fb\u9664\u6bcf\u4e2atoken\u7684\u635f\u5931\n            losses = {}\n            for token in current_vocab:\n                if self.is_removable(token):  # \u4fdd\u62a4\u7279\u6b8atoken\n                    losses[token] = self.calculate_removal_loss(token, corpus)\n\n            # \u79fb\u9664\u635f\u5931\u6700\u5c0f\u7684token\n            if losses:\n                token_to_remove = min(losses, key=losses.get)\n                current_vocab.remove(token_to_remove)\n                del self.token_probs[token_to_remove]\n\n                # \u91cd\u65b0\u8ba1\u7b97\u6982\u7387\n                self.update_probabilities(current_vocab, corpus)\n\n        self.vocab = {token: i for i, token in enumerate(current_vocab)}\n\n    def calculate_removal_loss(self, token, corpus):\n        \"\"\"\u8ba1\u7b97\u79fb\u9664token\u7684\u4f3c\u7136\u5ea6\u635f\u5931\"\"\"\n        total_loss = 0\n\n        for text in corpus:\n            # \u8ba1\u7b97\u6709token\u65f6\u7684\u6700\u4f18\u5206\u8bcd\u4f3c\u7136\u5ea6\n            with_token = self.get_best_segmentation(text, include_token=token)\n\n            # \u8ba1\u7b97\u65e0token\u65f6\u7684\u6700\u4f18\u5206\u8bcd\u4f3c\u7136\u5ea6  \n            without_token = self.get_best_segmentation(text, exclude_token=token)\n\n            # \u635f\u5931 = \u65e0token\u4f3c\u7136\u5ea6 - \u6709token\u4f3c\u7136\u5ea6\n            loss = without_token['log_prob'] - with_token['log_prob']\n            total_loss += loss\n\n        return total_loss\n\n    def get_best_segmentation(self, text, include_token=None, exclude_token=None):\n        \"\"\"\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u627e\u6700\u4f18\u5206\u8bcd\"\"\"\n        n = len(text)\n\n        # dp[i] = (\u6700\u4f18\u5bf9\u6570\u4f3c\u7136\u5ea6, \u5206\u8bcd\u65b9\u6848)\n        dp = [(-float('inf'), [])] * (n + 1)\n        dp[0] = (0.0, [])\n\n        for i in range(n + 1):\n            if dp[i][0] == -float('inf'):\n                continue\n\n            for j in range(i + 1, n + 1):\n                token = text[i:j]\n\n                # \u68c0\u67e5token\u662f\u5426\u53ef\u7528\n                if exclude_token and token == exclude_token:\n                    continue\n                if include_token and token not in self.token_probs and token != include_token:\n                    continue\n                if token not in self.token_probs:\n                    continue\n\n                # \u8ba1\u7b97\u65b0\u7684\u4f3c\u7136\u5ea6\n                token_prob = self.token_probs.get(token, 1e-10)\n                new_prob = dp[i][0] + math.log(token_prob)\n\n                if new_prob &gt; dp[j][0]:\n                    dp[j] = (new_prob, dp[i][1] + [token])\n\n        return {'log_prob': dp[n][0], 'tokens': dp[n][1]}\n\n    def tokenize(self, text):\n        \"\"\"\u4f7f\u7528\u8bad\u7ec3\u597d\u7684Unigram\u6a21\u578b\u5206\u8bcd\"\"\"\n        result = self.get_best_segmentation(text)\n        return result['tokens']\n\n# SentencePiece\u4f7f\u7528\u7684Unigram\u793a\u4f8b\ndef demo_unigram():\n    corpus = [\"hello world\", \"hello universe\", \"hi world\"]\n\n    unigram = UnigramTokenizer()\n    unigram.train(corpus, vocab_size=20)\n\n    text = \"hello world\"\n    tokens = unigram.tokenize(text)\n    print(f\"'{text}' -&gt; {tokens}\")\n\ndemo_unigram()\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#4-bpe-byte-level-bpe","title":"4. \u5b57\u8282\u7ea7BPE (Byte-level BPE)","text":"<p>\u521b\u65b0\u70b9: \u5728UTF-8\u5b57\u8282\u7ea7\u522b\u8fdb\u884cBPE</p> <pre><code>class ByteLevelBPE:\n    def __init__(self):\n        # UTF-8\u5b57\u8282\u5230\u53ef\u6253\u5370\u5b57\u7b26\u7684\u6620\u5c04\n        self.byte_encoder = self.bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n\n    def bytes_to_unicode(self):\n        \"\"\"\u521b\u5efa\u5b57\u8282\u5230Unicode\u5b57\u7b26\u7684\u6620\u5c04\"\"\"\n        bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"\u00a1\"), ord(\"\u00ac\")+1))+list(range(ord(\"\u00ae\"), ord(\"\u00ff\")+1))\n        cs = bs[:]\n        n = 0\n        for b in range(2**8):\n            if b not in bs:\n                bs.append(b)\n                cs.append(2**8+n)\n                n += 1\n        cs = [chr(n) for n in cs]\n        return dict(zip(bs, cs))\n\n    def encode_text_to_bytes(self, text):\n        \"\"\"\u5c06\u6587\u672c\u7f16\u7801\u4e3a\u5b57\u8282\u7ea7\u8868\u793a\"\"\"\n        byte_encoded = []\n        for char in text:\n            # \u83b7\u53d6UTF-8\u5b57\u8282\n            utf8_bytes = char.encode('utf-8')\n            for byte in utf8_bytes:\n                byte_encoded.append(self.byte_encoder[byte])\n        return ''.join(byte_encoded)\n\n    def decode_bytes_to_text(self, byte_string):\n        \"\"\"\u5c06\u5b57\u8282\u7ea7\u8868\u793a\u89e3\u7801\u56de\u6587\u672c\"\"\"\n        bytes_list = []\n        for char in byte_string:\n            bytes_list.append(self.byte_decoder[char])\n\n        byte_array = bytes(bytes_list)\n        return byte_array.decode('utf-8', errors='replace')\n\n    def train(self, corpus, vocab_size):\n        \"\"\"\u5728\u5b57\u8282\u7ea7\u522b\u8bad\u7ec3BPE\"\"\"\n        # 1. \u5c06\u6240\u6709\u6587\u672c\u8f6c\u6362\u4e3a\u5b57\u8282\u7ea7\u8868\u793a\n        byte_corpus = []\n        for text in corpus:\n            byte_text = self.encode_text_to_bytes(text)\n            byte_corpus.append(byte_text)\n\n        # 2. \u5728\u5b57\u8282\u7ea7\u522b\u5e94\u7528\u6807\u51c6BPE\n        self.bpe = BPETokenizer()\n        self.bpe.train(byte_corpus, vocab_size)\n\n    def tokenize(self, text):\n        \"\"\"\u5b57\u8282\u7ea7BPE\u5206\u8bcd\"\"\"\n        # \u8f6c\u6362\u4e3a\u5b57\u8282\u7ea7\u8868\u793a\n        byte_text = self.encode_text_to_bytes(text)\n\n        # \u5e94\u7528BPE\n        byte_tokens = self.bpe.tokenize(byte_text)\n\n        return byte_tokens\n\n# GPT-2\u4f7f\u7528\u7684Byte-level BPE\ndef demo_byte_bpe():\n    text = \"Hello, \u4e16\u754c!\"  # \u5305\u542b\u4e2d\u6587\n\n    bbpe = ByteLevelBPE()\n    byte_text = bbpe.encode_text_to_bytes(text)\n    print(f\"\u5b57\u8282\u7ea7\u7f16\u7801: {byte_text}\")\n\n    decoded = bbpe.decode_bytes_to_text(byte_text)\n    print(f\"\u89e3\u7801\u7ed3\u679c: {decoded}\")\n\ndemo_byte_bpe()\n</code></pre> <p>\u5b57\u8282\u7ea7BPE\u4f18\u52bf: - \u901a\u7528\u6027\u5f3a\uff0c\u652f\u6301\u6240\u6709\u8bed\u8a00 - \u8bcd\u6c47\u8868\u7d27\u51d1 - GPT-2/3/4\u7684\u6807\u51c6\u9009\u62e9</p>"},{"location":"fundamentals/transformer/tokenizer/#tokenizer_2","title":"\ud83d\udcac \u73b0\u4ee3Tokenizer\u53d1\u5c55\u8d8b\u52bf","text":""},{"location":"fundamentals/transformer/tokenizer/#1-large-concept-models","title":"1. \u5927\u6982\u5ff5\u6a21\u578b (Large Concept Models)","text":"<p>\u7a81\u7834\u6027\u601d\u8def: \u8d85\u8d8atoken\u7ea7\u522b\uff0c\u76f4\u63a5\u5904\u7406\u6982\u5ff5\u7ea7\u522b</p> <pre><code>class ConceptLevelTokenizer:\n    \"\"\"\u6982\u5ff5\u7ea7\u522b\u7684tokenizer (\u7406\u8bba\u6a21\u578b)\"\"\"\n\n    def __init__(self, concept_encoder):\n        self.concept_encoder = concept_encoder  # \u9884\u8bad\u7ec3\u7684\u6982\u5ff5\u7f16\u7801\u5668\n\n    def encode_to_concepts(self, text):\n        \"\"\"\u5c06\u6587\u672c\u76f4\u63a5\u7f16\u7801\u4e3a\u6982\u5ff5\u5411\u91cf\"\"\"\n        # \u4f7f\u7528\u53e5\u5b50\u7ea7\u522b\u7684\u7f16\u7801\u5668\n        sentences = self.split_to_sentences(text)\n\n        concept_vectors = []\n        for sentence in sentences:\n            # \u5c06\u53e5\u5b50\u7f16\u7801\u4e3a\u6982\u5ff5\u5411\u91cf\u800c\u975etoken\u5e8f\u5217\n            concept_vec = self.concept_encoder.encode(sentence)\n            concept_vectors.append(concept_vec)\n\n        return concept_vectors\n\n    def decode_from_concepts(self, concept_vectors):\n        \"\"\"\u4ece\u6982\u5ff5\u5411\u91cf\u89e3\u7801\u56de\u6587\u672c\"\"\"\n        sentences = []\n        for concept_vec in concept_vectors:\n            sentence = self.concept_encoder.decode(concept_vec)\n            sentences.append(sentence)\n\n        return ' '.join(sentences)\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#2","title":"2. \u52a8\u6001\u4e0a\u4e0b\u6587\u5206\u8bcd","text":"<p>\u6838\u5fc3\u601d\u60f3: \u6839\u636e\u4e0a\u4e0b\u6587\u52a8\u6001\u8c03\u6574\u5206\u8bcd\u7b56\u7565</p> <pre><code>class ContextAwareTokenizer:\n    \"\"\"\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u52a8\u6001\u5206\u8bcd\u5668\"\"\"\n\n    def __init__(self, base_tokenizer, context_model):\n        self.base_tokenizer = base_tokenizer\n        self.context_model = context_model\n\n    def tokenize_with_context(self, text, context=\"\"):\n        \"\"\"\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u52a8\u6001\u5206\u8bcd\"\"\"\n\n        # 1. \u5206\u6790\u4e0a\u4e0b\u6587\u786e\u5b9a\u5206\u8bcd\u7b56\u7565\n        context_features = self.analyze_context(context)\n\n        # 2. \u6839\u636e\u4e0a\u4e0b\u6587\u9009\u62e9\u5206\u8bcd\u7c92\u5ea6\n        if context_features['domain'] == 'technical':\n            # \u6280\u672f\u6587\u672c\uff1a\u66f4\u7ec6\u7c92\u5ea6\u5206\u8bcd\n            granularity = 'fine'\n        elif context_features['domain'] == 'casual':\n            # \u65e5\u5e38\u5bf9\u8bdd\uff1a\u8f83\u7c97\u7c92\u5ea6\u5206\u8bcd\n            granularity = 'coarse'\n        else:\n            granularity = 'medium'\n\n        # 3. \u5e94\u7528\u52a8\u6001\u5206\u8bcd\n        tokens = self.adaptive_tokenize(text, granularity)\n\n        return tokens\n\n    def adaptive_tokenize(self, text, granularity):\n        \"\"\"\u81ea\u9002\u5e94\u5206\u8bcd\"\"\"\n        if granularity == 'fine':\n            # \u66f4\u591a\u5b50\u8bcd\u5206\u5272\n            return self.base_tokenizer.tokenize(text, merge_threshold=0.3)\n        elif granularity == 'coarse':\n            # \u66f4\u5c11\u5b50\u8bcd\u5206\u5272\n            return self.base_tokenizer.tokenize(text, merge_threshold=0.8)\n        else:\n            # \u6807\u51c6\u5206\u8bcd\n            return self.base_tokenizer.tokenize(text)\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#3-tokenizer","title":"3. \u591a\u6a21\u6001Tokenizer","text":"<p>\u6269\u5c55\u601d\u8def: \u7edf\u4e00\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u3001\u97f3\u9891\u7b49\u591a\u79cd\u6a21\u6001</p> <pre><code>class MultimodalTokenizer:\n    \"\"\"\u591a\u6a21\u6001\u7edf\u4e00tokenizer\"\"\"\n\n    def __init__(self):\n        self.text_tokenizer = BPETokenizer()\n        self.image_tokenizer = ImagePatchTokenizer()\n        self.audio_tokenizer = AudioSegmentTokenizer()\n\n        # \u7edf\u4e00\u8bcd\u6c47\u8868\n        self.unified_vocab = self.build_unified_vocab()\n\n    def tokenize_multimodal(self, inputs):\n        \"\"\"\u591a\u6a21\u6001\u8f93\u5165\u7684\u7edf\u4e00\u5206\u8bcd\"\"\"\n        unified_tokens = []\n\n        for modality, data in inputs.items():\n            if modality == 'text':\n                tokens = self.text_tokenizer.tokenize(data)\n                # \u6dfb\u52a0\u6a21\u6001\u6807\u8bc6\n                unified_tokens.extend([f\"&lt;text&gt;{token}\" for token in tokens])\n\n            elif modality == 'image':\n                patches = self.image_tokenizer.tokenize(data)\n                unified_tokens.extend([f\"&lt;image&gt;{patch}\" for patch in patches])\n\n            elif modality == 'audio':\n                segments = self.audio_tokenizer.tokenize(data)\n                unified_tokens.extend([f\"&lt;audio&gt;{segment}\" for segment in segments])\n\n        return unified_tokens\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#_5","title":"\ud83d\udcac \u9762\u8bd5\u95ee\u9898\u89e3\u7b54","text":""},{"location":"fundamentals/transformer/tokenizer/#q1-bpewordpieceunigram","title":"Q1: BPE\u3001WordPiece\u3001Unigram\u7684\u6838\u5fc3\u533a\u522b\u662f\u4ec0\u4e48\uff1f","text":"<p>\u7b80\u6d01\u5bf9\u6bd4:</p> \u7ef4\u5ea6 BPE WordPiece Unigram \u6838\u5fc3\u7b56\u7565 \u9891\u7387\u9a71\u52a8\u5408\u5e76 \u4f3c\u7136\u5ea6\u9a71\u52a8\u5408\u5e76 \u6982\u7387\u9a71\u52a8\u526a\u679d \u8bad\u7ec3\u65b9\u5411 \u4ece\u5b57\u7b26\u5411\u4e0a\u6784\u5efa \u4ece\u5b57\u7b26\u5411\u4e0a\u6784\u5efa \u4ece\u5b8c\u6574\u8bcd\u6c47\u5411\u4e0b\u526a\u679d \u9009\u62e9\u6807\u51c6 \u5b57\u7b26\u5bf9\u9891\u7387 \u8bed\u8a00\u6a21\u578b\u4f3c\u7136\u5ea6 Token\u79fb\u9664\u635f\u5931 \u4ee3\u8868\u6a21\u578b GPT\u7cfb\u5217 BERT\u7cfb\u5217 T5\u3001mT5"},{"location":"fundamentals/transformer/tokenizer/#q2-byte-level-bpe","title":"Q2: \u4e3a\u4ec0\u4e48\u73b0\u4ee3\u5927\u6a21\u578b\u591a\u91c7\u7528Byte-level BPE\uff1f","text":"<p>\u6838\u5fc3\u4f18\u52bf:</p> <ol> <li>\u901a\u7528\u6027: \u652f\u6301\u6240\u6709\u8bed\u8a00\u548c\u5b57\u7b26\uff0c\u65e0\u9700\u7279\u6b8a\u9884\u5904\u7406</li> <li>\u7d27\u51d1\u6027: \u57fa\u7840\u8bcd\u6c47\u8868\u53ea\u9700256\u4e2a\u5b57\u8282</li> <li>\u9c81\u68d2\u6027: \u80fd\u5904\u7406\u4efb\u4f55UTF-8\u7f16\u7801\u7684\u6587\u672c</li> <li>\u4e00\u81f4\u6027: \u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u7684\u5904\u7406\u5b8c\u5168\u4e00\u81f4</li> </ol> <p>\u6280\u672f\u7ec6\u8282: <pre><code># \u4f20\u7edfBPE\u53ef\u80fd\u9047\u5230\u7684\u95ee\u9898\ntext = \"caf\u00e9\"  # \u5305\u542b\u91cd\u97f3\u5b57\u7b26\n# \u53ef\u80fd\u5bfc\u81f4\u7f16\u7801\u4e0d\u4e00\u81f4\u6216OOV\u95ee\u9898\n\n# Byte-level BPE\u7684\u89e3\u51b3\u65b9\u6848\nbytes_representation = text.encode('utf-8')  # [99, 97, 102, 195, 169]\n# \u6bcf\u4e2a\u5b57\u8282\u90fd\u6709\u56fa\u5b9a\u7684\u6620\u5c04\uff0c\u4fdd\u8bc1\u4e00\u81f4\u6027\n</code></pre></p>"},{"location":"fundamentals/transformer/tokenizer/#q3-tokenizer","title":"Q3: Tokenizer\u7684\u8bcd\u6c47\u8868\u5927\u5c0f\u5982\u4f55\u9009\u62e9\uff1f","text":"<p>\u6743\u8861\u8003\u8651:</p> <p>\u8bcd\u6c47\u8868\u8fc7\u5c0f: - \u5e8f\u5217\u53d8\u957f\uff0c\u8ba1\u7b97\u6210\u672c\u589e\u52a0 - \u8bed\u4e49\u4fe1\u606f\u4e22\u5931 - \u957f\u8ddd\u79bb\u4f9d\u8d56\u5efa\u6a21\u56f0\u96be</p> <p>\u8bcd\u6c47\u8868\u8fc7\u5927: - \u5d4c\u5165\u5c42\u53c2\u6570\u6fc0\u589e - \u7a00\u6709token\u8bad\u7ec3\u4e0d\u5145\u5206 - \u63a8\u7406\u65f6\u5185\u5b58\u5360\u7528\u5927</p> <p>\u7ecf\u9a8c\u6cd5\u5219: <pre><code># \u5e38\u89c1\u914d\u7f6e\nmodel_configs = {\n    'GPT-2': {'vocab_size': 50257, 'algorithm': 'Byte-level BPE'},\n    'BERT': {'vocab_size': 30522, 'algorithm': 'WordPiece'},\n    'T5': {'vocab_size': 32128, 'algorithm': 'SentencePiece Unigram'},\n    'LLaMA': {'vocab_size': 32000, 'algorithm': 'SentencePiece BPE'}\n}\n\n# \u9009\u62e9\u539f\u5219\uff1a\n# 1. \u82f1\u6587\uff1a20k-50k\u8f83\u4e3a\u5408\u9002\n# 2. \u591a\u8bed\u8a00\uff1a50k-100k\n# 3. \u4ee3\u7801\uff1a\u7279\u6b8a\u8003\u8651\uff0c\u53ef\u80fd\u9700\u8981\u66f4\u5927\u8bcd\u6c47\u8868\n</code></pre></p>"},{"location":"fundamentals/transformer/tokenizer/#q4-tokenizer","title":"Q4: \u5982\u4f55\u8bc4\u4f30Tokenizer\u7684\u597d\u574f\uff1f","text":"<p>\u8bc4\u4f30\u7ef4\u5ea6:</p> <ol> <li>\u538b\u7f29\u6548\u7387: <code>compression_ratio = original_chars / num_tokens</code></li> <li>\u8bcd\u6c47\u8986\u76d6\u7387: \u6d4b\u8bd5\u96c6\u4e2dUNK token\u7684\u6bd4\u4f8b</li> <li>\u8bed\u4e49\u4fdd\u6301\u5ea6: \u91cd\u8981\u8bcd\u6c47\u662f\u5426\u88ab\u5408\u7406\u5206\u5272</li> <li>fertility: \u5e73\u5747\u6bcf\u4e2a\u8bcd\u88ab\u5206\u6210\u591a\u5c11\u4e2atoken</li> </ol> <pre><code>def evaluate_tokenizer(tokenizer, test_corpus):\n    \"\"\"tokenizer\u8bc4\u4f30\u51fd\u6570\"\"\"\n\n    total_chars = sum(len(text) for text in test_corpus)\n    total_tokens = sum(len(tokenizer.tokenize(text)) for text in test_corpus)\n\n    # \u538b\u7f29\u6bd4\n    compression_ratio = total_chars / total_tokens\n\n    # UNK\u6bd4\u4f8b\n    unk_count = sum(text.count('[UNK]') for text in \n                   [' '.join(tokenizer.tokenize(text)) for text in test_corpus])\n    unk_ratio = unk_count / total_tokens\n\n    # Fertility (\u8bcd\u6c47\u5206\u5272\u5ea6)\n    word_tokens = []\n    for text in test_corpus:\n        words = text.split()\n        for word in words:\n            tokens = tokenizer.tokenize(word)\n            word_tokens.append(len(tokens))\n\n    fertility = sum(word_tokens) / len(word_tokens)\n\n    return {\n        'compression_ratio': compression_ratio,\n        'unk_ratio': unk_ratio,\n        'fertility': fertility\n    }\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#_6","title":"\ud83d\udcbb \u5b8c\u6574\u5b9e\u73b0\u793a\u4f8b","text":"<pre><code># \u73b0\u4ee3Tokenizer\u7684\u5b8c\u6574\u5b9e\u73b0\u793a\u4f8b\nfrom transformers import AutoTokenizer\n\nclass ModernTokenizerExample:\n    \"\"\"\u73b0\u4ee3tokenizer\u4f7f\u7528\u793a\u4f8b\"\"\"\n\n    def __init__(self, model_name=\"gpt2\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def demonstrate_tokenization(self):\n        \"\"\"\u6f14\u793a\u5404\u79cdtokenization\u573a\u666f\"\"\"\n\n        test_cases = [\n            \"Hello, world!\",\n            \"artificial intelligence\",\n            \"\u672a\u6765\u7684\u4eba\u5de5\u667a\u80fd\",\n            \"caf\u00e9 r\u00e9sum\u00e9 na\u00efve\",\n            \"COVID-19 vaccination\",\n            \"user@example.com\",\n            \"print('Hello, World!')\"\n        ]\n\n        print(\"=== Tokenization\u6f14\u793a ===\")\n        for text in test_cases:\n            tokens = self.tokenizer.tokenize(text)\n            token_ids = self.tokenizer.encode(text)\n            decoded = self.tokenizer.decode(token_ids)\n\n            print(f\"\\n\u539f\u6587: {text}\")\n            print(f\"Tokens: {tokens}\")\n            print(f\"Token IDs: {token_ids}\")\n            print(f\"\u89e3\u7801: {decoded}\")\n            print(f\"\u538b\u7f29\u6bd4: {len(text)/len(tokens):.2f}\")\n\n    def analyze_special_tokens(self):\n        \"\"\"\u5206\u6790\u7279\u6b8atoken\u7684\u5904\u7406\"\"\"\n\n        special_cases = [\n            \"Mr. Smith went to the U.S.A.\",  # \u7f29\u5199\n            \"She said, \\\"Hello!\\\"\",          # \u5f15\u53f7\n            \"Visit https://example.com\",      # URL\n            \"Temperature: 25.6\u00b0C\",           # \u7b26\u53f7\u548c\u6570\u5b57\n            \"   extra    spaces   \",         # \u591a\u4f59\u7a7a\u683c\n        ]\n\n        print(\"\\n=== \u7279\u6b8a\u60c5\u51b5\u5904\u7406 ===\")\n        for text in special_cases:\n            tokens = self.tokenizer.tokenize(text)\n            print(f\"'{text}' -&gt; {tokens}\")\n\n    def compare_tokenizers(self):\n        \"\"\"\u5bf9\u6bd4\u4e0d\u540ctokenizer\"\"\"\n\n        models = ['bert-base-uncased', 'gpt2', 'facebook/bart-base']\n        test_text = \"The quick brown fox jumps over the lazy dog.\"\n\n        print(\"\\n=== Tokenizer\u5bf9\u6bd4 ===\")\n        for model_name in models:\n            try:\n                tokenizer = AutoTokenizer.from_pretrained(model_name)\n                tokens = tokenizer.tokenize(test_text)\n\n                print(f\"\\n{model_name}:\")\n                print(f\"Vocab size: {tokenizer.vocab_size}\")\n                print(f\"Tokens: {tokens}\")\n                print(f\"Token count: {len(tokens)}\")\n\n            except Exception as e:\n                print(f\"Error loading {model_name}: {e}\")\n\n# \u8fd0\u884c\u6f14\u793a\nif __name__ == \"__main__\":\n    demo = ModernTokenizerExample()\n    demo.demonstrate_tokenization()\n    demo.analyze_special_tokens()\n    demo.compare_tokenizers()\n</code></pre>"},{"location":"fundamentals/transformer/tokenizer/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c","text":"<ul> <li>[ ] \u7406\u89e3Tokenizer\u7684\u57fa\u672c\u5de5\u4f5c\u6d41\u7a0b</li> <li>[ ] \u638c\u63e1BPE\u3001WordPiece\u3001Unigram\u7684\u6838\u5fc3\u7b97\u6cd5</li> <li>[ ] \u4e86\u89e3Byte-level BPE\u7684\u4f18\u52bf</li> <li>[ ] \u80fd\u5206\u6790\u4e0d\u540ctokenizer\u7684\u9002\u7528\u573a\u666f</li> <li>[ ] \u7406\u89e3tokenizer\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd</li> </ul>"},{"location":"fundamentals/transformer/tokenizer/#_8","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u4e0a\u4e00\u8282\uff1a\u8bed\u8a00\u6a21\u578b\u67b6\u6784</li> <li>\u4e0b\u4e00\u8282\uff1aAttention\u5347\u7ea7\u6280\u672f</li> <li>\u8fd4\u56de\uff1aTransformer\u57fa\u7840\u6982\u89c8</li> </ul>"},{"location":"getting-started/","title":"\ud83c\udfaf LLM\u5b66\u4e60\u6307\u5357","text":"<p>\u6b22\u8fce\u6765\u5230\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u81ea\u5b66\u6307\u5357\uff01\u8fd9\u662f\u4e00\u4e2a\u4e13\u4e3a\u6280\u672f\u9762\u8bd5\u8bbe\u8ba1\u768415\u5929\u7cfb\u7edf\u5316\u5b66\u4e60\u8def\u5f84\u3002</p> <p>\ud83d\udea7 \u5f53\u524d\u8fdb\u5ea6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b8c\u6210 </p> <p>\ud83d\udccd \u5df2\u8986\u76d6\u5185\u5bb9\uff1a\u6a21\u578b\u67b6\u6784\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u524d\u6cbf\u6280\u672f(DeepSeek)\u3001\u5e94\u7528\u5b9e\u6218 \ud83d\udccd \u6b63\u5728\u5f00\u53d1\uff1a\u6570\u636e\u5904\u7406\u3001\u6a21\u578b\u8bad\u7ec3\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u63a8\u7406\u4f18\u5316\u7b49\u6838\u5fc3\u9886\u57df \ud83d\udccd \u5b8c\u6574\u7248\u672c\uff1a\u9884\u8ba135-40\u5929\u5168\u6808LLM\u6280\u672f\u4f53\u7cfb (2025\u5e74Q2)</p>"},{"location":"getting-started/#_1","title":"\ud83d\ude80 \u4e3a\u4ec0\u4e48\u9009\u62e9\u8fd9\u4e2a\u5b66\u4e60\u6307\u5357\uff1f","text":""},{"location":"getting-started/#_2","title":"\u2728 \u9762\u8bd5\u5bfc\u5411\u8bbe\u8ba1","text":"<ul> <li>\ud83c\udfaf \u9ad8\u9891\u8003\u70b9\u8986\u76d6\uff1a\u91cd\u70b9\u5173\u6ce8\u6280\u672f\u9762\u8bd5\u4e2d\u7684\u6838\u5fc3\u95ee\u9898</li> <li>\ud83d\udcdd \u95ee\u7b54\u5f0f\u5b66\u4e60\uff1a\u6bcf\u4e2a\u7ae0\u8282\u90fd\u5305\u542b\u9762\u8bd5\u95ee\u9898\u548c\u6807\u51c6\u7b54\u6848</li> <li>\ud83e\udde0 \u7406\u8bba+\u5b9e\u8df5\uff1a\u65e2\u6709\u539f\u7406\u89e3\u91ca\u53c8\u6709\u4ee3\u7801\u5b9e\u73b0</li> </ul>"},{"location":"getting-started/#_3","title":"\ud83d\udd25 \u6280\u672f\u5185\u5bb9\u5168\u9762","text":"<ul> <li>\u57fa\u7840\u624e\u5b9e\uff1a\u4eceTransformer\u57fa\u7840\u5230\u73b0\u4ee3\u67b6\u6784\u6f14\u8fdb</li> <li>\u524d\u6cbf\u6280\u672f\uff1a\u5305\u542bDeepSeek\u7b49\u6700\u65b0\u6280\u672f\u521b\u65b0</li> <li>\u5de5\u7a0b\u5bfc\u5411\uff1a\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u548c\u6027\u80fd\u4f18\u5316</li> </ul>"},{"location":"getting-started/#_4","title":"\u23f1\ufe0f \u5b66\u4e60\u65f6\u95f4\u5408\u7406","text":"<ul> <li>15\u5929\u89c4\u5212\uff1a\u79d1\u5b66\u5206\u914d\u5b66\u4e60\u65f6\u95f4\uff0c\u5e73\u8861\u6df1\u5ea6\u4e0e\u5e7f\u5ea6</li> <li>\u7075\u6d3b\u5b89\u6392\uff1a\u53ef\u6839\u636e\u4e2a\u4eba\u65f6\u95f4\u8c03\u6574\u5b66\u4e60\u8282\u594f</li> <li>\u68c0\u9a8c\u6807\u51c6\uff1a\u6bcf\u8282\u90fd\u6709\u660e\u786e\u7684\u638c\u63e1\u6807\u51c6</li> </ul>"},{"location":"getting-started/#_5","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84\u5bfc\u822a","text":""},{"location":"getting-started/#105","title":"\ud83c\udfd7\ufe0f \u6a21\u578b\u57fa\u7840 (10.5\u5929)","text":"\u7ae0\u8282 \u6838\u5fc3\u5185\u5bb9 \u65f6\u95f4 \u96be\u5ea6 \u91cd\u8981\u6027 \u7b2c1\u8282 Transformer\u57fa\u7840 Attention + FFN + \u7f16\u7801\u5668-\u89e3\u7801\u5668 + Tokenizer 3\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25\ud83d\udd25 \u7b2c2\u8282 Attention\u5347\u7ea7 MHA\u2192MQA\u2192GQA\u2192MLA + KV Cache + RoPE 3\u5929 \u2b50\u2b50\u2b50 \ud83d\udd25\ud83d\udd25\ud83d\udd25 \u7b2c3\u8282 LLM\u5347\u7ea7\u6280\u672f MOE\u67b6\u6784 + \u5206\u5e03\u5f0f\u8bad\u7ec3 1.5\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25 \u7b2c4\u8282 DeepSeek\u6280\u672f MLA + DeepSeek MoE + MTP 3\u5929 \u2b50\u2b50\u2b50\u2b50 \ud83d\udd25\ud83d\udd25\ud83d\udd25\ud83d\udd25"},{"location":"getting-started/#45","title":"\ud83d\udee0\ufe0f \u5e94\u7528\u5b9e\u6218 (4.5\u5929)","text":"\u7ae0\u8282 \u6838\u5fc3\u5185\u5bb9 \u65f6\u95f4 \u96be\u5ea6 \u91cd\u8981\u6027 \u7b2c5\u8282 Context Engineering \u4e0a\u4e0b\u6587\u5de5\u7a0b + \u63d0\u793a\u8bcd\u8bbe\u8ba1 1.5\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25\ud83d\udd25 \u7b2c6\u8282 RAG\u4e0eAgent RAG\u6280\u672f + AI Agent\u6846\u67b6 1.5\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25 \u7b2c7\u8282 CoT\u4e0e\u8bc4\u6d4b \u601d\u7ef4\u94fe + LangChain + \u6a21\u578b\u8bc4\u6d4b 1.5\u5929 \u2b50\u2b50 \ud83d\udd25\ud83d\udd25"},{"location":"getting-started/#_6","title":"\ud83c\udfaf \u5b66\u4e60\u7b56\u7565\u5efa\u8bae","text":""},{"location":"getting-started/#7","title":"\ud83d\udcd6 \u7b2c\u4e00\u9636\u6bb5\uff1a\u57fa\u7840\u7406\u8bba (\u524d7\u5929)","text":"<ol> <li>\u91cd\u70b9\u638c\u63e1\uff1a\u7b2c1-2\u8282\u7684\u6838\u5fc3\u6982\u5ff5</li> <li>\u5b66\u4e60\u65b9\u6cd5\uff1a\u5148\u7406\u8bba\u540e\u4ee3\u7801\uff0c\u591a\u505a\u7b14\u8bb0</li> <li>\u65f6\u95f4\u5206\u914d\uff1a\u7406\u8bba\u7406\u89e360% + \u4ee3\u7801\u5b9e\u8df540%</li> </ol>"},{"location":"getting-started/#8-11","title":"\ud83d\udd2c \u7b2c\u4e8c\u9636\u6bb5\uff1a\u524d\u6cbf\u6280\u672f (\u7b2c8-11\u5929)","text":"<ol> <li>\u91cd\u70b9\u638c\u63e1\uff1a\u7b2c4\u8282DeepSeek\u6280\u672f\uff08\u9762\u8bd5\u70ed\u70b9\uff09</li> <li>\u5b66\u4e60\u65b9\u6cd5\uff1a\u5bf9\u6bd4\u5b66\u4e60\uff0c\u7406\u89e3\u6280\u672f\u6f14\u8fdb\u903b\u8f91</li> <li>\u65f6\u95f4\u5206\u914d\uff1a\u6280\u672f\u539f\u740670% + \u5e94\u7528\u573a\u666f30%</li> </ol>"},{"location":"getting-started/#12-15","title":"\ud83d\udee0\ufe0f \u7b2c\u4e09\u9636\u6bb5\uff1a\u5e94\u7528\u5b9e\u6218 (\u7b2c12-15\u5929)","text":"<ol> <li>\u91cd\u70b9\u638c\u63e1\uff1a\u7b2c5\u8282\u4e0a\u4e0b\u6587\u5de5\u7a0b\uff08\u5b9e\u7528\u6027\u5f3a\uff09</li> <li>\u5b66\u4e60\u65b9\u6cd5\uff1a\u5b9e\u8df5\u5bfc\u5411\uff0c\u591a\u505a\u9879\u76ee\u7ec3\u4e60</li> <li>\u65f6\u95f4\u5206\u914d\uff1a\u6982\u5ff5\u7406\u89e340% + \u5b9e\u9645\u5e94\u752860%</li> </ol>"},{"location":"getting-started/#_7","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":""},{"location":"getting-started/#_8","title":"\ud83e\udde0 \u7406\u8bba\u638c\u63e1","text":"<ul> <li>[ ] \u80fd\u7528\u81ea\u5df1\u7684\u8bdd\u89e3\u91ca\u6bcf\u4e2a\u6280\u672f\u7684\u6838\u5fc3\u539f\u7406</li> <li>[ ] \u80fd\u753b\u51faTransformer\u5b8c\u6574\u67b6\u6784\u56fe</li> <li>[ ] \u80fd\u8bf4\u660e\u4e0d\u540c\u6280\u672f\u7684\u4f18\u7f3a\u70b9\u548c\u9002\u7528\u573a\u666f</li> </ul>"},{"location":"getting-started/#_9","title":"\ud83d\udcbb \u4ee3\u7801\u5b9e\u8df5","text":"<ul> <li>[ ] \u80fd\u72ec\u7acb\u5b9e\u73b0Self-Attention\u673a\u5236</li> <li>[ ] \u80fd\u5b8c\u6210KV Cache\u4f18\u5316\u4ee3\u7801</li> <li>[ ] \u80fd\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\u6a21\u677f</li> </ul>"},{"location":"getting-started/#_10","title":"\ud83c\udfa4 \u9762\u8bd5\u51c6\u5907","text":"<ul> <li>[ ] \u80fd\u56de\u7b54\u6240\u6709\u7ae0\u8282\u7684\u6838\u5fc3\u9762\u8bd5\u95ee\u9898</li> <li>[ ] \u80fd\u89e3\u91ca\u6280\u672f\u9009\u62e9\u80cc\u540e\u7684\u5de5\u7a0b\u8003\u91cf</li> <li>[ ] \u80fd\u63cf\u8ff0\u5177\u4f53\u7684\u9879\u76ee\u5e94\u7528\u7ecf\u9a8c</li> </ul>"},{"location":"getting-started/#_11","title":"\ud83d\udca1 \u5b66\u4e60\u5efa\u8bae","text":""},{"location":"getting-started/#_12","title":"\ud83d\udd0d \u6df1\u5ea6\u4f18\u5148","text":"<ul> <li>\u91cd\u70b9\u6280\u672f\u6df1\u5165\u7406\u89e3\uff1aAttention\u673a\u5236\u3001DeepSeek\u521b\u65b0</li> <li>\u5173\u952e\u6982\u5ff5\u53cd\u590d\u7ec3\u4e60\uff1a\u901a\u8fc7\u4ee3\u7801\u52a0\u6df1\u7406\u89e3</li> <li>\u9762\u8bd5\u95ee\u9898\u591a\u6b21\u6f14\u7ec3\uff1a\u786e\u4fdd\u8868\u8fbe\u6e05\u6670\u6d41\u7545</li> </ul>"},{"location":"getting-started/#_13","title":"\ud83d\udd17 \u5173\u8054\u5b66\u4e60","text":"<ul> <li>\u6280\u672f\u6f14\u8fdb\u8109\u7edc\uff1a\u7406\u89e3\u4eceMHA\u5230MLA\u7684\u53d1\u5c55\u903b\u8f91</li> <li>\u5bf9\u6bd4\u5206\u6790\u65b9\u6cd5\uff1a\u901a\u8fc7\u5bf9\u6bd4\u52a0\u6df1\u4e0d\u540c\u6280\u672f\u7684\u7406\u89e3</li> <li>\u5b9e\u9645\u5e94\u7528\u601d\u8003\uff1a\u6bcf\u4e2a\u6280\u672f\u90fd\u8981\u601d\u8003\u5e94\u7528\u573a\u666f</li> </ul>"},{"location":"getting-started/#_14","title":"\u26a1 \u9ad8\u6548\u5b66\u4e60","text":"<ul> <li>\u65f6\u95f4\u7ba1\u7406\uff1a\u8bbe\u5b9a\u660e\u786e\u7684\u6bcf\u65e5\u5b66\u4e60\u76ee\u6807</li> <li>\u7b14\u8bb0\u6574\u7406\uff1a\u5efa\u7acb\u4e2a\u4eba\u7684\u77e5\u8bc6\u4f53\u7cfb</li> <li>\u5b9a\u671f\u590d\u4e60\uff1a\u5de9\u56fa\u5df2\u5b66\u5185\u5bb9\uff0c\u907f\u514d\u9057\u5fd8</li> </ul>"},{"location":"getting-started/#llm_1","title":"\ud83c\udf89 \u5f00\u59cb\u4f60\u7684LLM\u5b66\u4e60\u4e4b\u65c5","text":"<p>\u9009\u62e9\u4e00\u4e2a\u611f\u5174\u8da3\u7684\u7ae0\u8282\u5f00\u59cb\u5b66\u4e60\u5427\uff01\u5efa\u8bae\u4ece\u7b2c1\u8282 Transformer\u57fa\u7840\u5f00\u59cb\uff0c\u5faa\u5e8f\u6e10\u8fdb\u5730\u638c\u63e1\u6574\u4e2a\u77e5\u8bc6\u4f53\u7cfb\u3002</p> <p>\u8bb0\u4f4f\uff1a\u771f\u77e5\u8bc6\u6765\u81ea\u4e8e\u7406\u89e3\u539f\u7406\uff0c\u800c\u4e0d\u662f\u6b7b\u8bb0\u786c\u80cc\u3002 </p>"},{"location":"getting-started/#_15","title":"\ud83d\udd2e \u5373\u5c06\u63a8\u51fa\u7684\u5185\u5bb9","text":""},{"location":"getting-started/#7-9","title":"\ud83d\udcc8 \u7b2c\u4e8c\u9636\u6bb5\uff1a\u6570\u636e\u4e0e\u8bad\u7ec3 (\u9884\u8ba17-9\u5929)","text":"<ul> <li>\u6570\u636e\u5de5\u7a0b\uff1a\u6536\u96c6\u3001\u6e05\u6d17\u3001\u9884\u5904\u7406\u3001\u8d28\u91cf\u63a7\u5236</li> <li>\u9884\u8bad\u7ec3\u6280\u672f\uff1a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u3001\u7a33\u5b9a\u6027\u4f18\u5316</li> <li>\u8bad\u7ec3\u76d1\u63a7\uff1aLoss\u5206\u6790\u3001\u8c03\u8bd5\u6280\u5de7\u3001\u6548\u7387\u4f18\u5316</li> </ul>"},{"location":"getting-started/#6-8","title":"\ud83c\udfaf \u7b2c\u4e09\u9636\u6bb5\uff1a\u5bf9\u9f50\u4e0e\u4f18\u5316 (\u9884\u8ba16-8\u5929)","text":"<ul> <li>\u5f3a\u5316\u5b66\u4e60\uff1aRLHF\u3001DPO\u3001Constitutional AI</li> <li>\u5b89\u5168\u5bf9\u9f50\uff1a\u4ef7\u503c\u5bf9\u9f50\u3001\u6709\u5bb3\u5185\u5bb9\u8fc7\u6ee4\u3001\u504f\u89c1\u7f13\u89e3</li> <li>\u63a8\u7406\u4f18\u5316\uff1a\u91cf\u5316\u3001\u526a\u679d\u3001\u63a8\u7406\u52a0\u901f\u3001\u90e8\u7f72\u7b56\u7565</li> </ul>"},{"location":"getting-started/#5-7","title":"\ud83c\udf0d \u7b2c\u56db\u9636\u6bb5\uff1a\u8fdb\u9636\u5e94\u7528 (\u9884\u8ba15-7\u5929)","text":"<ul> <li>\u591a\u6a21\u6001\u6280\u672f\uff1aVision-Language\u3001\u97f3\u9891\u5904\u7406</li> <li>\u4e13\u4e1a\u9886\u57df\uff1a\u4ee3\u7801\u751f\u6210\u3001\u79d1\u5b66\u8ba1\u7b97\u3001\u533b\u7597\u5e94\u7528  </li> <li>\u5de5\u7a0b\u5316\uff1aMLOps\u3001\u4ea7\u54c1\u5316\u3001\u5546\u4e1a\u5316</li> </ul>"},{"location":"getting-started/#5-6","title":"\ud83d\udcca \u7b2c\u4e94\u9636\u6bb5\uff1a\u8bc4\u6d4b\u4e0e\u5206\u6790 (\u9884\u8ba15-6\u5929)","text":"<ul> <li>\u8bc4\u6d4b\u4f53\u7cfb\uff1a\u57fa\u51c6\u6d4b\u8bd5\u3001\u80fd\u529b\u5206\u6790\u3001\u5931\u6548\u6a21\u5f0f</li> <li>\u53ef\u89e3\u91ca\u6027\uff1a\u6a21\u578b\u5206\u6790\u3001\u51b3\u7b56\u900f\u660e\u5ea6</li> <li>\u7814\u7a76\u524d\u6cbf\uff1a\u6700\u65b0\u8bba\u6587\u89e3\u8bfb\u3001\u6280\u672f\u8d8b\u52bf</li> </ul> <p>\u73b0\u5728\u5c31\u5f00\u59cb\u5b66\u4e60\u5427\uff01 \u5f53\u524d\u768415\u5929\u5185\u5bb9\u5df2\u7ecf\u8db3\u591f\u5e94\u5bf9\u5927\u90e8\u5206LLM\u6280\u672f\u9762\u8bd5\u3002</p> <p>\u795d\u4f60\u5b66\u4e60\u987a\u5229\uff0c\u9762\u8bd5\u6210\u529f\uff01\ud83c\udfaf</p>"},{"location":"interview/","title":"\u9762\u8bd5\u9898\u5e93","text":""},{"location":"interview/#_2","title":"\ud83c\udfaf \u76ee\u6807","text":"<p>\u6c47\u603bLLM\u76f8\u5173\u7684\u9ad8\u9891\u9762\u8bd5\u95ee\u9898\uff0c\u5e2e\u52a9\u5feb\u901f\u590d\u4e60\u548c\u51c6\u5907\u3002</p>"},{"location":"interview/#_3","title":"\ud83d\udcda \u6309\u4e3b\u9898\u5206\u7c7b","text":""},{"location":"interview/#_4","title":"\u6a21\u578b\u57fa\u7840","text":"<ul> <li>Attention\u8ba1\u7b97\u516c\u5f0f\u548c\u539f\u7406</li> <li>Encoder-Only vs Decoder-Only\u67b6\u6784</li> <li>MHA/MQA/GQA/MLA\u7684\u533a\u522b</li> <li>KV Cache\u5de5\u4f5c\u539f\u7406</li> <li>RoPE\u4f4d\u7f6e\u7f16\u7801\u63a8\u5bfc</li> </ul>"},{"location":"interview/#_5","title":"\u6a21\u578b\u5e94\u7528","text":"<ul> <li>RAG\u7684\u5de5\u4f5c\u6d41\u7a0b</li> <li>Agent\u9762\u8bd5\u516b\u80a1\u6587 - \u5168\u9762\u8986\u76d6Agent\u76f8\u5173\u9762\u8bd5\u9898</li> <li>CoT\u63d0\u5347\u63a8\u7406\u7684\u539f\u7406</li> <li>\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7684\u6838\u5fc3\u6280\u5de7</li> </ul>"},{"location":"interview/#_6","title":"\u5de5\u7a0b\u5b9e\u8df5","text":"<ul> <li>\u6a21\u578b\u90e8\u7f72\u4f18\u5316</li> <li>\u63a8\u7406\u52a0\u901f\u6280\u672f</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3\u7b56\u7565</li> <li>\u6a21\u578b\u8bc4\u6d4b\u65b9\u6cd5</li> </ul>"},{"location":"interview/#top-10","title":"\ud83d\udd25 \u9ad8\u9891\u95ee\u9898 Top 10","text":"<ol> <li>Attention\u673a\u5236\u7684\u6570\u5b66\u516c\u5f0f\u662f\u4ec0\u4e48\uff1f</li> <li>\u4e3a\u4ec0\u4e48\u8981\u9664\u4ee5\u221ad_k\uff1f</li> <li>GPT\u548cBERT\u7684\u67b6\u6784\u533a\u522b\uff1f</li> <li>KV Cache\u5982\u4f55\u52a0\u901f\u63a8\u7406\uff1f</li> <li>\u4ec0\u4e48\u662fRoPE\uff1f\u5982\u4f55\u63a8\u5bfc\uff1f</li> <li>MQA\u76f8\u6bd4MHA\u7684\u4f18\u52bf\uff1f</li> <li>Pre-Norm vs Post-Norm\uff1f</li> <li>RAG\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1f</li> <li>Agent\u548c\u666e\u901aLLM\u7684\u533a\u522b\uff1f</li> <li>\u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u63d0\u793a\u8bcd\uff1f</li> </ol>"},{"location":"interview/#_7","title":"\ud83d\udca1 \u7b54\u9898\u6280\u5de7","text":"<ol> <li>\u5148\u8bf4\u6838\u5fc3\uff0c\u518d\u5c55\u5f00\u7ec6\u8282</li> <li>\u7ed3\u5408\u6570\u5b66\u516c\u5f0f\u548c\u4ee3\u7801\u5b9e\u73b0</li> <li>\u63d0\u53ca\u5b9e\u9645\u5e94\u7528\u548c\u5de5\u7a0b\u8003\u8651</li> <li>\u5bf9\u6bd4\u4e0d\u540c\u65b9\u6848\u7684\u4f18\u52a3</li> </ol> <p>\u6bcf\u4e2a\u95ee\u9898\u90fd\u5728\u5bf9\u5e94\u7ae0\u8282\u6709\u8be6\u7ec6\u89e3\u7b54\uff0c\u5efa\u8bae\u7ed3\u5408\u5b66\u4e60\uff01</p>"},{"location":"interview/agent-interview/","title":"Agent\u9762\u8bd5\u516b\u80a1\u6587","text":""},{"location":"interview/agent-interview/#_1","title":"\ud83c\udfaf \u76ee\u6807","text":"<p>\u603b\u7ed3\u5927\u6a21\u578bAgent\u76f8\u5173\u7684\u9762\u8bd5\u95ee\u9898\u548c\u7b54\u6848\uff0c\u5e2e\u52a9\u5feb\u901f\u590d\u4e60\u6838\u5fc3\u6982\u5ff5\u3002</p>"},{"location":"interview/agent-interview/#agent_1","title":"\ud83d\udcda Agent\u57fa\u7840\u6982\u5ff5\u7bc7","text":""},{"location":"interview/agent-interview/#q1-agentai","title":"Q1. \u4ec0\u4e48\u662f\u5927\u6a21\u578bAgent\uff1f\u5b83\u4e0e\u4f20\u7edf\u7684AI\u7cfb\u7edf\u6709\u4ec0\u4e48\u4e0d\u540c\uff1f","text":"<p>\u6838\u5fc3\u5b9a\u4e49: \u5927\u6a21\u578bAgent\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\uff0c\u80fd\u591f\u81ea\u4e3b\u89e3\u6790\u4efb\u52a1\u3001\u8c03\u7528\u5de5\u5177\u3001\u6267\u884c\u63a8\u7406\uff0c\u5e76\u4e0e\u73af\u5883\u4ea4\u4e92\u3002</p> <p>\u4e3b\u8981\u7279\u70b9: - \u57fa\u4e8eLLM\u7684\u51b3\u7b56: \u5229\u7528\u5927\u6a21\u578b\u7684\u81ea\u56de\u5f52\u751f\u6210\u80fd\u529b\u8fdb\u884c\u63a8\u7406\uff0c\u800c\u975e\u4f20\u7edf\u7684\u624b\u5de5\u7f16\u5199\u89c4\u5219\u6216\u5f3a\u5316\u5b66\u4e60\u7b56\u7565 - \u52a8\u6001\u5de5\u5177\u8c03\u7528: \u53ef\u4ee5\u6839\u636e\u4efb\u52a1\u9700\u8981\u8c03\u7528API\u3001\u6570\u636e\u5e93\u3001\u641c\u7d22\u5f15\u64ce\u6216\u5916\u90e8\u8ba1\u7b97\u5de5\u5177 - \u4e0a\u4e0b\u6587\u8bb0\u5fc6: \u901a\u8fc7\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u6216\u5916\u90e8\u5b58\u50a8\uff08\u5982RAG\u3001\u5411\u91cf\u6570\u636e\u5e93\uff09\u7ef4\u62a4\u957f\u671f\u8bb0\u5fc6 - \u53ef\u6269\u5c55\u6027: \u53ef\u4ee5\u65e0\u7f1d\u9002\u914d\u4e0d\u540c\u4efb\u52a1\uff0c\u800c\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3</p> <p>\u4e0e\u4f20\u7edfAI\u7684\u533a\u522b: - \u4f20\u7edfAI: \u4f9d\u8d56\u56fa\u5b9a\u7684\u89c4\u5219\u6216\u6a21\u578b\uff08\u5982\u5206\u7c7b\u5668\u3001\u77e5\u8bc6\u56fe\u8c31\uff09\uff0c\u9002\u7528\u4e8e\u7279\u5b9a\u4efb\u52a1\uff0c\u4f46\u6cdb\u5316\u80fd\u529b\u8f83\u5f31 - LLM Agent: \u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u5b9e\u73b0\u901a\u7528\u4efb\u52a1\u5904\u7406\uff0c\u5e76\u80fd\u52a8\u6001\u8c03\u7528\u5de5\u5177\u89e3\u51b3\u590d\u6742\u95ee\u9898</p>"},{"location":"interview/agent-interview/#q2-llm-agent","title":"Q2. LLM Agent\u7684\u57fa\u672c\u67b6\u6784\u6709\u54ea\u4e9b\u7ec4\u6210\u90e8\u5206\uff1f","text":"<p>\u5178\u578b\u67b6\u6784\u5305\u62ec:</p> <ol> <li> <p>\u4efb\u52a1\u89e3\u6790\u6a21\u5757\uff08Task Parser\uff09: \u901a\u8fc7LLM\u89e3\u6790\u8f93\u5165\u7684\u4efb\u52a1\u6216\u7528\u6237\u6307\u4ee4\uff0c\u8bc6\u522b\u76ee\u6807\u548c\u6f5c\u5728\u5b50\u4efb\u52a1</p> </li> <li> <p>\u8ba1\u5212\u4e0e\u63a8\u7406\u6a21\u5757\uff08Planning &amp; Reasoning\uff09:</p> </li> <li>\u91c7\u7528\u57fa\u4e8eChain-of-Thought\uff08CoT\uff09</li> <li> <p>\u6216ReAct\uff08Reason + Act\uff09\u7b49\u6280\u672f\u8fdb\u884c\u591a\u6b65\u63a8\u7406\uff0c\u786e\u4fdd\u4efb\u52a1\u6267\u884c\u7684\u5408\u7406\u6027</p> </li> <li> <p>\u5de5\u5177\u8c03\u7528\uff08Tool Use / API Calling\uff09: \u901a\u8fc7\u63d2\u4ef6\u673a\u5236\u6216API\uff0c\u8c03\u7528\u641c\u7d22\u5f15\u64ce\u3001\u6570\u636e\u5e93\u3001\u4ee3\u7801\u6267\u884c\u73af\u5883\u3001\u8ba1\u7b97\u5f15\u64ce</p> </li> <li> <p>\u8bb0\u5fc6\u7ba1\u7406\uff08Memory &amp; Retrieval\uff09: \u7ef4\u62a4\u77ed\u671f\u8bb0\u5fc6\uff08Session Context\uff09\u548c\u957f\u671f\u8bb0\u5fc6\uff08\u5411\u91cf\u6570\u636e\u5e93\u3001\u77e5\u8bc6\u5e93\uff09\u4ee5\u652f\u6301\u8fde\u7eed\u5bf9\u8bdd\u6216\u957f\u671f\u4efb\u52a1</p> </li> <li> <p>\u6267\u884c\u53cd\u9988\uff08Execution &amp; Feedback\uff09: \u89c2\u5bdf\u6267\u884c\u7ed3\u679c\uff0c\u8fdb\u884c\u81ea\u6211\u7ea0\u9519\uff08Self-Refinement\uff09\u6216\u5143\u63a8\u7406\uff08Meta-Reasoning\uff09\u4ee5\u4f18\u5316\u4efb\u52a1\u6267\u884c\u6d41\u7a0b</p> </li> </ol>"},{"location":"interview/agent-interview/#q3-llm-agent","title":"Q3. LLM Agent\u5982\u4f55\u8fdb\u884c\u51b3\u7b56\uff1f","text":"<p>\u4e3b\u8981\u51b3\u7b56\u65b9\u6cd5:</p> <ol> <li>\u57fa\u4e8eChain-of-Thought\uff08CoT\uff09\u63a8\u7406:</li> <li>\u901a\u8fc7\u663e\u5f0f\u7684\u9010\u6b65\u63a8\u7406\uff0c\u4f7f\u6a21\u578b\u5728\u751f\u6210\u7b54\u6848\u524d\u5148\u5c55\u5f00\u63a8\u7406\u6b65\u9aa4</li> <li> <p>\u4f8b\u5982\uff1a\u7528\u6237\u95ee\u67d0\u57ce\u5e02GDP\u662f\u5426\u6bd4\u5168\u56fd\u5e73\u5747\u503c\u9ad8 \u2192 Agent\uff08CoT\uff09\uff1a\u9996\u5148\u83b7\u53d6\u8be5\u57ce\u5e02\u7684GDP\u6570\u636e \u2192 \u83b7\u53d6\u5168\u56fdGDP\u5e73\u5747\u503c \u2192 \u8fdb\u884c\u6bd4\u8f83 \u2192 \u751f\u6210\u7b54\u6848</p> </li> <li> <p>\u57fa\u4e8eReAct\uff08Reasoning + Acting\uff09\u6846\u67b6:</p> </li> <li>\u7ed3\u5408\u903b\u8f91\u63a8\u7406\u4e0e\u884c\u52a8\u6267\u884c\uff08\u5982API\u67e5\u8be2\u3001\u6570\u636e\u5e93\u68c0\u7d22\uff09\uff0c\u907f\u514d\u6a21\u578b\u76f4\u63a5\"\u80e1\u7f16\"\u7b54\u6848</li> <li> <p>\u4f8b\u5982\uff1a\u67e5\u8be2\u67d0\u516c\u53f82023\u5e74\u8d22\u62a5\u6570\u636e \u2192 \u601d\u8003\uff1a\"\u6211\u9700\u8981\u627e\u5230\u8be5\u516c\u53f8\u7684\u8d22\u62a5\u7f51\u7ad9\" \u2192 \u884c\u52a8\uff1a\"\u8c03\u7528Google\u641c\u7d22API\" \u2192 \u89c2\u5bdf\uff1a\"\u627e\u5230\u4e86SEC\u5907\u6848\u6570\u636e\" \u2192 \u751f\u6210\u6700\u7ec8\u7b54\u6848</p> </li> <li> <p>\u57fa\u4e8eSelf-Reflection / Self-Correction:</p> </li> <li>Agent\u751f\u6210\u521d\u6b65\u7b54\u6848\u540e\uff0c\u53ef\u56de\u987e\u81ea\u5df1\u7684\u63a8\u7406\u8fc7\u7a0b\u5e76\u8fdb\u884c\u4fee\u6b63</li> </ol>"},{"location":"interview/agent-interview/#q4-llm-agent","title":"Q4. \u5982\u4f55\u8ba9LLM Agent\u5177\u5907\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\uff1f","text":"<p>\u4e3b\u8981\u65b9\u5f0f:</p> <ol> <li>\u5411\u91cf\u6570\u636e\u5e93\uff08Vector Database\uff09+ RAG:</li> <li>\u5c06\u5386\u53f2\u5bf9\u8bdd\u6216\u77e5\u8bc6\u5b58\u5165\u5411\u91cf\u6570\u636e\u5e93\uff08\u5982FAISS\u3001ChromaDB\uff09</li> <li> <p>\u5728\u4ea4\u4e92\u65f6\u68c0\u7d22\u76f8\u5173\u5185\u5bb9\uff0c\u5408\u5e76\u8fdbLLM\u7684\u8f93\u5165\u4e0a\u4e0b\u6587</p> </li> <li> <p>Memory Transformer / Hierarchical Memory:</p> </li> <li>\u77ed\u671f\u8bb0\u5fc6\uff08Session Context\uff09\uff1a\u4fdd\u7559\u6700\u8fd1\u7684\u5bf9\u8bdd\u5185\u5bb9</li> <li> <p>\u957f\u671f\u8bb0\u5fc6\uff08Long-Term Embeddings\uff09\uff1a\u91cd\u8981\u4fe1\u606f\u5b58\u5165\u5916\u90e8\u5b58\u50a8\uff0c\u5e76\u5728\u5fc5\u8981\u65f6\u53ec\u56de</p> </li> <li> <p>Fine-tuning + Knowledge Distillation:</p> </li> <li>\u9884\u8bad\u7ec3LLM\u4f7f\u5176\u638c\u63e1\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\uff0c\u63d0\u9ad8\u5728\u8be5\u9886\u57df\u7684\u56de\u7b54\u51c6\u786e\u6027</li> </ol>"},{"location":"interview/agent-interview/#q5-llm-agentapi","title":"Q5. LLM Agent\u5982\u4f55\u8fdb\u884c\u52a8\u6001API\u8c03\u7528\uff1f","text":"<p>\u5e38\u7528\u65b9\u5f0f:</p> <ol> <li> <p>\u63d2\u4ef6\u673a\u5236\uff08Plugins\uff09: OpenAI Plugin\u3001LangChain Agents\u5141\u8bb8LLM\u76f4\u63a5\u8c03\u7528API</p> </li> <li> <p>\u52a8\u6001\u51fd\u6570\u8c03\u7528\uff08Function Calling\uff09: \u901a\u8fc7OpenAI GPT-4 Turbo\u7684function-calling\u673a\u5236\uff0c\u81ea\u52a8\u89e3\u6790JSON\u7ed3\u6784\u5e76\u8c03\u7528\u76f8\u5e94API\uff1a    <pre><code>{\n  \"name\": \"search_stock_price\",\n  \"parameters\": {\n    \"ticker\": \"AAPL\"\n  }\n}\n</code></pre></p> </li> <li> <p>\u4ee3\u7801\u89e3\u91ca\u5668\uff08Code Interpreter\uff09: \u901a\u8fc7Python\u8fd0\u884c\u73af\u5883\u6267\u884c\u8ba1\u7b97\u3001\u6570\u636e\u5904\u7406\u7b49\u4efb\u52a1</p> </li> </ol>"},{"location":"interview/agent-interview/#agent_2","title":"\ud83d\udcda \u4e3b\u6d41Agent\u6846\u67b6\u7bc7","text":""},{"location":"interview/agent-interview/#q1-llm-agent","title":"Q1. \u5e02\u9762\u4e0a\u6709\u54ea\u4e9b\u4e3b\u6d41\u7684LLM Agent\u6846\u67b6\uff1f","text":"<p>\u4e3b\u8981\u6846\u67b6\u5bf9\u6bd4:</p> \u6846\u67b6 \u4e3b\u8981\u7279\u70b9 \u9002\u7528\u573a\u666f LangChain \u94fe\u5f0f\u8c03\u7528\u3001\u5de5\u5177\u6574\u5408\u3001\u5185\u5b58\u7ba1\u7406\u3001\u4ee3\u7406\u673a\u5236 \u901a\u7528Agent\u5f00\u53d1 LlamaIndex \u6570\u636e\u7d22\u5f15\u3001\u67e5\u8be2\u8def\u7531\u3001\u5411\u91cf\u5b58\u50a8\u96c6\u6210 RAG\u589e\u5f3a\u5e94\u7528 AutoGPT \u81ea\u4e3b\u6027\u3001\u957f\u8bb0\u5fc6\u3001\u591a\u5de5\u5177\u8c03\u7528 \u81ea\u4e3b\u4efb\u52a1\u6267\u884c BabyAGI \u57fa\u4e8eOpenAI + Pinecone\u7684\u4efb\u52a1\u961f\u5217\u63a7\u5236 \u6700\u5c0f\u5316\u81ea\u4e3bAgent CrewAI \u591a\u667a\u80fd\u4f53\u67b6\u6784\u3001\u89d2\u8272\u5206\u5de5\u3001LangChain\u517c\u5bb9 \u56e2\u961f\u534f\u4f5c\u4efb\u52a1 LangGraph \u57fa\u4e8eDAG\u7684\u5de5\u4f5c\u6d41\u7ba1\u7406\u3001\u72b6\u6001\u7ba1\u7406 \u590d\u6742\u4efb\u52a1\u7f16\u6392"},{"location":"interview/agent-interview/#q2-langchain","title":"Q2. LangChain\u7684\u6838\u5fc3\u7ec4\u4ef6\u6709\u54ea\u4e9b\uff1f","text":"<p>\u4e3b\u8981\u7ec4\u4ef6:</p> <ol> <li>Models\uff08\u6a21\u578b\uff09: \u9002\u914dOpenAI\u3001Anthropic\u3001Mistral\u3001Llama\u53ca\u672c\u5730LLM</li> <li>Prompt Templates\uff08\u63d0\u793a\u8bcd\u6a21\u677f\uff09: \u5141\u8bb8\u7528\u6237\u521b\u5efa\u52a8\u6001\u63d0\u793a\u8bcd\uff0c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b</li> <li>Memory\uff08\u8bb0\u5fc6\uff09:</li> <li>\u77ed\u671f\u8bb0\u5fc6\uff1a\u5b58\u50a8\u5bf9\u8bdd\u4e0a\u4e0b\u6587</li> <li>\u957f\u671f\u8bb0\u5fc6\uff1a\u7ed3\u5408\u5411\u91cf\u6570\u636e\u5e93\u6301\u4e45\u5316\u5b58\u50a8</li> <li>Chains\uff08\u94fe\u5f0f\u8c03\u7528\uff09:</li> <li>Simple Chains\uff1a\u5355\u6b65\u4efb\u52a1</li> <li>Sequential Chains\uff1a\u4e32\u8054\u591a\u4e2a\u6b65\u9aa4</li> <li>Agents\uff08\u667a\u80fd\u4f53\uff09: \u901a\u8fc7ReAct\u6846\u67b6\uff0cAgent\u9009\u62e9\u5408\u9002\u7684\u5de5\u5177\u5b8c\u6210\u4efb\u52a1</li> <li>Tools\uff08\u5de5\u5177\uff09: \u8bbf\u95eeAPI\u3001Google\u641c\u7d22\u3001SQL\u6570\u636e\u5e93\u7b49</li> </ol>"},{"location":"interview/agent-interview/#q3-langchain-agent","title":"Q3. LangChain Agent\u7684\u4e3b\u8981\u7c7b\u578b\u6709\u54ea\u4e9b\uff1f","text":"<p>Agent\u7c7b\u578b:</p> <ol> <li>Zero-shot ReAct Agent: LLM\u76f4\u63a5\u51b3\u5b9a\u5de5\u5177\u8c03\u7528\uff0c\u4e0d\u4f7f\u7528\u989d\u5916\u63d0\u793a\u4fe1\u606f</li> <li>Conversational ReAct Agent: \u7ed3\u5408\u4f1a\u8bdd\u8bb0\u5fc6\uff0c\u4f7fAgent\u4fdd\u6301\u4e0a\u4e0b\u6587</li> <li>Structured Chat Agent: \u9002\u7528\u4e8e\u7ed3\u6784\u5316\u5bf9\u8bdd\uff0c\u5982\u8868\u5355\u586b\u5145</li> <li>Self-Reflective Agent: \u5177\u5907\u81ea\u6211\u53cd\u9988\u673a\u5236\uff0c\u53ef\u4fee\u6b63\u9519\u8bef\u56de\u7b54</li> </ol>"},{"location":"interview/agent-interview/#agent_3","title":"\ud83d\udcda Agent\u5c40\u9650\u6027\u4e0e\u4f18\u5316\u7bc7","text":""},{"location":"interview/agent-interview/#q1-llm-agent_1","title":"Q1. LLM Agent\u4e3b\u8981\u6709\u54ea\u4e9b\u5c40\u9650\u6027\uff1f","text":"<p>\u4e3b\u8981\u5c40\u9650:</p> <ul> <li>\u5e7b\u89c9\u95ee\u9898\uff08Hallucination\uff09: \u6a21\u578b\u53ef\u80fd\u751f\u6210\u865a\u5047\u4fe1\u606f</li> <li>\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236: \u65e0\u6cd5\u957f\u671f\u8bb0\u5fc6\u5927\u91cf\u5386\u53f2\u4fe1\u606f</li> <li>\u8ba1\u7b97\u6210\u672c\u9ad8: \u63a8\u7406\u6d88\u8017\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90</li> <li>\u7f3a\u4e4f\u5b9e\u65f6\u6570\u636e: \u9700\u7ed3\u5408\u5916\u90e8API\u83b7\u53d6\u6700\u65b0\u4fe1\u606f</li> </ul>"},{"location":"interview/agent-interview/#q2-llm-agent_1","title":"Q2. \u5982\u4f55\u8861\u91cfLLM Agent\u7684\u6027\u80fd\uff1f","text":"<p>\u5e38\u89c1\u8bc4\u4f30\u6307\u6807:</p> <ul> <li>\u4efb\u52a1\u6210\u529f\u7387\uff08Task Completion Rate\uff09</li> <li>\u5de5\u5177\u8c03\u7528\u51c6\u786e\u7387\uff08Tool Usage Accuracy\uff09</li> <li>\u63a8\u7406\u8d28\u91cf\uff08Reasoning Quality\uff09</li> <li>\u7528\u6237\u6ee1\u610f\u5ea6\uff08User Satisfaction\uff09</li> </ul>"},{"location":"interview/agent-interview/#q3-llm-agent_1","title":"Q3. \u5982\u4f55\u4f18\u5316LLM Agent\u7684\u6027\u80fd\uff1f","text":"<p>\u4f18\u5316\u65b9\u6cd5:</p> <ul> <li>\u51cf\u5c11API\u8c03\u7528\u6b21\u6570: \u4f7f\u7528\u7f13\u5b58\uff08\u5982LangChain\u7684LLMCache\uff09</li> <li>\u4f18\u5316\u63d0\u793a\u8bcd: \u63d0\u524d\u63d0\u4f9b\u793a\u4f8b\uff0c\u63d0\u9ad8\u63a8\u7406\u80fd\u529b</li> <li>\u9009\u62e9\u5408\u9002\u7684LLM: \u4efb\u52a1\u7b80\u5355\u65f6\u4f7f\u7528Claude Instant\u4ee3\u66ffGPT-4\uff0c\u964d\u4f4e\u6210\u672c</li> </ul>"},{"location":"interview/agent-interview/#_2","title":"\ud83d\udcda \u5b9e\u9645\u5e94\u7528\u7bc7","text":""},{"location":"interview/agent-interview/#q1-llm-agent_2","title":"Q1. LLM Agent\u5728\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u5178\u578b\u573a\u666f\uff1f","text":"<p>\u5e94\u7528\u573a\u666f:</p> <ul> <li>\u667a\u80fd\u5ba2\u670d\uff08LangChain + RAG\uff09</li> <li>\u8d22\u52a1\u6570\u636e\u5206\u6790\uff08LlamaIndex + SQL\uff09</li> <li>\u81ea\u52a8\u5316\u7814\u7a76\u52a9\u624b\uff08CrewAI + AutoGPT\uff09</li> <li>\u6587\u6863\u89e3\u6790\uff08OCR + LangChain\uff09</li> </ul>"},{"location":"interview/agent-interview/#q2-agent","title":"Q2. \u8bbe\u8ba1\u4e00\u4e2a\u533b\u5b66\u95ee\u7b54Agent\u9700\u8981\u54ea\u4e9b\u5173\u952e\u7ec4\u4ef6\uff1f","text":"<p>\u5173\u952e\u7ec4\u4ef6:</p> <ul> <li>\u533b\u5b66\u77e5\u8bc6\u5e93\uff08\u7528\u4e8e\u67e5\u8be2\u6807\u51c6\u7b54\u6848\uff09</li> <li>\u75c5\u5386\u89e3\u6790\u6a21\u5757\uff08\u5904\u7406\u975e\u7ed3\u6784\u5316\u6587\u672c\uff09</li> <li>\u5de5\u5177API\uff08\u5982UpToDate\u3001PubMed\u67e5\u8be2\uff09</li> <li>\u957f\u8bb0\u5fc6\u673a\u5236\uff08RAG + \u5411\u91cf\u6570\u636e\u5e93\uff09</li> <li>\u9a8c\u8bc1\u673a\u5236\uff08\u51cf\u5c11\u5e7b\u89c9\u98ce\u9669\uff09</li> </ul>"},{"location":"interview/agent-interview/#agentmanus","title":"\ud83d\udcda \u56fd\u4ea7Agent\u6846\u67b6\uff1aManus","text":""},{"location":"interview/agent-interview/#q1-manus","title":"Q1. Manus\u7684\u6838\u5fc3\u529f\u80fd\u6709\u54ea\u4e9b\uff1f","text":"<p>\u4e3b\u8981\u529f\u80fd:</p> <ul> <li>\u4efb\u52a1\u7406\u89e3\u4e0e\u5206\u6790: \u91c7\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u89e3\u6790\u7528\u6237\u6307\u4ee4\uff0c\u652f\u6301\u8de8\u8bed\u8a00\u610f\u56fe\u8bc6\u522b</li> <li>\u4efb\u52a1\u5206\u89e3: \u5c06\u590d\u6742\u76ee\u6807\u8f6c\u5316\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u7ed3\u6784\uff0c\u81ea\u52a8\u751f\u6210\u5173\u952e\u8def\u5f84</li> <li>\u6267\u884c\u8ba1\u5212\u5236\u5b9a: \u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u4f18\u5316\u4efb\u52a1\u4f18\u5148\u7ea7\u961f\u5217</li> <li>\u5f02\u5e38\u5904\u7406: \u5b9e\u65f6\u68c0\u6d4b\u6267\u884c\u504f\u5dee\u5e76\u89e6\u53d1\u5907\u7528\u65b9\u6848</li> </ul>"},{"location":"interview/agent-interview/#q2-manus","title":"Q2. Manus\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\u8bbe\u8ba1\u7279\u70b9\uff1f","text":"<p>\u67b6\u6784\u7279\u70b9: - \u91c7\u7528\u591a\u4ee3\u7406\u534f\u4f5c\u7684\u67b6\u6784\u8bbe\u8ba1 - \u7528\u6237\u53ea\u4e0e\u6267\u884c\u4ee3\u7406\u901a\u4fe1\uff0c\u6267\u884c\u4ee3\u7406\u4e0d\u77e5\u9053\u5176\u4ed6\u4ee3\u7406\u7684\u8be6\u7ec6\u4fe1\u606f - \u8fd9\u79cd\u8bbe\u8ba1\u6709\u52a9\u4e8e\u63a7\u5236\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u907f\u514d\u56e0\u8fc7\u591a\u4fe1\u606f\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d</p>"},{"location":"interview/agent-interview/#_3","title":"\ud83d\udca1 \u9762\u8bd5\u6280\u5de7","text":"<ol> <li>\u5148\u8bf4\u6838\u5fc3\u6982\u5ff5\uff0c\u518d\u5c55\u5f00\u6280\u672f\u7ec6\u8282</li> <li>\u7ed3\u5408\u5177\u4f53\u6846\u67b6\u548c\u5b9e\u73b0\u65b9\u5f0f</li> <li>\u63d0\u53ca\u5b9e\u9645\u5e94\u7528\u573a\u666f\u548c\u5de5\u7a0b\u8003\u8651</li> <li>\u5bf9\u6bd4\u4e0d\u540c\u65b9\u6848\u7684\u4f18\u52a3\u52bf</li> </ol>"},{"location":"interview/agent-interview/#_4","title":"\ud83d\udd17 \u76f8\u5173\u94fe\u63a5","text":"<ul> <li>\u8fd4\u56de\u9762\u8bd5\u9898\u5e93</li> <li>Context Engineering\u5b9e\u8df5</li> <li>RAG\u4e0eAgent\u5e94\u7528</li> </ul>"},{"location":"reference/markdown/","title":"Markdown \u8bed\u6cd5\u53c2\u8003","text":"<p>\u5b8c\u6574\u7684 Markdown \u8bed\u6cd5\u53c2\u8003\u6307\u5357\u3002</p>"},{"location":"reference/markdown/#_1","title":"\u57fa\u7840\u8bed\u6cd5","text":""},{"location":"reference/markdown/#_2","title":"\u6807\u9898","text":"<pre><code># H1 \u6807\u9898\n## H2 \u6807\u9898\n### H3 \u6807\u9898\n#### H4 \u6807\u9898\n##### H5 \u6807\u9898\n###### H6 \u6807\u9898\n</code></pre>"},{"location":"reference/markdown/#_3","title":"\u6587\u672c\u6837\u5f0f","text":"\u6837\u5f0f \u8bed\u6cd5 \u793a\u4f8b \u7c97\u4f53 <code>**\u6587\u672c**</code> \u6216 <code>__\u6587\u672c__</code> \u7c97\u4f53\u6587\u672c \u659c\u4f53 <code>*\u6587\u672c*</code> \u6216 <code>_\u6587\u672c_</code> \u659c\u4f53\u6587\u672c \u5220\u9664\u7ebf <code>~~\u6587\u672c~~</code> ~~\u5220\u9664\u7ebf\u6587\u672c~~ \u884c\u5185\u4ee3\u7801 <code>`\u4ee3\u7801`</code> <code>\u4ee3\u7801</code>"},{"location":"reference/markdown/#_4","title":"\u5217\u8868","text":""},{"location":"reference/markdown/#_5","title":"\u65e0\u5e8f\u5217\u8868","text":"<pre><code>- \u9879\u76ee 1\n- \u9879\u76ee 2\n  - \u5b50\u9879\u76ee 2.1\n  - \u5b50\u9879\u76ee 2.2\n- \u9879\u76ee 3\n</code></pre>"},{"location":"reference/markdown/#_6","title":"\u6709\u5e8f\u5217\u8868","text":"<pre><code>1. \u7b2c\u4e00\u9879\n2. \u7b2c\u4e8c\u9879\n   1. \u5b50\u9879\u76ee 2.1\n   2. \u5b50\u9879\u76ee 2.2\n3. \u7b2c\u4e09\u9879\n</code></pre>"},{"location":"reference/markdown/#_7","title":"\u94fe\u63a5\u548c\u56fe\u7247","text":"<pre><code>[\u94fe\u63a5\u6587\u672c](https://example.com)\n[\u5185\u90e8\u94fe\u63a5](../index.md)\n![\u56fe\u7247\u63cf\u8ff0](image.png)\n</code></pre>"},{"location":"reference/markdown/#_8","title":"\u8868\u683c","text":"<pre><code>| \u8868\u59341 | \u8868\u59342 | \u8868\u59343 |\n|-------|-------|-------|\n| \u5355\u5143\u683c1 | \u5355\u5143\u683c2 | \u5355\u5143\u683c3 |\n| \u5355\u5143\u683c4 | \u5355\u5143\u683c5 | \u5355\u5143\u683c6 |\n</code></pre>"},{"location":"reference/markdown/#_9","title":"\u6269\u5c55\u8bed\u6cd5","text":""},{"location":"reference/markdown/#_10","title":"\u4ee3\u7801\u5757","text":"<pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n</code></pre>"},{"location":"reference/markdown/#_11","title":"\u544a\u793a\u6846\uff08\u9700\u8981\u6269\u5c55\uff09","text":"<pre><code>!!! note \"\u6ce8\u91ca\"\n    \u8fd9\u662f\u4e00\u4e2a\u6ce8\u91ca\u6846\u3002\n\n!!! tip \"\u63d0\u793a\"\n    \u8fd9\u662f\u4e00\u4e2a\u63d0\u793a\u6846\u3002\n\n!!! warning \"\u8b66\u544a\"\n    \u8fd9\u662f\u4e00\u4e2a\u8b66\u544a\u6846\u3002\n\n!!! danger \"\u5371\u9669\"\n    \u8fd9\u662f\u4e00\u4e2a\u5371\u9669\u8b66\u544a\u6846\u3002\n</code></pre>"},{"location":"reference/markdown/#_12","title":"\u4efb\u52a1\u5217\u8868","text":"<pre><code>- [x] \u5df2\u5b8c\u6210\u7684\u4efb\u52a1\n- [ ] \u672a\u5b8c\u6210\u7684\u4efb\u52a1\n- [ ] \u53e6\u4e00\u4e2a\u672a\u5b8c\u6210\u7684\u4efb\u52a1\n</code></pre>"},{"location":"reference/markdown/#_13","title":"\u811a\u6ce8","text":"<pre><code>\u8fd9\u91cc\u6709\u4e00\u4e2a\u811a\u6ce8\u5f15\u7528[^1]\u3002\n\n[^1]: \u8fd9\u662f\u811a\u6ce8\u5185\u5bb9\u3002\n</code></pre>"},{"location":"reference/markdown/#_14","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u4f7f\u7528\u63cf\u8ff0\u6027\u6807\u9898 - \u8ba9\u8bfb\u8005\u5feb\u901f\u4e86\u89e3\u5185\u5bb9</li> <li>\u4fdd\u6301\u4e00\u81f4\u7684\u683c\u5f0f - \u7edf\u4e00\u7684\u6837\u5f0f\u8ba9\u6587\u6863\u66f4\u4e13\u4e1a</li> <li>\u9002\u5ea6\u4f7f\u7528\u683c\u5f0f - \u8fc7\u591a\u7684\u683c\u5f0f\u4f1a\u5f71\u54cd\u53ef\u8bfb\u6027</li> <li>\u6dfb\u52a0\u76ee\u5f55\u94fe\u63a5 - \u5e2e\u52a9\u7528\u6237\u5feb\u901f\u5bfc\u822a</li> </ol> <p>!!! tip \"\u7f16\u5199\u6280\u5de7\"     \u5b9a\u671f\u9884\u89c8\u60a8\u7684\u6587\u6863\uff0c\u786e\u4fdd\u683c\u5f0f\u6b63\u786e\u6e32\u67d3\u3002</p>"},{"location":"training/parameter-efficient/lora/","title":"LoRA\u53c2\u6570\u9ad8\u6548\u5fae\u8c03","text":""},{"location":"training/parameter-efficient/lora/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3LoRA(Low-Rank Adaptation)\u6280\u672f\u539f\u7406\uff0c\u638c\u63e1\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u7684\u6838\u5fc3\u65b9\u6cd5\uff0c\u5b66\u4f1a\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u5e94\u7528LoRA\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - LoRA\u7684\u6838\u5fc3\u539f\u7406\u662f\u4ec0\u4e48\uff1f\u4e3a\u4ec0\u4e48\u80fd\u51cf\u5c11\u53c2\u6570\u91cf\uff1f - LoRA\u4e2d\u7684\u79e9(rank)\u5982\u4f55\u9009\u62e9\uff1f - LoRA vs \u5168\u53c2\u6570\u5fae\u8c03\u7684\u4f18\u52a3\u5bf9\u6bd4\uff1f - QLoRA\u662f\u4ec0\u4e48\uff1f\u4e0eLoRA\u6709\u4ec0\u4e48\u533a\u522b\uff1f</p>"},{"location":"training/parameter-efficient/lora/#lora_1","title":"\ud83c\udfd7\ufe0f LoRA\u6280\u672f\u539f\u7406","text":""},{"location":"training/parameter-efficient/lora/#_2","title":"\u6838\u5fc3\u601d\u60f3","text":"<p>LoRA\u57fa\u4e8e\u4e00\u4e2a\u5173\u952e\u89c2\u5bdf\uff1a\u6a21\u578b\u5fae\u8c03\u65f6\u7684\u6743\u91cd\u66f4\u65b0\u5177\u6709\u4f4e\u79e9\u7279\u6027\uff0c\u53ef\u4ee5\u7528\u4e24\u4e2a\u5c0f\u77e9\u9635\u7684\u4e58\u79ef\u6765\u8fd1\u4f3c\u3002</p> <pre><code>\u4f20\u7edf\u5168\u53c2\u6570\u5fae\u8c03 vs LoRA\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502        \u5168\u53c2\u6570\u5fae\u8c03                \u2502    \u2502           LoRA\u65b9\u6cd5              \u2502\n\u2502                                 \u2502    \u2502                                 \u2502\n\u2502  W\u2080 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 W\u2080 + \u2206W          \u2502    \u2502  W\u2080 (\u51bb\u7ed3)                      \u2502\n\u2502       \u8bad\u7ec3\u2206W                    \u2502    \u2502    +                            \u2502\n\u2502   \u53c2\u6570\u91cf: 100%                  \u2502    \u2502  \u2206W = B \u00d7 A                     \u2502 \n\u2502                                 \u2502    \u2502       \u8bad\u7ec3B,A                   \u2502\n\u2502                                 \u2502    \u2502  \u53c2\u6570\u91cf: &lt;1%                    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/parameter-efficient/lora/#_3","title":"\u6570\u5b66\u8868\u8fbe","text":"<p>\u539f\u59cb\u7ebf\u6027\u5c42\uff1a \\(\\(h = W_0 x\\)\\)</p> <p>LoRA\u589e\u5f3a\u540e\uff1a \\(\\(h = W_0 x + \\Delta W x = W_0 x + B A x\\)\\)</p> <p>\u5176\u4e2d\uff1a - \\(W_0 \\in \\mathbb{R}^{d \\times k}\\)\uff1a\u539f\u59cb\u6743\u91cd\u77e9\u9635(\u51bb\u7ed3) - \\(A \\in \\mathbb{R}^{r \\times k}\\)\uff1a\u4e0b\u6295\u5f71\u77e9\u9635 - \\(B \\in \\mathbb{R}^{d \\times r}\\)\uff1a\u4e0a\u6295\u5f71\u77e9\u9635 - \\(r \\ll \\min(d,k)\\)\uff1a\u4f4e\u79e9\u7ef4\u5ea6</p> <p>\u53c2\u6570\u51cf\u5c11\u6bd4\u4f8b\uff1a \\(\\(\\frac{\\text{LoRA\u53c2\u6570\u91cf}}{\\text{\u539f\u59cb\u53c2\u6570\u91cf}} = \\frac{r(d+k)}{d \\times k}\\)\\)</p>"},{"location":"training/parameter-efficient/lora/#lora_2","title":"LoRA\u67b6\u6784\u56fe","text":"<pre><code>LoRA\u6a21\u5757\u7ed3\u6784\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    \u8f93\u5165 x                                    \u2502\n\u2502                      \u2502                                      \u2502\n\u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                              \u2502\n\u2502              \u2502               \u2502                              \u2502\n\u2502        \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\n\u2502        \u2502   W\u2080(\u51bb\u7ed3)  \u2502   \u2502  LoRA\u5206\u652f  \u2502                        \u2502\n\u2502        \u2502           \u2502   \u2502           \u2502                        \u2502\n\u2502        \u2502           \u2502   \u2502    \u250c\u2500A\u2500\u2510   \u2502                        \u2502\n\u2502        \u2502           \u2502   \u2502    \u2502   \u2502   \u2502                        \u2502\n\u2502        \u2502           \u2502   \u2502    \u2514\u2500\u252c\u2500\u2518   \u2502                        \u2502\n\u2502        \u2502           \u2502   \u2502      \u2502     \u2502                        \u2502\n\u2502        \u2502           \u2502   \u2502    \u250c\u2500\u25bc\u2500\u2510   \u2502                        \u2502\n\u2502        \u2502           \u2502   \u2502 \u03b1/r\u2502 B \u2502   \u2502                        \u2502\n\u2502        \u2502           \u2502   \u2502    \u2514\u2500\u252c\u2500\u2518   \u2502                        \u2502\n\u2502        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n\u2502              \u2502               \u2502                              \u2502\n\u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                              \u2502\n\u2502                      \u25bc                                      \u2502\n\u2502                  h = W\u2080x + BAx                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/parameter-efficient/lora/#lora_3","title":"\ud83d\udcbb LoRA\u5b9e\u73b0\u8be6\u89e3","text":""},{"location":"training/parameter-efficient/lora/#_4","title":"\u57fa\u7840\u5b9e\u73b0","text":"<pre><code>import torch\nimport torch.nn as nn\nimport math\n\nclass LoRALayer(nn.Module):\n    \"\"\"LoRA\u5c42\u5b9e\u73b0\"\"\"\n\n    def __init__(self, in_features, out_features, rank=4, alpha=16, dropout=0.1):\n        super().__init__()\n\n        self.rank = rank\n        self.alpha = alpha\n        self.scaling = alpha / rank  # \u7f29\u653e\u56e0\u5b50\n\n        # LoRA\u77e9\u9635\n        self.lora_A = nn.Parameter(torch.zeros(rank, in_features))\n        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n        self.dropout = nn.Dropout(dropout)\n\n        # \u521d\u59cb\u5316A\u4e3a\u968f\u673a\u5c0f\u503c\uff0cB\u4e3a0\n        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n        nn.init.zeros_(self.lora_B)\n\n    def forward(self, x):\n        \"\"\"\u524d\u5411\u4f20\u64ad\"\"\"\n        # LoRA\u8def\u5f84: B @ A @ x\n        lora_output = (self.dropout(x) @ self.lora_A.T @ self.lora_B.T) * self.scaling\n        return lora_output\n\nclass LoRALinear(nn.Module):\n    \"\"\"\u5e26LoRA\u7684\u7ebf\u6027\u5c42\"\"\"\n\n    def __init__(self, linear_layer, rank=4, alpha=16, dropout=0.1):\n        super().__init__()\n\n        # \u51bb\u7ed3\u539f\u59cb\u5c42\n        self.linear = linear_layer\n        for param in self.linear.parameters():\n            param.requires_grad = False\n\n        # \u6dfb\u52a0LoRA\n        self.lora = LoRALayer(\n            linear_layer.in_features,\n            linear_layer.out_features,\n            rank=rank,\n            alpha=alpha,\n            dropout=dropout\n        )\n\n    def forward(self, x):\n        # \u539f\u59cb\u8f93\u51fa + LoRA\u8f93\u51fa\n        return self.linear(x) + self.lora(x)\n\n# \u4f7f\u7528\u793a\u4f8b\noriginal_linear = nn.Linear(768, 768)\nlora_linear = LoRALinear(original_linear, rank=16, alpha=32)\n\nprint(f\"\u539f\u59cb\u53c2\u6570\u91cf: {sum(p.numel() for p in original_linear.parameters()):,}\")\nprint(f\"LoRA\u53c2\u6570\u91cf: {sum(p.numel() for p in lora_linear.lora.parameters()):,}\")\nprint(f\"\u53c2\u6570\u51cf\u5c11\u6bd4\u4f8b: {100 * (1 - sum(p.numel() for p in lora_linear.lora.parameters()) / sum(p.numel() for p in original_linear.parameters())):.1f}%\")\n</code></pre>"},{"location":"training/parameter-efficient/lora/#lora_4","title":"\u9ad8\u7ea7LoRA\u5b9e\u73b0","text":"<pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport torch\n\nclass AdvancedLoRATrainer:\n    \"\"\"\u9ad8\u7ea7LoRA\u8bad\u7ec3\u5668\"\"\"\n\n    def __init__(self, model_name, lora_config=None):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            load_in_8bit=True  # \u91cf\u5316\u52a0\u8f7d\n        )\n\n        # \u9ed8\u8ba4LoRA\u914d\u7f6e\n        if lora_config is None:\n            lora_config = LoraConfig(\n                r=16,  # \u79e9\n                lora_alpha=32,  # \u7f29\u653e\u56e0\u5b50\n                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n                               \"gate_proj\", \"up_proj\", \"down_proj\"],  # \u76ee\u6807\u6a21\u5757\n                lora_dropout=0.05,  # Dropout\n                bias=\"none\",  # \u504f\u7f6e\u5904\u7406\n                task_type=TaskType.CAUSAL_LM,  # \u4efb\u52a1\u7c7b\u578b\n                inference_mode=False,  # \u8bad\u7ec3\u6a21\u5f0f\n            )\n\n        # \u5e94\u7528LoRA\n        self.model = get_peft_model(self.model, lora_config)\n\n        # \u6253\u5370\u53ef\u8bad\u7ec3\u53c2\u6570\n        self.print_trainable_parameters()\n\n    def print_trainable_parameters(self):\n        \"\"\"\u6253\u5370\u53ef\u8bad\u7ec3\u53c2\u6570\u7edf\u8ba1\"\"\"\n        trainable_params = 0\n        all_param = 0\n\n        for _, param in self.model.named_parameters():\n            all_param += param.numel()\n            if param.requires_grad:\n                trainable_params += param.numel()\n\n        print(f\"\u53ef\u8bad\u7ec3\u53c2\u6570: {trainable_params:,}\")\n        print(f\"\u603b\u53c2\u6570\u91cf: {all_param:,}\")\n        print(f\"\u53ef\u8bad\u7ec3\u6bd4\u4f8b: {100 * trainable_params / all_param:.2f}%\")\n\n    def adaptive_rank_selection(self, target_modules):\n        \"\"\"\u81ea\u9002\u5e94\u79e9\u9009\u62e9\"\"\"\n        # \u6839\u636e\u4e0d\u540c\u6a21\u5757\u9009\u62e9\u4e0d\u540c\u7684\u79e9\n        rank_config = {\n            \"q_proj\": 32,    # Query\u6295\u5f71\u9700\u8981\u66f4\u9ad8\u79e9\n            \"k_proj\": 16,    # Key\u6295\u5f71\u4e2d\u7b49\u79e9  \n            \"v_proj\": 32,    # Value\u6295\u5f71\u9700\u8981\u66f4\u9ad8\u79e9\n            \"o_proj\": 16,    # \u8f93\u51fa\u6295\u5f71\u4e2d\u7b49\u79e9\n            \"gate_proj\": 8,  # Gate\u6295\u5f71\u8f83\u4f4e\u79e9\n            \"up_proj\": 16,   # Up\u6295\u5f71\u4e2d\u7b49\u79e9\n            \"down_proj\": 8,  # Down\u6295\u5f71\u8f83\u4f4e\u79e9\n        }\n\n        return {module: rank_config.get(module, 16) for module in target_modules}\n\n# \u4f7f\u7528PEFT\u5e93\u7684\u5b8c\u6574\u793a\u4f8b\ndef setup_lora_training(model_name, custom_config=None):\n    \"\"\"\u8bbe\u7f6eLoRA\u8bad\u7ec3\u73af\u5883\"\"\"\n\n    # \u81ea\u5b9a\u4e49\u914d\u7f6e\u793a\u4f8b\n    if custom_config is None:\n        custom_config = {\n            \"r\": 16,\n            \"lora_alpha\": 32,\n            \"target_modules\": [\n                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                \"gate_proj\", \"up_proj\", \"down_proj\"\n            ],\n            \"lora_dropout\": 0.05,\n            \"bias\": \"none\",\n            \"task_type\": TaskType.CAUSAL_LM,\n        }\n\n    # \u521b\u5efaLoRA\u914d\u7f6e\n    lora_config = LoraConfig(**custom_config)\n\n    # \u52a0\u8f7d\u548c\u914d\u7f6e\u6a21\u578b\n    trainer = AdvancedLoRATrainer(model_name, lora_config)\n\n    return trainer\n\n# \u8bad\u7ec3\u914d\u7f6e\u793a\u4f8b\ntrainer = setup_lora_training(\n    \"microsoft/DialoGPT-medium\",\n    custom_config={\n        \"r\": 32,  # \u66f4\u9ad8\u7684\u79e9\u7528\u4e8e\u66f4\u590d\u6742\u7684\u4efb\u52a1\n        \"lora_alpha\": 64,\n        \"target_modules\": [\"q_proj\", \"v_proj\", \"o_proj\"],\n        \"lora_dropout\": 0.1,\n        \"bias\": \"none\",\n    }\n)\n</code></pre>"},{"location":"training/parameter-efficient/lora/#lora_5","title":"\ud83c\udf9b\ufe0f LoRA\u8d85\u53c2\u6570\u8c03\u4f18","text":""},{"location":"training/parameter-efficient/lora/#_5","title":"\u5173\u952e\u8d85\u53c2\u6570\u89e3\u6790","text":"<pre><code>class LoRAHyperparameterGuide:\n    \"\"\"LoRA\u8d85\u53c2\u6570\u8c03\u4f18\u6307\u5357\"\"\"\n\n    @staticmethod\n    def recommend_rank(task_complexity, model_size):\n        \"\"\"\u63a8\u8350\u79e9\u5927\u5c0f\"\"\"\n\n        # \u57fa\u7840\u63a8\u8350\n        base_rank = {\n            \"simple\": 4,    # \u7b80\u5355\u4efb\u52a1(\u5206\u7c7b\u7b49)\n            \"medium\": 16,   # \u4e2d\u7b49\u4efb\u52a1(\u5bf9\u8bdd\u7b49)  \n            \"complex\": 32   # \u590d\u6742\u4efb\u52a1(\u4ee3\u7801\u751f\u6210\u7b49)\n        }.get(task_complexity, 16)\n\n        # \u6839\u636e\u6a21\u578b\u5927\u5c0f\u8c03\u6574\n        size_multiplier = {\n            \"small\": 0.5,   # &lt;1B\u53c2\u6570\n            \"medium\": 1.0,  # 1B-7B\u53c2\u6570\n            \"large\": 1.5,   # 7B-13B\u53c2\u6570\n            \"xlarge\": 2.0   # &gt;13B\u53c2\u6570\n        }.get(model_size, 1.0)\n\n        return int(base_rank * size_multiplier)\n\n    @staticmethod\n    def recommend_alpha(rank):\n        \"\"\"\u63a8\u8350alpha\u503c\"\"\"\n        # \u7ecf\u9a8c\u516c\u5f0f: alpha = 2 * rank\n        return 2 * rank\n\n    @staticmethod\n    def recommend_dropout(dataset_size):\n        \"\"\"\u63a8\u8350dropout\u503c\"\"\"\n        if dataset_size &lt; 1000:\n            return 0.1  # \u5c0f\u6570\u636e\u96c6\uff0c\u66f4\u9ad8dropout\n        elif dataset_size &lt; 10000:\n            return 0.05  # \u4e2d\u7b49\u6570\u636e\u96c6\n        else:\n            return 0.01  # \u5927\u6570\u636e\u96c6\uff0c\u8f83\u4f4edropout\n\n# \u4f7f\u7528\u793a\u4f8b\nguide = LoRAHyperparameterGuide()\n\n# \u4e3a7B\u6a21\u578b\u7684\u5bf9\u8bdd\u4efb\u52a1\u63a8\u8350\u53c2\u6570\nrecommended_rank = guide.recommend_rank(\"medium\", \"large\")\nrecommended_alpha = guide.recommend_alpha(recommended_rank)\nrecommended_dropout = guide.recommend_dropout(5000)\n\nprint(f\"\u63a8\u8350\u914d\u7f6e:\")\nprint(f\"Rank: {recommended_rank}\")\nprint(f\"Alpha: {recommended_alpha}\")\nprint(f\"Dropout: {recommended_dropout}\")\n</code></pre>"},{"location":"training/parameter-efficient/lora/#_6","title":"\u76ee\u6807\u6a21\u5757\u9009\u62e9\u7b56\u7565","text":"<pre><code>def select_target_modules(model_architecture, task_type):\n    \"\"\"\u667a\u80fd\u9009\u62e9\u76ee\u6807\u6a21\u5757\"\"\"\n\n    # \u4e0d\u540c\u67b6\u6784\u7684\u6a21\u5757\u6620\u5c04\n    module_maps = {\n        \"llama\": {\n            \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n            \"ffn\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n            \"all\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n                   \"gate_proj\", \"up_proj\", \"down_proj\"]\n        },\n        \"gpt2\": {\n            \"attention\": [\"c_attn\", \"c_proj\"],\n            \"ffn\": [\"c_fc\"],\n            \"all\": [\"c_attn\", \"c_proj\", \"c_fc\"]\n        },\n        \"bert\": {\n            \"attention\": [\"query\", \"key\", \"value\", \"dense\"],\n            \"ffn\": [\"intermediate.dense\", \"output.dense\"],\n            \"all\": [\"query\", \"key\", \"value\", \"dense\", \n                   \"intermediate.dense\", \"output.dense\"]\n        }\n    }\n\n    # \u4efb\u52a1\u7279\u5b9a\u63a8\u8350\n    task_recommendations = {\n        \"generation\": \"all\",      # \u751f\u6210\u4efb\u52a1\u4f7f\u7528\u6240\u6709\u6a21\u5757\n        \"classification\": \"attention\",  # \u5206\u7c7b\u4efb\u52a1\u4e3b\u8981\u7528attention\n        \"qa\": \"all\",             # \u95ee\u7b54\u4efb\u52a1\u4f7f\u7528\u6240\u6709\u6a21\u5757\n        \"summarization\": \"attention\"  # \u6458\u8981\u4efb\u52a1\u4e3b\u8981\u7528attention\n    }\n\n    arch_modules = module_maps.get(model_architecture, module_maps[\"llama\"])\n    module_type = task_recommendations.get(task_type, \"all\")\n\n    return arch_modules[module_type]\n\n# \u4f7f\u7528\u793a\u4f8b\ntarget_modules = select_target_modules(\"llama\", \"generation\")\nprint(f\"\u63a8\u8350\u76ee\u6807\u6a21\u5757: {target_modules}\")\n</code></pre>"},{"location":"training/parameter-efficient/lora/#qloralora","title":"\ud83d\ude80 QLoRA\u91cf\u5316LoRA","text":""},{"location":"training/parameter-efficient/lora/#qlora","title":"QLoRA\u6838\u5fc3\u6280\u672f","text":"<pre><code>from transformers import BitsAndBytesConfig\nimport torch\n\nclass QLoRATrainer:\n    \"\"\"QLoRA\u8bad\u7ec3\u5668 - 4bit\u91cf\u5316 + LoRA\"\"\"\n\n    def __init__(self, model_name):\n        # 4bit\u91cf\u5316\u914d\u7f6e\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,                    # 4bit\u91cf\u5316\n            bnb_4bit_use_double_quant=True,       # \u53cc\u91cd\u91cf\u5316\n            bnb_4bit_quant_type=\"nf4\",           # NF4\u91cf\u5316\u7c7b\u578b\n            bnb_4bit_compute_dtype=torch.bfloat16  # \u8ba1\u7b97\u6570\u636e\u7c7b\u578b\n        )\n\n        # \u52a0\u8f7d\u91cf\u5316\u6a21\u578b\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            trust_remote_code=True\n        )\n\n        # LoRA\u914d\u7f6e\n        lora_config = LoraConfig(\n            r=64,  # QLoRA\u901a\u5e38\u4f7f\u7528\u66f4\u9ad8\u7684\u79e9\n            lora_alpha=128,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                           \"gate_proj\", \"up_proj\", \"down_proj\"],\n            lora_dropout=0.05,\n            bias=\"none\",\n            task_type=TaskType.CAUSAL_LM,\n        )\n\n        # \u5e94\u7528LoRA\u5230\u91cf\u5316\u6a21\u578b\n        self.model = get_peft_model(self.model, lora_config)\n\n    def memory_usage_comparison(self):\n        \"\"\"\u663e\u5b58\u4f7f\u7528\u5bf9\u6bd4\"\"\"\n        return {\n            \"Full Fine-tuning (16bit)\": \"~60GB for 7B model\",\n            \"LoRA (16bit)\": \"~24GB for 7B model\", \n            \"QLoRA (4bit)\": \"~9GB for 7B model\",\n            \"Memory Reduction\": \"85% vs Full Fine-tuning\"\n        }\n\n# QLoRA vs LoRA\u5bf9\u6bd4\ndef compare_approaches():\n    \"\"\"\u5bf9\u6bd4\u4e0d\u540c\u65b9\u6cd5\u7684\u8d44\u6e90\u9700\u6c42\"\"\"\n\n    comparison = {\n        \"\u65b9\u6cd5\": [\"\u5168\u53c2\u6570\u5fae\u8c03\", \"LoRA\", \"QLoRA\"],\n        \"\u663e\u5b58\u9700\u6c42(7B)\": [\"60GB\", \"24GB\", \"9GB\"],\n        \"\u8bad\u7ec3\u65f6\u95f4\": [\"\u57fa\u51c6\", \"1.2x\", \"1.5x\"],\n        \"\u53c2\u6570\u91cf\": [\"100%\", \"&lt;1%\", \"&lt;1%\"],\n        \"\u7cbe\u5ea6\u635f\u5931\": [\"0%\", \"~1%\", \"~2%\"],\n        \"\u786c\u4ef6\u8981\u6c42\": [\"A100 80GB\", \"V100 32GB\", \"RTX 3090\"]\n    }\n\n    return comparison\n</code></pre>"},{"location":"training/parameter-efficient/lora/#qlora_1","title":"QLoRA\u8bad\u7ec3\u5b9e\u73b0","text":"<pre><code>from trl import SFTTrainer, SFTConfig\nfrom datasets import load_dataset\n\ndef train_with_qlora(model_name, dataset_name, output_dir):\n    \"\"\"\u4f7f\u7528QLoRA\u8fdb\u884c\u8bad\u7ec3\"\"\"\n\n    # 1. \u8bbe\u7f6eQLoRA\n    qlora_trainer = QLoRATrainer(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    # 2. \u52a0\u8f7d\u6570\u636e\n    dataset = load_dataset(dataset_name, split=\"train\")\n\n    # 3. \u8bad\u7ec3\u914d\u7f6e\n    training_config = SFTConfig(\n        output_dir=output_dir,\n        num_train_epochs=3,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=4,\n        warmup_steps=100,\n        learning_rate=2e-4,\n        fp16=False,  # QLoRA\u63a8\u8350\u4f7f\u7528bf16\n        bf16=True,\n        logging_steps=10,\n        save_steps=500,\n        eval_steps=500,\n        max_seq_length=512,\n        packing=True,\n    )\n\n    # 4. \u521b\u5efa\u8bad\u7ec3\u5668\n    trainer = SFTTrainer(\n        model=qlora_trainer.model,\n        tokenizer=tokenizer,\n        args=training_config,\n        train_dataset=dataset,\n        max_seq_length=512,\n    )\n\n    # 5. \u5f00\u59cb\u8bad\u7ec3\n    trainer.train()\n    trainer.save_model()\n\n    return trainer\n\n# \u4f7f\u7528\u793a\u4f8b\n# trainer = train_with_qlora(\n#     \"meta-llama/Llama-2-7b-hf\",\n#     \"alpaca\",\n#     \"./qlora_output\"\n# )\n</code></pre>"},{"location":"training/parameter-efficient/lora/#lora_6","title":"\ud83d\udcca LoRA\u6027\u80fd\u5206\u6790","text":""},{"location":"training/parameter-efficient/lora/#_7","title":"\u6548\u679c\u8bc4\u4f30","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\n\nclass LoRAAnalyzer:\n    \"\"\"LoRA\u6027\u80fd\u5206\u6790\u5668\"\"\"\n\n    def __init__(self):\n        # \u5b9e\u9a8c\u6570\u636e (\u57fa\u4e8e\u771f\u5b9ebenchmark)\n        self.rank_performance = {\n            \"ranks\": [1, 2, 4, 8, 16, 32, 64, 128],\n            \"accuracy\": [0.65, 0.72, 0.81, 0.87, 0.91, 0.93, 0.94, 0.94],\n            \"parameters\": [0.01, 0.02, 0.04, 0.08, 0.16, 0.32, 0.64, 1.28]  # % of total\n        }\n\n    def plot_rank_vs_performance(self):\n        \"\"\"\u7ed8\u5236\u79e9\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\"\"\"\n        ranks = self.rank_performance[\"ranks\"]\n        accuracy = self.rank_performance[\"accuracy\"]\n        params = self.rank_performance[\"parameters\"]\n\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n        # \u7cbe\u5ea6vs\u79e9\n        ax1.plot(ranks, accuracy, 'b-o', linewidth=2, markersize=6)\n        ax1.set_xlabel('LoRA Rank')\n        ax1.set_ylabel('Accuracy')\n        ax1.set_title('Accuracy vs LoRA Rank')\n        ax1.grid(True, alpha=0.3)\n        ax1.set_xscale('log', base=2)\n\n        # \u53c2\u6570\u91cfvs\u79e9\n        ax2.plot(ranks, params, 'r-s', linewidth=2, markersize=6)\n        ax2.set_xlabel('LoRA Rank')\n        ax2.set_ylabel('Parameters (%)')\n        ax2.set_title('Parameter Overhead vs LoRA Rank')\n        ax2.grid(True, alpha=0.3)\n        ax2.set_xscale('log', base=2)\n\n        plt.tight_layout()\n        return fig\n\n    def efficiency_analysis(self):\n        \"\"\"\u6548\u7387\u5206\u6790\"\"\"\n        analysis = {\n            \"\u6700\u4f73\u6027\u4ef7\u6bd4\u79e9\": 16,\n            \"\u539f\u56e0\": \"\u572816\u79e9\u65f6\u8fbe\u523091%\u7cbe\u5ea6\uff0c\u4ec5\u4f7f\u75280.16%\u53c2\u6570\",\n            \"\u751c\u871c\u70b9\": \"rank=8\u523032\u4e4b\u95f4\",\n            \"\u8fc7\u62df\u5408\u98ce\u9669\": \"rank&gt;64\u65f6\u8fb9\u9645\u6536\u76ca\u9012\u51cf\"\n        }\n        return analysis\n\n    def task_specific_recommendations(self):\n        \"\"\"\u4efb\u52a1\u7279\u5b9a\u63a8\u8350\"\"\"\n        return {\n            \"\u5206\u7c7b\u4efb\u52a1\": {\n                \"\u63a8\u8350\u79e9\": \"4-8\",\n                \"\u539f\u56e0\": \"\u5206\u7c7b\u901a\u5e38\u4e0d\u9700\u8981\u592a\u591a\u8868\u8fbe\u80fd\u529b\"\n            },\n            \"\u751f\u6210\u4efb\u52a1\": {\n                \"\u63a8\u8350\u79e9\": \"16-32\", \n                \"\u539f\u56e0\": \"\u751f\u6210\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u8868\u8fbe\u80fd\u529b\"\n            },\n            \"\u4ee3\u7801\u751f\u6210\": {\n                \"\u63a8\u8350\u79e9\": \"32-64\",\n                \"\u539f\u56e0\": \"\u4ee3\u7801\u751f\u6210\u9700\u8981\u7cbe\u786e\u7684\u8bed\u6cd5\u7406\u89e3\"\n            },\n            \"\u591a\u6a21\u6001\": {\n                \"\u63a8\u8350\u79e9\": \"64-128\",\n                \"\u539f\u56e0\": \"\u591a\u6a21\u6001\u878d\u5408\u9700\u8981\u66f4\u9ad8\u7ef4\u5ea6\u8868\u793a\"\n            }\n        }\n\n# \u5206\u6790\u793a\u4f8b\nanalyzer = LoRAAnalyzer()\nefficiency = analyzer.efficiency_analysis()\nrecommendations = analyzer.task_specific_recommendations()\n\nprint(\"\u6548\u7387\u5206\u6790:\", efficiency)\nprint(\"\\n\u4efb\u52a1\u63a8\u8350:\", recommendations)\n</code></pre>"},{"location":"training/parameter-efficient/lora/#lora_7","title":"\ud83d\udee0\ufe0f LoRA\u5b9e\u6218\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"training/parameter-efficient/lora/#_8","title":"\u8bad\u7ec3\u6280\u5de7","text":"<pre><code>class LoRABestPractices:\n    \"\"\"LoRA\u6700\u4f73\u5b9e\u8df5\u6307\u5357\"\"\"\n\n    @staticmethod\n    def initialize_lora_weights(lora_A, lora_B):\n        \"\"\"\u6700\u4f73\u6743\u91cd\u521d\u59cb\u5316\"\"\"\n        # A\u77e9\u9635: \u4f7f\u7528Kaiming\u5747\u5300\u5206\u5e03\n        nn.init.kaiming_uniform_(lora_A, a=math.sqrt(5))\n\n        # B\u77e9\u9635: \u521d\u59cb\u5316\u4e3a0\u786e\u4fdd\u5f00\u59cb\u65f6\u2206W=0\n        nn.init.zeros_(lora_B)\n\n        return lora_A, lora_B\n\n    @staticmethod\n    def learning_rate_strategy(base_lr=2e-4):\n        \"\"\"\u5b66\u4e60\u7387\u7b56\u7565\"\"\"\n        return {\n            \"LoRA\u5c42\u5b66\u4e60\u7387\": base_lr,          # LoRA\u5c42\u4f7f\u7528\u8f83\u9ad8\u5b66\u4e60\u7387\n            \"\u9884\u8bad\u7ec3\u5c42\u5b66\u4e60\u7387\": base_lr / 10,    # \u9884\u8bad\u7ec3\u5c42\u4f7f\u7528\u8f83\u4f4e\u5b66\u4e60\u7387(\u5982\u679c\u4e0d\u51bb\u7ed3)\n            \"\u8c03\u5ea6\u5668\": \"cosine\",               # \u4f7f\u7528cosine\u8c03\u5ea6\n            \"\u70ed\u8eab\u6b65\u6570\": \"\u603b\u6b65\u6570\u768410%\"           # \u9002\u5f53\u70ed\u8eab\n        }\n\n    @staticmethod\n    def data_efficiency_tips():\n        \"\"\"\u6570\u636e\u6548\u7387\u63d0\u5347\u5efa\u8bae\"\"\"\n        return {\n            \"\u6570\u636e\u8d28\u91cf\": \"\u5b81\u5c11\u52ff\u6ee5\uff0c\u9ad8\u8d28\u91cf\u6570\u636e\u6bd4\u5927\u91cf\u4f4e\u8d28\u91cf\u6570\u636e\u66f4\u91cd\u8981\",\n            \"\u6570\u636e\u683c\u5f0f\": \"\u786e\u4fdd\u8f93\u5165\u683c\u5f0f\u4e0e\u9884\u8bad\u7ec3\u9636\u6bb5\u4e00\u81f4\",\n            \"\u5e8f\u5217\u957f\u5ea6\": \"\u4f7f\u7528\u6a21\u578b\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u4ee5\u5145\u5206\u5229\u7528\u6ce8\u610f\u529b\",\n            \"\u6279\u91cf\u5927\u5c0f\": \"\u5728\u663e\u5b58\u5141\u8bb8\u60c5\u51b5\u4e0b\u5c3d\u91cf\u589e\u5927batch size\"\n        }\n\n    @staticmethod\n    def common_pitfalls():\n        \"\"\"\u5e38\u89c1\u9677\u9631\u548c\u89e3\u51b3\u65b9\u6848\"\"\"\n        return {\n            \"\u9677\u96311\": {\n                \"\u95ee\u9898\": \"\u79e9\u8bbe\u7f6e\u8fc7\u4f4e\u5bfc\u81f4\u6b20\u62df\u5408\",\n                \"\u89e3\u51b3\": \"\u9010\u6b65\u589e\u52a0\u79e9\u76f4\u5230\u6027\u80fd\u9971\u548c\"\n            },\n            \"\u9677\u96312\": {\n                \"\u95ee\u9898\": \"\u76ee\u6807\u6a21\u5757\u9009\u62e9\u4e0d\u5f53\",\n                \"\u89e3\u51b3\": \"\u4eceattention\u5f00\u59cb\uff0c\u9010\u6b65\u52a0\u5165FFN\u6a21\u5757\"\n            },\n            \"\u9677\u96313\": {\n                \"\u95ee\u9898\": \"\u5b66\u4e60\u7387\u8bbe\u7f6e\u4e0d\u5f53\",\n                \"\u89e3\u51b3\": \"LoRA\u901a\u5e38\u9700\u8981\u6bd4\u5168\u53c2\u6570\u5fae\u8c03\u66f4\u9ad8\u7684\u5b66\u4e60\u7387\"\n            },\n            \"\u9677\u96314\": {\n                \"\u95ee\u9898\": \"\u5fd8\u8bb0\u7f29\u653e\u56e0\u5b50alpha\",\n                \"\u89e3\u51b3\": \"alpha\u901a\u5e38\u8bbe\u4e3a2*rank\uff0c\u53ef\u5fae\u8c03\"\n            }\n        }\n\n# \u5b9e\u8df5\u6307\u5bfc\npractices = LoRABestPractices()\nlr_strategy = practices.learning_rate_strategy()\npitfalls = practices.common_pitfalls()\n\nprint(\"\u5b66\u4e60\u7387\u7b56\u7565:\", lr_strategy)\nprint(\"\\n\u5e38\u89c1\u9677\u9631:\", pitfalls)\n</code></pre>"},{"location":"training/parameter-efficient/lora/#lora_8","title":"LoRA\u6a21\u578b\u5408\u5e76\u4e0e\u90e8\u7f72","text":"<pre><code>def merge_and_deploy_lora(base_model_path, lora_weights_path, output_path):\n    \"\"\"\u5408\u5e76LoRA\u6743\u91cd\u5e76\u90e8\u7f72\"\"\"\n\n    # 1. \u52a0\u8f7d\u57fa\u7840\u6a21\u578b\n    base_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n\n    # 2. \u52a0\u8f7dLoRA\u6743\u91cd\n    model = PeftModel.from_pretrained(base_model, lora_weights_path)\n\n    # 3. \u5408\u5e76\u6743\u91cd\n    merged_model = model.merge_and_unload()\n\n    # 4. \u4fdd\u5b58\u5408\u5e76\u540e\u7684\u6a21\u578b\n    merged_model.save_pretrained(output_path)\n    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n    tokenizer.save_pretrained(output_path)\n\n    print(f\"\u6a21\u578b\u5df2\u5408\u5e76\u5e76\u4fdd\u5b58\u5230: {output_path}\")\n\n    # 5. \u63a8\u7406\u6027\u80fd\u6d4b\u8bd5\n    test_inference_speed(merged_model, tokenizer)\n\n    return merged_model\n\ndef test_inference_speed(model, tokenizer):\n    \"\"\"\u6d4b\u8bd5\u63a8\u7406\u901f\u5ea6\"\"\"\n    import time\n\n    model.eval()\n    test_inputs = [\"Hello, how are you?\", \"Explain machine learning\", \"Write a Python function\"]\n\n    total_time = 0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for input_text in test_inputs:\n            inputs = tokenizer(input_text, return_tensors=\"pt\")\n\n            start_time = time.time()\n            outputs = model.generate(\n                **inputs, \n                max_new_tokens=50,\n                do_sample=False\n            )\n            end_time = time.time()\n\n            generated_tokens = outputs.shape[1] - inputs.input_ids.shape[1]\n            generation_time = end_time - start_time\n\n            total_time += generation_time\n            total_tokens += generated_tokens\n\n            print(f\"\u8f93\u5165: {input_text}\")\n            print(f\"\u751f\u6210\u65f6\u95f4: {generation_time:.2f}s\")\n            print(f\"\u751f\u6210token\u6570: {generated_tokens}\")\n            print(f\"\u901f\u5ea6: {generated_tokens/generation_time:.2f} tokens/s\\n\")\n\n    avg_speed = total_tokens / total_time\n    print(f\"\u5e73\u5747\u751f\u6210\u901f\u5ea6: {avg_speed:.2f} tokens/s\")\n\n# \u4f7f\u7528\u793a\u4f8b\n# merged_model = merge_and_deploy_lora(\n#     \"meta-llama/Llama-2-7b-hf\",\n#     \"./lora_weights\",\n#     \"./merged_model\"\n# )\n</code></pre>"},{"location":"training/parameter-efficient/lora/#_9","title":"\ud83c\udfaf \u9762\u8bd5\u95ee\u7b54\u603b\u7ed3","text":""},{"location":"training/parameter-efficient/lora/#q1-lora","title":"Q1: LoRA\u7684\u6838\u5fc3\u539f\u7406\u662f\u4ec0\u4e48\uff1f\u4e3a\u4ec0\u4e48\u80fd\u51cf\u5c11\u53c2\u6570\u91cf\uff1f","text":"<p>A: LoRA\u57fa\u4e8e\u4f4e\u79e9\u5047\u8bbe\uff0c\u8ba4\u4e3a\u6a21\u578b\u5fae\u8c03\u65f6\u7684\u6743\u91cd\u66f4\u65b0\u2206W\u5177\u6709\u4f4e\u79e9\u7279\u6027\uff0c\u53ef\u4ee5\u5206\u89e3\u4e3a\u2206W=BA\u4e24\u4e2a\u5c0f\u77e9\u9635\u3002\u8fd9\u6837\u53ea\u9700\u8bad\u7ec3BA\u800c\u4e0d\u662f\u6574\u4e2a\u2206W\uff0c\u53c2\u6570\u91cf\u4eced\u00d7k\u51cf\u5c11\u5230r\u00d7(d+k)\uff0c\u5176\u4e2dr&lt;&lt;min(d,k)\u3002</p>"},{"location":"training/parameter-efficient/lora/#q2-lorarank","title":"Q2: LoRA\u4e2d\u7684\u79e9(rank)\u5982\u4f55\u9009\u62e9\uff1f","text":"<p>A: - \u7b80\u5355\u4efb\u52a1(\u5206\u7c7b): rank=4-8 - \u4e2d\u7b49\u4efb\u52a1(\u5bf9\u8bdd): rank=16-32 - \u590d\u6742\u4efb\u52a1(\u4ee3\u7801\u751f\u6210): rank=32-64 - \u751c\u871c\u70b9: \u901a\u5e38rank=16\u5728\u6027\u80fd\u548c\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861</p>"},{"location":"training/parameter-efficient/lora/#q3-lora-vs","title":"Q3: LoRA vs \u5168\u53c2\u6570\u5fae\u8c03\u7684\u4f18\u52a3\u5bf9\u6bd4\uff1f","text":"<p>A: - \u4f18\u52bf: \u53c2\u6570\u91cf\u51cf\u5c1199%+\u3001\u663e\u5b58\u9700\u6c42\u964d\u4f4e60%+\u3001\u8bad\u7ec3\u901f\u5ea6\u66f4\u5feb\u3001\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8 - \u52a3\u52bf: \u6027\u80fd\u7565\u6709\u635f\u5931(\u901a\u5e381-3%)\u3001\u5bf9\u67d0\u4e9b\u590d\u6742\u4efb\u52a1\u6548\u679c\u4e0d\u5982\u5168\u53c2\u6570\u5fae\u8c03 - \u9002\u7528\u573a\u666f: \u8d44\u6e90\u53d7\u9650\u3001\u591a\u4efb\u52a1\u573a\u666f\u3001\u5feb\u901f\u9002\u5e94</p>"},{"location":"training/parameter-efficient/lora/#q4-qloralora","title":"Q4: QLoRA\u662f\u4ec0\u4e48\uff1f\u4e0eLoRA\u6709\u4ec0\u4e48\u533a\u522b\uff1f","text":"<p>A: QLoRA = 4bit\u91cf\u5316 + LoRA\uff0c\u5728LoRA\u57fa\u7840\u4e0a\u52a0\u51654bit\u91cf\u5316\u6280\u672f\uff1a - \u663e\u5b58\u4f18\u52bf: 7B\u6a21\u578b\u4ece24GB\u964d\u52309GB - \u6027\u80fd\u635f\u5931: \u6bd4LoRA\u591a1-2%\u7cbe\u5ea6\u635f\u5931 - \u786c\u4ef6\u95e8\u69db: \u6d88\u8d39\u7ea7\u663e\u5361\u5c31\u80fd\u8bad\u7ec3\u5927\u6a21\u578b</p>"},{"location":"training/parameter-efficient/lora/#_10","title":"\ud83d\ude80 \u5b66\u4e60\u5efa\u8bae","text":"<ol> <li>\u7406\u8bba\u5148\u884c: \u7406\u89e3\u4f4e\u79e9\u5206\u89e3\u7684\u6570\u5b66\u539f\u7406</li> <li>\u52a8\u624b\u5b9e\u8df5: \u4ece\u7b80\u5355\u4efb\u52a1\u5f00\u59cb\uff0c\u9010\u6b65\u638c\u63e1\u8d85\u53c2\u6570\u8c03\u4f18</li> <li>\u5bf9\u6bd4\u5b9e\u9a8c: \u6bd4\u8f83\u4e0d\u540crank\u548calpha\u7684\u6548\u679c</li> <li>\u751f\u4ea7\u5e94\u7528: \u5b66\u4e60\u6a21\u578b\u5408\u5e76\u548c\u90e8\u7f72\u6280\u5de7</li> </ol> <p>LoRA\u662f\u76ee\u524d\u6700\u5b9e\u7528\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\uff0c\u662f2024\u5e74LLM\u5de5\u7a0b\u5e08\u5fc5\u5907\u6280\u80fd\uff01</p>"},{"location":"training/post-training/","title":"\u7b2c9\u8282\uff1a\u540e\u8bad\u7ec3\u6280\u672f\u4f53\u7cfb","text":""},{"location":"training/post-training/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u5168\u9762\u638c\u63e1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u6280\u672f\u6808\uff0c\u5305\u62ec\u540e\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6838\u5fc3\u65b9\u6cd5\uff0c\u7406\u89e3\u73b0\u4ee3\u524d\u6cbf\u6a21\u578b\u7684\u5b8c\u6574\u8bad\u7ec3\u6d41\u7a0b\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - \u540e\u8bad\u7ec3\u5305\u542b\u54ea\u4e9b\u5173\u952e\u9636\u6bb5\uff1f - \u540e\u9884\u8bad\u7ec3\u4e0e\u76d1\u7763\u5fae\u8c03\u7684\u533a\u522b\uff1f - \u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u540e\u8bad\u7ec3\u6570\u636e\u6d41\u7a0b\uff1f - RLHF\u5728\u540e\u8bad\u7ec3\u4e2d\u7684\u5177\u4f53\u4f5c\u7528\uff1f</p>"},{"location":"training/post-training/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a4-5\u5929</p> <ul> <li>Day 1: \u540e\u9884\u8bad\u7ec3\u6280\u672f + \u9886\u57df\u9002\u5e94</li> <li>Day 2: \u76d1\u7763\u5fae\u8c03\u65b9\u6cd5 + \u6307\u4ee4\u5fae\u8c03</li> <li>Day 3: \u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50 + \u504f\u597d\u4f18\u5316</li> <li>Day 4: \u9ad8\u7ea7\u540e\u8bad\u7ec3\u6280\u672f + \u8fed\u4ee3\u4f18\u5316</li> <li>Day 5: \u5b9e\u6218\u9879\u76ee + \u6548\u679c\u8bc4\u4f30</li> </ul>"},{"location":"training/post-training/#_3","title":"\ud83c\udfd7\ufe0f \u540e\u8bad\u7ec3\u6280\u672f\u67b6\u6784","text":""},{"location":"training/post-training/#_4","title":"\u73b0\u4ee3\u540e\u8bad\u7ec3\u6d41\u7a0b","text":"<pre><code>\u5b8c\u6574\u540e\u8bad\u7ec3Pipeline (2024)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        \u5927\u6a21\u578b\u540e\u8bad\u7ec3\u6280\u672f\u6808                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u9884\u8bad\u7ec3\u57fa\u5ea7\u6a21\u578b (Base Model)                                             \u2502\n\u2502         \u2502                                                               \u2502\n\u2502         \u25bc                                                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502   \u540e\u9884\u8bad\u7ec3      \u2502\u2500\u2500\u2500\u25b6\u2502   \u76d1\u7763\u5fae\u8c03      \u2502\u2500\u2500\u2500\u25b6\u2502   \u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50   \u2502       \u2502\n\u2502  \u2502 Post-Pretraining\u2502    \u2502      SFT       \u2502    \u2502      RLHF      \u2502       \u2502\n\u2502  \u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502       \u2502\n\u2502  \u2502\u2022 \u9886\u57df\u7ee7\u7eed\u8bad\u7ec3    \u2502    \u2502\u2022 \u6307\u4ee4\u9075\u5faa\u8bad\u7ec3    \u2502    \u2502\u2022 \u4eba\u7c7b\u504f\u597d\u5bf9\u9f50    \u2502       \u2502\n\u2502  \u2502\u2022 \u77e5\u8bc6\u6ce8\u5165       \u2502    \u2502\u2022 \u683c\u5f0f\u89c4\u8303\u5316     \u2502    \u2502\u2022 \u5b89\u5168\u6027\u63d0\u5347     \u2502       \u2502\n\u2502  \u2502\u2022 \u80fd\u529b\u6269\u5c55       \u2502    \u2502\u2022 \u4efb\u52a1\u9002\u5e94       \u2502    \u2502\u2022 \u6709\u7528\u6027\u4f18\u5316     \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502         \u2502                       \u2502                       \u2502               \u2502\n\u2502         \u25bc                       \u25bc                       \u25bc               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502   \u8bc4\u4f30\u4e0e\u8fed\u4ee3     \u2502    \u2502   \u6a21\u578b\u5408\u5e76      \u2502    \u2502   \u90e8\u7f72\u4f18\u5316      \u2502       \u2502\n\u2502  \u2502                 \u2502    \u2502                 \u2502    \u2502                 \u2502       \u2502\n\u2502  \u2502\u2022 \u80fd\u529b\u57fa\u51c6\u6d4b\u8bd5    \u2502    \u2502\u2022 \u6743\u91cd\u878d\u5408       \u2502    \u2502\u2022 \u63a8\u7406\u52a0\u901f       \u2502       \u2502\n\u2502  \u2502\u2022 \u5b89\u5168\u6027\u68c0\u6d4b     \u2502    \u2502\u2022 \u7248\u672c\u7ba1\u7406       \u2502    \u2502\u2022 \u91cf\u5316\u90e8\u7f72       \u2502       \u2502\n\u2502  \u2502\u2022 \u7528\u6237\u53cd\u9988\u6536\u96c6    \u2502    \u2502\u2022 A/B\u6d4b\u8bd5       \u2502    \u2502\u2022 \u8fb9\u7f18\u8ba1\u7b97       \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/post-training/#_5","title":"\u6280\u672f\u53d1\u5c55\u8d8b\u52bf","text":"\u9636\u6bb5 2022\u5e74\u65b9\u6cd5 2023\u5e74\u65b9\u6cd5 2024\u5e74\u65b9\u6cd5 2025\u5e74\u8d8b\u52bf \u6570\u636e \u4eba\u5de5\u6807\u6ce8 \u534a\u81ea\u52a8\u5316 \u5927\u89c4\u6a21\u5408\u6210 \u81ea\u9002\u5e94\u751f\u6210 \u8bad\u7ec3 \u5355\u9636\u6bb5SFT SFT+RLHF \u591a\u8f6e\u8fed\u4ee3 \u6301\u7eed\u5b66\u4e60 \u5bf9\u9f50 \u7b80\u5355RLHF DPO\u7b80\u5316 Constitutional AI \u81ea\u4e3b\u5bf9\u9f50 \u8bc4\u4f30 \u9759\u6001\u57fa\u51c6 \u52a8\u6001\u8bc4\u6d4b \u5bf9\u6297\u6d4b\u8bd5 \u5b9e\u65f6\u76d1\u63a7"},{"location":"training/post-training/#_6","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"training/post-training/#1","title":"1. \u540e\u9884\u8bad\u7ec3\u6280\u672f","text":"<p>\u91cd\u70b9\u638c\u63e1\uff1a\u9886\u57df\u9002\u5e94\u548c\u77e5\u8bc6\u6ce8\u5165</p> <ul> <li>\u6838\u5fc3\u7406\u5ff5</li> <li>\u5728\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e4b\u95f4\u7684\u6865\u6881\u9636\u6bb5</li> <li>\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u6570\u636e\u7ee7\u7eed\u9884\u8bad\u7ec3</li> <li> <p>\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u589e\u5f3a\u4e13\u4e1a\u77e5\u8bc6</p> </li> <li> <p>\u6280\u672f\u8981\u70b9</p> </li> <li>\u6570\u636e\u914d\u6bd4\u7b56\u7565\uff1a\u9886\u57df\u6570\u636e\u4e0e\u901a\u7528\u6570\u636e1:5\u6df7\u5408</li> <li>\u8bad\u7ec3\u7b56\u7565\uff1a\u4f4e\u5b66\u4e60\u7387\u3001\u5355epoch\u3001\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8</li> <li> <p>\u9002\u7528\u573a\u666f\uff1a\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u91d1\u878d\u7b49\u4e13\u4e1a\u9886\u57df</p> </li> <li> <p>\u5b9e\u65bd\u6d41\u7a0b</p> </li> <li>\u9886\u57df\u8bed\u6599\u6536\u96c6\u4e0e\u6e05\u6d17</li> <li>\u4e0e\u901a\u7528\u6570\u636e\u6df7\u5408\u8bad\u7ec3</li> <li>\u80fd\u529b\u8bc4\u4f30\u4e0e\u8c03\u6574</li> </ul>"},{"location":"training/post-training/#2","title":"2. \u76d1\u7763\u5fae\u8c03\u65b9\u6cd5","text":"<p>\u91cd\u70b9\u638c\u63e1\uff1a\u6307\u4ee4\u5fae\u8c03\u548c\u683c\u5f0f\u5316\u8bad\u7ec3</p> <ul> <li>\u6307\u4ee4\u5fae\u8c03(Instruction Tuning)</li> <li>\u6307\u4ee4-\u56de\u7b54\u5bf9\u6570\u636e\u6784\u5efa</li> <li>\u591a\u4efb\u52a1\u7edf\u4e00\u683c\u5f0f\u8bbe\u8ba1</li> <li> <p>Template\u5de5\u7a0b\u548c\u683c\u5f0f\u6807\u51c6\u5316</p> </li> <li> <p>\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03</p> </li> <li>\u5355\u4efb\u52a1vs\u591a\u4efb\u52a1\u7b56\u7565</li> <li>\u6570\u636e\u589e\u5f3a\u6280\u672f</li> <li> <p>\u8d1f\u6837\u672c\u6316\u6398\u65b9\u6cd5</p> </li> <li> <p>\u8bad\u7ec3\u6280\u5de7</p> </li> <li>\u5b66\u4e60\u7387\u8c03\u5ea6\u7b56\u7565</li> <li>\u6570\u636e\u6df7\u5408\u6bd4\u4f8b</li> <li>\u8fc7\u62df\u5408\u9632\u6b62\u65b9\u6cd5</li> </ul>"},{"location":"training/post-training/#3","title":"3. \u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50","text":"<p>\u91cd\u70b9\u638c\u63e1\uff1aRLHF\u5b8c\u6574\u6d41\u7a0b</p> <ul> <li>\u4e09\u9636\u6bb5RLHF</li> <li>SFT\u76d1\u7763\u5fae\u8c03\u57fa\u7840</li> <li>\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u6280\u5de7</li> <li> <p>PPO\u7b56\u7565\u4f18\u5316\u5b9e\u73b0</p> </li> <li> <p>\u73b0\u4ee3\u5bf9\u9f50\u65b9\u6cd5</p> </li> <li>DPO\u76f4\u63a5\u504f\u597d\u4f18\u5316</li> <li>Constitutional AI\u89c4\u5219\u5bf9\u9f50</li> <li> <p>RLAIF\u81ea\u52a8\u5316\u53cd\u9988</p> </li> <li> <p>\u9ad8\u7ea7\u6280\u672f</p> </li> <li>\u591a\u76ee\u6807\u4f18\u5316</li> <li>\u5b89\u5168\u6027\u7ea6\u675f</li> <li>\u957f\u671f\u4e00\u81f4\u6027\u4fdd\u6301</li> </ul>"},{"location":"training/post-training/#4","title":"4. \u9ad8\u7ea7\u540e\u8bad\u7ec3\u6280\u672f","text":"<p>\u91cd\u70b9\u638c\u63e1\uff1a\u524d\u6cbf\u4f18\u5316\u65b9\u6cd5</p> <ul> <li>\u8fed\u4ee3\u4f18\u5316\u7b56\u7565</li> <li>\u591a\u8f6e\u6570\u636e\u751f\u6210\u4e0e\u7b5b\u9009</li> <li>\u6a21\u578b\u81ea\u4e3e\u5b66\u4e60</li> <li> <p>\u5728\u7ebf\u5b66\u4e60\u4e0e\u9002\u5e94</p> </li> <li> <p>\u6570\u636e\u5de5\u7a0b</p> </li> <li>\u5408\u6210\u6570\u636e\u751f\u6210</li> <li>\u8d28\u91cf\u8bc4\u4f30\u4e0e\u8fc7\u6ee4</li> <li> <p>\u591a\u6837\u6027\u4fdd\u8bc1\u673a\u5236</p> </li> <li> <p>\u6a21\u578b\u878d\u5408\u6280\u672f</p> </li> <li>\u6743\u91cd\u63d2\u503c\u65b9\u6cd5</li> <li>\u4e13\u5bb6\u6a21\u578b\u7ec4\u5408</li> <li>\u52a8\u6001\u8def\u7531\u673a\u5236</li> </ul>"},{"location":"training/post-training/#_7","title":"\ud83d\udd2c \u6838\u5fc3\u6280\u672f\u6df1\u5ea6\u89e3\u6790","text":""},{"location":"training/post-training/#_8","title":"\u540e\u8bad\u7ec3\u6570\u636e\u751f\u6210\u6d41\u7a0b","text":"<pre><code>class PostTrainingDataPipeline:\n    \"\"\"\u540e\u8bad\u7ec3\u6570\u636e\u751f\u6210\u6d41\u7a0b\"\"\"\n\n    def __init__(self, base_model, quality_threshold=0.8):\n        self.base_model = base_model\n        self.quality_threshold = quality_threshold\n        self.data_pipeline = []\n\n    def generate_instructions(self, seed_topics, num_per_topic=100):\n        \"\"\"\u751f\u6210\u591a\u6837\u5316\u6307\u4ee4\u6570\u636e\"\"\"\n\n        instruction_templates = [\n            \"\u8bf7\u89e3\u91ca{topic}\u7684\u57fa\u672c\u6982\u5ff5\",\n            \"\u5982\u4f55\u5728{topic}\u9886\u57df\u89e3\u51b3{problem}\",\n            \"\u5206\u6790{topic}\u4e2d\u7684{aspect}\",\n            \"\u6bd4\u8f83{topic}\u7684\u4e0d\u540c\u65b9\u6cd5\",\n            \"\u603b\u7ed3{topic}\u7684\u6700\u4f73\u5b9e\u8df5\"\n        ]\n\n        generated_instructions = []\n\n        for topic in seed_topics:\n            for template in instruction_templates:\n                for _ in range(num_per_topic // len(instruction_templates)):\n                    # \u4f7f\u7528\u6a21\u578b\u751f\u6210\u5177\u4f53\u6307\u4ee4\n                    prompt = f\"\u57fa\u4e8e\u6a21\u677f'{template}'\u548c\u4e3b\u9898'{topic}'\uff0c\u751f\u6210\u4e00\u4e2a\u5177\u4f53\u7684\u6307\u4ee4\uff1a\"\n\n                    instruction = self.base_model.generate(\n                        prompt, \n                        max_length=100,\n                        temperature=0.8\n                    )\n\n                    generated_instructions.append({\n                        'topic': topic,\n                        'instruction': instruction,\n                        'template': template\n                    })\n\n        return generated_instructions\n\n    def generate_responses(self, instructions):\n        \"\"\"\u4e3a\u6307\u4ee4\u751f\u6210\u9ad8\u8d28\u91cf\u56de\u7b54\"\"\"\n\n        instruction_response_pairs = []\n\n        for item in instructions:\n            instruction = item['instruction']\n\n            # \u751f\u6210\u591a\u4e2a\u5019\u9009\u56de\u7b54\n            candidates = []\n            for temp in [0.3, 0.7, 1.0]:  # \u4e0d\u540c\u6e29\u5ea6\u91c7\u6837\n                response = self.base_model.generate(\n                    instruction,\n                    max_length=512,\n                    temperature=temp,\n                    top_p=0.9\n                )\n                candidates.append(response)\n\n            # \u8d28\u91cf\u8bc4\u4f30\u9009\u62e9\u6700\u4f73\u56de\u7b54\n            best_response = self.select_best_response(instruction, candidates)\n\n            if self.evaluate_quality(instruction, best_response) &gt; self.quality_threshold:\n                instruction_response_pairs.append({\n                    'instruction': instruction,\n                    'response': best_response,\n                    'topic': item['topic'],\n                    'quality_score': self.evaluate_quality(instruction, best_response)\n                })\n\n        return instruction_response_pairs\n\n    def select_best_response(self, instruction, candidates):\n        \"\"\"\u9009\u62e9\u6700\u4f73\u56de\u7b54\"\"\"\n        scores = []\n\n        for response in candidates:\n            score = 0\n\n            # \u76f8\u5173\u6027\u8bc4\u5206\n            score += self.relevance_score(instruction, response) * 0.4\n\n            # \u5b8c\u6574\u6027\u8bc4\u5206\n            score += self.completeness_score(response) * 0.3\n\n            # \u6d41\u7545\u6027\u8bc4\u5206\n            score += self.fluency_score(response) * 0.3\n\n            scores.append(score)\n\n        best_idx = np.argmax(scores)\n        return candidates[best_idx]\n\n    def create_preference_pairs(self, instruction_response_pairs):\n        \"\"\"\u521b\u5efa\u504f\u597d\u5bf9\u6570\u636e\"\"\"\n        preference_pairs = []\n\n        for i, pair1 in enumerate(instruction_response_pairs):\n            for j, pair2 in enumerate(instruction_response_pairs):\n                if i &gt;= j or pair1['instruction'] != pair2['instruction']:\n                    continue\n\n                # \u57fa\u4e8e\u8d28\u91cf\u5206\u6570\u521b\u5efa\u504f\u597d\u5bf9\n                if pair1['quality_score'] &gt; pair2['quality_score']:\n                    preference_pairs.append({\n                        'prompt': pair1['instruction'],\n                        'chosen': pair1['response'],\n                        'rejected': pair2['response'],\n                        'quality_diff': pair1['quality_score'] - pair2['quality_score']\n                    })\n\n        return preference_pairs\n</code></pre>"},{"location":"training/post-training/#_9","title":"\u8fed\u4ee3\u8bad\u7ec3\u6846\u67b6","text":"<pre><code>class IterativePostTraining:\n    \"\"\"\u8fed\u4ee3\u540e\u8bad\u7ec3\u6846\u67b6\"\"\"\n\n    def __init__(self, base_model, iterations=5):\n        self.base_model = base_model\n        self.current_model = base_model\n        self.iterations = iterations\n        self.training_history = []\n\n    def iteration_cycle(self, iteration_num):\n        \"\"\"\u5355\u6b21\u8fed\u4ee3\u5468\u671f\"\"\"\n\n        print(f\"\u5f00\u59cb\u7b2c{iteration_num}\u8f6e\u8fed\u4ee3\u8bad\u7ec3...\")\n\n        # 1. \u6570\u636e\u751f\u6210\u9636\u6bb5\n        synthetic_data = self.generate_synthetic_data()\n\n        # 2. \u6570\u636e\u8d28\u91cf\u7b5b\u9009\n        high_quality_data = self.filter_high_quality_data(synthetic_data)\n\n        # 3. \u504f\u597d\u6570\u636e\u6784\u5efa\n        preference_data = self.build_preference_data(high_quality_data)\n\n        # 4. \u6a21\u578b\u8bad\u7ec3\n        improved_model = self.train_model(preference_data)\n\n        # 5. \u6a21\u578b\u8bc4\u4f30\n        eval_results = self.evaluate_model(improved_model)\n\n        # 6. \u66f4\u65b0\u5f53\u524d\u6700\u4f73\u6a21\u578b\n        if eval_results['overall_score'] &gt; self.get_current_score():\n            self.current_model = improved_model\n            print(f\"\u7b2c{iteration_num}\u8f6e\u8bad\u7ec3\u6210\u529f\uff0c\u6027\u80fd\u63d0\u5347!\")\n\n        # \u8bb0\u5f55\u8bad\u7ec3\u5386\u53f2\n        self.training_history.append({\n            'iteration': iteration_num,\n            'data_size': len(high_quality_data),\n            'eval_results': eval_results,\n            'improvement': eval_results['overall_score'] - self.get_current_score()\n        })\n\n        return improved_model, eval_results\n\n    def multi_iteration_training(self):\n        \"\"\"\u591a\u8f6e\u8fed\u4ee3\u8bad\u7ec3\"\"\"\n\n        for i in range(1, self.iterations + 1):\n            try:\n                model, results = self.iteration_cycle(i)\n\n                # \u65e9\u505c\u68c0\u67e5\n                if self.should_early_stop():\n                    print(f\"\u7b2c{i}\u8f6e\u540e\u8fbe\u5230\u6536\u655b\uff0c\u63d0\u524d\u505c\u6b62\")\n                    break\n\n            except Exception as e:\n                print(f\"\u7b2c{i}\u8f6e\u8bad\u7ec3\u5931\u8d25: {e}\")\n                continue\n\n        return self.current_model, self.training_history\n\n    def should_early_stop(self, patience=2):\n        \"\"\"\u65e9\u505c\u5224\u65ad\"\"\"\n        if len(self.training_history) &lt; patience + 1:\n            return False\n\n        recent_improvements = [\n            h['improvement'] for h in self.training_history[-patience:]\n        ]\n\n        # \u5982\u679c\u8fde\u7eed\u51e0\u8f6e\u6ca1\u6709\u663e\u8457\u63d0\u5347\uff0c\u5219\u65e9\u505c\n        return all(imp &lt; 0.01 for imp in recent_improvements)\n</code></pre>"},{"location":"training/post-training/#_10","title":"\u524d\u6cbf\u6a21\u578b\u540e\u8bad\u7ec3\u7b56\u7565","text":"<pre><code>class FrontierModelPostTraining:\n    \"\"\"\u524d\u6cbf\u6a21\u578b\u540e\u8bad\u7ec3\u7b56\u7565 (\u57fa\u4e8e2024\u5e74\u6700\u65b0\u7814\u7a76)\"\"\"\n\n    def __init__(self):\n        self.training_stages = [\n            \"post_pretraining\",\n            \"instruction_tuning\", \n            \"preference_optimization\",\n            \"safety_alignment\",\n            \"capability_enhancement\"\n        ]\n\n    def modern_post_training_recipe(self):\n        \"\"\"\u73b0\u4ee3\u540e\u8bad\u7ec3\u914d\u65b9\"\"\"\n\n        recipe = {\n            \"\u6570\u636e\u7b56\u7565\": {\n                \"\u5408\u6210\u6570\u636e\u6bd4\u4f8b\": \"70-80%\",\n                \"\u4eba\u5de5\u6570\u636e\u6bd4\u4f8b\": \"20-30%\",\n                \"\u6570\u636e\u8d28\u91cf\u7b5b\u9009\": \"\u591a\u8f6e\u8fc7\u6ee4\uff0c\u4fdd\u7559top 20%\",\n                \"\u591a\u6837\u6027\u4fdd\u8bc1\": \"topic clustering + balanced sampling\"\n            },\n\n            \"\u8bad\u7ec3\u7b56\u7565\": {\n                \"\u603b\u8f6e\u6570\": \"5-6\u8f6e\u8fed\u4ee3\",\n                \"\u6bcf\u8f6e\u6570\u636e\u91cf\": \"10K-100K samples\",\n                \"\u5b66\u4e60\u7387\u8c03\u5ea6\": \"cosine with warmup\",\n                \"\u6279\u91cf\u5927\u5c0f\": \"adaptive based on data quality\"\n            },\n\n            \"\u5bf9\u9f50\u65b9\u6cd5\": {\n                \"\u4e3b\u8981\u65b9\u6cd5\": \"RLHF + DPO hybrid\",\n                \"\u5b89\u5168\u7ea6\u675f\": \"Constitutional AI rules\",\n                \"\u8bc4\u4f30\u6307\u6807\": \"helpfulness + harmlessness + honesty\"\n            },\n\n            \"\u8d28\u91cf\u63a7\u5236\": {\n                \"\u81ea\u52a8\u8bc4\u4f30\": \"LLM-as-judge for initial filtering\",\n                \"\u4eba\u5de5\u9a8c\u8bc1\": \"critical samples manual review\", \n                \"A/B\u6d4b\u8bd5\": \"continuous model comparison\",\n                \"\u7ea2\u961f\u6d4b\u8bd5\": \"adversarial safety probing\"\n            }\n        }\n\n        return recipe\n\n    def data_quality_pyramid(self):\n        \"\"\"\u6570\u636e\u8d28\u91cf\u91d1\u5b57\u5854\"\"\"\n\n        return {\n            \"\u9876\u5c42 (5%)\": {\n                \"\u63cf\u8ff0\": \"\u4e13\u5bb6\u7ea7\u9ad8\u8d28\u91cf\u6570\u636e\",\n                \"\u6765\u6e90\": \"\u9886\u57df\u4e13\u5bb6\u6807\u6ce8 + \u7cbe\u5fc3\u8bbe\u8ba1prompt\",\n                \"\u7528\u9014\": \"\u5173\u952e\u80fd\u529b\u8bad\u7ec3 + \u6700\u7ec8\u5bf9\u9f50\",\n                \"\u6210\u672c\": \"\u6781\u9ad8\"\n            },\n\n            \"\u4e2d\u5c42 (25%)\": {\n                \"\u63cf\u8ff0\": \"\u7ecf\u8fc7\u7b5b\u9009\u7684\u4f18\u8d28\u5408\u6210\u6570\u636e\", \n                \"\u6765\u6e90\": \"\u9ad8\u8d28\u91cf\u6a21\u578b\u751f\u6210 + \u81ea\u52a8\u7b5b\u9009\",\n                \"\u7528\u9014\": \"\u4e3b\u8981\u80fd\u529b\u8bad\u7ec3 + \u4e00\u822c\u5bf9\u9f50\",\n                \"\u6210\u672c\": \"\u4e2d\u7b49\"\n            },\n\n            \"\u5e95\u5c42 (70%)\": {\n                \"\u63cf\u8ff0\": \"\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\",\n                \"\u6765\u6e90\": \"\u6279\u91cf\u751f\u6210 + \u57fa\u7840\u8fc7\u6ee4\",\n                \"\u7528\u9014\": \"\u57fa\u7840\u80fd\u529b\u8bad\u7ec3 + \u77e5\u8bc6\u83b7\u53d6\", \n                \"\u6210\u672c\": \"\u4f4e\"\n            }\n        }\n\n    def scalable_rlhf_approach(self):\n        \"\"\"\u53ef\u6269\u5c55\u7684RLHF\u65b9\u6cd5\"\"\"\n\n        approach = {\n            \"Phase 1 - \u6570\u636e\u6536\u96c6\": {\n                \"\u65b9\u6cd5\": \"\u5927\u89c4\u6a21\u5408\u6210\u6307\u4ee4\u751f\u6210\",\n                \"\u89c4\u6a21\": \"100K-1M instructions\",\n                \"\u8d28\u91cf\": \"\u81ea\u52a8\u8fc7\u6ee4 + \u91c7\u6837\u9a8c\u8bc1\"\n            },\n\n            \"Phase 2 - \u504f\u597d\u6807\u6ce8\": {\n                \"\u65b9\u6cd5\": \"AI\u8f85\u52a9 + \u4eba\u5de5\u9a8c\u8bc1\",\n                \"\u89c4\u6a21\": \"10K-50K preference pairs\",\n                \"\u8d28\u91cf\": \"\u591a\u6807\u6ce8\u8005\u4e00\u81f4\u6027\u68c0\u67e5\"\n            },\n\n            \"Phase 3 - \u5956\u52b1\u5efa\u6a21\": {\n                \"\u65b9\u6cd5\": \"robust reward model training\",\n                \"\u9a8c\u8bc1\": \"out-of-distribution testing\",\n                \"\u8fed\u4ee3\": \"\u5b9a\u671f\u66f4\u65b0\u5956\u52b1\u6a21\u578b\"\n            },\n\n            \"Phase 4 - \u7b56\u7565\u4f18\u5316\": {\n                \"\u65b9\u6cd5\": \"PPO + DPO\u7ed3\u5408\",\n                \"\u7ea6\u675f\": \"KL\u6563\u5ea6 + safety constraints\",\n                \"\u76d1\u63a7\": \"\u5b9e\u65f6\u6027\u80fd\u548c\u5b89\u5168\u6027\u8ffd\u8e2a\"\n            }\n        }\n\n        return approach\n</code></pre>"},{"location":"training/post-training/#_11","title":"\ud83d\udcca \u540e\u8bad\u7ec3\u6548\u679c\u8bc4\u4f30","text":""},{"location":"training/post-training/#_12","title":"\u7efc\u5408\u8bc4\u4f30\u6846\u67b6","text":"<pre><code>class PostTrainingEvaluator:\n    \"\"\"\u540e\u8bad\u7ec3\u6548\u679c\u8bc4\u4f30\u5668\"\"\"\n\n    def __init__(self):\n        self.evaluation_dimensions = [\n            \"capability\", \"alignment\", \"safety\", \"efficiency\"\n        ]\n\n    def comprehensive_evaluation(self, model_before, model_after, test_suites):\n        \"\"\"\u7efc\u5408\u8bc4\u4f30\u5bf9\u6bd4\"\"\"\n\n        results = {}\n\n        for dimension in self.evaluation_dimensions:\n            results[dimension] = self.evaluate_dimension(\n                dimension, model_before, model_after, test_suites[dimension]\n            )\n\n        # \u8ba1\u7b97\u603b\u4f53\u6539\u8fdb\u5206\u6570\n        overall_improvement = self.calculate_overall_improvement(results)\n\n        return {\n            'dimension_results': results,\n            'overall_improvement': overall_improvement,\n            'recommendations': self.generate_recommendations(results)\n        }\n\n    def evaluate_capability(self, model, test_suite):\n        \"\"\"\u80fd\u529b\u8bc4\u4f30\"\"\"\n\n        capability_scores = {}\n\n        # \u8bed\u8a00\u7406\u89e3\u80fd\u529b\n        capability_scores['understanding'] = self.test_reading_comprehension(model, test_suite['reading'])\n\n        # \u751f\u6210\u80fd\u529b\n        capability_scores['generation'] = self.test_text_generation(model, test_suite['generation'])\n\n        # \u63a8\u7406\u80fd\u529b\n        capability_scores['reasoning'] = self.test_logical_reasoning(model, test_suite['reasoning'])\n\n        # \u77e5\u8bc6\u8fd0\u7528\n        capability_scores['knowledge'] = self.test_factual_knowledge(model, test_suite['knowledge'])\n\n        return capability_scores\n\n    def evaluate_alignment(self, model, test_suite):\n        \"\"\"\u5bf9\u9f50\u7a0b\u5ea6\u8bc4\u4f30\"\"\"\n\n        alignment_scores = {}\n\n        # \u6709\u7528\u6027 (Helpfulness)\n        alignment_scores['helpfulness'] = self.test_helpfulness(model, test_suite['helpful'])\n\n        # \u65e0\u5bb3\u6027 (Harmlessness)  \n        alignment_scores['harmlessness'] = self.test_harmlessness(model, test_suite['harmful'])\n\n        # \u8bda\u5b9e\u6027 (Honesty)\n        alignment_scores['honesty'] = self.test_honesty(model, test_suite['truthfulness'])\n\n        return alignment_scores\n\n    def benchmark_comparison(self, models, benchmarks):\n        \"\"\"\u57fa\u51c6\u6d4b\u8bd5\u5bf9\u6bd4\"\"\"\n\n        benchmark_results = {}\n\n        for benchmark_name, benchmark_data in benchmarks.items():\n            benchmark_results[benchmark_name] = {}\n\n            for model_name, model in models.items():\n                score = self.run_benchmark(model, benchmark_data)\n                benchmark_results[benchmark_name][model_name] = score\n\n        return benchmark_results\n\n    def generate_training_report(self, training_history, final_results):\n        \"\"\"\u751f\u6210\u8bad\u7ec3\u62a5\u544a\"\"\"\n\n        report = {\n            \"\u8bad\u7ec3\u6982\u51b5\": {\n                \"\u603b\u8f6e\u6570\": len(training_history),\n                \"\u603b\u8bad\u7ec3\u65f6\u95f4\": sum(h.get('training_time', 0) for h in training_history),\n                \"\u6570\u636e\u4f7f\u7528\u91cf\": sum(h.get('data_size', 0) for h in training_history),\n                \"\u6700\u7ec8\u6539\u8fdb\u5e45\u5ea6\": final_results['overall_improvement']\n            },\n\n            \"\u6027\u80fd\u8d8b\u52bf\": {\n                \"\u80fd\u529b\u63d0\u5347\u66f2\u7ebf\": [h['eval_results']['capability'] for h in training_history],\n                \"\u5bf9\u9f50\u6539\u5584\u66f2\u7ebf\": [h['eval_results']['alignment'] for h in training_history],\n                \"\u5b89\u5168\u6027\u53d8\u5316\": [h['eval_results']['safety'] for h in training_history]\n            },\n\n            \"\u5173\u952e\u53d1\u73b0\": self.extract_key_insights(training_history, final_results),\n\n            \"\u6539\u8fdb\u5efa\u8bae\": final_results['recommendations']\n        }\n\n        return report\n</code></pre>"},{"location":"training/post-training/#_13","title":"\ud83c\udfaf \u9762\u8bd5\u95ee\u7b54\u603b\u7ed3","text":""},{"location":"training/post-training/#q1","title":"Q1: \u540e\u8bad\u7ec3\u5305\u542b\u54ea\u4e9b\u5173\u952e\u9636\u6bb5\uff1f","text":"<p>A: \u73b0\u4ee3\u540e\u8bad\u7ec3\u901a\u5e38\u5305\u542b4\u4e2a\u6838\u5fc3\u9636\u6bb5\uff1a 1. \u540e\u9884\u8bad\u7ec3: \u9886\u57df\u9002\u5e94\u548c\u77e5\u8bc6\u6ce8\u5165 2. \u76d1\u7763\u5fae\u8c03: \u6307\u4ee4\u9075\u5faa\u548c\u683c\u5f0f\u89c4\u8303 3. \u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50: \u4eba\u7c7b\u504f\u597d\u4f18\u5316 4. \u5b89\u5168\u6027\u5bf9\u9f50: Constitutional AI\u7b49\u65b9\u6cd5</p>"},{"location":"training/post-training/#q2","title":"Q2: \u540e\u9884\u8bad\u7ec3\u4e0e\u76d1\u7763\u5fae\u8c03\u7684\u533a\u522b\uff1f","text":"<p>A:  - \u540e\u9884\u8bad\u7ec3: \u7ee7\u7eed\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f7f\u7528\u9886\u57df\u6570\u636e\uff0c\u76ee\u6807\u662f\u77e5\u8bc6\u6ce8\u5165 - \u76d1\u7763\u5fae\u8c03: \u4efb\u52a1\u5bfc\u5411\u8bad\u7ec3\uff0c\u4f7f\u7528\u6307\u4ee4-\u56de\u7b54\u5bf9\uff0c\u76ee\u6807\u662f\u80fd\u529b\u9002\u5e94 - \u6570\u636e\u89c4\u6a21: \u540e\u9884\u8bad\u7ec3\u901a\u5e38\u9700\u8981B\u7ea7token\uff0cSFT\u9700\u8981K-M\u7ea7\u6837\u672c</p>"},{"location":"training/post-training/#q3","title":"Q3: \u5982\u4f55\u8bbe\u8ba1\u6709\u6548\u7684\u540e\u8bad\u7ec3\u6570\u636e\u6d41\u7a0b\uff1f","text":"<p>A: - \u6570\u636e\u8d28\u91cf\u91d1\u5b57\u5854: 5%\u4e13\u5bb6\u6570\u636e + 25%\u4f18\u8d28\u5408\u6210\u6570\u636e + 70%\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e - \u8fed\u4ee3\u751f\u6210: \u591a\u8f6e\u6570\u636e\u751f\u6210\u3001\u7b5b\u9009\u3001\u8bad\u7ec3\u5faa\u73af - \u8d28\u91cf\u63a7\u5236: LLM-as-judge + \u4eba\u5de5\u9a8c\u8bc1 + A/B\u6d4b\u8bd5</p>"},{"location":"training/post-training/#q4-rlhf","title":"Q4: RLHF\u5728\u540e\u8bad\u7ec3\u4e2d\u7684\u5177\u4f53\u4f5c\u7528\uff1f","text":"<p>A: - \u6838\u5fc3\u4ef7\u503c: \u5c06\u4eba\u7c7b\u504f\u597d\u8f6c\u5316\u4e3a\u53ef\u4f18\u5316\u7684\u76ee\u6807\u51fd\u6570 - \u5b9e\u9645\u6548\u679c: \"RLHF\u6bd4\u6307\u4ee4\u5fae\u8c03\u66f4\u53ef\u6269\u5c55\uff0c\u6210\u672c\u66f4\u4f4e\uff0c\u6548\u679c\u66f4\u597d\" - \u73b0\u4ee3\u8d8b\u52bf: RLHF + DPO\u6df7\u5408\uff0c\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316</p>"},{"location":"training/post-training/#_14","title":"\ud83d\ude80 \u5b66\u4e60\u5efa\u8bae","text":"<ol> <li>\u5168\u5c40\u89c6\u89d2: \u7406\u89e3\u540e\u8bad\u7ec3\u5728\u6574\u4e2a\u6a21\u578b\u751f\u547d\u5468\u671f\u4e2d\u7684\u4f5c\u7528</li> <li>\u5b9e\u8df5\u5bfc\u5411: \u91cd\u70b9\u638c\u63e1\u6570\u636e\u5de5\u7a0b\u548c\u8bad\u7ec3\u6d41\u7a0b</li> <li>\u524d\u6cbf\u8ddf\u8e2a: \u5173\u6ce82024\u5e74\u7684\u8fed\u4ee3\u8bad\u7ec3\u548c\u5408\u6210\u6570\u636e\u6280\u672f</li> <li>\u6548\u679c\u8bc4\u4f30: \u5b66\u4f1a\u5efa\u7acb\u5b8c\u6574\u7684\u8bc4\u4f30\u4f53\u7cfb</li> </ol> <p>\u540e\u8bad\u7ec3\u6280\u672f\u662f\u73b0\u4ee3LLM\u7684\u6838\u5fc3\u7ade\u4e89\u529b\uff0c\u4e5f\u662f2024\u5e74\u6700\u6d3b\u8dc3\u7684\u7814\u7a76\u9886\u57df\uff01</p>"},{"location":"training/post-training/post-pretraining/","title":"\u540e\u9884\u8bad\u7ec3\u6280\u672f","text":""},{"location":"training/post-training/post-pretraining/#_2","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3\u540e\u9884\u8bad\u7ec3(Post-Pretraining)\u6280\u672f\uff0c\u638c\u63e1\u9886\u57df\u9002\u5e94\u548c\u77e5\u8bc6\u6ce8\u5165\u7684\u6838\u5fc3\u65b9\u6cd5\uff0c\u5b66\u4f1a\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u57fa\u7840\u4e0a\u8fdb\u884c\u6709\u6548\u7684\u9886\u57df\u589e\u5f3a\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - \u4ec0\u4e48\u662f\u540e\u9884\u8bad\u7ec3\uff1f\u4e0e\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6709\u4ec0\u4e48\u533a\u522b\uff1f - \u540e\u9884\u8bad\u7ec3\u7684\u6570\u636e\u914d\u6bd4\u7b56\u7565\u662f\u4ec0\u4e48\uff1f - \u5982\u4f55\u9632\u6b62\u540e\u9884\u8bad\u7ec3\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\uff1f - \u540e\u9884\u8bad\u7ec3\u5728\u4ec0\u4e48\u573a\u666f\u4e0b\u6700\u6709\u4ef7\u503c\uff1f</p>"},{"location":"training/post-training/post-pretraining/#_3","title":"\ud83c\udfd7\ufe0f \u540e\u9884\u8bad\u7ec3\u6838\u5fc3\u6982\u5ff5","text":""},{"location":"training/post-training/post-pretraining/#_4","title":"\u5b9a\u4e49\u4e0e\u5b9a\u4f4d","text":"<p>\u540e\u9884\u8bad\u7ec3(Post-Pretraining)\u662f\u6307\u5728\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u4f7f\u7528\u7279\u5b9a\u9886\u57df\u6570\u636e\u8fdb\u884c\u7ee7\u7eed\u9884\u8bad\u7ec3\u7684\u8fc7\u7a0b\uff0c\u76ee\u6807\u662f\u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u589e\u5f3a\u7279\u5b9a\u9886\u57df\u7684\u77e5\u8bc6\u548c\u80fd\u529b\u3002</p> <pre><code>\u8bad\u7ec3\u9636\u6bb5\u5b9a\u4f4d\u56fe\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    \u5927\u6a21\u578b\u5b8c\u6574\u8bad\u7ec3\u6d41\u7a0b                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u9884\u8bad\u7ec3        \u540e\u9884\u8bad\u7ec3       \u76d1\u7763\u5fae\u8c03      \u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50          \u2502\n\u2502 Pretraining \u2192 Post-Pretraining \u2192 SFT \u2192 RLHF                    \u2502\n\u2502                                                                 \u2502\n\u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502 \u2502\u901a\u7528\u8bed\u6599  \u2502   \u2502 \u9886\u57df+\u901a\u7528    \u2502   \u2502\u6307\u4ee4\u5bf9  \u2502   \u2502 \u504f\u597d\u6570\u636e     \u2502      \u2502\n\u2502 \u2502CommonCrawl\u2502  \u2502 \u533b\u7597+Common \u2502   \u2502Instruct\u2502   \u2502 Human Pref  \u2502      \u2502\n\u2502 \u2502C4, \u4e66\u7c4d\u7b49 \u2502   \u2502 \u6cd5\u5f8b+Books  \u2502   \u2502Dataset \u2502   \u2502 RLHF Data   \u2502      \u2502\n\u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502      \u25b2              \u25b2             \u25b2             \u25b2               \u2502\n\u2502  \u57fa\u7840\u80fd\u529b        \u9886\u57df\u589e\u5f3a       \u4efb\u52a1\u9002\u5e94       \u4ef7\u503c\u5bf9\u9f50            \u2502\n\u2502  \u8bed\u8a00\u7406\u89e3        \u4e13\u4e1a\u77e5\u8bc6       \u6307\u4ee4\u9075\u5faa       \u4eba\u7c7b\u504f\u597d            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/post-training/post-pretraining/#_5","title":"\u6838\u5fc3\u4ef7\u503c\u4e3b\u5f20","text":"<ol> <li>\u77e5\u8bc6\u6ce8\u5165: \u5411\u6a21\u578b\u6ce8\u5165\u7279\u5b9a\u9886\u57df\u7684\u4e13\u4e1a\u77e5\u8bc6</li> <li>\u80fd\u529b\u4fdd\u6301: \u7ef4\u6301\u539f\u6709\u7684\u901a\u7528\u8bed\u8a00\u80fd\u529b</li> <li>\u6210\u672c\u6548\u76ca: \u76f8\u6bd4\u4ece\u5934\u8bad\u7ec3\uff0c\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c</li> <li>\u7075\u6d3b\u9002\u5e94: \u53ef\u9488\u5bf9\u4e0d\u540c\u9886\u57df\u5feb\u901f\u5b9a\u5236</li> </ol>"},{"location":"training/post-training/post-pretraining/#_6","title":"\ud83d\udcca \u6280\u672f\u539f\u7406\u4e0e\u7b56\u7565","text":""},{"location":"training/post-training/post-pretraining/#_7","title":"\u6570\u636e\u914d\u6bd4\u9ec4\u91d1\u6cd5\u5219","text":"<p>\u57fa\u4e8e\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6700\u4f73\u5b9e\u8df5\uff1a</p> <pre><code>class PostPretrainingDataStrategy:\n    \"\"\"\u540e\u9884\u8bad\u7ec3\u6570\u636e\u7b56\u7565\"\"\"\n\n    def __init__(self, domain_data_size, general_data_size):\n        self.domain_data = domain_data_size\n        self.general_data = general_data_size\n\n    def optimal_mixing_ratio(self):\n        \"\"\"\u6700\u4f18\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\"\"\"\n\n        # \u9ec4\u91d1\u6bd4\u4f8b: \u9886\u57df\u6570\u636e:\u901a\u7528\u6570\u636e = 1:5\n        recommended_ratio = {\n            \"\u9886\u57df\u7279\u5b9a\u6570\u636e\": \"16.7%\",\n            \"\u901a\u7528\u6570\u636e\": \"83.3%\", \n            \"\u6df7\u5408\u7b56\u7565\": \"\u6bcf\u4e2abatch\u5185\u968f\u673a\u6df7\u5408\",\n            \"\u7406\u8bba\u4f9d\u636e\": \"\u5e73\u8861\u4e13\u4e1a\u6027\u4e0e\u901a\u7528\u6027\uff0c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\"\n        }\n\n        return recommended_ratio\n\n    def data_preprocessing_pipeline(self, domain_corpus, general_corpus):\n        \"\"\"\u6570\u636e\u9884\u5904\u7406\u6d41\u7a0b\"\"\"\n\n        # 1. \u9886\u57df\u6570\u636e\u5904\u7406\n        domain_processed = self.process_domain_data(domain_corpus)\n\n        # 2. \u901a\u7528\u6570\u636e\u91c7\u6837\n        general_sampled = self.sample_general_data(general_corpus, \n                                                  target_size=len(domain_processed) * 5)\n\n        # 3. \u6570\u636e\u53bb\u91cd\n        dedup_domain = self.deduplicate_data(domain_processed)\n        dedup_general = self.deduplicate_data(general_sampled)\n\n        # 4. \u8d28\u91cf\u8fc7\u6ee4\n        high_quality_domain = self.quality_filter(dedup_domain, threshold=0.8)\n        high_quality_general = self.quality_filter(dedup_general, threshold=0.7)\n\n        # 5. \u6df7\u5408\u7b56\u7565\n        mixed_dataset = self.create_mixed_dataset(high_quality_domain, high_quality_general)\n\n        return mixed_dataset\n\n    def create_mixed_dataset(self, domain_data, general_data):\n        \"\"\"\u521b\u5efa\u6df7\u5408\u6570\u636e\u96c6\"\"\"\n        import random\n\n        mixed_data = []\n\n        # \u786e\u4fdd\u6bcf\u4e2abatch\u90fd\u6709\u9002\u5f53\u6bd4\u4f8b\u7684\u6df7\u5408\n        batch_size = 1000\n        domain_per_batch = batch_size // 6  # ~16.7%\n        general_per_batch = batch_size - domain_per_batch  # ~83.3%\n\n        domain_idx, general_idx = 0, 0\n\n        while domain_idx &lt; len(domain_data) and general_idx &lt; len(general_data):\n            batch_data = []\n\n            # \u6dfb\u52a0\u9886\u57df\u6570\u636e\n            batch_data.extend(domain_data[domain_idx:domain_idx + domain_per_batch])\n            domain_idx += domain_per_batch\n\n            # \u6dfb\u52a0\u901a\u7528\u6570\u636e\n            batch_data.extend(general_data[general_idx:general_idx + general_per_batch])\n            general_idx += general_per_batch\n\n            # \u968f\u673a\u6253\u4e71batch\u5185\u6570\u636e\n            random.shuffle(batch_data)\n            mixed_data.extend(batch_data)\n\n        return mixed_data\n\n# \u5b9e\u9645\u5e94\u7528\u793a\u4f8b\nstrategy = PostPretrainingDataStrategy(\n    domain_data_size=1000000,  # 100\u4e07\u6761\u533b\u7597\u6570\u636e\n    general_data_size=5000000  # 500\u4e07\u6761\u901a\u7528\u6570\u636e\n)\n\noptimal_ratio = strategy.optimal_mixing_ratio()\nprint(\"\u6700\u4f18\u6df7\u5408\u6bd4\u4f8b:\", optimal_ratio)\n</code></pre>"},{"location":"training/post-training/post-pretraining/#_8","title":"\u8bad\u7ec3\u8d85\u53c2\u6570\u914d\u7f6e","text":"<pre><code>class PostPretrainingConfig:\n    \"\"\"\u540e\u9884\u8bad\u7ec3\u914d\u7f6e\u7ba1\u7406\"\"\"\n\n    def __init__(self, model_size=\"7B\"):\n        self.model_size = model_size\n\n    def get_training_config(self):\n        \"\"\"\u83b7\u53d6\u8bad\u7ec3\u914d\u7f6e\"\"\"\n\n        base_config = {\n            # \u5b66\u4e60\u7387\u7b56\u7565\n            \"learning_rate\": 1e-5,  # \u6bd4\u9884\u8bad\u7ec3\u4f4e1-2\u4e2a\u6570\u91cf\u7ea7\n            \"min_learning_rate\": 1e-6,\n            \"warmup_ratio\": 0.03,   # 3%\u7684warmup\n            \"lr_scheduler\": \"cosine\",\n\n            # \u8bad\u7ec3\u7b56\u7565\n            \"epochs\": 1,            # \u5355\u8f6e\u8bad\u7ec3\u907f\u514d\u8fc7\u62df\u5408\n            \"batch_size\": self._get_batch_size(),\n            \"gradient_accumulation\": self._get_grad_accumulation(),\n            \"max_grad_norm\": 1.0,\n\n            # \u6570\u636e\u7b56\u7565\n            \"max_seq_length\": 2048,\n            \"data_mixture_ratio\": {\"domain\": 0.167, \"general\": 0.833},\n\n            # \u6b63\u5219\u5316\n            \"weight_decay\": 0.1,\n            \"dropout\": 0.1,\n\n            # \u76d1\u63a7\u6307\u6807\n            \"eval_steps\": 500,\n            \"save_steps\": 1000,\n            \"logging_steps\": 100\n        }\n\n        return base_config\n\n    def _get_batch_size(self):\n        \"\"\"\u6839\u636e\u6a21\u578b\u5927\u5c0f\u8c03\u6574batch size\"\"\"\n        size_mapping = {\n            \"1B\": 64,\n            \"3B\": 32, \n            \"7B\": 16,\n            \"13B\": 8,\n            \"30B\": 4,\n            \"70B\": 2\n        }\n        return size_mapping.get(self.model_size, 16)\n\n    def _get_grad_accumulation(self):\n        \"\"\"\u68af\u5ea6\u7d2f\u79ef\u6b65\u6570\"\"\"\n        # \u786e\u4fdd\u6709\u6548batch size\u8db3\u591f\u5927\n        target_effective_batch = 256\n        actual_batch = self._get_batch_size()\n        return max(1, target_effective_batch // actual_batch)\n\n    def catastrophic_forgetting_prevention(self):\n        \"\"\"\u707e\u96be\u6027\u9057\u5fd8\u9632\u6b62\u7b56\u7565\"\"\"\n\n        strategies = {\n            \"\u6570\u636e\u5c42\u9762\": {\n                \"\u901a\u7528\u6570\u636e\u6df7\u5408\": \"\u4fdd\u630183.3%\u901a\u7528\u6570\u636e\u6bd4\u4f8b\",\n                \"\u6570\u636e\u8d28\u91cf\": \"\u786e\u4fdd\u9886\u57df\u6570\u636e\u8d28\u91cf\uff0c\u907f\u514d\u566a\u58f0\u6570\u636e\",\n                \"\u5e8f\u5217\u957f\u5ea6\": \"\u4f7f\u7528\u5b8c\u6574\u5e8f\u5217\uff0c\u907f\u514d\u622a\u65ad\u4e22\u5931\u4e0a\u4e0b\u6587\"\n            },\n\n            \"\u8bad\u7ec3\u5c42\u9762\": {\n                \"\u5b66\u4e60\u7387\": \"\u4f7f\u7528\u8f83\u5c0f\u5b66\u4e60\u7387(1e-5)\uff0c\u6e29\u548c\u66f4\u65b0\",\n                \"\u8bad\u7ec3\u8f6e\u6b21\": \"\u5355\u8f6e\u8bad\u7ec3\uff0c\u907f\u514d\u8fc7\u5ea6\u62df\u5408\u9886\u57df\u6570\u636e\",\n                \"\u68af\u5ea6\u88c1\u526a\": \"\u9632\u6b62\u68af\u5ea6\u7206\u70b8\u5bfc\u81f4\u7684\u7a81\u53d8\"\n            },\n\n            \"\u8bc4\u4f30\u5c42\u9762\": {\n                \"\u901a\u7528\u80fd\u529b\u76d1\u63a7\": \"\u5b9a\u671f\u5728\u901a\u7528\u57fa\u51c6\u4e0a\u8bc4\u4f30\",\n                \"\u9886\u57df\u80fd\u529b\u8bc4\u4f30\": \"\u9a8c\u8bc1\u9886\u57df\u589e\u5f3a\u6548\u679c\",\n                \"\u65e9\u505c\u673a\u5236\": \"\u53d1\u73b0\u9057\u5fd8\u65f6\u53ca\u65f6\u505c\u6b62\"\n            }\n        }\n\n        return strategies\n\n# \u914d\u7f6e\u793a\u4f8b\nconfig = PostPretrainingConfig(\"7B\")\ntraining_config = config.get_training_config()\nforgetting_prevention = config.catastrophic_forgetting_prevention()\n\nprint(\"\u8bad\u7ec3\u914d\u7f6e:\", training_config)\nprint(\"\u9057\u5fd8\u9632\u6b62\u7b56\u7565:\", forgetting_prevention)\n</code></pre>"},{"location":"training/post-training/post-pretraining/#_9","title":"\ud83d\udcbb \u5b9e\u9645\u5b9e\u73b0\u6848\u4f8b","text":""},{"location":"training/post-training/post-pretraining/#_10","title":"\u533b\u7597\u9886\u57df\u540e\u9884\u8bad\u7ec3\u793a\u4f8b","text":"<pre><code>import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\nfrom datasets import Dataset, concatenate_datasets\nimport numpy as np\n\nclass MedicalPostPretraining:\n    \"\"\"\u533b\u7597\u9886\u57df\u540e\u9884\u8bad\u7ec3\u5b9e\u73b0\"\"\"\n\n    def __init__(self, base_model_name=\"meta-llama/Llama-2-7b-hf\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            base_model_name,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            load_in_8bit=True  # \u5185\u5b58\u4f18\u5316\n        )\n\n        # \u6dfb\u52a0padding token\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def prepare_medical_data(self, medical_texts, general_texts):\n        \"\"\"\u51c6\u5907\u533b\u7597\u9886\u57df\u6df7\u5408\u6570\u636e\"\"\"\n\n        # 1. \u5904\u7406\u533b\u7597\u6587\u672c\n        medical_processed = []\n        for text in medical_texts:\n            if self._is_high_quality_medical(text):\n                medical_processed.append({\"text\": text, \"source\": \"medical\"})\n\n        # 2. \u91c7\u6837\u901a\u7528\u6587\u672c (5\u500d\u533b\u7597\u6570\u636e\u91cf)\n        target_general_size = len(medical_processed) * 5\n        general_sampled = np.random.choice(\n            general_texts, \n            size=min(target_general_size, len(general_texts)),\n            replace=False\n        )\n\n        general_processed = [\n            {\"text\": text, \"source\": \"general\"} \n            for text in general_sampled\n        ]\n\n        # 3. \u521b\u5efa\u6df7\u5408\u6570\u636e\u96c6\n        all_data = medical_processed + general_processed\n        np.random.shuffle(all_data)  # \u968f\u673a\u6253\u4e71\n\n        return Dataset.from_list(all_data)\n\n    def _is_high_quality_medical(self, text):\n        \"\"\"\u533b\u7597\u6587\u672c\u8d28\u91cf\u68c0\u67e5\"\"\"\n\n        # \u57fa\u672c\u957f\u5ea6\u68c0\u67e5\n        if len(text.split()) &lt; 50 or len(text.split()) &gt; 2000:\n            return False\n\n        # \u533b\u7597\u672f\u8bed\u5bc6\u5ea6\u68c0\u67e5\n        medical_terms = [\n            \"patient\", \"diagnosis\", \"treatment\", \"symptom\", \"disease\",\n            \"medication\", \"therapy\", \"clinical\", \"medical\", \"health\",\n            \"\u75c5\u4eba\", \"\u8bca\u65ad\", \"\u6cbb\u7597\", \"\u75c7\u72b6\", \"\u75be\u75c5\", \"\u836f\u7269\", \"\u4e34\u5e8a\", \"\u533b\u7597\"\n        ]\n\n        term_count = sum(1 for term in medical_terms if term.lower() in text.lower())\n        term_density = term_count / len(text.split())\n\n        # \u81f3\u5c11\u5305\u542b\u4e00\u5b9a\u5bc6\u5ea6\u7684\u533b\u7597\u672f\u8bed\n        return term_density &gt; 0.02\n\n    def tokenize_function(self, examples):\n        \"\"\"\u6570\u636etokenization\"\"\"\n\n        # Tokenize\u6587\u672c\n        tokenized = self.tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding=False,\n            max_length=2048,\n            return_tensors=None\n        )\n\n        # \u5bf9\u4e8e\u56e0\u679c\u8bed\u8a00\u6a21\u578b\uff0clabels\u5c31\u662finput_ids\n        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n\n        return tokenized\n\n    def create_data_collator(self):\n        \"\"\"\u521b\u5efa\u6570\u636e\u6574\u7406\u5668\"\"\"\n        from transformers import DataCollatorForLanguageModeling\n\n        return DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer,\n            mlm=False,  # \u4e0d\u4f7f\u7528masked language modeling\n            pad_to_multiple_of=8  # \u4f18\u5316GPU\u5185\u5b58\u5bf9\u9f50\n        )\n\n    def train(self, train_dataset, eval_dataset, output_dir):\n        \"\"\"\u6267\u884c\u540e\u9884\u8bad\u7ec3\"\"\"\n\n        # \u8bad\u7ec3\u914d\u7f6e\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n\n            # \u57fa\u672c\u8bbe\u7f6e\n            num_train_epochs=1,\n            per_device_train_batch_size=4,\n            per_device_eval_batch_size=4,\n            gradient_accumulation_steps=16,  # \u6709\u6548batch size = 64\n\n            # \u5b66\u4e60\u7387\u8bbe\u7f6e\n            learning_rate=1e-5,\n            warmup_ratio=0.03,\n            lr_scheduler_type=\"cosine\",\n\n            # \u4f18\u5316\u8bbe\u7f6e\n            bf16=True,  # \u4f7f\u7528bfloat16\n            gradient_checkpointing=True,  # \u8282\u7701\u663e\u5b58\n            max_grad_norm=1.0,\n            weight_decay=0.1,\n\n            # \u8bc4\u4f30\u548c\u4fdd\u5b58\n            evaluation_strategy=\"steps\",\n            eval_steps=500,\n            save_steps=1000,\n            save_total_limit=3,\n\n            # \u65e5\u5fd7\u8bbe\u7f6e\n            logging_steps=100,\n            logging_dir=f\"{output_dir}/logs\",\n            report_to=\"tensorboard\",\n\n            # \u5176\u4ed6\u8bbe\u7f6e\n            dataloader_pin_memory=True,\n            remove_unused_columns=False,\n        )\n\n        # \u521b\u5efa\u8bad\u7ec3\u5668\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=train_dataset,\n            eval_dataset=eval_dataset,\n            data_collator=self.create_data_collator(),\n            tokenizer=self.tokenizer,\n        )\n\n        # \u5f00\u59cb\u8bad\u7ec3\n        trainer.train()\n\n        # \u4fdd\u5b58\u6a21\u578b\n        trainer.save_model()\n\n        return trainer\n\n# \u4f7f\u7528\u793a\u4f8b\ndef run_medical_post_pretraining():\n    \"\"\"\u8fd0\u884c\u533b\u7597\u9886\u57df\u540e\u9884\u8bad\u7ec3\"\"\"\n\n    # 1. \u51c6\u5907\u6570\u636e (\u793a\u4f8b\u6570\u636e)\n    medical_texts = [\n        \"\u60a3\u8005\u4e3b\u8bc9\u80f8\u75db3\u5929\uff0c\u4f34\u6709\u547c\u5438\u56f0\u96be\u3002\u4f53\u683c\u68c0\u67e5\u53d1\u73b0\u5fc3\u7387\u589e\u5feb...\",\n        \"The patient presents with acute myocardial infarction...\",\n        # ... \u66f4\u591a\u533b\u7597\u6587\u672c\n    ]\n\n    general_texts = [\n        \"Today is a beautiful day for outdoor activities...\",\n        \"Machine learning is transforming various industries...\",\n        # ... \u66f4\u591a\u901a\u7528\u6587\u672c\n    ]\n\n    # 2. \u521d\u59cb\u5316\u8bad\u7ec3\u5668\n    trainer = MedicalPostPretraining()\n\n    # 3. \u51c6\u5907\u6570\u636e\n    train_data = trainer.prepare_medical_data(medical_texts, general_texts)\n    tokenized_train = train_data.map(trainer.tokenize_function, batched=True)\n\n    # \u5212\u5206\u8bad\u7ec3\u548c\u9a8c\u8bc1\u96c6\n    train_test_split = tokenized_train.train_test_split(test_size=0.1)\n    train_dataset = train_test_split[\"train\"]\n    eval_dataset = train_test_split[\"test\"]\n\n    # 4. \u6267\u884c\u8bad\u7ec3\n    trained_model = trainer.train(\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        output_dir=\"./medical_llama_7b\"\n    )\n\n    return trained_model\n\n# \u6267\u884c\u8bad\u7ec3\n# trained_model = run_medical_post_pretraining()\n</code></pre>"},{"location":"training/post-training/post-pretraining/#_11","title":"\u6548\u679c\u8bc4\u4f30\u4e0e\u76d1\u63a7","text":"<pre><code>class PostPretrainingEvaluator:\n    \"\"\"\u540e\u9884\u8bad\u7ec3\u6548\u679c\u8bc4\u4f30\u5668\"\"\"\n\n    def __init__(self, base_model, finetuned_model, tokenizer):\n        self.base_model = base_model\n        self.finetuned_model = finetuned_model\n        self.tokenizer = tokenizer\n\n    def domain_knowledge_evaluation(self, domain_questions):\n        \"\"\"\u9886\u57df\u77e5\u8bc6\u8bc4\u4f30\"\"\"\n\n        results = {\n            \"base_model_scores\": [],\n            \"finetuned_model_scores\": [],\n            \"improvements\": []\n        }\n\n        for question in domain_questions:\n            # \u57fa\u7840\u6a21\u578b\u56de\u7b54\n            base_answer = self.generate_answer(self.base_model, question)\n            base_score = self.evaluate_domain_answer(question, base_answer)\n\n            # \u5fae\u8c03\u6a21\u578b\u56de\u7b54\n            ft_answer = self.generate_answer(self.finetuned_model, question)\n            ft_score = self.evaluate_domain_answer(question, ft_answer)\n\n            # \u8bb0\u5f55\u7ed3\u679c\n            results[\"base_model_scores\"].append(base_score)\n            results[\"finetuned_model_scores\"].append(ft_score)\n            results[\"improvements\"].append(ft_score - base_score)\n\n        # \u8ba1\u7b97\u7edf\u8ba1\u6307\u6807\n        avg_improvement = np.mean(results[\"improvements\"])\n        improvement_std = np.std(results[\"improvements\"])\n\n        return {\n            \"average_improvement\": avg_improvement,\n            \"improvement_std\": improvement_std,\n            \"improvement_rate\": np.mean(np.array(results[\"improvements\"]) &gt; 0),\n            \"detailed_results\": results\n        }\n\n    def general_capability_retention(self, general_benchmarks):\n        \"\"\"\u901a\u7528\u80fd\u529b\u4fdd\u6301\u8bc4\u4f30\"\"\"\n\n        retention_scores = {}\n\n        for benchmark_name, benchmark_data in general_benchmarks.items():\n            base_score = self.run_benchmark(self.base_model, benchmark_data)\n            ft_score = self.run_benchmark(self.finetuned_model, benchmark_data)\n\n            retention_rate = ft_score / base_score if base_score &gt; 0 else 0\n\n            retention_scores[benchmark_name] = {\n                \"base_score\": base_score,\n                \"finetuned_score\": ft_score,\n                \"retention_rate\": retention_rate,\n                \"acceptable\": retention_rate &gt; 0.95  # \u4fdd\u630195%\u4ee5\u4e0a\u4e3aacceptable\n            }\n\n        return retention_scores\n\n    def catastrophic_forgetting_detection(self, test_samples):\n        \"\"\"\u707e\u96be\u6027\u9057\u5fd8\u68c0\u6d4b\"\"\"\n\n        forgetting_indicators = {\n            \"\u8bed\u6cd5\u80fd\u529b\u4e0b\u964d\": self.test_grammar_capability(test_samples[\"grammar\"]),\n            \"\u5e38\u8bc6\u63a8\u7406\u9000\u5316\": self.test_commonsense_reasoning(test_samples[\"commonsense\"]),\n            \"\u57fa\u7840\u77e5\u8bc6\u4e22\u5931\": self.test_factual_knowledge(test_samples[\"facts\"]),\n            \"\u903b\u8f91\u63a8\u7406\u80fd\u529b\": self.test_logical_reasoning(test_samples[\"logic\"])\n        }\n\n        # \u8ba1\u7b97\u603b\u4f53\u9057\u5fd8\u98ce\u9669\n        forgetting_risk = self.calculate_forgetting_risk(forgetting_indicators)\n\n        return {\n            \"individual_indicators\": forgetting_indicators,\n            \"overall_forgetting_risk\": forgetting_risk,\n            \"risk_level\": self.categorize_risk_level(forgetting_risk)\n        }\n\n    def calculate_forgetting_risk(self, indicators):\n        \"\"\"\u8ba1\u7b97\u9057\u5fd8\u98ce\u9669\u5206\u6570\"\"\"\n\n        risk_weights = {\n            \"\u8bed\u6cd5\u80fd\u529b\u4e0b\u964d\": 0.3,\n            \"\u5e38\u8bc6\u63a8\u7406\u9000\u5316\": 0.3, \n            \"\u57fa\u7840\u77e5\u8bc6\u4e22\u5931\": 0.2,\n            \"\u903b\u8f91\u63a8\u7406\u80fd\u529b\": 0.2\n        }\n\n        total_risk = 0\n        for indicator, score in indicators.items():\n            # score\u8d8a\u4f4e\u98ce\u9669\u8d8a\u9ad8\n            risk = (1 - score) * risk_weights[indicator]\n            total_risk += risk\n\n        return total_risk\n\n    def generate_evaluation_report(self, domain_eval, retention_eval, forgetting_eval):\n        \"\"\"\u751f\u6210\u8bc4\u4f30\u62a5\u544a\"\"\"\n\n        report = {\n            \"\u540e\u9884\u8bad\u7ec3\u6548\u679c\u603b\u7ed3\": {\n                \"\u9886\u57df\u80fd\u529b\u63d0\u5347\": f\"{domain_eval['average_improvement']:.3f} (+{domain_eval['improvement_rate']*100:.1f}% cases improved)\",\n                \"\u901a\u7528\u80fd\u529b\u4fdd\u6301\": f\"\u5e73\u5747\u4fdd\u6301\u7387: {np.mean([r['retention_rate'] for r in retention_eval.values()]):.3f}\",\n                \"\u9057\u5fd8\u98ce\u9669\u8bc4\u4f30\": forgetting_eval['risk_level']\n            },\n\n            \"\u8be6\u7ec6\u5206\u6790\": {\n                \"\u6700\u4f73\u63d0\u5347\u9886\u57df\": self.find_best_improvements(domain_eval),\n                \"\u9700\u8981\u5173\u6ce8\u7684\u901a\u7528\u80fd\u529b\": self.find_concerning_retentions(retention_eval),\n                \"\u9057\u5fd8\u98ce\u9669\u70b9\": self.find_forgetting_risks(forgetting_eval)\n            },\n\n            \"\u6539\u8fdb\u5efa\u8bae\": self.generate_improvement_suggestions(\n                domain_eval, retention_eval, forgetting_eval\n            )\n        }\n\n        return report\n\n# \u8bc4\u4f30\u4f7f\u7528\u793a\u4f8b\ndef evaluate_medical_post_pretraining(base_model, finetuned_model, tokenizer):\n    \"\"\"\u533b\u7597\u9886\u57df\u540e\u9884\u8bad\u7ec3\u8bc4\u4f30\"\"\"\n\n    evaluator = PostPretrainingEvaluator(base_model, finetuned_model, tokenizer)\n\n    # \u51c6\u5907\u8bc4\u4f30\u6570\u636e\n    medical_questions = [\n        \"What are the symptoms of myocardial infarction?\",\n        \"Explain the mechanism of action of ACE inhibitors\",\n        # ... \u66f4\u591a\u533b\u7597\u95ee\u9898\n    ]\n\n    general_benchmarks = {\n        \"common_sense\": load_commonsense_qa(),\n        \"reading_comprehension\": load_reading_comprehension(),\n        \"arithmetic\": load_arithmetic_tasks()\n    }\n\n    forgetting_test_samples = {\n        \"grammar\": load_grammar_tests(),\n        \"commonsense\": load_commonsense_tests(),\n        \"facts\": load_factual_tests(),\n        \"logic\": load_logic_tests()\n    }\n\n    # \u6267\u884c\u8bc4\u4f30\n    domain_results = evaluator.domain_knowledge_evaluation(medical_questions)\n    retention_results = evaluator.general_capability_retention(general_benchmarks)\n    forgetting_results = evaluator.catastrophic_forgetting_detection(forgetting_test_samples)\n\n    # \u751f\u6210\u62a5\u544a\n    evaluation_report = evaluator.generate_evaluation_report(\n        domain_results, retention_results, forgetting_results\n    )\n\n    print(\"\u540e\u9884\u8bad\u7ec3\u8bc4\u4f30\u62a5\u544a:\")\n    print(json.dumps(evaluation_report, indent=2, ensure_ascii=False))\n\n    return evaluation_report\n\n# \u8fd0\u884c\u8bc4\u4f30\n# evaluation_report = evaluate_medical_post_pretraining(base_model, finetuned_model, tokenizer)\n</code></pre>"},{"location":"training/post-training/post-pretraining/#_12","title":"\ud83d\udccb \u6700\u4f73\u5b9e\u8df5\u603b\u7ed3","text":""},{"location":"training/post-training/post-pretraining/#_13","title":"\u6210\u529f\u8981\u7d20\u6e05\u5355","text":"<pre><code>def post_pretraining_checklist():\n    \"\"\"\u540e\u9884\u8bad\u7ec3\u6210\u529f\u8981\u7d20\u6e05\u5355\"\"\"\n\n    return {\n        \"\u6570\u636e\u51c6\u5907\": {\n            \"\u2713 \u9886\u57df\u6570\u636e\u8d28\u91cf\": \"\u786e\u4fdd\u6570\u636e\u6765\u6e90\u6743\u5a01\u3001\u5185\u5bb9\u51c6\u786e\",\n            \"\u2713 \u6570\u636e\u89c4\u6a21\u5145\u8db3\": \"\u81f3\u5c1110\u4ebftoken\u7684\u9886\u57df\u6570\u636e\",\n            \"\u2713 \u901a\u7528\u6570\u636e\u6df7\u5408\": \"\u4fdd\u63011:5\u7684\u9886\u57df:\u901a\u7528\u6bd4\u4f8b\",\n            \"\u2713 \u6570\u636e\u53bb\u91cd\u5904\u7406\": \"\u907f\u514d\u91cd\u590d\u6570\u636e\u5bfc\u81f4\u8fc7\u62df\u5408\",\n            \"\u2713 \u683c\u5f0f\u7edf\u4e00\u5316\": \"\u4fdd\u6301\u4e0e\u9884\u8bad\u7ec3\u9636\u6bb5\u4e00\u81f4\u7684\u683c\u5f0f\"\n        },\n\n        \"\u8bad\u7ec3\u914d\u7f6e\": {\n            \"\u2713 \u5b66\u4e60\u7387\u8bbe\u7f6e\": \"\u4f7f\u75281e-5\u5de6\u53f3\u7684\u8f83\u4f4e\u5b66\u4e60\u7387\",\n            \"\u2713 \u5355\u8f6e\u8bad\u7ec3\": \"\u907f\u514d\u591a\u8f6e\u8bad\u7ec3\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\",\n            \"\u2713 \u6279\u91cf\u5927\u5c0f\": \"\u786e\u4fdd\u6709\u6548\u6279\u91cf\u5927\u5c0f\u8db3\u591f\u5927\",\n            \"\u2713 \u5e8f\u5217\u957f\u5ea6\": \"\u4f7f\u7528\u6a21\u578b\u652f\u6301\u7684\u6700\u5927\u5e8f\u5217\u957f\u5ea6\",\n            \"\u2713 \u6b63\u5219\u5316\u7b56\u7565\": \"\u9002\u5f53\u7684dropout\u548cweight decay\"\n        },\n\n        \"\u76d1\u63a7\u8bc4\u4f30\": {\n            \"\u2713 \u9886\u57df\u80fd\u529b\u8ddf\u8e2a\": \"\u5b9a\u671f\u8bc4\u4f30\u9886\u57df\u7279\u5b9a\u4efb\u52a1\u8868\u73b0\",\n            \"\u2713 \u901a\u7528\u80fd\u529b\u76d1\u63a7\": \"\u786e\u4fdd\u901a\u7528benchmark\u4e0d\u9000\u5316\",\n            \"\u2713 \u9057\u5fd8\u98ce\u9669\u68c0\u6d4b\": \"\u76d1\u63a7\u707e\u96be\u6027\u9057\u5fd8\u6307\u6807\",\n            \"\u2713 \u65e9\u505c\u673a\u5236\": \"\u8bbe\u7f6e\u5408\u7406\u7684\u65e9\u505c\u6761\u4ef6\",\n            \"\u2713 \u5b9a\u671fcheckpointing\": \"\u4fdd\u5b58\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u8282\u70b9\"\n        }\n    }\n\n# \u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848\ndef common_issues_solutions():\n    \"\"\"\u5e38\u89c1\u95ee\u9898\u4e0e\u89e3\u51b3\u65b9\u6848\"\"\"\n\n    return {\n        \"\u95ee\u98981: \u707e\u96be\u6027\u9057\u5fd8\": {\n            \"\u75c7\u72b6\": \"\u901a\u7528\u80fd\u529b\u663e\u8457\u4e0b\u964d\uff0c\u57fa\u7840\u4efb\u52a1\u8868\u73b0\u53d8\u5dee\",\n            \"\u539f\u56e0\": \"\u9886\u57df\u6570\u636e\u6bd4\u4f8b\u8fc7\u9ad8\uff0c\u5b66\u4e60\u7387\u8fc7\u5927\uff0c\u8bad\u7ec3\u8fc7\u4e45\",\n            \"\u89e3\u51b3\": \"\u964d\u4f4e\u9886\u57df\u6570\u636e\u6bd4\u4f8b\uff0c\u51cf\u5c0f\u5b66\u4e60\u7387\uff0c\u5355\u8f6e\u8bad\u7ec3\"\n        },\n\n        \"\u95ee\u98982: \u9886\u57df\u80fd\u529b\u63d0\u5347\u4e0d\u660e\u663e\": {\n            \"\u75c7\u72b6\": \"\u5728\u9886\u57df\u4efb\u52a1\u4e0a\u8868\u73b0\u6ca1\u6709\u660e\u663e\u6539\u5584\",\n            \"\u539f\u56e0\": \"\u9886\u57df\u6570\u636e\u8d28\u91cf\u5dee\uff0c\u6570\u636e\u91cf\u4e0d\u8db3\uff0c\u8bad\u7ec3\u4e0d\u5145\u5206\",\n            \"\u89e3\u51b3\": \"\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\uff0c\u589e\u52a0\u6570\u636e\u91cf\uff0c\u9002\u5f53\u589e\u52a0\u8bad\u7ec3\u6b65\u6570\"\n        },\n\n        \"\u95ee\u98983: \u8bad\u7ec3\u4e0d\u7a33\u5b9a\": {\n            \"\u75c7\u72b6\": \"\u635f\u5931\u9707\u8361\uff0c\u8bad\u7ec3\u53d1\u6563\uff0c\u6027\u80fd\u5ffd\u9ad8\u5ffd\u4f4e\",\n            \"\u539f\u56e0\": \"\u5b66\u4e60\u7387\u8fc7\u9ad8\uff0c\u6279\u91cf\u5927\u5c0f\u4e0d\u5408\u9002\uff0c\u6570\u636e\u8d28\u91cf\u95ee\u9898\",\n            \"\u89e3\u51b3\": \"\u964d\u4f4e\u5b66\u4e60\u7387\uff0c\u8c03\u6574\u6279\u91cf\u5927\u5c0f\uff0c\u6e05\u6d17\u6570\u636e\"\n        },\n\n        \"\u95ee\u98984: \u8d44\u6e90\u4e0d\u8db3\": {\n            \"\u75c7\u72b6\": \"\u663e\u5b58\u4e0d\u591f\uff0c\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\",\n            \"\u539f\u56e0\": \"\u6a21\u578b\u592a\u5927\uff0c\u6279\u91cf\u5927\u5c0f\u8fc7\u5927\uff0c\u5e8f\u5217\u957f\u5ea6\u8fc7\u957f\",\n            \"\u89e3\u51b3\": \"\u4f7f\u7528\u91cf\u5316\uff0c\u51cf\u5c0f\u6279\u91cf\uff0c\u68af\u5ea6\u7d2f\u79ef\uff0c\u6a21\u578b\u5e76\u884c\"\n        }\n    }\n</code></pre>"},{"location":"training/post-training/post-pretraining/#_14","title":"\ud83c\udfaf \u9762\u8bd5\u95ee\u7b54\u603b\u7ed3","text":""},{"location":"training/post-training/post-pretraining/#q1","title":"Q1: \u4ec0\u4e48\u662f\u540e\u9884\u8bad\u7ec3\uff1f\u4e0e\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6709\u4ec0\u4e48\u533a\u522b\uff1f","text":"<p>A: \u540e\u9884\u8bad\u7ec3\u662f\u5728\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u57fa\u7840\u4e0a\uff0c\u4f7f\u7528\u9886\u57df\u7279\u5b9a\u6570\u636e\u8fdb\u884c\u7ee7\u7eed\u9884\u8bad\u7ec3\u7684\u8fc7\u7a0b\uff1a - \u4e0e\u9884\u8bad\u7ec3\u533a\u522b: \u9884\u8bad\u7ec3\u4f7f\u7528\u901a\u7528\u8bed\u6599\uff0c\u540e\u9884\u8bad\u7ec3\u4f7f\u7528\u9886\u57df+\u901a\u7528\u6df7\u5408\u6570\u636e - \u4e0e\u5fae\u8c03\u533a\u522b: \u5fae\u8c03\u4f7f\u7528\u4efb\u52a1\u6807\u6ce8\u6570\u636e\uff0c\u540e\u9884\u8bad\u7ec3\u4ecd\u4f7f\u7528\u81ea\u76d1\u7763\u5b66\u4e60 - \u76ee\u6807: \u5728\u4fdd\u6301\u901a\u7528\u80fd\u529b\u57fa\u7840\u4e0a\u6ce8\u5165\u9886\u57df\u77e5\u8bc6</p>"},{"location":"training/post-training/post-pretraining/#q2","title":"Q2: \u540e\u9884\u8bad\u7ec3\u7684\u6570\u636e\u914d\u6bd4\u7b56\u7565\u662f\u4ec0\u4e48\uff1f","text":"<p>A: \u7ecf\u9a8c\u9a8c\u8bc1\u7684\u9ec4\u91d1\u6bd4\u4f8b\u662f\u9886\u57df:\u901a\u7528 = 1:5 - \u7406\u8bba\u4f9d\u636e: \u5e73\u8861\u9886\u57df\u589e\u5f3a\u4e0e\u80fd\u529b\u4fdd\u6301 - \u5b9e\u65bd\u65b9\u6cd5: \u6bcf\u4e2abatch\u5185\u968f\u673a\u6df7\u5408\uff0c\u786e\u4fdd\u6bd4\u4f8b\u7a33\u5b9a - \u8c03\u4f18\u7a7a\u95f4: \u53ef\u6839\u636e\u5177\u4f53\u9700\u6c42\u57281:3\u52301:8\u4e4b\u95f4\u8c03\u6574</p>"},{"location":"training/post-training/post-pretraining/#q3","title":"Q3: \u5982\u4f55\u9632\u6b62\u540e\u9884\u8bad\u7ec3\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\uff1f","text":"<p>A: \u591a\u5c42\u9762\u9632\u62a4\u7b56\u7565\uff1a - \u6570\u636e\u5c42\u9762: \u4fdd\u6301\u8db3\u591f\u6bd4\u4f8b\u7684\u901a\u7528\u6570\u636e\u6df7\u5408 - \u8bad\u7ec3\u5c42\u9762: \u4f7f\u7528\u8f83\u5c0f\u5b66\u4e60\u7387\uff0c\u5355\u8f6e\u8bad\u7ec3\uff0c\u68af\u5ea6\u88c1\u526a - \u76d1\u63a7\u5c42\u9762: \u5b9a\u671f\u8bc4\u4f30\u901a\u7528\u57fa\u51c6\uff0c\u8bbe\u7f6e\u65e9\u505c\u673a\u5236</p>"},{"location":"training/post-training/post-pretraining/#q4","title":"Q4: \u540e\u9884\u8bad\u7ec3\u5728\u4ec0\u4e48\u573a\u666f\u4e0b\u6700\u6709\u4ef7\u503c\uff1f","text":"<p>A: \u6700\u9002\u7528\u7684\u4e09\u4e2a\u573a\u666f\uff1a 1. \u6709\u5927\u91cf\u9886\u57df\u8bed\u6599 (&gt;10\u4ebftoken)\u4e14\u8d28\u91cf\u9ad8 2. \u9886\u57df\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1 (\u5982\u533b\u7597\u3001\u6cd5\u5f8b\u3001\u91d1\u878d) 3. \u9700\u8981\u4fdd\u6301\u901a\u7528\u80fd\u529b \u7684\u4e13\u4e1a\u5e94\u7528</p>"},{"location":"training/post-training/post-pretraining/#_15","title":"\ud83d\ude80 \u5b66\u4e60\u5efa\u8bae","text":"<ol> <li>\u7406\u8bba\u7406\u89e3: \u6df1\u5165\u7406\u89e3\u540e\u9884\u8bad\u7ec3\u7684\u5b9a\u4f4d\u548c\u4ef7\u503c</li> <li>\u5b9e\u8df5\u9a8c\u8bc1: \u5728\u5c0f\u89c4\u6a21\u6570\u636e\u4e0a\u9a8c\u8bc1\u914d\u6bd4\u548c\u8d85\u53c2\u6570</li> <li>\u76d1\u63a7\u4f53\u7cfb: \u5efa\u7acb\u5b8c\u6574\u7684\u80fd\u529b\u8bc4\u4f30\u548c\u9057\u5fd8\u68c0\u6d4b\u673a\u5236</li> <li>\u9886\u57df\u5e94\u7528: \u9009\u62e9\u5177\u4f53\u9886\u57df\u6df1\u5165\u5b9e\u8df5</li> </ol> <p>\u540e\u9884\u8bad\u7ec3\u662f\u8fde\u63a5\u901a\u7528\u6a21\u578b\u4e0e\u4e13\u4e1a\u5e94\u7528\u7684\u91cd\u8981\u6865\u6881\uff0c\u662f\u73b0\u4ee3LLM\u843d\u5730\u7684\u5173\u952e\u6280\u672f\uff01</p>"},{"location":"training/post-training/supervised-finetuning/","title":"\u76d1\u7763\u5fae\u8c03\u6280\u672f","text":""},{"location":"training/post-training/supervised-finetuning/#_2","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u5168\u9762\u638c\u63e1\u76d1\u7763\u5fae\u8c03(Supervised Fine-Tuning, SFT)\u6280\u672f\uff0c\u7406\u89e3\u6307\u4ee4\u5fae\u8c03\u7684\u6838\u5fc3\u539f\u7406\uff0c\u5b66\u4f1a\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u6570\u636e\u96c6\u548c\u5b9e\u73b0\u6709\u6548\u7684\u5fae\u8c03\u6d41\u7a0b\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - SFT\u5728\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f - \u6307\u4ee4\u5fae\u8c03\u4e0e\u4f20\u7edf\u5fae\u8c03\u6709\u4ec0\u4e48\u533a\u522b\uff1f - \u5982\u4f55\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u6570\u636e\u96c6\uff1f - SFT\u8bad\u7ec3\u4e2d\u5e38\u89c1\u7684\u95ee\u9898\u548c\u89e3\u51b3\u65b9\u6848\uff1f</p>"},{"location":"training/post-training/supervised-finetuning/#sft","title":"\ud83c\udfd7\ufe0f SFT\u6838\u5fc3\u6982\u5ff5","text":""},{"location":"training/post-training/supervised-finetuning/#_3","title":"\u5b9a\u4e49\u4e0e\u4f5c\u7528","text":"<p>\u76d1\u7763\u5fae\u8c03(SFT)\u662f\u4f7f\u7528\u6307\u4ee4-\u56de\u7b54\u5bf9\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u9075\u5faa\u6307\u4ee4\u548c\u4ea7\u751f\u671f\u671b\u8f93\u51fa\u7684\u8fc7\u7a0b\uff0c\u662f\u8fde\u63a5\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u6865\u6881\u3002</p> <pre><code>SFT\u5728\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u4f4d\u7f6e\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    \u5b8c\u6574LLM\u8bad\u7ec3\u6d41\u7a0b                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u9884\u8bad\u7ec3 \u2192 \u540e\u9884\u8bad\u7ec3 \u2192 \u76d1\u7763\u5fae\u8c03(SFT) \u2192 \u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50                 \u2502\n\u2502                        \u25b2                                        \u2502\n\u2502                   \u5173\u952e\u8f6c\u6362\u70b9                                      \u2502\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502   \u8bed\u8a00\u5efa\u6a21       \u2502\u2500\u2500\u25b6\u2502   \u6307\u4ee4\u9075\u5faa       \u2502\u2500\u2500\u25b6\u2502   \u4eba\u7c7b\u504f\u597d       \u2502  \u2502\n\u2502  \u2502 \u4e0b\u4e00\u8bcd\u9884\u6d4b       \u2502   \u2502 \u4efb\u52a1\u5b8c\u6210        \u2502   \u2502 \u4ef7\u503c\u5bf9\u9f50        \u2502  \u2502\n\u2502  \u2502 \u65e0\u76d1\u7763\u5b66\u4e60       \u2502   \u2502 \u76d1\u7763\u5b66\u4e60        \u2502   \u2502 \u5f3a\u5316\u5b66\u4e60        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u25b2                      \u25b2                      \u25b2          \u2502\n\u2502    \u901a\u7528\u8bed\u8a00\u80fd\u529b              \u6307\u4ee4\u9075\u5faa\u80fd\u529b              \u5bf9\u9f50\u80fd\u529b     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/post-training/supervised-finetuning/#_4","title":"\u6838\u5fc3\u4ef7\u503c","text":"<ol> <li>\u80fd\u529b\u8f6c\u6362: \u4ece\u8bed\u8a00\u5efa\u6a21\u80fd\u529b\u8f6c\u6362\u4e3a\u4efb\u52a1\u6267\u884c\u80fd\u529b</li> <li>\u683c\u5f0f\u89c4\u8303: \u6559\u4f1a\u6a21\u578b\u6807\u51c6\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f</li> <li>\u6307\u4ee4\u9075\u5faa: \u57f9\u517b\u9075\u5faa\u7528\u6237\u6307\u4ee4\u7684\u57fa\u7840\u80fd\u529b</li> <li>\u591a\u4efb\u52a1\u7edf\u4e00: \u7528\u7edf\u4e00\u683c\u5f0f\u5904\u7406\u591a\u79cd\u4e0d\u540c\u4efb\u52a1</li> </ol>"},{"location":"training/post-training/supervised-finetuning/#_5","title":"\ud83d\udcca \u6307\u4ee4\u6570\u636e\u6784\u5efa","text":""},{"location":"training/post-training/supervised-finetuning/#_6","title":"\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u7684\u7279\u5f81","text":"<pre><code>class InstructionDataBuilder:\n    \"\"\"\u6307\u4ee4\u6570\u636e\u6784\u5efa\u5668\"\"\"\n\n    def __init__(self):\n        self.quality_criteria = {\n            \"\u591a\u6837\u6027\": \"\u6db5\u76d6\u4e0d\u540c\u9886\u57df\u3001\u4efb\u52a1\u7c7b\u578b\u3001\u96be\u5ea6\u5c42\u7ea7\",\n            \"\u51c6\u786e\u6027\": \"\u56de\u7b54\u51c6\u786e\u3001\u4e8b\u5b9e\u6b63\u786e\u3001\u903b\u8f91\u6e05\u6670\",\n            \"\u5b8c\u6574\u6027\": \"\u56de\u7b54\u5b8c\u6574\u3001\u7ed3\u6784\u5316\u3001\u6ee1\u8db3\u6307\u4ee4\u8981\u6c42\",\n            \"\u4e00\u81f4\u6027\": \"\u683c\u5f0f\u7edf\u4e00\u3001\u98ce\u683c\u4e00\u81f4\u3001\u6807\u51c6\u89c4\u8303\"\n        }\n\n    def instruction_template_design(self):\n        \"\"\"\u6307\u4ee4\u6a21\u677f\u8bbe\u8ba1\"\"\"\n\n        templates = {\n            \"\u57fa\u7840\u95ee\u7b54\u6a21\u677f\": {\n                \"\u683c\u5f0f\": \"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\",\n                \"\u9002\u7528\": \"\u7b80\u5355\u95ee\u7b54\u3001\u77e5\u8bc6\u67e5\u8be2\u7c7b\u4efb\u52a1\",\n                \"\u793a\u4f8b\": {\n                    \"instruction\": \"\u89e3\u91ca\u4ec0\u4e48\u662f\u673a\u5668\u5b66\u4e60\",\n                    \"response\": \"\u673a\u5668\u5b66\u4e60\u662f\u4e00\u79cd\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u901a\u8fc7\u7b97\u6cd5\u8ba9\u8ba1\u7b97\u673a\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u89c4\u5f8b...\"\n                }\n            },\n\n            \"\u591a\u8f6e\u5bf9\u8bdd\u6a21\u677f\": {\n                \"\u683c\u5f0f\": \"### Conversation:\\n{conversation_history}\\n\\n### Human:\\n{human_input}\\n\\n### Assistant:\\n{assistant_response}\",\n                \"\u9002\u7528\": \"\u5bf9\u8bdd\u7cfb\u7edf\u3001\u804a\u5929\u673a\u5668\u4eba\",\n                \"\u793a\u4f8b\": {\n                    \"conversation_history\": \"\u4e4b\u524d\u7684\u5bf9\u8bdd\u5386\u53f2\",\n                    \"human_input\": \"\u7528\u6237\u5f53\u524d\u8f93\u5165\",\n                    \"assistant_response\": \"\u52a9\u624b\u56de\u5e94\"\n                }\n            },\n\n            \"\u4efb\u52a1\u5bfc\u5411\u6a21\u677f\": {\n                \"\u683c\u5f0f\": \"### Task:\\n{task_description}\\n\\n### Input:\\n{input_data}\\n\\n### Output:\\n{expected_output}\",\n                \"\u9002\u7528\": \"\u7ed3\u6784\u5316\u4efb\u52a1\u3001\u6570\u636e\u5904\u7406\",\n                \"\u793a\u4f8b\": {\n                    \"task_description\": \"\u5c06\u4ee5\u4e0b\u6587\u672c\u7ffb\u8bd1\u6210\u82f1\u6587\",\n                    \"input_data\": \"\u4f60\u597d\uff0c\u4e16\u754c\",\n                    \"expected_output\": \"Hello, world\"\n                }\n            },\n\n            \"\u601d\u7ef4\u94fe\u6a21\u677f\": {\n                \"\u683c\u5f0f\": \"### Problem:\\n{problem}\\n\\n### Solution:\\n{reasoning_steps}\\n\\n### Answer:\\n{final_answer}\",\n                \"\u9002\u7528\": \"\u63a8\u7406\u4efb\u52a1\u3001\u6570\u5b66\u95ee\u9898\u3001\u903b\u8f91\u5206\u6790\",\n                \"\u793a\u4f8b\": {\n                    \"problem\": \"\u8ba1\u7b97 (3+5) \u00d7 2 = ?\",\n                    \"reasoning_steps\": \"\u9996\u5148\u8ba1\u7b97\u62ec\u53f7\u5185: 3+5=8\\n\u7136\u540e\u4e58\u4ee52: 8\u00d72=16\",\n                    \"final_answer\": \"16\"\n                }\n            }\n        }\n\n        return templates\n\n    def data_diversity_strategy(self):\n        \"\"\"\u6570\u636e\u591a\u6837\u6027\u7b56\u7565\"\"\"\n\n        diversity_dimensions = {\n            \"\u4efb\u52a1\u7c7b\u578b\u591a\u6837\u6027\": {\n                \"\u6587\u672c\u751f\u6210\": [\"\u521b\u610f\u5199\u4f5c\", \"\u6458\u8981\u751f\u6210\", \"\u7eed\u5199\u8865\u5168\"],\n                \"\u4fe1\u606f\u62bd\u53d6\": [\"\u5b9e\u4f53\u8bc6\u522b\", \"\u5173\u7cfb\u62bd\u53d6\", \"\u5173\u952e\u8bcd\u63d0\u53d6\"],\n                \"\u6587\u672c\u5206\u6790\": [\"\u60c5\u611f\u5206\u6790\", \"\u4e3b\u9898\u5206\u7c7b\", \"\u610f\u56fe\u7406\u89e3\"],\n                \"\u63a8\u7406\u95ee\u7b54\": [\"\u5e38\u8bc6\u63a8\u7406\", \"\u6570\u5b66\u8ba1\u7b97\", \"\u903b\u8f91\u63a8\u7406\"],\n                \"\u4ee3\u7801\u76f8\u5173\": [\"\u4ee3\u7801\u751f\u6210\", \"\u4ee3\u7801\u89e3\u91ca\", \"Bug\u4fee\u590d\"],\n                \"\u521b\u610f\u4efb\u52a1\": [\"\u5934\u8111\u98ce\u66b4\", \"\u6545\u4e8b\u521b\u4f5c\", \"\u8bd7\u6b4c\u521b\u4f5c\"]\n            },\n\n            \"\u9886\u57df\u8986\u76d6\u591a\u6837\u6027\": {\n                \"\u79d1\u6280\u9886\u57df\": \"\u8ba1\u7b97\u673a\u3001\u4eba\u5de5\u667a\u80fd\u3001\u751f\u7269\u6280\u672f\",\n                \"\u5546\u4e1a\u9886\u57df\": \"\u91d1\u878d\u3001\u7ba1\u7406\u3001\u8425\u9500\",\n                \"\u6559\u80b2\u9886\u57df\": \"\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u3001\u5386\u53f2\",\n                \"\u751f\u6d3b\u9886\u57df\": \"\u5065\u5eb7\u3001\u7f8e\u98df\u3001\u65c5\u6e38\",\n                \"\u6587\u5316\u9886\u57df\": \"\u6587\u5b66\u3001\u827a\u672f\u3001\u54f2\u5b66\"\n            },\n\n            \"\u96be\u5ea6\u5c42\u7ea7\u591a\u6837\u6027\": {\n                \"\u7b80\u5355\": \"\u57fa\u7840\u6982\u5ff5\u3001\u7b80\u5355\u95ee\u7b54\u3001\u76f4\u63a5\u67e5\u8be2\",\n                \"\u4e2d\u7b49\": \"\u5206\u6790\u89e3\u91ca\u3001\u591a\u6b65\u63a8\u7406\u3001\u7efc\u5408\u5e94\u7528\",\n                \"\u56f0\u96be\": \"\u590d\u6742\u63a8\u7406\u3001\u521b\u65b0\u601d\u8003\u3001\u4e13\u4e1a\u6df1\u5ea6\"\n            },\n\n            \"\u8bed\u8a00\u98ce\u683c\u591a\u6837\u6027\": {\n                \"\u6b63\u5f0f\u98ce\u683c\": \"\u5b66\u672f\u8bba\u6587\u3001\u5546\u52a1\u62a5\u544a\u3001\u5b98\u65b9\u6587\u6863\",\n                \"\u975e\u6b63\u5f0f\u98ce\u683c\": \"\u65e5\u5e38\u5bf9\u8bdd\u3001\u793e\u4ea4\u5a92\u4f53\u3001\u4e2a\u4eba\u535a\u5ba2\",\n                \"\u4e13\u4e1a\u98ce\u683c\": \"\u6280\u672f\u6587\u6863\u3001\u533b\u5b66\u62a5\u544a\u3001\u6cd5\u5f8b\u6761\u6587\",\n                \"\u521b\u610f\u98ce\u683c\": \"\u6587\u5b66\u521b\u4f5c\u3001\u5e7f\u544a\u6587\u6848\u3001\u827a\u672f\u8bc4\u8bba\"\n            }\n        }\n\n        return diversity_dimensions\n\n    def generate_instruction_data(self, seed_topics, num_per_topic=50):\n        \"\"\"\u751f\u6210\u6307\u4ee4\u6570\u636e\"\"\"\n\n        instruction_data = []\n        templates = self.instruction_template_design()\n\n        # \u6307\u4ee4\u751f\u6210\u63d0\u793a\u6a21\u677f\n        generation_prompts = {\n            \"\u95ee\u7b54\u7c7b\": \"\u57fa\u4e8e\u4e3b\u9898'{topic}'\uff0c\u751f\u6210\u4e00\u4e2a\u9700\u8981\u8be6\u7ec6\u89e3\u91ca\u7684\u95ee\u9898\uff1a\",\n            \"\u4efb\u52a1\u7c7b\": \"\u57fa\u4e8e\u4e3b\u9898'{topic}'\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u5177\u4f53\u7684\u4efb\u52a1\u6307\u4ee4\uff1a\",\n            \"\u63a8\u7406\u7c7b\": \"\u57fa\u4e8e\u4e3b\u9898'{topic}'\uff0c\u521b\u5efa\u4e00\u4e2a\u9700\u8981\u903b\u8f91\u63a8\u7406\u7684\u95ee\u9898\uff1a\",\n            \"\u521b\u610f\u7c7b\": \"\u57fa\u4e8e\u4e3b\u9898'{topic}'\uff0c\u8bbe\u8ba1\u4e00\u4e2a\u521b\u610f\u751f\u6210\u4efb\u52a1\uff1a\"\n        }\n\n        for topic in seed_topics:\n            for prompt_type, prompt_template in generation_prompts.items():\n                for i in range(num_per_topic // len(generation_prompts)):\n\n                    # \u751f\u6210\u6307\u4ee4\n                    instruction_prompt = prompt_template.format(topic=topic)\n                    instruction = self.generate_with_llm(instruction_prompt)\n\n                    # \u751f\u6210\u56de\u7b54\n                    response_prompt = f\"\u8bf7\u8be6\u7ec6\u56de\u7b54\u4ee5\u4e0b\u95ee\u9898\uff1a\\n{instruction}\"\n                    response = self.generate_with_llm(response_prompt)\n\n                    # \u8d28\u91cf\u68c0\u67e5\n                    if self.quality_check(instruction, response):\n                        instruction_data.append({\n                            \"instruction\": instruction,\n                            \"response\": response,\n                            \"topic\": topic,\n                            \"type\": prompt_type,\n                            \"quality_score\": self.calculate_quality_score(instruction, response)\n                        })\n\n        return instruction_data\n\n    def quality_check(self, instruction, response):\n        \"\"\"\u8d28\u91cf\u68c0\u67e5\"\"\"\n\n        # \u57fa\u7840\u957f\u5ea6\u68c0\u67e5\n        if len(instruction.split()) &lt; 5 or len(response.split()) &lt; 10:\n            return False\n\n        # \u76f8\u5173\u6027\u68c0\u67e5\n        relevance_score = self.calculate_relevance(instruction, response)\n        if relevance_score &lt; 0.7:\n            return False\n\n        # \u5b8c\u6574\u6027\u68c0\u67e5\n        completeness_score = self.calculate_completeness(response)\n        if completeness_score &lt; 0.6:\n            return False\n\n        # \u5b89\u5168\u6027\u68c0\u67e5\n        if self.contains_harmful_content(instruction + \" \" + response):\n            return False\n\n        return True\n\n    def create_balanced_dataset(self, instruction_data, target_size=10000):\n        \"\"\"\u521b\u5efa\u5e73\u8861\u6570\u636e\u96c6\"\"\"\n\n        # \u6309\u7c7b\u578b\u548c\u4e3b\u9898\u5206\u7ec4\n        grouped_data = {}\n        for item in instruction_data:\n            key = f\"{item['type']}_{item['topic']}\"\n            if key not in grouped_data:\n                grouped_data[key] = []\n            grouped_data[key].append(item)\n\n        # \u8ba1\u7b97\u6bcf\u7ec4\u76ee\u6807\u6570\u91cf\n        num_groups = len(grouped_data)\n        per_group_target = target_size // num_groups\n\n        # \u4ece\u6bcf\u7ec4\u91c7\u6837\n        balanced_data = []\n        for group, items in grouped_data.items():\n            # \u6309\u8d28\u91cf\u5206\u6570\u6392\u5e8f\uff0c\u9009\u62e9\u9ad8\u8d28\u91cf\u6837\u672c\n            sorted_items = sorted(items, key=lambda x: x['quality_score'], reverse=True)\n            selected = sorted_items[:min(per_group_target, len(sorted_items))]\n            balanced_data.extend(selected)\n\n        return balanced_data\n\n# \u4f7f\u7528\u793a\u4f8b\nbuilder = InstructionDataBuilder()\ntemplates = builder.instruction_template_design()\ndiversity = builder.data_diversity_strategy()\n\nprint(\"\u6307\u4ee4\u6a21\u677f:\", templates)\nprint(\"\u591a\u6837\u6027\u7b56\u7565:\", diversity)\n</code></pre>"},{"location":"training/post-training/supervised-finetuning/#_7","title":"\u6570\u636e\u589e\u5f3a\u6280\u672f","text":"<pre><code>class InstructionDataAugmentation:\n    \"\"\"\u6307\u4ee4\u6570\u636e\u589e\u5f3a\u6280\u672f\"\"\"\n\n    def __init__(self, base_llm):\n        self.base_llm = base_llm\n\n    def paraphrase_augmentation(self, instruction, response):\n        \"\"\"\u6539\u5199\u589e\u5f3a\"\"\"\n\n        # \u6307\u4ee4\u6539\u5199\n        paraphrase_prompts = [\n            f\"\u8bf7\u7528\u4e0d\u540c\u7684\u65b9\u5f0f\u8868\u8fbe\u4ee5\u4e0b\u6307\u4ee4\uff0c\u4fdd\u6301\u542b\u4e49\u4e0d\u53d8\uff1a\\n{instruction}\",\n            f\"\u5c06\u4ee5\u4e0b\u6307\u4ee4\u6539\u5199\u5f97\u66f4\u52a0\u6b63\u5f0f\uff1a\\n{instruction}\",\n            f\"\u5c06\u4ee5\u4e0b\u6307\u4ee4\u6539\u5199\u5f97\u66f4\u52a0\u7b80\u6d01\uff1a\\n{instruction}\"\n        ]\n\n        augmented_pairs = []\n\n        for prompt in paraphrase_prompts:\n            new_instruction = self.base_llm.generate(prompt)\n\n            # \u68c0\u67e5\u6539\u5199\u8d28\u91cf\n            if self.is_valid_paraphrase(instruction, new_instruction):\n                # \u4e3a\u65b0\u6307\u4ee4\u751f\u6210\u5bf9\u5e94\u56de\u7b54\n                new_response = self.base_llm.generate(\n                    f\"\u8bf7\u56de\u7b54\uff1a{new_instruction}\"\n                )\n\n                augmented_pairs.append({\n                    \"instruction\": new_instruction,\n                    \"response\": new_response,\n                    \"augmentation_type\": \"paraphrase\",\n                    \"original_instruction\": instruction\n                })\n\n        return augmented_pairs\n\n    def difficulty_augmentation(self, instruction, response):\n        \"\"\"\u96be\u5ea6\u8c03\u8282\u589e\u5f3a\"\"\"\n\n        # \u7b80\u5316\u7248\u672c\n        simplify_prompt = f\"\"\"\n        \u5c06\u4ee5\u4e0b\u6307\u4ee4\u7b80\u5316\uff0c\u4f7f\u5176\u66f4\u5bb9\u6613\u7406\u89e3\uff1a\n\n        \u539f\u6307\u4ee4\uff1a{instruction}\n\n        \u7b80\u5316\u6307\u4ee4\uff1a\n        \"\"\"\n\n        simple_instruction = self.base_llm.generate(simplify_prompt)\n        simple_response = self.base_llm.generate(f\"\u8bf7\u7b80\u5355\u56de\u7b54\uff1a{simple_instruction}\")\n\n        # \u590d\u6742\u7248\u672c  \n        complicate_prompt = f\"\"\"\n        \u5c06\u4ee5\u4e0b\u6307\u4ee4\u6269\u5c55\u5f97\u66f4\u52a0\u5177\u4f53\u548c\u590d\u6742\uff1a\n\n        \u539f\u6307\u4ee4\uff1a{instruction}\n\n        \u6269\u5c55\u6307\u4ee4\uff1a\n        \"\"\"\n\n        complex_instruction = self.base_llm.generate(complicate_prompt)\n        complex_response = self.base_llm.generate(f\"\u8bf7\u8be6\u7ec6\u56de\u7b54\uff1a{complex_instruction}\")\n\n        return [\n            {\n                \"instruction\": simple_instruction,\n                \"response\": simple_response,\n                \"augmentation_type\": \"simplification\",\n                \"difficulty_level\": \"easy\"\n            },\n            {\n                \"instruction\": complex_instruction, \n                \"response\": complex_response,\n                \"augmentation_type\": \"complication\",\n                \"difficulty_level\": \"hard\"\n            }\n        ]\n\n    def format_augmentation(self, instruction, response):\n        \"\"\"\u683c\u5f0f\u53d8\u6362\u589e\u5f3a\"\"\"\n\n        format_variations = []\n\n        # \u95ee\u7b54\u683c\u5f0f\n        qa_format = {\n            \"instruction\": f\"\u95ee\u9898\uff1a{instruction}\",\n            \"response\": f\"\u56de\u7b54\uff1a{response}\",\n            \"format_type\": \"qa_format\"\n        }\n\n        # \u5bf9\u8bdd\u683c\u5f0f\n        dialog_format = {\n            \"instruction\": f\"\u7528\u6237\uff1a{instruction}\",\n            \"response\": f\"\u52a9\u624b\uff1a{response}\",\n            \"format_type\": \"dialog_format\"\n        }\n\n        # \u7ed3\u6784\u5316\u683c\u5f0f\n        structured_format = {\n            \"instruction\": f\"\u4efb\u52a1\uff1a{instruction}\\n\u8981\u6c42\uff1a\u8bf7\u63d0\u4f9b\u8be6\u7ec6\u56de\u7b54\",\n            \"response\": f\"\u89e3\u7b54\uff1a\\n{response}\",\n            \"format_type\": \"structured_format\"\n        }\n\n        format_variations.extend([qa_format, dialog_format, structured_format])\n\n        return format_variations\n\n    def context_augmentation(self, instruction, response):\n        \"\"\"\u4e0a\u4e0b\u6587\u589e\u5f3a\"\"\"\n\n        # \u6dfb\u52a0\u80cc\u666f\u4fe1\u606f\n        context_enhanced = []\n\n        contexts = [\n            \"\u5728\u5b66\u672f\u7814\u7a76\u73af\u5883\u4e2d\uff0c\",\n            \"\u5728\u65e5\u5e38\u751f\u6d3b\u573a\u666f\u4e0b\uff0c\", \n            \"\u5728\u5546\u4e1a\u5e94\u7528\u4e2d\uff0c\",\n            \"\u5728\u6559\u80b2\u6559\u5b66\u4e2d\uff0c\"\n        ]\n\n        for context in contexts:\n            contextualized_instruction = context + instruction.lower()\n\n            # \u751f\u6210\u9002\u5e94\u4e0a\u4e0b\u6587\u7684\u56de\u7b54\n            context_response = self.base_llm.generate(\n                f\"\u5728{context[:-1]}\u7684\u80cc\u666f\u4e0b\uff0c\u8bf7\u56de\u7b54\uff1a{instruction}\"\n            )\n\n            context_enhanced.append({\n                \"instruction\": contextualized_instruction,\n                \"response\": context_response,\n                \"augmentation_type\": \"context_enhancement\",\n                \"context\": context.strip(\"\uff0c\")\n            })\n\n        return context_enhanced\n\n# \u6570\u636e\u589e\u5f3a\u4f7f\u7528\u793a\u4f8b\ndef augment_instruction_dataset(original_data, target_multiplier=3):\n    \"\"\"\u6570\u636e\u589e\u5f3a\u6d41\u7a0b\"\"\"\n\n    augmenter = InstructionDataAugmentation(base_llm)\n    augmented_data = []\n\n    for item in original_data:\n        instruction = item[\"instruction\"]\n        response = item[\"response\"]\n\n        # \u539f\u59cb\u6570\u636e\n        augmented_data.append(item)\n\n        # \u6539\u5199\u589e\u5f3a\n        paraphrases = augmenter.paraphrase_augmentation(instruction, response)\n        augmented_data.extend(paraphrases)\n\n        # \u96be\u5ea6\u589e\u5f3a\n        difficulty_variants = augmenter.difficulty_augmentation(instruction, response)\n        augmented_data.extend(difficulty_variants)\n\n        # \u683c\u5f0f\u589e\u5f3a\n        format_variants = augmenter.format_augmentation(instruction, response)\n        augmented_data.extend(format_variants)\n\n        # \u4e0a\u4e0b\u6587\u589e\u5f3a\n        context_variants = augmenter.context_augmentation(instruction, response)\n        augmented_data.extend(context_variants)\n\n        # \u63a7\u5236\u589e\u5f3a\u500d\u6570\n        if len(augmented_data) &gt;= len(original_data) * target_multiplier:\n            break\n\n    return augmented_data[:len(original_data) * target_multiplier]\n</code></pre>"},{"location":"training/post-training/supervised-finetuning/#sft_1","title":"\ud83d\ude80 SFT\u8bad\u7ec3\u5b9e\u73b0","text":""},{"location":"training/post-training/supervised-finetuning/#_8","title":"\u5b8c\u6574\u8bad\u7ec3\u6d41\u7a0b","text":"<pre><code>from transformers import (\n    AutoTokenizer, AutoModelForCausalLM, \n    Trainer, TrainingArguments,\n    DataCollatorForLanguageModeling\n)\nfrom datasets import Dataset\nimport torch\n\nclass SupervisedFineTuner:\n    \"\"\"\u76d1\u7763\u5fae\u8c03\u8bad\u7ec3\u5668\"\"\"\n\n    def __init__(self, model_name, max_seq_length=2048):\n        self.model_name = model_name\n        self.max_seq_length = max_seq_length\n\n        # \u521d\u59cb\u5316tokenizer\u548cmodel\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        # \u8bbe\u7f6epad token\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n        # \u52a0\u8f7d\u6a21\u578b\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            load_in_8bit=True,  # 8bit\u91cf\u5316\u8282\u7701\u663e\u5b58\n            trust_remote_code=True\n        )\n\n        # \u542f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9\u8282\u7701\u663e\u5b58\n        self.model.gradient_checkpointing_enable()\n\n    def format_instruction_data(self, instruction_data, template_type=\"alpaca\"):\n        \"\"\"\u683c\u5f0f\u5316\u6307\u4ee4\u6570\u636e\"\"\"\n\n        formatted_data = []\n\n        templates = {\n            \"alpaca\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\",\n\n            \"vicuna\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\\nUSER: {instruction}\\nASSISTANT: {response}\",\n\n            \"chinese\": \"\u4ee5\u4e0b\u662f\u4e00\u4e2a\u63cf\u8ff0\u4efb\u52a1\u7684\u6307\u4ee4\u3002\u8bf7\u7f16\u5199\u4e00\u4e2a\u9002\u5f53\u5b8c\u6210\u8bf7\u6c42\u7684\u56de\u7b54\u3002\\n\\n### \u6307\u4ee4\uff1a\\n{instruction}\\n\\n### \u56de\u7b54\uff1a\\n{response}\",\n\n            \"simple\": \"### Question:\\n{instruction}\\n\\n### Answer:\\n{response}\"\n        }\n\n        template = templates.get(template_type, templates[\"alpaca\"])\n\n        for item in instruction_data:\n            formatted_text = template.format(\n                instruction=item[\"instruction\"],\n                response=item[\"response\"]\n            )\n\n            formatted_data.append({\n                \"text\": formatted_text,\n                \"instruction\": item[\"instruction\"],\n                \"response\": item[\"response\"]\n            })\n\n        return formatted_data\n\n    def tokenize_function(self, examples):\n        \"\"\"Tokenization\u51fd\u6570\"\"\"\n\n        # Tokenize\u5b8c\u6574\u6587\u672c\n        tokenized = self.tokenizer(\n            examples[\"text\"],\n            truncation=True,\n            padding=False,\n            max_length=self.max_seq_length,\n            return_tensors=None\n        )\n\n        # \u4e3a\u6307\u4ee4\u5fae\u8c03\u8bbe\u7f6elabels\n        # \u53ea\u5bf9response\u90e8\u5206\u8ba1\u7b97loss\uff0cinstruction\u90e8\u5206mask\u6389\n        input_ids = tokenized[\"input_ids\"]\n        labels = []\n\n        for i, text in enumerate(examples[\"text\"]):\n            # \u627e\u5230response\u5f00\u59cb\u4f4d\u7f6e\n            if \"### Response:\" in text:\n                response_start = text.find(\"### Response:\") + len(\"### Response:\")\n            elif \"ASSISTANT:\" in text:\n                response_start = text.find(\"ASSISTANT:\") + len(\"ASSISTANT:\")\n            elif \"### \u56de\u7b54\uff1a\" in text:\n                response_start = text.find(\"### \u56de\u7b54\uff1a\") + len(\"### \u56de\u7b54\uff1a\")\n            else:\n                response_start = len(text) // 2  # \u9ed8\u8ba4\u4ece\u4e2d\u95f4\u5f00\u59cb\n\n            # Tokenize\u5230response\u5f00\u59cb\u4f4d\u7f6e\u7684\u6587\u672c\n            prefix_tokens = self.tokenizer(\n                text[:response_start],\n                truncation=True,\n                padding=False,\n                max_length=self.max_seq_length,\n                return_tensors=None\n            )[\"input_ids\"]\n\n            # \u521b\u5efalabels\uff1ainstruction\u90e8\u5206\u4e3a-100\uff0cresponse\u90e8\u5206\u4e3a\u6b63\u5e38token\n            label = [-100] * len(prefix_tokens) + input_ids[i][len(prefix_tokens):]\n\n            # \u786e\u4fdd\u957f\u5ea6\u4e00\u81f4\n            if len(label) &gt; len(input_ids[i]):\n                label = label[:len(input_ids[i])]\n            elif len(label) &lt; len(input_ids[i]):\n                label.extend(input_ids[i][len(label):])\n\n            labels.append(label)\n\n        tokenized[\"labels\"] = labels\n        return tokenized\n\n    def create_data_collator(self):\n        \"\"\"\u521b\u5efa\u6570\u636e\u6574\u7406\u5668\"\"\"\n\n        return DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer,\n            mlm=False,  # \u4e0d\u4f7f\u7528\u63a9\u7801\u8bed\u8a00\u6a21\u578b\n            pad_to_multiple_of=8,  # \u4e3a\u4e86tensor core\u4f18\u5316\n        )\n\n    def train(self, train_data, eval_data=None, output_dir=\"./sft_output\", \n              epochs=3, batch_size=4, learning_rate=2e-4):\n        \"\"\"\u6267\u884cSFT\u8bad\u7ec3\"\"\"\n\n        # \u683c\u5f0f\u5316\u6570\u636e\n        formatted_train = self.format_instruction_data(train_data)\n        train_dataset = Dataset.from_list(formatted_train)\n\n        # Tokenize\u6570\u636e\n        tokenized_train = train_dataset.map(\n            self.tokenize_function,\n            batched=True,\n            remove_columns=train_dataset.column_names\n        )\n\n        # \u5904\u7406\u9a8c\u8bc1\u6570\u636e\n        eval_dataset = None\n        if eval_data:\n            formatted_eval = self.format_instruction_data(eval_data)\n            eval_dataset = Dataset.from_list(formatted_eval)\n            tokenized_eval = eval_dataset.map(\n                self.tokenize_function,\n                batched=True,\n                remove_columns=eval_dataset.column_names\n            )\n            eval_dataset = tokenized_eval\n\n        # \u8bad\u7ec3\u53c2\u6570\n        training_args = TrainingArguments(\n            output_dir=output_dir,\n\n            # \u57fa\u7840\u8bad\u7ec3\u8bbe\u7f6e\n            num_train_epochs=epochs,\n            per_device_train_batch_size=batch_size,\n            per_device_eval_batch_size=batch_size,\n            gradient_accumulation_steps=8,  # \u6709\u6548batch size = batch_size * 8\n\n            # \u4f18\u5316\u5668\u8bbe\u7f6e\n            learning_rate=learning_rate,\n            weight_decay=0.01,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n\n            # \u7cbe\u5ea6\u548c\u5185\u5b58\u4f18\u5316\n            bf16=True,\n            gradient_checkpointing=True,\n            max_grad_norm=1.0,\n\n            # \u8bc4\u4f30\u548c\u4fdd\u5b58\n            evaluation_strategy=\"steps\" if eval_dataset else \"no\",\n            eval_steps=500,\n            save_strategy=\"steps\",\n            save_steps=1000,\n            save_total_limit=3,\n            load_best_model_at_end=True if eval_dataset else False,\n            metric_for_best_model=\"eval_loss\" if eval_dataset else None,\n\n            # \u65e5\u5fd7\u8bbe\u7f6e\n            logging_dir=f\"{output_dir}/logs\",\n            logging_steps=100,\n            report_to=\"tensorboard\",\n\n            # \u5176\u4ed6\u4f18\u5316\n            dataloader_pin_memory=True,\n            dataloader_num_workers=4,\n            remove_unused_columns=False,\n        )\n\n        # \u521b\u5efa\u8bad\u7ec3\u5668\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=tokenized_train,\n            eval_dataset=eval_dataset,\n            data_collator=self.create_data_collator(),\n            tokenizer=self.tokenizer,\n        )\n\n        # \u5f00\u59cb\u8bad\u7ec3\n        print(\"\u5f00\u59cbSFT\u8bad\u7ec3...\")\n        train_result = trainer.train()\n\n        # \u4fdd\u5b58\u6a21\u578b\n        trainer.save_model()\n        trainer.save_state()\n\n        print(f\"\u8bad\u7ec3\u5b8c\u6210\uff01\u6a21\u578b\u4fdd\u5b58\u5728: {output_dir}\")\n        print(f\"\u8bad\u7ec3\u7edf\u8ba1: {train_result.metrics}\")\n\n        return trainer, train_result\n\n# \u591a\u4efb\u52a1SFT\u8bad\u7ec3\nclass MultiTaskSFTTrainer(SupervisedFineTuner):\n    \"\"\"\u591a\u4efb\u52a1SFT\u8bad\u7ec3\u5668\"\"\"\n\n    def __init__(self, model_name, task_weights=None):\n        super().__init__(model_name)\n        self.task_weights = task_weights or {}\n\n    def prepare_multitask_data(self, task_datasets):\n        \"\"\"\u51c6\u5907\u591a\u4efb\u52a1\u6570\u636e\"\"\"\n\n        all_data = []\n        task_info = {}\n\n        for task_name, task_data in task_datasets.items():\n            # \u8ba1\u7b97\u4efb\u52a1\u6743\u91cd\n            weight = self.task_weights.get(task_name, 1.0)\n\n            # \u6839\u636e\u6743\u91cd\u91c7\u6837\u6570\u636e\n            sample_size = int(len(task_data) * weight)\n            sampled_data = np.random.choice(task_data, sample_size, replace=False)\n\n            # \u6dfb\u52a0\u4efb\u52a1\u6807\u8bc6\n            for item in sampled_data:\n                item_with_task = item.copy()\n                item_with_task[\"task\"] = task_name\n                all_data.append(item_with_task)\n\n            task_info[task_name] = {\n                \"original_size\": len(task_data),\n                \"sampled_size\": sample_size,\n                \"weight\": weight\n            }\n\n        # \u968f\u673a\u6253\u4e71\n        np.random.shuffle(all_data)\n\n        print(\"\u591a\u4efb\u52a1\u6570\u636e\u7edf\u8ba1:\")\n        for task, info in task_info.items():\n            print(f\"  {task}: {info['sampled_size']} \u6837\u672c (\u6743\u91cd: {info['weight']})\")\n\n        return all_data, task_info\n\n    def task_aware_formatting(self, instruction_data):\n        \"\"\"\u4efb\u52a1\u611f\u77e5\u7684\u683c\u5f0f\u5316\"\"\"\n\n        task_templates = {\n            \"qa\": \"Question: {instruction}\\nAnswer: {response}\",\n            \"summarization\": \"Summarize the following text:\\n{instruction}\\n\\nSummary: {response}\",\n            \"translation\": \"Translate to English:\\n{instruction}\\n\\nTranslation: {response}\",\n            \"classification\": \"Classify the following text:\\n{instruction}\\n\\nCategory: {response}\",\n            \"generation\": \"Complete the following:\\n{instruction}\\n\\nCompletion: {response}\"\n        }\n\n        formatted_data = []\n\n        for item in instruction_data:\n            task = item.get(\"task\", \"general\")\n            template = task_templates.get(task, task_templates[\"qa\"])\n\n            formatted_text = template.format(\n                instruction=item[\"instruction\"],\n                response=item[\"response\"]\n            )\n\n            formatted_data.append({\n                \"text\": formatted_text,\n                \"task\": task,\n                \"instruction\": item[\"instruction\"],\n                \"response\": item[\"response\"]\n            })\n\n        return formatted_data\n\n# \u4f7f\u7528\u793a\u4f8b\ndef run_sft_training():\n    \"\"\"\u8fd0\u884cSFT\u8bad\u7ec3\u793a\u4f8b\"\"\"\n\n    # \u51c6\u5907\u8bad\u7ec3\u6570\u636e\n    train_data = [\n        {\n            \"instruction\": \"\u89e3\u91ca\u4ec0\u4e48\u662f\u6df1\u5ea6\u5b66\u4e60\",\n            \"response\": \"\u6df1\u5ea6\u5b66\u4e60\u662f\u673a\u5668\u5b66\u4e60\u7684\u4e00\u4e2a\u5206\u652f\uff0c\u5b83\u4f7f\u7528\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u6765\u5b66\u4e60\u6570\u636e\u7684\u590d\u6742\u6a21\u5f0f...\"\n        },\n        {\n            \"instruction\": \"\u5199\u4e00\u4e2aPython\u51fd\u6570\u8ba1\u7b97\u6590\u6ce2\u90a3\u5951\u6570\u5217\",\n            \"response\": \"def fibonacci(n):\\n    if n &lt;= 1:\\n        return n\\n    return fibonacci(n-1) + fibonacci(n-2)\"\n        },\n        # ... \u66f4\u591a\u8bad\u7ec3\u6570\u636e\n    ]\n\n    # \u521b\u5efa\u8bad\u7ec3\u5668\n    trainer = SupervisedFineTuner(\"microsoft/DialoGPT-medium\")\n\n    # \u6267\u884c\u8bad\u7ec3\n    trained_model, results = trainer.train(\n        train_data=train_data,\n        eval_data=train_data[:100],  # \u4f7f\u7528\u90e8\u5206\u6570\u636e\u4f5c\u4e3a\u9a8c\u8bc1\u96c6\n        output_dir=\"./my_sft_model\",\n        epochs=3,\n        batch_size=2,\n        learning_rate=2e-4\n    )\n\n    return trained_model, results\n\n# \u8fd0\u884c\u8bad\u7ec3\n# model, results = run_sft_training()\n</code></pre>"},{"location":"training/post-training/supervised-finetuning/#sft_2","title":"\ud83d\udcca SFT\u6548\u679c\u8bc4\u4f30","text":""},{"location":"training/post-training/supervised-finetuning/#_9","title":"\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6","text":"<pre><code>class SFTEvaluator:\n    \"\"\"SFT\u6548\u679c\u8bc4\u4f30\u5668\"\"\"\n\n    def __init__(self, base_model, sft_model, tokenizer):\n        self.base_model = base_model\n        self.sft_model = sft_model\n        self.tokenizer = tokenizer\n\n    def instruction_following_evaluation(self, test_instructions):\n        \"\"\"\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u8bc4\u4f30\"\"\"\n\n        results = {\n            \"base_model\": [],\n            \"sft_model\": [],\n            \"improvement_scores\": []\n        }\n\n        for instruction in test_instructions:\n            # \u57fa\u7840\u6a21\u578b\u54cd\u5e94\n            base_response = self.generate_response(self.base_model, instruction)\n            base_score = self.evaluate_instruction_following(instruction, base_response)\n\n            # SFT\u6a21\u578b\u54cd\u5e94\n            sft_response = self.generate_response(self.sft_model, instruction)\n            sft_score = self.evaluate_instruction_following(instruction, sft_response)\n\n            # \u8bb0\u5f55\u7ed3\u679c\n            results[\"base_model\"].append({\n                \"instruction\": instruction,\n                \"response\": base_response,\n                \"score\": base_score\n            })\n\n            results[\"sft_model\"].append({\n                \"instruction\": instruction,\n                \"response\": sft_response,\n                \"score\": sft_score\n            })\n\n            results[\"improvement_scores\"].append(sft_score - base_score)\n\n        # \u8ba1\u7b97\u603b\u4f53\u6307\u6807\n        avg_improvement = np.mean(results[\"improvement_scores\"])\n        improvement_rate = np.mean([s &gt; 0 for s in results[\"improvement_scores\"]])\n\n        return {\n            \"average_improvement\": avg_improvement,\n            \"improvement_rate\": improvement_rate,\n            \"detailed_results\": results\n        }\n\n    def evaluate_instruction_following(self, instruction, response):\n        \"\"\"\u8bc4\u4f30\u6307\u4ee4\u9075\u5faa\u8d28\u91cf\"\"\"\n\n        score = 0.0\n\n        # 1. \u683c\u5f0f\u68c0\u67e5 (0-0.3\u5206)\n        format_score = self.check_response_format(response)\n        score += format_score * 0.3\n\n        # 2. \u76f8\u5173\u6027\u68c0\u67e5 (0-0.4\u5206)\n        relevance_score = self.check_relevance(instruction, response)\n        score += relevance_score * 0.4\n\n        # 3. \u5b8c\u6574\u6027\u68c0\u67e5 (0-0.2\u5206)\n        completeness_score = self.check_completeness(instruction, response)\n        score += completeness_score * 0.2\n\n        # 4. \u8d28\u91cf\u68c0\u67e5 (0-0.1\u5206)\n        quality_score = self.check_response_quality(response)\n        score += quality_score * 0.1\n\n        return score\n\n    def task_specific_evaluation(self, task_test_suites):\n        \"\"\"\u4efb\u52a1\u7279\u5b9a\u8bc4\u4f30\"\"\"\n\n        task_results = {}\n\n        for task_name, test_suite in task_test_suites.items():\n            print(f\"\u8bc4\u4f30\u4efb\u52a1: {task_name}\")\n\n            task_scores = {\n                \"base_model\": [],\n                \"sft_model\": []\n            }\n\n            for test_case in test_suite:\n                instruction = test_case[\"instruction\"]\n                expected = test_case.get(\"expected\", \"\")\n\n                # \u751f\u6210\u56de\u7b54\n                base_response = self.generate_response(self.base_model, instruction)\n                sft_response = self.generate_response(self.sft_model, instruction)\n\n                # \u4efb\u52a1\u7279\u5b9a\u8bc4\u4f30\n                base_score = self.task_specific_score(\n                    task_name, instruction, base_response, expected\n                )\n                sft_score = self.task_specific_score(\n                    task_name, instruction, sft_response, expected\n                )\n\n                task_scores[\"base_model\"].append(base_score)\n                task_scores[\"sft_model\"].append(sft_score)\n\n            # \u8ba1\u7b97\u4efb\u52a1\u5e73\u5747\u5206\n            task_results[task_name] = {\n                \"base_avg\": np.mean(task_scores[\"base_model\"]),\n                \"sft_avg\": np.mean(task_scores[\"sft_model\"]),\n                \"improvement\": np.mean(task_scores[\"sft_model\"]) - np.mean(task_scores[\"base_model\"])\n            }\n\n        return task_results\n\n    def output_format_analysis(self, test_instructions):\n        \"\"\"\u8f93\u51fa\u683c\u5f0f\u5206\u6790\"\"\"\n\n        format_metrics = {\n            \"\u7ed3\u6784\u5316\u7a0b\u5ea6\": [],\n            \"\u957f\u5ea6\u9002\u4e2d\u6027\": [],\n            \"\u8bed\u8a00\u6d41\u7545\u6027\": [],\n            \"\u4e13\u4e1a\u6027\": []\n        }\n\n        for instruction in test_instructions:\n            sft_response = self.generate_response(self.sft_model, instruction)\n\n            # \u5206\u6790\u5404\u4e2a\u7ef4\u5ea6\n            format_metrics[\"\u7ed3\u6784\u5316\u7a0b\u5ea6\"].append(\n                self.analyze_structure(sft_response)\n            )\n            format_metrics[\"\u957f\u5ea6\u9002\u4e2d\u6027\"].append(\n                self.analyze_length_appropriateness(instruction, sft_response)\n            )\n            format_metrics[\"\u8bed\u8a00\u6d41\u7545\u6027\"].append(\n                self.analyze_fluency(sft_response)\n            )\n            format_metrics[\"\u4e13\u4e1a\u6027\"].append(\n                self.analyze_professionalism(sft_response)\n            )\n\n        # \u8ba1\u7b97\u5e73\u5747\u5206\n        avg_metrics = {\n            metric: np.mean(scores) \n            for metric, scores in format_metrics.items()\n        }\n\n        return avg_metrics\n\n    def generate_evaluation_report(self, instruction_eval, task_eval, format_eval):\n        \"\"\"\u751f\u6210\u8bc4\u4f30\u62a5\u544a\"\"\"\n\n        report = {\n            \"SFT\u8bad\u7ec3\u6548\u679c\u603b\u7ed3\": {\n                \"\u6307\u4ee4\u9075\u5faa\u6539\u5584\": f\"{instruction_eval['average_improvement']:.3f}\u5206 ({instruction_eval['improvement_rate']*100:.1f}%\u6837\u672c\u6539\u5584)\",\n                \"\u4efb\u52a1\u80fd\u529b\u63d0\u5347\": {\n                    task: f\"{results['improvement']:.3f}\u5206\" \n                    for task, results in task_eval.items()\n                },\n                \"\u8f93\u51fa\u683c\u5f0f\u8d28\u91cf\": {\n                    metric: f\"{score:.3f}\u5206\" \n                    for metric, score in format_eval.items()\n                }\n            },\n\n            \"\u5173\u952e\u53d1\u73b0\": {\n                \"\u6700\u4f73\u6539\u5584\u4efb\u52a1\": max(task_eval.items(), key=lambda x: x[1]['improvement'])[0],\n                \"\u9700\u8981\u5173\u6ce8\u7684\u4efb\u52a1\": [\n                    task for task, results in task_eval.items() \n                    if results['improvement'] &lt; 0.1\n                ],\n                \"\u683c\u5f0f\u5316\u8d28\u91cf\": \"\u826f\u597d\" if np.mean(list(format_eval.values())) &gt; 0.7 else \"\u9700\u8981\u6539\u8fdb\"\n            },\n\n            \"\u6539\u8fdb\u5efa\u8bae\": self.generate_improvement_suggestions(\n                instruction_eval, task_eval, format_eval\n            )\n        }\n\n        return report\n\n# \u8bc4\u4f30\u4f7f\u7528\u793a\u4f8b\ndef evaluate_sft_model(base_model_path, sft_model_path, tokenizer):\n    \"\"\"\u8bc4\u4f30SFT\u6a21\u578b\u6548\u679c\"\"\"\n\n    # \u52a0\u8f7d\u6a21\u578b\n    base_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n    sft_model = AutoModelForCausalLM.from_pretrained(sft_model_path)\n\n    # \u521b\u5efa\u8bc4\u4f30\u5668\n    evaluator = SFTEvaluator(base_model, sft_model, tokenizer)\n\n    # \u51c6\u5907\u6d4b\u8bd5\u6570\u636e\n    test_instructions = [\n        \"\u89e3\u91ca\u91cf\u5b50\u8ba1\u7b97\u7684\u57fa\u672c\u539f\u7406\",\n        \"\u5199\u4e00\u4e2a\u6392\u5e8f\u7b97\u6cd5\u7684Python\u5b9e\u73b0\",\n        \"\u5206\u6790\u8fd9\u6bb5\u4ee3\u7801\u7684\u65f6\u95f4\u590d\u6742\u5ea6\",\n        # ... \u66f4\u591a\u6d4b\u8bd5\u6307\u4ee4\n    ]\n\n    task_test_suites = {\n        \"\u95ee\u7b54\": [\n            {\"instruction\": \"\u4ec0\u4e48\u662f\u673a\u5668\u5b66\u4e60\uff1f\", \"expected\": \"\u8be6\u7ec6\u89e3\u91ca\"},\n            {\"instruction\": \"\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u5386\u53f2\", \"expected\": \"\u5386\u53f2\u4ecb\u7ecd\"}\n        ],\n        \"\u4ee3\u7801\u751f\u6210\": [\n            {\"instruction\": \"\u5199\u4e00\u4e2a\u5feb\u901f\u6392\u5e8f\u51fd\u6570\", \"expected\": \"Python\u4ee3\u7801\"},\n            {\"instruction\": \"\u5b9e\u73b0\u4e8c\u53c9\u6811\u904d\u5386\", \"expected\": \"\u7b97\u6cd5\u5b9e\u73b0\"}\n        ]\n    }\n\n    # \u6267\u884c\u8bc4\u4f30\n    instruction_results = evaluator.instruction_following_evaluation(test_instructions)\n    task_results = evaluator.task_specific_evaluation(task_test_suites)\n    format_results = evaluator.output_format_analysis(test_instructions)\n\n    # \u751f\u6210\u62a5\u544a\n    evaluation_report = evaluator.generate_evaluation_report(\n        instruction_results, task_results, format_results\n    )\n\n    print(\"SFT\u8bc4\u4f30\u62a5\u544a:\")\n    print(json.dumps(evaluation_report, indent=2, ensure_ascii=False))\n\n    return evaluation_report\n</code></pre>"},{"location":"training/post-training/supervised-finetuning/#_10","title":"\ud83c\udfaf \u9762\u8bd5\u95ee\u7b54\u603b\u7ed3","text":""},{"location":"training/post-training/supervised-finetuning/#q1-sft","title":"Q1: SFT\u5728\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u4f5c\u7528\u662f\u4ec0\u4e48\uff1f","text":"<p>A: SFT\u662f\u5173\u952e\u7684\u80fd\u529b\u8f6c\u6362\u9636\u6bb5\uff1a - \u529f\u80fd\u8f6c\u6362: \u4ece\u8bed\u8a00\u5efa\u6a21\u8f6c\u4e3a\u4efb\u52a1\u6267\u884c - \u683c\u5f0f\u89c4\u8303: \u6559\u4f1a\u6a21\u578b\u6807\u51c6\u8f93\u5165\u8f93\u51fa\u683c\u5f0f - \u6307\u4ee4\u9075\u5faa: \u5efa\u7acb\u57fa\u7840\u7684\u6307\u4ee4\u7406\u89e3\u548c\u6267\u884c\u80fd\u529b - \u540e\u7eed\u57fa\u7840: \u4e3aRLHF\u7b49\u540e\u7eed\u5bf9\u9f50\u8bad\u7ec3\u63d0\u4f9b\u57fa\u7840</p>"},{"location":"training/post-training/supervised-finetuning/#q2","title":"Q2: \u6307\u4ee4\u5fae\u8c03\u4e0e\u4f20\u7edf\u5fae\u8c03\u6709\u4ec0\u4e48\u533a\u522b\uff1f","text":"<p>A: - \u6570\u636e\u683c\u5f0f: \u6307\u4ee4\u5fae\u8c03\u4f7f\u7528\u6307\u4ee4-\u56de\u7b54\u5bf9\uff0c\u4f20\u7edf\u5fae\u8c03\u4f7f\u7528\u6807\u6ce8\u6570\u636e - \u8bad\u7ec3\u76ee\u6807: \u6307\u4ee4\u5fae\u8c03\u8bad\u7ec3\u901a\u7528\u6307\u4ee4\u9075\u5faa\uff0c\u4f20\u7edf\u5fae\u8c03\u8bad\u7ec3\u7279\u5b9a\u4efb\u52a1 - \u6cdb\u5316\u80fd\u529b: \u6307\u4ee4\u5fae\u8c03\u5177\u5907\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u4f20\u7edf\u5fae\u8c03\u5c40\u9650\u4e8e\u8bad\u7ec3\u4efb\u52a1 - \u5e94\u7528\u8303\u56f4: \u6307\u4ee4\u5fae\u8c03\u9002\u7528\u4e8e\u5bf9\u8bdd\u52a9\u624b\uff0c\u4f20\u7edf\u5fae\u8c03\u9002\u7528\u4e8e\u4e13\u95e8\u4efb\u52a1</p>"},{"location":"training/post-training/supervised-finetuning/#q3","title":"Q3: \u5982\u4f55\u6784\u5efa\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4\u6570\u636e\u96c6\uff1f","text":"<p>A: \u56db\u4e2a\u5173\u952e\u7ef4\u5ea6\uff1a - \u591a\u6837\u6027: \u4efb\u52a1\u7c7b\u578b\u3001\u9886\u57df\u3001\u96be\u5ea6\u3001\u98ce\u683c\u7684\u5168\u9762\u8986\u76d6 - \u51c6\u786e\u6027: \u56de\u7b54\u51c6\u786e\u3001\u4e8b\u5b9e\u6b63\u786e\u3001\u903b\u8f91\u6e05\u6670 - \u5b8c\u6574\u6027: \u56de\u7b54\u5b8c\u6574\u3001\u7ed3\u6784\u5316\u3001\u6ee1\u8db3\u6307\u4ee4\u8981\u6c42 - \u4e00\u81f4\u6027: \u683c\u5f0f\u7edf\u4e00\u3001\u98ce\u683c\u4e00\u81f4\u3001\u6807\u51c6\u89c4\u8303</p>"},{"location":"training/post-training/supervised-finetuning/#q4-sft","title":"Q4: SFT\u8bad\u7ec3\u4e2d\u5e38\u89c1\u7684\u95ee\u9898\u548c\u89e3\u51b3\u65b9\u6848\uff1f","text":"<p>A: - \u8fc7\u62df\u5408: \u4f7f\u7528\u9a8c\u8bc1\u96c6\u65e9\u505c\u3001\u589e\u52a0\u6570\u636e\u591a\u6837\u6027\u3001\u9002\u5f53\u6b63\u5219\u5316 - \u683c\u5f0f\u4e0d\u89c4\u8303: \u7edf\u4e00\u6570\u636e\u6a21\u677f\u3001\u6807\u51c6\u5316\u9884\u5904\u7406\u3001\u8d28\u91cf\u68c0\u67e5 - \u80fd\u529b\u4e0d\u5747\u8861: \u5e73\u8861\u91c7\u6837\u3001\u4efb\u52a1\u6743\u91cd\u3001\u591a\u8f6e\u8bad\u7ec3 - \u8ba1\u7b97\u8d44\u6e90\u4e0d\u8db3: \u91cf\u5316\u8bad\u7ec3\u3001\u68af\u5ea6\u7d2f\u79ef\u3001\u6a21\u578b\u5e76\u884c</p>"},{"location":"training/post-training/supervised-finetuning/#_11","title":"\ud83d\ude80 \u5b66\u4e60\u5efa\u8bae","text":"<ol> <li>\u7406\u89e3\u6838\u5fc3: \u6df1\u5165\u7406\u89e3SFT\u5728\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u7684\u5173\u952e\u4f5c\u7528</li> <li>\u6570\u636e\u4e3a\u738b: \u91cd\u70b9\u638c\u63e1\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u7684\u6784\u5efa\u65b9\u6cd5</li> <li>\u5b9e\u8df5\u9a8c\u8bc1: \u5728\u5b9e\u9645\u6570\u636e\u4e0a\u5b8c\u6574\u8dd1\u901aSFT\u8bad\u7ec3\u6d41\u7a0b</li> <li>\u6548\u679c\u8bc4\u4f30: \u5efa\u7acb\u591a\u7ef4\u5ea6\u7684SFT\u6548\u679c\u8bc4\u4f30\u4f53\u7cfb</li> </ol> <p>SFT\u662f\u8fde\u63a5\u9884\u8bad\u7ec3\u4e0e\u5e94\u7528\u7684\u5173\u952e\u6865\u6881\uff0c\u662f\u73b0\u4ee3LLM\u4e0d\u53ef\u6216\u7f3a\u7684\u6838\u5fc3\u6280\u672f\uff01</p>"},{"location":"training/rlhf-alignment/","title":"\u7b2c8\u8282\uff1a\u5f3a\u5316\u5b66\u4e60\u4e0e\u5bf9\u9f50\u6280\u672f","text":""},{"location":"training/rlhf-alignment/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u6df1\u5165\u638c\u63e1\u73b0\u4ee3LLM\u5bf9\u9f50\u6280\u672f\uff0c\u5305\u62ecRLHF\u3001DPO\u3001Constitutional AI\u7b49\u6838\u5fc3\u65b9\u6cd5\uff0c\u7406\u89e3\u5f3a\u5316\u5b66\u4e60\u5728LLM\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u5177\u5907\u6784\u5efa\u5b8c\u6574\u5bf9\u9f50\u7cfb\u7edf\u7684\u80fd\u529b\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - RLHF\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u8be6\u89e3 - DPO\u76f8\u6bd4RLHF\u7684\u4f18\u52bf\u548c\u52a3\u52bf - Constitutional AI\u7684\u6838\u5fc3\u7406\u5ff5\u548c\u5b9e\u73b0 - \u5956\u52b1\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u6311\u6218 - \u4e3b\u6d41RLHF\u6846\u67b6\u7684\u9009\u62e9\u548c\u4f7f\u7528</p>"},{"location":"training/rlhf-alignment/#_2","title":"\ud83d\udcc5 \u5b66\u4e60\u8ba1\u5212","text":"<p>\u5efa\u8bae\u5b66\u4e60\u65f6\u95f4\uff1a3-4\u5929</p> <ul> <li>Day 1: RLHF\u6838\u5fc3\u6280\u672f + PPO\u7b97\u6cd5\u539f\u7406</li> <li>Day 2: DPO\u6280\u672f + Constitutional AI\u65b9\u6cd5  </li> <li>Day 3: \u5956\u52b1\u6a21\u578b\u8bad\u7ec3 + \u8bc4\u4f30\u6280\u672f</li> <li>Day 4: \u5b9e\u73b0\u6846\u67b6\u638c\u63e1 + \u73b0\u4ee3\u8fed\u4ee3\u8bad\u7ec3</li> </ul>"},{"location":"training/rlhf-alignment/#llm","title":"\ud83c\udfae \u5f3a\u5316\u5b66\u4e60\u5728LLM\u4e2d\u7684\u9769\u547d","text":""},{"location":"training/rlhf-alignment/#_3","title":"\u6838\u5fc3\u4ef7\u503c","text":"<p>\u5f3a\u5316\u5b66\u4e60\u4ece\u4eba\u7c7b\u53cd\u9988(RLHF)\u6280\u672f\u9769\u547d\u6027\u5730\u6539\u53d8\u4e86\u5927\u6a21\u578b\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f7f\u5f97AI\u7cfb\u7edf\u80fd\u591f\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u3002</p> <pre><code>LLM\u5bf9\u9f50\u6280\u672f\u6f14\u8fdb\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   \u4f20\u7edf\u65b9\u6cd5      \u2502\u2500\u2500\u2500\u25b6\u2502   RLHF\u65f6\u4ee3      \u2502\u2500\u2500\u2500\u25b6\u2502   \u73b0\u4ee3\u5bf9\u9f50      \u2502\n\u2502                \u2502    \u2502                 \u2502    \u2502                 \u2502\n\u2502 \u2022 \u76d1\u7763\u5b66\u4e60      \u2502    \u2502 \u2022 \u4eba\u7c7b\u53cd\u9988      \u2502    \u2502 \u2022 DPO\u7b80\u5316       \u2502\n\u2502 \u2022 \u89c4\u5219\u7ea6\u675f      \u2502    \u2502 \u2022 PPO\u4f18\u5316       \u2502    \u2502 \u2022 Constitutional\u2502\n\u2502 \u2022 \u786c\u7f16\u7801\u4ef7\u503c    \u2502    \u2502 \u2022 \u5956\u52b1\u5efa\u6a21      \u2502    \u2502 \u2022 RLAIF\u81ea\u52a8\u5316   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n        2020\u5e74\u524d              2020-2022\u5e74           2023\u5e74\u81f3\u4eca\n</code></pre>"},{"location":"training/rlhf-alignment/#_4","title":"\u6280\u672f\u6210\u719f\u5ea6\u77e9\u9635","text":"\u6280\u672f \u6210\u719f\u5ea6 \u5de5\u4e1a\u5e94\u7528 \u5b66\u672f\u7814\u7a76 \u672a\u6765\u6f5c\u529b RLHF \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2606\u2606 DPO \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2606 Constitutional AI \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 RLAIF \u2605\u2605\u2605\u2606\u2606 \u2605\u2605\u2606\u2606\u2606 \u2605\u2605\u2605\u2605\u2606 \u2605\u2605\u2605\u2605\u2605 Self-Rewarding \u2605\u2605\u2606\u2606\u2606 \u2605\u2606\u2606\u2606\u2606 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605"},{"location":"training/rlhf-alignment/#_5","title":"\ud83d\udcda \u5b66\u4e60\u8def\u5f84","text":""},{"location":"training/rlhf-alignment/#1-rlhf","title":"1. RLHF\u6838\u5fc3\u6280\u672f","text":"<p>\u91cd\u70b9\u638c\u63e1\uff1a\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\u7684\u5b8c\u6574\u6d41\u7a0b</p> <ul> <li>\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b</li> <li>SFT\u76d1\u7763\u5fae\u8c03\uff1a\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u8bad\u7ec3</li> <li>\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\uff1a\u4eba\u7c7b\u504f\u597d\u5efa\u6a21</li> <li> <p>PPO\u5f3a\u5316\u5b66\u4e60\uff1a\u7b56\u7565\u4f18\u5316\u4e0e\u5bf9\u9f50</p> </li> <li> <p>PPO\u7b97\u6cd5\u6df1\u5ea6\u89e3\u6790</p> </li> <li>\u4e3a\u4ec0\u4e48\u9009\u62e9PPO\uff1a\u7a33\u5b9a\u6027\u4e0e\u6548\u7387\u5e73\u8861</li> <li>clip\u673a\u5236\uff1a\u9632\u6b62\u7b56\u7565\u66f4\u65b0\u8fc7\u5927</li> <li> <p>KL\u6563\u5ea6\u7ea6\u675f\uff1a\u4fdd\u6301\u4e0e\u53c2\u8003\u6a21\u578b\u7684\u8ddd\u79bb</p> </li> <li> <p>\u5173\u952e\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848</p> </li> <li>\u5956\u52b1\u9ed1\u5ba2\uff1a\u6a21\u578b\u5229\u7528\u5956\u52b1\u51fd\u6570\u6f0f\u6d1e</li> <li>\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1a\u68af\u5ea6\u7206\u70b8\u548c\u6536\u655b\u95ee\u9898</li> <li>\u8ba1\u7b97\u8d44\u6e90\uff1a\u591a\u6a21\u578b\u5e76\u884c\u8bad\u7ec3\u9700\u6c42</li> </ul>"},{"location":"training/rlhf-alignment/#2-dpoconstitutional-ai","title":"2. DPO\u4e0eConstitutional AI","text":"<p>\u91cd\u70b9\u638c\u63e1\uff1a\u73b0\u4ee3\u5316\u7684\u5bf9\u9f50\u65b9\u6cd5</p> <ul> <li>DPO\u76f4\u63a5\u504f\u597d\u4f18\u5316</li> <li>\u6838\u5fc3\u4f18\u52bf\uff1a\u7b80\u5316\u8bad\u7ec3\u6d41\u7a0b\uff0c\u65e0\u9700\u5956\u52b1\u6a21\u578b</li> <li>\u6570\u5b66\u539f\u7406\uff1aBradley-Terry\u6a21\u578b\u7684\u76f4\u63a5\u4f18\u5316</li> <li> <p>\u5b9e\u73b0\u6280\u5de7\uff1a\u53c2\u8003\u6a21\u578b\u51bb\u7ed3\uff0c\u504f\u597d\u6570\u636e\u5904\u7406</p> </li> <li> <p>Constitutional AI\u65b9\u6cd5</p> </li> <li>\u7406\u5ff5\u521b\u65b0\uff1a\u901a\u8fc7\u89c4\u5219Constitution\u6307\u5bfc\u884c\u4e3a</li> <li>\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u81ea\u6211\u6279\u8bc4\u6539\u8fdb + RLAIF\u8bad\u7ec3</li> <li> <p>\u89c4\u5219\u8bbe\u8ba1\uff1a\u5982\u4f55\u6784\u5efa\u6709\u6548\u7684Constitution</p> </li> <li> <p>RLAIF\u6280\u672f</p> </li> <li>AI\u53cd\u9988vs\u4eba\u7c7b\u53cd\u9988\uff1a\u53ef\u6269\u5c55\u6027\u4e0e\u8d28\u91cf\u5e73\u8861</li> <li>\u81ea\u52a8\u5316\u4f18\u52bf\uff1a\u6210\u672c\u964d\u4f4e\uff0c\u89c4\u6a21\u6269\u5927</li> <li>\u8d28\u91cf\u63a7\u5236\uff1a\u786e\u4fddAI\u8bc4\u5224\u7684\u53ef\u9760\u6027</li> </ul>"},{"location":"training/rlhf-alignment/#3","title":"3. \u5956\u52b1\u6a21\u578b\u8bad\u7ec3","text":"<p>\u91cd\u70b9\u638c\u63e1\uff1aRLHF\u7684\u6838\u5fc3\u7ec4\u4ef6</p> <ul> <li>Bradley-Terry\u6a21\u578b\u7406\u8bba</li> <li>\u504f\u597d\u6982\u7387\u5efa\u6a21\uff1aP(A&gt;B) = \u03c3(r(A)-r(B))</li> <li>\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\uff1a\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u76ee\u6807</li> <li> <p>\u6570\u636e\u683c\u5f0f\uff1a\u4e09\u5143\u7ec4(prompt, chosen, rejected)</p> </li> <li> <p>\u8bad\u7ec3\u6280\u5de7\u4e0e\u6700\u4f73\u5b9e\u8df5</p> </li> <li>\u6570\u636e\u8d28\u91cf\u63a7\u5236\uff1a\u957f\u5ea6\u8fc7\u6ee4\u3001\u591a\u6837\u6027\u68c0\u67e5</li> <li>\u6a21\u578b\u67b6\u6784\uff1a\u5206\u7c7b\u5934\u8bbe\u8ba1\uff0cdropout\u9632\u8fc7\u62df\u5408</li> <li> <p>\u8d85\u53c2\u6570\u8c03\u4f18\uff1a\u5b66\u4e60\u7387\u3001batch size\u9009\u62e9</p> </li> <li> <p>\u8bc4\u4f30\u4e0e\u8bca\u65ad\u65b9\u6cd5</p> </li> <li>\u51c6\u786e\u7387\u8bc4\u4f30\uff1a\u504f\u597d\u9884\u6d4b\u6b63\u786e\u7387</li> <li>\u6821\u51c6\u7a0b\u5ea6\uff1a\u671f\u671b\u6821\u51c6\u8bef\u5dee(ECE)</li> <li> <p>RewardBench\u57fa\u51c6\uff1a\u5de5\u4e1a\u6807\u51c6\u8bc4\u6d4b</p> </li> <li> <p>\u524d\u6cbf\u6280\u672f\u8d8b\u52bf</p> </li> <li>Self-Rewarding\uff1a\u6a21\u578b\u81ea\u6211\u5956\u52b1\u673a\u5236</li> <li>CLoud\u65b9\u6cd5\uff1a\u5148\u6279\u8bc4\u540e\u8bc4\u5206\u7b56\u7565</li> <li>Ensemble\u6280\u672f\uff1a\u591a\u6a21\u578b\u96c6\u6210\u63d0\u5347\u9c81\u68d2\u6027</li> </ul>"},{"location":"training/rlhf-alignment/#4","title":"4. \u5b9e\u73b0\u6846\u67b6\u4e0e\u5b9e\u6218","text":"<p>\u91cd\u70b9\u638c\u63e1\uff1a\u5de5\u7a0b\u5b9e\u73b0\u548c\u6846\u67b6\u9009\u62e9</p> <ul> <li>\u4e3b\u6d41\u6846\u67b6\u5bf9\u6bd4\u5206\u6790</li> <li>TRL\uff1a\u6613\u7528\u6027\u597d\uff0cHuggingFace\u751f\u6001</li> <li>OpenRLHF\uff1a\u9ad8\u6027\u80fd\uff0c\u652f\u6301\u5927\u89c4\u6a21\u8bad\u7ec3</li> <li>TRLX\uff1a\u7075\u6d3b\u53ef\u5b9a\u5236\uff0c\u7814\u7a76\u5bfc\u5411</li> <li> <p>DeepSpeed-Chat\uff1a\u6781\u81f4\u6027\u80fd\u4f18\u5316</p> </li> <li> <p>\u5b8c\u6574\u5b9e\u73b0\u6d41\u7a0b</p> </li> <li>\u73af\u5883\u914d\u7f6e\uff1a\u4f9d\u8d56\u5b89\u88c5\uff0cGPU\u8981\u6c42</li> <li>\u6570\u636e\u51c6\u5907\uff1a\u683c\u5f0f\u8f6c\u6362\uff0c\u8d28\u91cf\u68c0\u67e5</li> <li> <p>\u4e09\u9636\u6bb5\u8bad\u7ec3\uff1aSFT \u2192 RM \u2192 PPO\u5b8c\u6574\u6d41\u7a0b</p> </li> <li> <p>\u6027\u80fd\u4f18\u5316\u6280\u672f</p> </li> <li>\u5185\u5b58\u4f18\u5316\uff1a\u68af\u5ea6\u68c0\u67e5\u70b9\uff0cCPU\u5378\u8f7d</li> <li>\u8ba1\u7b97\u4f18\u5316\uff1a\u6df7\u5408\u7cbe\u5ea6\uff0c\u6a21\u578b\u5e76\u884c</li> <li>\u5206\u5e03\u5f0f\u8bad\u7ec3\uff1a\u591a\u5361\u591a\u673a\u534f\u540c</li> </ul>"},{"location":"training/rlhf-alignment/#_6","title":"\ud83d\udd2c \u6280\u672f\u6df1\u5ea6\u5206\u6790","text":""},{"location":"training/rlhf-alignment/#2024","title":"2024\u5e74\u540e\u8bad\u7ec3\u6280\u672f\u9769\u547d","text":"<p>\u57fa\u4e8e\u6700\u65b0\u7814\u7a76\uff0c\u73b0\u4ee3\u540e\u8bad\u7ec3\u5448\u73b0\u4ee5\u4e0b\u8d8b\u52bf\uff1a</p> <pre><code>\u73b0\u4ee3\u540e\u8bad\u7ec3\u6280\u672f\u6808 (2024)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    \u6570\u636e\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u5408\u6210\u6570\u636e\u751f\u6210 \u2192 \u8d28\u91cf\u7b5b\u9009 \u2192 \u591a\u8f6e\u8bad\u7ec3 \u2192 \u6a21\u578b\u81ea\u4e3e \u2192 \u6301\u7eed\u6539\u8fdb        \u2502\n\u2502       \u25b2              \u25b2          \u25b2          \u25b2          \u25b2         \u2502\n\u2502  LLM\u8f85\u52a9\u751f\u6210    AI\u8d28\u91cf\u8bc4\u4f30   PPO/DPO\u6df7\u5408  Self-Rewarding  \u5728\u7ebf\u5b66\u4e60 \u2502\n\u2502                                                                 \u2502\n\u2502  \u5173\u952e\u521b\u65b0\u70b9\uff1a                                                    \u2502\n\u2502  1. \u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\uff1a70-80%\u7684\u8bad\u7ec3\u6570\u636e\u6765\u81ea\u6a21\u578b\u751f\u6210                  \u2502\n\u2502  2. \u8fed\u4ee3\u4f18\u5316\u5faa\u73af\uff1a5-6\u8f6e\u6301\u7eed\u6539\u8fdb\uff0c\u6bcf\u8f6e10K-100K\u6837\u672c               \u2502\n\u2502  3. \u8d28\u91cf\u81f3\u4e0a\u539f\u5219\uff1a\u6570\u636e\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u91cd\u8981                          \u2502\n\u2502  4. RLHF\u53ef\u6269\u5c55\u6027\uff1a\u6bd4\u6307\u4ee4\u5fae\u8c03\u66f4\u5bb9\u6613\u6269\u5c55\uff0c\u6210\u672c\u66f4\u4f4e                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/rlhf-alignment/#_7","title":"\u524d\u6cbf\u6a21\u578b\u8bad\u7ec3\u914d\u65b9","text":"<p>\u57fa\u4e8eDeepSeek R1\u3001OpenAI o1\u7b49\u524d\u6cbf\u6a21\u578b\u7684\u8bad\u7ec3\u5b9e\u8df5\uff1a</p> \u7ef4\u5ea6 \u4f20\u7edf\u65b9\u6cd5 2024\u73b0\u4ee3\u65b9\u6cd5 \u5173\u952e\u521b\u65b0 \u6570\u636e\u6765\u6e90 \u4eba\u5de5\u6807\u6ce8\u4e3a\u4e3b \u5408\u6210\u6570\u636e\u4e3a\u4e3b(70-80%) LLM\u8f85\u52a9\u751f\u6210 \u8bad\u7ec3\u8f6e\u6570 \u5355\u8f6e\u8bad\u7ec3 \u591a\u8f6e\u8fed\u4ee3(5-6\u8f6e) \u6301\u7eed\u6539\u8fdb \u8d28\u91cf\u63a7\u5236 \u968f\u673a\u91c7\u6837 \u4e25\u683c\u7b5b\u9009(\u4fdd\u7559top 20%) AI\u8f85\u52a9\u8bc4\u4f30 \u5bf9\u9f50\u65b9\u6cd5 RLHF RLHF + DPO\u6df7\u5408 \u7075\u6d3b\u7ec4\u5408 \u8bc4\u4f30\u4f53\u7cfb \u9759\u6001\u57fa\u51c6 \u52a8\u6001\u8bc4\u6d4b + \u7ea2\u961f\u6d4b\u8bd5 \u5bf9\u6297\u6027\u8bc4\u4f30"},{"location":"training/rlhf-alignment/#rlhf-vs","title":"RLHF vs \u4f20\u7edf\u65b9\u6cd5\u5bf9\u6bd4","text":"<pre><code>\u6280\u672f\u7279\u5f81\u5bf9\u6bd4\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    \u4f20\u7edf\u76d1\u7763\u5b66\u4e60   \u2502      RLHF       \u2502       DPO        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 \u6570\u636e\u9700\u6c42: \u5927\u91cf    \u2502 \u6570\u636e\u9700\u6c42: \u4e2d\u7b49    \u2502 \u6570\u636e\u9700\u6c42: \u5c11\u91cf    \u2502\n\u2502 \u6807\u6ce8\u6210\u672c: \u9ad8      \u2502 \u6807\u6ce8\u6210\u672c: \u5f88\u9ad8    \u2502 \u6807\u6ce8\u6210\u672c: \u9ad8      \u2502\n\u2502 \u8bad\u7ec3\u590d\u6742\u5ea6: \u4f4e    \u2502 \u8bad\u7ec3\u590d\u6742\u5ea6: \u5f88\u9ad8  \u2502 \u8bad\u7ec3\u590d\u6742\u5ea6: \u4e2d    \u2502\n\u2502 \u5bf9\u9f50\u6548\u679c: \u4e00\u822c    \u2502 \u5bf9\u9f50\u6548\u679c: \u4f18\u79c0    \u2502 \u5bf9\u9f50\u6548\u679c: \u826f\u597d    \u2502\n\u2502 \u8ba1\u7b97\u8d44\u6e90: \u4e2d\u7b49    \u2502 \u8ba1\u7b97\u8d44\u6e90: \u5de8\u5927    \u2502 \u8ba1\u7b97\u8d44\u6e90: \u8f83\u5927    \u2502\n\u2502 \u8bad\u7ec3\u7a33\u5b9a\u6027: \u597d    \u2502 \u8bad\u7ec3\u7a33\u5b9a\u6027: \u5dee    \u2502 \u8bad\u7ec3\u7a33\u5b9a\u6027: \u8f83\u597d  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/rlhf-alignment/#_8","title":"\u6210\u672c\u6548\u76ca\u5206\u6790","text":"\u65b9\u6cd5 \u6570\u636e\u6807\u6ce8\u6210\u672c \u8ba1\u7b97\u6210\u672c \u5f00\u53d1\u5468\u671f \u6548\u679c\u8d28\u91cf \u603b\u4f53ROI \u76d1\u7763\u5fae\u8c03 $10K $1K 1\u5468 70% \u2605\u2605\u2605\u2606\u2606 RLHF $100K $50K 2\u6708 95% \u2605\u2605\u2605\u2605\u2606 DPO $50K $10K 3\u5468 85% \u2605\u2605\u2605\u2605\u2605 Constitutional AI $20K $15K 1\u6708 88% \u2605\u2605\u2605\u2605\u2606"},{"location":"training/rlhf-alignment/#_9","title":"\ud83d\udcc8 \u5b9e\u9645\u5e94\u7528\u6848\u4f8b","text":""},{"location":"training/rlhf-alignment/#_10","title":"\u6210\u529f\u6848\u4f8b\u5206\u6790","text":"<ol> <li>ChatGPT\u7cfb\u5217</li> <li>\u6280\u672f\u6808\uff1aSFT + RLHF(PPO)</li> <li>\u521b\u65b0\u70b9\uff1a\u5927\u89c4\u6a21\u4eba\u7c7b\u53cd\u9988\u6536\u96c6</li> <li> <p>\u6548\u679c\uff1a\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\u548c\u5b89\u5168\u6027</p> </li> <li> <p>Claude\u7cfb\u5217</p> </li> <li>\u6280\u672f\u6808\uff1aConstitutional AI + RLAIF</li> <li>\u521b\u65b0\u70b9\uff1a\u89c4\u5219\u9a71\u52a8\u7684\u4ef7\u503c\u5bf9\u9f50</li> <li> <p>\u6548\u679c\uff1a\u66f4\u597d\u7684\u65e0\u5bb3\u6027\u548c\u8bda\u5b9e\u6027</p> </li> <li> <p>Llama2-Chat</p> </li> <li>\u6280\u672f\u6808\uff1aSFT + RLHF</li> <li>\u521b\u65b0\u70b9\uff1a\u5f00\u6e90RLHF\u6700\u4f73\u5b9e\u8df5</li> <li>\u6548\u679c\uff1a\u5f00\u6e90\u6a21\u578b\u5bf9\u9f50\u6807\u6746</li> </ol>"},{"location":"training/rlhf-alignment/#_11","title":"\u5de5\u4e1a\u90e8\u7f72\u8003\u91cf","text":"<pre><code># \u90e8\u7f72\u51b3\u7b56\u6811\ndef choose_alignment_method(budget, timeline, quality_requirement):\n    \"\"\"\u6839\u636e\u5b9e\u9645\u7ea6\u675f\u9009\u62e9\u5bf9\u9f50\u65b9\u6cd5\"\"\"\n\n    if budget &lt; 50000:  # \u9884\u7b97\u6709\u9650\n        if timeline &lt; 1:  # \u65f6\u95f4\u7d27\u6025\n            return \"\u76d1\u7763\u5fae\u8c03 + \u7b80\u5355\u89c4\u5219\"\n        else:\n            return \"DPO\u65b9\u6cd5\"\n\n    elif quality_requirement &gt; 90:  # \u9ad8\u8d28\u91cf\u8981\u6c42\n        if budget &gt; 200000:  # \u9884\u7b97\u5145\u8db3\n            return \"\u5b8c\u6574RLHF\u6d41\u7a0b\"\n        else:\n            return \"Constitutional AI\"\n\n    else:  # \u5e73\u8861\u8003\u8651\n        return \"DPO + \u540e\u5904\u7406\u89c4\u5219\"\n</code></pre>"},{"location":"training/rlhf-alignment/#_12","title":"\u2705 \u5b66\u4e60\u68c0\u9a8c\u6807\u51c6","text":"<p>\u5b8c\u6210\u4ee5\u4e0b\u56db\u4e2a\u5c42\u6b21\u624d\u7b97\u638c\u63e1\u672c\u8282\uff1a</p>"},{"location":"training/rlhf-alignment/#_13","title":"\ud83e\udde0 \u7406\u8bba\u7406\u89e3\u5c42","text":"<ul> <li>[ ] \u80fd\u6e05\u6670\u89e3\u91caRLHF\u4e09\u9636\u6bb5\u7684\u5de5\u4f5c\u539f\u7406\u548c\u5fc5\u8981\u6027</li> <li>[ ] \u7406\u89e3PPO\u7b97\u6cd5\u7684\u6570\u5b66\u539f\u7406\u548cclip\u673a\u5236</li> <li>[ ] \u638c\u63e1Bradley-Terry\u6a21\u578b\u5728\u5956\u52b1\u5efa\u6a21\u4e2d\u7684\u5e94\u7528</li> <li>[ ] \u7406\u89e3DPO\u76f8\u5bf9RLHF\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027</li> </ul>"},{"location":"training/rlhf-alignment/#_14","title":"\ud83d\udcbb \u5b9e\u8df5\u80fd\u529b\u5c42","text":"<ul> <li>[ ] \u80fd\u4f7f\u7528TRL\u6846\u67b6\u5b8c\u6210\u5b8c\u6574RLHF\u8bad\u7ec3\u6d41\u7a0b</li> <li>[ ] \u80fd\u8bad\u7ec3\u548c\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u7684\u8d28\u91cf</li> <li>[ ] \u80fd\u5b9e\u73b0DPO\u8bad\u7ec3\u5e76\u4e0eRLHF\u6548\u679c\u5bf9\u6bd4</li> <li>[ ] \u80fd\u8bbe\u8ba1Constitutional\u89c4\u5219\u5e76\u5e94\u7528RLAIF</li> </ul>"},{"location":"training/rlhf-alignment/#_15","title":"\ud83d\udd27 \u5de5\u7a0b\u5e94\u7528\u5c42","text":"<ul> <li>[ ] \u80fd\u9009\u62e9\u5408\u9002\u7684\u6846\u67b6\u548c\u5de5\u5177\u94fe</li> <li>[ ] \u80fd\u5904\u7406\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u548c\u6027\u80fd\u95ee\u9898</li> <li>[ ] \u80fd\u8bbe\u8ba1\u5b8c\u6574\u7684\u8bad\u7ec3\u76d1\u63a7\u548c\u8bc4\u4f30\u4f53\u7cfb</li> <li>[ ] \u80fd\u6392\u67e5\u548c\u89e3\u51b3\u8bad\u7ec3\u4e2d\u7684\u5e38\u89c1\u95ee\u9898</li> </ul>"},{"location":"training/rlhf-alignment/#_16","title":"\ud83c\udfa4 \u9762\u8bd5\u51c6\u5907\u5c42","text":"<ul> <li>[ ] \u80fd\u56de\u7b54\u6240\u6709\u6838\u5fc3\u6280\u672f\u7684\u9762\u8bd5\u95ee\u9898</li> <li>[ ] \u80fd\u5206\u6790\u4e0d\u540c\u65b9\u6cd5\u7684trade-off\u548c\u9002\u7528\u573a\u666f</li> <li>[ ] \u80fd\u63cf\u8ff0\u5177\u4f53\u7684\u9879\u76ee\u5b9e\u65bd\u7ecf\u9a8c</li> <li>[ ] \u80fd\u8ba8\u8bba\u6280\u672f\u53d1\u5c55\u8d8b\u52bf\u548c\u672a\u6765\u65b9\u5411</li> </ul>"},{"location":"training/rlhf-alignment/#_17","title":"\ud83d\udca1 \u5b66\u4e60\u5efa\u8bae","text":""},{"location":"training/rlhf-alignment/#_18","title":"\ud83c\udfaf \u5b66\u4e60\u91cd\u70b9\u6392\u5e8f","text":"<ol> <li>\u4f18\u5148\u7ea7P0: RLHF\u6838\u5fc3\u6d41\u7a0b (\u9762\u8bd5\u5fc5\u8003)</li> <li>\u4f18\u5148\u7ea7P1: DPO\u6280\u672f\u539f\u7406 (2024\u70ed\u70b9)</li> <li>\u4f18\u5148\u7ea7P1: \u5956\u52b1\u6a21\u578b\u8bad\u7ec3 (\u6280\u672f\u6838\u5fc3)</li> <li>\u4f18\u5148\u7ea7P2: \u6846\u67b6\u4f7f\u7528\u6280\u5de7 (\u5de5\u7a0b\u80fd\u529b)</li> </ol>"},{"location":"training/rlhf-alignment/#_19","title":"\ud83d\udcd6 \u63a8\u8350\u5b66\u4e60\u8def\u5f84","text":"<ul> <li>\u7406\u8bba\u5148\u884c: \u5148\u7406\u89e3\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u6982\u5ff5</li> <li>\u4ee3\u7801\u5b9e\u8df5: \u8dd1\u901aTRL\u7684\u5b8c\u6574RLHF Demo</li> <li>\u6846\u67b6\u5bf9\u6bd4: \u4e86\u89e3\u4e0d\u540c\u6846\u67b6\u7684\u4f18\u52a3\u52bf</li> <li>\u9879\u76ee\u5b9e\u6218: \u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8bad\u7ec3\u5bf9\u9f50\u6a21\u578b</li> </ul>"},{"location":"training/rlhf-alignment/#_20","title":"\u26a0\ufe0f \u5e38\u89c1\u5b66\u4e60\u8bef\u533a","text":"<ul> <li>\u8bef\u533a1: \u5ffd\u89c6\u5956\u52b1\u6a21\u578b\u7684\u91cd\u8981\u6027</li> <li>\u8bef\u533a2: \u53ea\u5173\u6ce8PPO\u7b97\u6cd5\uff0c\u4e0d\u7406\u89e3\u6574\u4f53\u6d41\u7a0b</li> <li>\u8bef\u533a3: \u8fc7\u5ea6\u8ff7\u4fe1\u67d0\u79cd\u6280\u672f\uff0c\u4e0d\u8003\u8651\u9002\u7528\u573a\u666f</li> <li>\u8bef\u533a4: \u53ea\u5173\u6ce8\u7406\u8bba\uff0c\u7f3a\u4e4f\u5b9e\u8df5\u7ecf\u9a8c</li> </ul>"},{"location":"training/rlhf-alignment/#_21","title":"\ud83d\udd2e \u524d\u6cbf\u6280\u672f\u8d8b\u52bf","text":""},{"location":"training/rlhf-alignment/#2024-2025","title":"2024-2025\u5e74\u53d1\u5c55\u65b9\u5411","text":"<ol> <li>\u7b97\u6cd5\u521b\u65b0</li> <li>Group Relative Policy Optimization (GRPO) - DeepSeek R1\u91c7\u7528</li> <li>Reinforced Token Optimization (RTO) - token\u7ea7\u5956\u52b1\u4f18\u5316</li> <li>Multi-step reasoning optimization - \u591a\u6b65\u63a8\u7406\u5f3a\u5316\u5b66\u4e60</li> <li> <p>Online DPO - \u5728\u7ebf\u504f\u597d\u4f18\u5316</p> </li> <li> <p>\u6570\u636e\u5de5\u7a0b\u9769\u547d</p> </li> <li>\u5408\u6210\u6570\u636e\u4e3b\u5bfc\uff1a70-80%\u8bad\u7ec3\u6570\u636e\u6765\u81eaAI\u751f\u6210</li> <li>\u8d28\u91cf\u7b5b\u9009\u81ea\u52a8\u5316\uff1aAI\u8bc4\u4f30 + \u4eba\u5de5\u9a8c\u8bc1</li> <li>\u8fed\u4ee3\u6570\u636e\u751f\u6210\uff1a\u6a21\u578b\u81ea\u4e3e\u6539\u8fdb\u6570\u636e\u8d28\u91cf</li> <li> <p>\u591a\u6837\u6027\u4fdd\u8bc1\u673a\u5236\uff1atopic clustering + balanced sampling</p> </li> <li> <p>\u8bad\u7ec3\u8303\u5f0f\u521b\u65b0</p> </li> <li>\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\uff1a5-6\u8f6e\u6301\u7eed\u6539\u8fdb</li> <li>\u6df7\u5408\u5bf9\u9f50\u65b9\u6cd5\uff1aRLHF + DPO + Constitutional AI</li> <li>\u81ea\u9002\u5e94\u8bad\u7ec3\uff1a\u6839\u636e\u6a21\u578b\u8868\u73b0\u52a8\u6001\u8c03\u6574</li> <li> <p>\u6301\u7eed\u5b66\u4e60\u80fd\u529b\uff1a\u5728\u7ebf\u66f4\u65b0\u548c\u9002\u5e94</p> </li> <li> <p>\u7cfb\u7edf\u4f18\u5316</p> </li> <li>\u66f4\u9ad8\u6548\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u67b6\u6784</li> <li>\u81ea\u52a8\u5316\u8d85\u53c2\u6570\u548c\u6570\u636e\u914d\u6bd4\u8c03\u4f18</li> <li>\u7aef\u5230\u7aef\u5bf9\u9f50\u7cfb\u7edf\u5e73\u53f0\u5316</li> <li> <p>\u5b9e\u65f6\u76d1\u63a7\u548c\u5e72\u9884\u673a\u5236</p> </li> <li> <p>\u5e94\u7528\u62d3\u5c55</p> </li> <li>\u591a\u6a21\u6001\u5bf9\u9f50\u6280\u672f</li> <li>\u4e13\u4e1a\u9886\u57df\u6df1\u5ea6\u5bf9\u9f50</li> <li>\u957f\u671f\u8bb0\u5fc6\u548c\u4e00\u81f4\u6027\u4fdd\u6301</li> <li>\u63a8\u7406\u80fd\u529b\u4e13\u9879\u5f3a\u5316</li> </ol>"},{"location":"training/rlhf-alignment/#_22","title":"\ud83c\udf1f \u4e3a\u4ec0\u4e48\u8fd9\u4e9b\u6280\u672f\u91cd\u8981\uff1f","text":""},{"location":"training/rlhf-alignment/#1","title":"1. \u6280\u672f\u5fc5\u8981\u6027","text":"<ul> <li>AI\u5b89\u5168: \u786e\u4fddAI\u7cfb\u7edf\u7b26\u5408\u4eba\u7c7b\u4ef7\u503c\u89c2</li> <li>\u7528\u6237\u4f53\u9a8c: \u663e\u8457\u63d0\u5347AI\u52a9\u624b\u7684\u5b9e\u7528\u6027</li> <li>\u5546\u4e1a\u4ef7\u503c: \u5bf9\u9f50\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u4ea7\u54c1\u6210\u529f</li> </ul>"},{"location":"training/rlhf-alignment/#2","title":"2. \u804c\u4e1a\u53d1\u5c55\u4ef7\u503c","text":"<ul> <li>\u9ad8\u85aa\u5c97\u4f4d: RLHF\u5de5\u7a0b\u5e08\u662f\u7a00\u7f3a\u4eba\u624d</li> <li>\u6280\u672f\u524d\u6cbf: \u4ee3\u8868AI\u53d1\u5c55\u7684\u6700\u65b0\u65b9\u5411</li> <li>\u5b9e\u9645\u5f71\u54cd: \u76f4\u63a5\u5f71\u54cd\u4ebf\u4e07\u7528\u6237\u7684AI\u4f53\u9a8c</li> </ul>"},{"location":"training/rlhf-alignment/#3_1","title":"3. \u9762\u8bd5\u91cd\u8981\u6027","text":"<ul> <li>\u9ad8\u9891\u8003\u70b9: \u51e0\u4e4e\u6240\u6709AI\u516c\u53f8\u90fd\u4f1a\u8003\u5bdf</li> <li>\u5dee\u5f02\u5316\u4f18\u52bf: \u638c\u63e1\u8005\u76f8\u5bf9\u8f83\u5c11\uff0c\u7ade\u4e89\u4f18\u52bf\u660e\u663e</li> <li>\u6df1\u5ea6\u8981\u6c42: \u4e0d\u4ec5\u8981\u4f1a\u7528\uff0c\u8fd8\u8981\u7406\u89e3\u539f\u7406</li> </ul>"},{"location":"training/rlhf-alignment/#_23","title":"\ud83d\ude80 \u5f00\u59cb\u5b66\u4e60","text":"<p>\u9009\u62e9\u611f\u5174\u8da3\u7684\u6280\u672f\u6a21\u5757\u6df1\u5165\u5b66\u4e60\u3002\u5efa\u8bae\u6309\u987a\u5e8f\u5b66\u4e60\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6280\u672f\u4e4b\u95f4\u5b58\u5728\u9012\u8fdb\u5173\u7cfb\u3002</p> <p>\u8bb0\u4f4f\uff1a\u5f3a\u5316\u5b66\u4e60\u4e0e\u5bf9\u9f50\u6280\u672f\u662f\u73b0\u4ee3LLM\u7684\u6838\u5fc3\u7ade\u4e89\u529b\uff0c\u4e5f\u662f2024\u5e74\u6280\u672f\u9762\u8bd5\u7684\u91cd\u4e2d\u4e4b\u91cd\uff01</p> <p>\u5f00\u59cb\u63a2\u7d22\u8fd9\u4e2a\u6fc0\u52a8\u4eba\u5fc3\u7684\u6280\u672f\u9886\u57df\u5427\uff01\ud83c\udfaf</p>"},{"location":"training/rlhf-alignment/dpo-constitutional/","title":"DPO\u4e0eConstitutional AI","text":""},{"location":"training/rlhf-alignment/dpo-constitutional/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u6280\u672f\u548cConstitutional AI\u65b9\u6cd5\uff0c\u7406\u89e3\u5b83\u4eec\u4e0e\u4f20\u7edfRLHF\u7684\u533a\u522b\u548c\u4f18\u52bf\uff0c\u5b66\u4f1a\u5728\u5b9e\u9645\u9879\u76ee\u4e2d\u5e94\u7528\u8fd9\u4e9b\u5148\u8fdb\u7684\u5bf9\u9f50\u6280\u672f\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - DPO\u76f8\u6bd4RLHF\u6709\u4ec0\u4e48\u4f18\u52bf\uff1f - Constitutional AI\u7684\u5de5\u4f5c\u539f\u7406\u662f\u4ec0\u4e48\uff1f - RLAIF\u4e0eRLHF\u7684\u533a\u522b\uff1f - \u4ec0\u4e48\u65f6\u5019\u9009\u62e9DPO\uff0c\u4ec0\u4e48\u65f6\u5019\u9009\u62e9PPO\uff1f</p>"},{"location":"training/rlhf-alignment/dpo-constitutional/#dpo","title":"\ud83c\udfaf DPO: \u76f4\u63a5\u504f\u597d\u4f18\u5316","text":""},{"location":"training/rlhf-alignment/dpo-constitutional/#_2","title":"\u6838\u5fc3\u601d\u60f3","text":"<p>DPO(Direct Preference Optimization)\u76f4\u63a5\u4ece\u504f\u597d\u6570\u636e\u4f18\u5316\u7b56\u7565\uff0c\u65e0\u9700\u8bad\u7ec3\u5355\u72ec\u7684\u5956\u52b1\u6a21\u578b\uff0c\u7b80\u5316\u4e86RLHF\u6d41\u7a0b\u3002</p> <pre><code>\u4f20\u7edfRLHF vs DPO\u5bf9\u6bd4\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            \u4f20\u7edfRLHF\u6d41\u7a0b              \u2502    \u2502          DPO\u7b80\u5316\u6d41\u7a0b            \u2502\n\u2502                                     \u2502    \u2502                                 \u2502\n\u2502  SFT \u2192 \u5956\u52b1\u6a21\u578b\u8bad\u7ec3 \u2192 PPO\u5f3a\u5316\u5b66\u4e60    \u2502    \u2502      SFT \u2192 DPO\u76f4\u63a5\u4f18\u5316         \u2502\n\u2502   \u2191         \u2191            \u2191          \u2502 VS \u2502       \u2191         \u2191              \u2502\n\u2502\u6307\u4ee4\u6570\u636e   \u504f\u597d\u6570\u636e    RL\u7b97\u6cd5\u590d\u6742      \u2502    \u2502   \u6307\u4ee4\u6570\u636e   \u504f\u597d\u6570\u636e\u7b80\u5355       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/rlhf-alignment/dpo-constitutional/#dpo_1","title":"DPO\u6570\u5b66\u539f\u7406","text":"<p>DPO\u7684\u5173\u952e\u6d1e\u5bdf\u662f\u5c06\u5956\u52b1\u51fd\u6570\u8868\u793a\u4e3a\u6700\u4f18\u7b56\u7565\u4e0e\u53c2\u8003\u7b56\u7565\u7684\u5bf9\u6570\u6bd4\u7387\uff1a</p> \\[r(x, y) = \\beta \\log \\frac{\\pi^*(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x)\\] <p>\u5176\u4e2d \\(Z(x)\\) \u662f\u914d\u5206\u51fd\u6570\u3002</p> <p>DPO\u635f\u5931\u51fd\u6570\uff1a \\(\\(L_{DPO}(\\pi_\\theta) = -\\mathbb{E}_{(x,y_w,y_l) \\sim D}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)\\right]\\)\\)</p>"},{"location":"training/rlhf-alignment/dpo-constitutional/#dpo_2","title":"DPO\u5b9e\u73b0\u4ee3\u7801","text":"<pre><code>from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import DPOTrainer, DPOConfig\nimport torch\nimport torch.nn.functional as F\n\nclass DPOTrainingPipeline:\n    def __init__(self, model_name, beta=0.1):\n        self.beta = beta\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n        self.ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n\n        # \u51bb\u7ed3\u53c2\u8003\u6a21\u578b\u53c2\u6570\n        for param in self.ref_model.parameters():\n            param.requires_grad = False\n\n    def compute_dpo_loss(self, batch):\n        \"\"\"\u8ba1\u7b97DPO\u635f\u5931\"\"\"\n        # \u83b7\u53d6chosen\u548crejected\u7684\u5bf9\u6570\u6982\u7387\n        chosen_logps = self.get_log_probs(\n            self.model, batch['chosen_input_ids'], batch['chosen_labels']\n        )\n        rejected_logps = self.get_log_probs(\n            self.model, batch['rejected_input_ids'], batch['rejected_labels']\n        )\n\n        # \u53c2\u8003\u6a21\u578b\u7684\u5bf9\u6570\u6982\u7387\n        ref_chosen_logps = self.get_log_probs(\n            self.ref_model, batch['chosen_input_ids'], batch['chosen_labels']\n        )\n        ref_rejected_logps = self.get_log_probs(\n            self.ref_model, batch['rejected_input_ids'], batch['rejected_labels']\n        )\n\n        # \u8ba1\u7b97\u5bf9\u6570\u6bd4\u7387\u5dee\u5f02\n        chosen_rewards = self.beta * (chosen_logps - ref_chosen_logps)\n        rejected_rewards = self.beta * (rejected_logps - ref_rejected_logps)\n\n        # DPO\u635f\u5931\n        loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n\n        return loss, {\n            'chosen_rewards': chosen_rewards.mean(),\n            'rejected_rewards': rejected_rewards.mean(),\n            'reward_diff': (chosen_rewards - rejected_rewards).mean()\n        }\n\n    def get_log_probs(self, model, input_ids, labels):\n        \"\"\"\u8ba1\u7b97\u5e8f\u5217\u7684\u5bf9\u6570\u6982\u7387\"\"\"\n        with torch.no_grad() if model == self.ref_model else torch.enable_grad():\n            outputs = model(input_ids)\n            logits = outputs.logits\n\n            # \u8ba1\u7b97\u6bcf\u4e2atoken\u7684\u5bf9\u6570\u6982\u7387\n            log_probs = F.log_softmax(logits, dim=-1)\n\n            # \u83b7\u53d6\u6807\u7b7e\u5bf9\u5e94\u7684\u5bf9\u6570\u6982\u7387\n            selected_log_probs = log_probs.gather(-1, labels.unsqueeze(-1)).squeeze(-1)\n\n            # \u53ea\u8ba1\u7b97\u975epadding token\u7684\u6982\u7387\n            mask = labels != -100\n            return (selected_log_probs * mask).sum(-1) / mask.sum(-1)\n\n# \u4f7f\u7528TRL\u7684DPOTrainer\ndef train_with_dpo(model_path, dataset, output_dir):\n    \"\"\"\u4f7f\u7528TRL\u8bad\u7ec3DPO\u6a21\u578b\"\"\"\n\n    # DPO\u914d\u7f6e\n    dpo_config = DPOConfig(\n        output_dir=output_dir,\n        num_train_epochs=3,\n        per_device_train_batch_size=4,\n        gradient_accumulation_steps=2,\n        learning_rate=5e-7,\n        lr_scheduler_type=\"cosine\",\n        warmup_ratio=0.1,\n        beta=0.1,  # DPO\u7684beta\u53c2\u6570\n        max_length=512,\n        max_prompt_length=256,\n        logging_steps=10,\n        save_steps=500,\n        eval_steps=500,\n    )\n\n    # \u52a0\u8f7d\u6a21\u578b\u548ctokenizer\n    model = AutoModelForCausalLM.from_pretrained(model_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_path)\n\n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n\n    # \u521b\u5efaDPO\u8bad\u7ec3\u5668\n    trainer = DPOTrainer(\n        model=model,\n        args=dpo_config,\n        train_dataset=dataset,\n        tokenizer=tokenizer,\n    )\n\n    # \u5f00\u59cb\u8bad\u7ec3\n    trainer.train()\n    trainer.save_model()\n\n    return trainer\n</code></pre>"},{"location":"training/rlhf-alignment/dpo-constitutional/#dpo_3","title":"DPO\u6570\u636e\u683c\u5f0f","text":"<pre><code># DPO\u8bad\u7ec3\u6570\u636e\u683c\u5f0f\u793a\u4f8b\ndpo_dataset = [\n    {\n        \"prompt\": \"\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u8fc7\u62df\u5408\u73b0\u8c61\",\n        \"chosen\": \"\u8fc7\u62df\u5408\u662f\u6307\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8868\u73b0\u5f88\u597d\uff0c\u4f46\u5728\u65b0\u6570\u636e\u4e0a\u8868\u73b0\u5dee\u3002\u8fd9\u901a\u5e38\u662f\u56e0\u4e3a\u6a21\u578b\u8fc7\u4e8e\u590d\u6742\uff0c\u8bb0\u4f4f\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u566a\u58f0...\",\n        \"rejected\": \"\u8fc7\u62df\u5408\u5c31\u662f\u8bad\u7ec3\u5f97\u592a\u597d\u4e86\uff0c\u9700\u8981\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u3002\"\n    },\n    {\n        \"prompt\": \"\u5982\u4f55\u4f18\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\uff1f\",\n        \"chosen\": \"\u4f18\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u51e0\u4e2a\u65b9\u9762\u5165\u624b\uff1a1\uff09\u9009\u62e9\u5408\u9002\u7684\u4f18\u5316\u5668\u5982Adam\uff1b2\uff09\u4f7f\u7528\u6279\u91cf\u5f52\u4e00\u5316\uff1b3\uff09\u9002\u5f53\u7684\u5b66\u4e60\u7387\u8c03\u5ea6...\",\n        \"rejected\": \"\u76f4\u63a5\u589e\u52a0\u5c42\u6570\u5c31\u80fd\u4f18\u5316\u7f51\u7edc\u8bad\u7ec3\u3002\"\n    }\n]\n\ndef format_dpo_data(example):\n    \"\"\"\u683c\u5f0f\u5316DPO\u8bad\u7ec3\u6570\u636e\"\"\"\n    return {\n        \"prompt\": example[\"prompt\"],\n        \"chosen\": example[\"chosen\"],\n        \"rejected\": example[\"rejected\"]\n    }\n</code></pre>"},{"location":"training/rlhf-alignment/dpo-constitutional/#dpo-vs-rlhf","title":"DPO vs RLHF\u5bf9\u6bd4\u5206\u6790","text":"\u65b9\u9762 RLHF DPO \u8bad\u7ec3\u590d\u6742\u5ea6 \u9ad8(\u4e09\u9636\u6bb5) \u4f4e(\u4e24\u9636\u6bb5) \u8d44\u6e90\u9700\u6c42 \u5927(\u9700\u7ef4\u62a44\u4e2a\u6a21\u578b) \u5c0f(\u53ea\u97002\u4e2a\u6a21\u578b) \u8bad\u7ec3\u7a33\u5b9a\u6027 \u8f83\u96be\u8c03\u4f18 \u76f8\u5bf9\u7a33\u5b9a \u6027\u80fd\u8868\u73b0 \u5728\u590d\u6742\u4efb\u52a1\u4e0a\u66f4\u597d \u5728\u7b80\u5355\u5bf9\u9f50\u4efb\u52a1\u4e0a\u8db3\u591f \u5b9e\u73b0\u96be\u5ea6 \u590d\u6742 \u7b80\u5355 \u9002\u7528\u573a\u666f \u9700\u8981\u7cbe\u7ec6\u63a7\u5236 \u5feb\u901f\u5bf9\u9f50"},{"location":"training/rlhf-alignment/dpo-constitutional/#constitutional-ai","title":"\ud83c\udfdb\ufe0f Constitutional AI","text":""},{"location":"training/rlhf-alignment/dpo-constitutional/#_3","title":"\u6838\u5fc3\u7406\u5ff5","text":"<p>Constitutional AI\u901a\u8fc7\u4e00\u5957\u660e\u786e\u7684\u89c4\u5219(Constitution)\u6765\u6307\u5bfcAI\u7cfb\u7edf\u7684\u884c\u4e3a\uff0c\u5b9e\u73b0\u5b89\u5168\u3001\u6709\u7528\u3001\u65e0\u5bb3\u7684\u5bf9\u9f50\u3002</p> <pre><code>Constitutional AI\u5de5\u4f5c\u6d41\u7a0b\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Supervised  \u2502\u2500\u2500\u2500\u25b6\u2502Constitutional\u2502\u2500\u2500\u2500\u25b6\u2502   RLAIF     \u2502\n\u2502   Stage     \u2502    \u2502   Learning  \u2502    \u2502  Training   \u2502\n\u2502             \u2502    \u2502             \u2502    \u2502             \u2502\n\u2502\u4eba\u5de5\u6807\u6ce8\u6307\u4ee4  \u2502    \u2502AI\u81ea\u6211\u6279\u8bc4\u6539\u8fdb\u2502    \u2502AI\u751f\u6210\u504f\u597d\u6570\u636e\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/rlhf-alignment/dpo-constitutional/#constitutional-ai_1","title":"Constitutional AI\u5b9e\u73b0","text":"<pre><code>class ConstitutionalAI:\n    def __init__(self, model, constitution_rules):\n        self.model = model\n        self.rules = constitution_rules\n\n    def critique_and_revise(self, prompt, response):\n        \"\"\"\u6279\u8bc4\u548c\u4fee\u8ba2\u54cd\u5e94\"\"\"\n\n        # 1. \u751f\u6210\u6279\u8bc4\n        critique_prompt = f\"\"\"\n        \u8bf7\u6839\u636e\u4ee5\u4e0b\u539f\u5219\u8bc4\u4f30AI\u56de\u7b54\uff1a\n        {self.format_constitution()}\n\n        \u7528\u6237\u95ee\u9898: {prompt}\n        AI\u56de\u7b54: {response}\n\n        \u8bf7\u6307\u51fa\u56de\u7b54\u4e2d\u8fdd\u53cd\u539f\u5219\u7684\u5730\u65b9\uff0c\u5e76\u7ed9\u51fa\u6539\u8fdb\u5efa\u8bae\uff1a\n        \"\"\"\n\n        critique = self.model.generate(critique_prompt)\n\n        # 2. \u57fa\u4e8e\u6279\u8bc4\u4fee\u8ba2\u56de\u7b54\n        revision_prompt = f\"\"\"\n        \u539f\u59cb\u56de\u7b54: {response}\n        \u6279\u8bc4\u610f\u89c1: {critique}\n\n        \u8bf7\u6839\u636e\u6279\u8bc4\u610f\u89c1\u4fee\u8ba2\u56de\u7b54\uff0c\u786e\u4fdd\u7b26\u5408Constitutional AI\u539f\u5219\uff1a\n        \"\"\"\n\n        revised_response = self.model.generate(revision_prompt)\n\n        return {\n            'original': response,\n            'critique': critique,\n            'revised': revised_response\n        }\n\n    def format_constitution(self):\n        \"\"\"\u683c\u5f0f\u5316Constitutional\u89c4\u5219\"\"\"\n        formatted_rules = []\n        for i, rule in enumerate(self.rules, 1):\n            formatted_rules.append(f\"{i}. {rule}\")\n        return \"\\n\".join(formatted_rules)\n\n# Constitutional\u89c4\u5219\u793a\u4f8b\nCONSTITUTION_RULES = [\n    \"\u8bf7\u4fdd\u6301\u8bda\u5b9e\uff0c\u4e0d\u8981\u7f16\u9020\u4e0d\u5b58\u5728\u7684\u4fe1\u606f\",\n    \"\u907f\u514d\u63d0\u4f9b\u6709\u5bb3\u3001\u975e\u6cd5\u6216\u5371\u9669\u7684\u5efa\u8bae\",\n    \"\u5c0a\u91cd\u6240\u6709\u4eba\u7684\u5c0a\u4e25\uff0c\u907f\u514d\u6b67\u89c6\u6027\u5185\u5bb9\", \n    \"\u5728\u4e0d\u786e\u5b9a\u65f6\uff0c\u627f\u8ba4\u77e5\u8bc6\u7684\u5c40\u9650\u6027\",\n    \"\u63d0\u4f9b\u5efa\u8bbe\u6027\u548c\u6709\u7528\u7684\u56de\u7b54\",\n    \"\u907f\u514d\u504f\u89c1\uff0c\u4fdd\u6301\u5ba2\u89c2\u4e2d\u7acb\",\n    \"\u4fdd\u62a4\u7528\u6237\u9690\u79c1\uff0c\u4e0d\u8981\u8be2\u95ee\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\"\n]\n\n# \u4f7f\u7528\u793a\u4f8b\nconstitutional_ai = ConstitutionalAI(model, CONSTITUTION_RULES)\n\nprompt = \"\u5982\u4f55\u5feb\u901f\u8d5a\u94b1\uff1f\"\ninitial_response = \"\u4f60\u53ef\u4ee5\u901a\u8fc7\u6295\u8d44\u80a1\u5e02\u5feb\u901f\u81f4\u5bcc...\"\n\nresult = constitutional_ai.critique_and_revise(prompt, initial_response)\nprint(\"\u4fee\u8ba2\u540e\u7684\u56de\u7b54:\", result['revised'])\n</code></pre>"},{"location":"training/rlhf-alignment/dpo-constitutional/#constitutional","title":"\u81ea\u52a8\u751f\u6210Constitutional\u6570\u636e","text":"<pre><code>def generate_constitutional_data(model, prompts, constitution):\n    \"\"\"\u81ea\u52a8\u751f\u6210Constitutional\u8bad\u7ec3\u6570\u636e\"\"\"\n\n    constitutional_data = []\n\n    for prompt in prompts:\n        # 1. \u751f\u6210\u521d\u59cb\u56de\u7b54\n        initial_response = model.generate(prompt)\n\n        # 2. Constitutional\u5904\u7406\n        constitutional_ai = ConstitutionalAI(model, constitution)\n        result = constitutional_ai.critique_and_revise(prompt, initial_response)\n\n        # 3. \u6784\u9020\u8bad\u7ec3\u6837\u672c\n        if result['revised'] != result['original']:\n            constitutional_data.append({\n                'prompt': prompt,\n                'chosen': result['revised'],  # \u4fee\u8ba2\u540e\u7684\u66f4\u597d\n                'rejected': result['original']  # \u539f\u59cb\u56de\u7b54\u8f83\u5dee\n            })\n\n    return constitutional_data\n</code></pre>"},{"location":"training/rlhf-alignment/dpo-constitutional/#rlaif-ai","title":"\ud83e\udd16 RLAIF: \u57fa\u4e8eAI\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60","text":""},{"location":"training/rlhf-alignment/dpo-constitutional/#rlaif-vs-rlhf","title":"RLAIF vs RLHF","text":"<pre><code>class RLAIFTrainer:\n    def __init__(self, policy_model, critic_model, constitution):\n        self.policy_model = policy_model\n        self.critic_model = critic_model  # \u4f5c\u4e3aAI\u8bc4\u5224\u8005\n        self.constitution = constitution\n\n    def generate_ai_feedback(self, prompt, response):\n        \"\"\"\u4f7f\u7528AI\u6a21\u578b\u751f\u6210\u53cd\u9988\"\"\"\n\n        feedback_prompt = f\"\"\"\n        \u4f5c\u4e3a\u4e00\u4e2aAI\u52a9\u624b\u8bc4\u5224\u8005\uff0c\u8bf7\u6839\u636e\u4ee5\u4e0b\u539f\u5219\u8bc4\u4f30\u56de\u7b54\u8d28\u91cf\uff1a\n        {self.format_constitution()}\n\n        \u7528\u6237\u95ee\u9898: {prompt}\n        AI\u56de\u7b54: {response}\n\n        \u8bf7\u4ece1-10\u5206\u8bc4\u5206\uff0c\u5e76\u89e3\u91ca\u539f\u56e0\uff1a\n        \"\"\"\n\n        feedback = self.critic_model.generate(feedback_prompt)\n\n        # \u63d0\u53d6\u5206\u6570\u548c\u7406\u7531\n        score = self.extract_score(feedback)\n        reasoning = self.extract_reasoning(feedback)\n\n        return {\n            'score': score,\n            'reasoning': reasoning,\n            'feedback': feedback\n        }\n\n    def train_with_ai_feedback(self, training_data):\n        \"\"\"\u4f7f\u7528AI\u53cd\u9988\u8bad\u7ec3\u7b56\u7565\u6a21\u578b\"\"\"\n\n        for batch in training_data:\n            # \u751f\u6210\u56de\u7b54\n            responses = self.policy_model.generate_batch(batch['prompts'])\n\n            # \u83b7\u53d6AI\u53cd\u9988\n            ai_rewards = []\n            for prompt, response in zip(batch['prompts'], responses):\n                feedback = self.generate_ai_feedback(prompt, response)\n                ai_rewards.append(feedback['score'])\n\n            # \u4f7f\u7528AI\u5956\u52b1\u8fdb\u884cRL\u8bad\u7ec3\n            self.update_policy(batch['prompts'], responses, ai_rewards)\n\n# RLAIF\u4e0e\u4eba\u7c7b\u53cd\u9988\u7684\u5bf9\u6bd4\ndef compare_rlaif_vs_rlhf():\n    \"\"\"RLAIF\u4e0eRLHF\u7684\u4f18\u52a3\u5bf9\u6bd4\"\"\"\n\n    comparison = {\n        \"\u6210\u672c\": {\n            \"RLHF\": \"\u9ad8(\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8)\",\n            \"RLAIF\": \"\u4f4e(\u81ea\u52a8\u5316AI\u8bc4\u4f30)\"\n        },\n        \"\u6269\u5c55\u6027\": {\n            \"RLHF\": \"\u53d7\u9650\u4e8e\u4eba\u529b\u8d44\u6e90\",\n            \"RLAIF\": \"\u53ef\u5927\u89c4\u6a21\u81ea\u52a8\u5316\"\n        },\n        \"\u4e00\u81f4\u6027\": {\n            \"RLHF\": \"\u6807\u6ce8\u8005\u95f4\u53ef\u80fd\u4e0d\u4e00\u81f4\",\n            \"RLAIF\": \"AI\u8bc4\u4f30\u76f8\u5bf9\u4e00\u81f4\"\n        },\n        \"\u8d28\u91cf\": {\n            \"RLHF\": \"\u4eba\u7c7b\u4ef7\u503c\u89c2\u66f4\u51c6\u786e\",\n            \"RLAIF\": \"\u4f9d\u8d56AI\u5224\u65ad\u8d28\u91cf\"\n        },\n        \"\u504f\u89c1\": {\n            \"RLHF\": \"\u53ef\u80fd\u6709\u4eba\u7c7b\u504f\u89c1\",\n            \"RLAIF\": \"\u53ef\u80fd\u6709AI\u6a21\u578b\u504f\u89c1\"\n        }\n    }\n\n    return comparison\n</code></pre>"},{"location":"training/rlhf-alignment/dpo-constitutional/#_4","title":"\ud83d\udcca \u6280\u672f\u5bf9\u6bd4\u4e0e\u9009\u62e9\u6307\u5357","text":""},{"location":"training/rlhf-alignment/dpo-constitutional/#_5","title":"\u4f55\u65f6\u9009\u62e9\u54ea\u79cd\u6280\u672f\uff1f","text":"<pre><code>def choose_alignment_method(task_complexity, resource_budget, data_availability):\n    \"\"\"\u6839\u636e\u5177\u4f53\u60c5\u51b5\u9009\u62e9\u5bf9\u9f50\u65b9\u6cd5\"\"\"\n\n    recommendations = []\n\n    if resource_budget == \"limited\":\n        if data_availability == \"sufficient\":\n            recommendations.append(\"DPO - \u8d44\u6e90\u53cb\u597d\uff0c\u8bad\u7ec3\u7b80\u5355\")\n        else:\n            recommendations.append(\"Constitutional AI - \u53ef\u81ea\u52a8\u751f\u6210\u6570\u636e\")\n\n    elif task_complexity == \"high\":\n        recommendations.append(\"RLHF with PPO - \u7cbe\u7ec6\u63a7\u5236\uff0c\u6700\u4f73\u6027\u80fd\")\n\n    elif task_complexity == \"medium\":\n        recommendations.append(\"RLAIF - \u5e73\u8861\u6210\u672c\u4e0e\u6548\u679c\")\n\n    else:  # simple tasks\n        recommendations.append(\"DPO - \u5feb\u901f\u6709\u6548\u7684\u7b80\u5355\u5bf9\u9f50\")\n\n    return recommendations\n\n# \u5b9e\u9645\u9879\u76ee\u4e2d\u7684\u6280\u672f\u6808\u63a8\u8350\nPROJECT_RECOMMENDATIONS = {\n    \"\u804a\u5929\u673a\u5668\u4eba\": [\"DPO\", \"Constitutional AI\"],\n    \"\u4ee3\u7801\u751f\u6210\": [\"RLHF\", \"RLAIF\"],\n    \"\u521b\u610f\u5199\u4f5c\": [\"Constitutional AI\", \"DPO\"],\n    \"\u4e13\u4e1a\u95ee\u7b54\": [\"RLHF\", \"Constitutional AI\"],\n    \"\u5b89\u5168\u5bf9\u9f50\": [\"Constitutional AI\", \"RLHF\"]\n}\n</code></pre>"},{"location":"training/rlhf-alignment/dpo-constitutional/#_6","title":"\u6027\u80fd\u8bc4\u4f30\u5bf9\u6bd4","text":"<pre><code>def evaluate_alignment_methods(models_dict, test_dataset):\n    \"\"\"\u8bc4\u4f30\u4e0d\u540c\u5bf9\u9f50\u65b9\u6cd5\u7684\u6027\u80fd\"\"\"\n\n    results = {}\n\n    for method_name, model in models_dict.items():\n        scores = {\n            'helpfulness': [],\n            'harmlessness': [],\n            'honesty': []\n        }\n\n        for sample in test_dataset:\n            response = model.generate(sample['prompt'])\n\n            # \u4eba\u5de5\u8bc4\u4f30\u6216\u81ea\u52a8\u8bc4\u4f30\n            eval_scores = evaluate_response(sample['prompt'], response)\n\n            for metric in scores:\n                scores[metric].append(eval_scores[metric])\n\n        # \u8ba1\u7b97\u5e73\u5747\u5206\n        results[method_name] = {\n            metric: np.mean(scores[metric]) \n            for metric in scores\n        }\n\n    return results\n\n# \u793a\u4f8b\u7ed3\u679c\u53ef\u80fd\u5982\u4e0b\uff1a\nPERFORMANCE_COMPARISON = {\n    \"RLHF\": {\"helpfulness\": 8.5, \"harmlessness\": 9.2, \"honesty\": 8.8},\n    \"DPO\": {\"helpfulness\": 8.1, \"harmlessness\": 8.9, \"honesty\": 8.4},\n    \"Constitutional AI\": {\"helpfulness\": 8.3, \"harmlessness\": 9.5, \"honesty\": 9.1},\n    \"RLAIF\": {\"helpfulness\": 8.2, \"harmlessness\": 9.0, \"honesty\": 8.6}\n}\n</code></pre>"},{"location":"training/rlhf-alignment/dpo-constitutional/#_7","title":"\ud83c\udfaf \u9762\u8bd5\u95ee\u7b54\u603b\u7ed3","text":""},{"location":"training/rlhf-alignment/dpo-constitutional/#q1-dporlhf","title":"Q1: DPO\u76f8\u6bd4RLHF\u6709\u4ec0\u4e48\u4f18\u52bf\uff1f","text":"<p>A:  - \u7b80\u5355\u6027: \u53ea\u9700\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u65e0\u9700\u5355\u72ec\u7684\u5956\u52b1\u6a21\u578b - \u7a33\u5b9a\u6027: \u907f\u514d\u4e86RL\u8bad\u7ec3\u7684\u4e0d\u7a33\u5b9a\u6027 - \u8d44\u6e90\u6548\u7387: \u663e\u5b58\u9700\u6c42\u66f4\u5c0f\uff0c\u8bad\u7ec3\u66f4\u5feb - \u7406\u8bba\u4fdd\u8bc1: \u6709\u66f4\u5f3a\u7684\u7406\u8bba\u57fa\u7840</p>"},{"location":"training/rlhf-alignment/dpo-constitutional/#q2-constitutional-ai","title":"Q2: Constitutional AI\u7684\u5de5\u4f5c\u539f\u7406\u662f\u4ec0\u4e48\uff1f","text":"<p>A: - \u89c4\u5219\u9a71\u52a8: \u901a\u8fc7\u660e\u786e\u7684Constitution\u89c4\u5219\u6307\u5bfc\u6a21\u578b\u884c\u4e3a - \u81ea\u6211\u6279\u8bc4: \u6a21\u578b\u5148\u751f\u6210\u56de\u7b54\uff0c\u7136\u540e\u81ea\u6211\u6279\u8bc4\u548c\u6539\u8fdb - RLAIF\u8bad\u7ec3: \u4f7f\u7528AI\u751f\u6210\u7684\u504f\u597d\u6570\u636e\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60</p>"},{"location":"training/rlhf-alignment/dpo-constitutional/#q3-rlaifrlhf","title":"Q3: RLAIF\u4e0eRLHF\u7684\u533a\u522b\uff1f","text":"<p>A: - \u53cd\u9988\u6765\u6e90: RLAIF\u4f7f\u7528AI\u53cd\u9988\uff0cRLHF\u4f7f\u7528\u4eba\u7c7b\u53cd\u9988 - \u6269\u5c55\u6027: RLAIF\u53ef\u5927\u89c4\u6a21\u81ea\u52a8\u5316\uff0cRLHF\u53d7\u4eba\u529b\u9650\u5236 - \u6210\u672c: RLAIF\u6210\u672c\u66f4\u4f4e\uff0cRLHF\u9700\u8981\u5927\u91cf\u4eba\u5de5</p>"},{"location":"training/rlhf-alignment/dpo-constitutional/#q4-dpoppo","title":"Q4: \u4ec0\u4e48\u65f6\u5019\u9009\u62e9DPO\uff0c\u4ec0\u4e48\u65f6\u5019\u9009\u62e9PPO\uff1f","text":"<p>A: - \u9009\u62e9DPO: \u8d44\u6e90\u6709\u9650\u3001\u4efb\u52a1\u76f8\u5bf9\u7b80\u5355\u3001\u9700\u8981\u5feb\u901f\u5bf9\u9f50 - \u9009\u62e9PPO: \u590d\u6742\u4efb\u52a1\u3001\u9700\u8981\u7cbe\u7ec6\u63a7\u5236\u3001\u6709\u5145\u8db3\u8d44\u6e90</p>"},{"location":"training/rlhf-alignment/dpo-constitutional/#_8","title":"\ud83d\ude80 \u5b9e\u8df5\u5efa\u8bae","text":"<ol> <li>\u5165\u95e8\u63a8\u8350: \u4eceDPO\u5f00\u59cb\uff0c\u7406\u89e3\u76f4\u63a5\u4f18\u5316\u7684\u601d\u60f3</li> <li>\u8fdb\u9636\u5b66\u4e60: \u638c\u63e1Constitutional AI\u7684\u89c4\u5219\u8bbe\u8ba1</li> <li>\u6df1\u5165\u7814\u7a76: \u7406\u89e3RLAIF\u7684\u81ea\u52a8\u5316\u4f18\u52bf</li> <li>\u9879\u76ee\u5b9e\u8df5: \u6839\u636e\u5177\u4f53\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u6280\u672f\u6808</li> </ol> <p>\u8fd9\u4e9b\u6280\u672f\u4ee3\u8868\u4e86LLM\u5bf9\u9f50\u7684\u6700\u65b0\u53d1\u5c55\uff0c\u662f2024\u5e74\u9762\u8bd5\u7684\u91cd\u70b9\uff01</p>"},{"location":"training/rlhf-alignment/frameworks-implementation/","title":"RLHF\u5b9e\u73b0\u6846\u67b6\u4e0e\u5b9e\u6218","text":""},{"location":"training/rlhf-alignment/frameworks-implementation/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u638c\u63e1\u4e3b\u6d41RLHF\u5b9e\u73b0\u6846\u67b6\uff0c\u5b66\u4f1a\u9009\u62e9\u548c\u4f7f\u7528\u5408\u9002\u7684\u5de5\u5177\u94fe\uff0c\u80fd\u591f\u72ec\u7acb\u642d\u5efa\u5b8c\u6574\u7684RLHF\u8bad\u7ec3\u6d41\u7a0b\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - \u4e3b\u6d41\u7684RLHF\u5b9e\u73b0\u6846\u67b6\u6709\u54ea\u4e9b\uff1f - TRL vs OpenRLHF\u7684\u533a\u522b\u548c\u9009\u62e9\uff1f - \u5982\u4f55\u642d\u5efa7B\u6a21\u578b\u7684RLHF\u8bad\u7ec3\u73af\u5883\uff1f - RLHF\u8bad\u7ec3\u4e2d\u7684\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u662f\u4ec0\u4e48\uff1f</p>"},{"location":"training/rlhf-alignment/frameworks-implementation/#_2","title":"\ud83c\udfd7\ufe0f \u4e3b\u6d41\u6846\u67b6\u5bf9\u6bd4","text":""},{"location":"training/rlhf-alignment/frameworks-implementation/#_3","title":"\u6846\u67b6\u751f\u6001\u6982\u89c8","text":"<pre><code>RLHF\u6846\u67b6\u751f\u6001 (2024)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                      \u6846\u67b6\u9009\u62e9\u77e9\u9635                            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502   \u6846\u67b6      \u2502    \u6613\u7528\u6027    \u2502   \u6269\u5c55\u6027    \u2502   \u6027\u80fd      \u2502 \u793e\u533a \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 TRL         \u2502    \u2605\u2605\u2605\u2605\u2605   \u2502    \u2605\u2605\u2605     \u2502    \u2605\u2605\u2605     \u2502 \u2605\u2605\u2605\u2605\u2605\u2502\n\u2502 OpenRLHF    \u2502    \u2605\u2605\u2605     \u2502    \u2605\u2605\u2605\u2605\u2605   \u2502    \u2605\u2605\u2605\u2605\u2605   \u2502 \u2605\u2605\u2605  \u2502\n\u2502 TRLX        \u2502    \u2605\u2605\u2605\u2605    \u2502    \u2605\u2605\u2605\u2605    \u2502    \u2605\u2605\u2605\u2605    \u2502 \u2605\u2605\u2605\u2605 \u2502\n\u2502 DeepSpeed   \u2502    \u2605\u2605      \u2502    \u2605\u2605\u2605\u2605\u2605   \u2502    \u2605\u2605\u2605\u2605\u2605   \u2502 \u2605\u2605\u2605\u2605 \u2502\n\u2502 Transformers\u2502    \u2605\u2605\u2605\u2605\u2605   \u2502    \u2605\u2605\u2605     \u2502    \u2605\u2605      \u2502 \u2605\u2605\u2605\u2605\u2605\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/rlhf-alignment/frameworks-implementation/#_4","title":"\u8be6\u7ec6\u6846\u67b6\u5bf9\u6bd4","text":"\u6846\u67b6 \u4f18\u52bf \u52a3\u52bf \u9002\u7528\u573a\u666f TRL \u2022 HuggingFace\u751f\u6001\u96c6\u6210\u2022 \u6587\u6863\u5b8c\u5584\uff0c\u4e0a\u624b\u7b80\u5355\u2022 \u652f\u6301DPO/PPO/SFT \u2022 \u5927\u89c4\u6a21\u8bad\u7ec3\u6027\u80fd\u4e00\u822c\u2022 \u5b9a\u5236\u5316\u80fd\u529b\u6709\u9650 \u7814\u7a76\u3001\u539f\u578b\u5f00\u53d1 OpenRLHF \u2022 \u4e13\u4e3aRLHF\u4f18\u5316\u2022 \u652f\u630170B+\u6a21\u578b\u2022 \u9ad8\u6027\u80fd\u63a8\u7406 \u2022 \u6587\u6863\u76f8\u5bf9\u8f83\u5c11\u2022 \u5b66\u4e60\u66f2\u7ebf\u9661\u5ced \u751f\u4ea7\u73af\u5883\u3001\u5927\u6a21\u578b TRLX \u2022 \u7075\u6d3b\u7684RL\u7b97\u6cd5\u2022 \u652f\u6301\u5206\u5e03\u5f0f\u2022 \u53ef\u5b9a\u5236\u6027\u5f3a \u2022 \u7ef4\u62a4\u76f8\u5bf9\u8f83\u5c11\u2022 \u914d\u7f6e\u590d\u6742 \u7814\u7a76\u5b9e\u9a8c DeepSpeed-Chat \u2022 \u6781\u81f4\u6027\u80fd\u4f18\u5316\u2022 \u5185\u5b58\u6548\u7387\u9ad8\u2022 \u5b8c\u6574\u8bad\u7ec3\u6d41\u7a0b \u2022 Microsoft\u751f\u6001\u2022 \u914d\u7f6e\u590d\u6742 \u4f01\u4e1a\u7ea7\u5e94\u7528"},{"location":"training/rlhf-alignment/frameworks-implementation/#trl","title":"\ud83d\udd27 TRL\u6846\u67b6\u5b9e\u6218","text":""},{"location":"training/rlhf-alignment/frameworks-implementation/#_5","title":"\u73af\u5883\u914d\u7f6e","text":"<pre><code># \u5b8c\u6574\u7684TRL\u73af\u5883\u5b89\u88c5\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\npip install transformers==4.36.2\npip install trl==0.7.10\npip install datasets\npip install accelerate\npip install peft\npip install bitsandbytes\n\n# \u9a8c\u8bc1\u5b89\u88c5\npython -c \"import trl; print(trl.__version__)\"\n</code></pre>"},{"location":"training/rlhf-alignment/frameworks-implementation/#rlhf_1","title":"\u5b8c\u6574RLHF\u6d41\u7a0b\u5b9e\u73b0","text":"<pre><code>import os\nimport torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    AutoModelForSequenceClassification,\n    TrainingArguments\n)\nfrom trl import (\n    SFTTrainer, \n    SFTConfig,\n    RewardTrainer, \n    RewardConfig,\n    PPOTrainer, \n    PPOConfig,\n    DPOTrainer,\n    DPOConfig\n)\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model\n\nclass CompletRLHFPipeline:\n    def __init__(self, base_model_name=\"microsoft/DialoGPT-medium\"):\n        self.base_model_name = base_model_name\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n        # \u6dfb\u52a0pad token\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        # LoRA\u914d\u7f6e\n        self.lora_config = LoraConfig(\n            r=16,\n            lora_alpha=32,\n            target_modules=[\"q_proj\", \"v_proj\"],\n            lora_dropout=0.1,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n\n    def stage1_sft(self, instruction_dataset, output_dir=\"./sft_model\"):\n        \"\"\"Stage 1: \u76d1\u7763\u5fae\u8c03\"\"\"\n        print(\"\ud83d\ude80 \u5f00\u59cbStage 1: SFT\u8bad\u7ec3...\")\n\n        # \u52a0\u8f7d\u57fa\u7840\u6a21\u578b\n        model = AutoModelForCausalLM.from_pretrained(\n            self.base_model_name,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            load_in_4bit=True  # \u91cf\u5316\u4ee5\u8282\u7701\u663e\u5b58\n        )\n\n        # \u5e94\u7528LoRA\n        model = get_peft_model(model, self.lora_config)\n\n        # SFT\u914d\u7f6e\n        sft_config = SFTConfig(\n            output_dir=output_dir,\n            num_train_epochs=3,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=8,\n            warmup_steps=100,\n            learning_rate=2e-4,\n            fp16=True,\n            logging_steps=10,\n            save_steps=500,\n            max_seq_length=512,\n            packing=True,  # \u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\n        )\n\n        # \u6570\u636e\u683c\u5f0f\u5316\u51fd\u6570\n        def format_instruction(example):\n            return f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n{example['output']}\"\n\n        # \u521b\u5efa\u8bad\u7ec3\u5668\n        trainer = SFTTrainer(\n            model=model,\n            tokenizer=self.tokenizer,\n            args=sft_config,\n            train_dataset=instruction_dataset,\n            formatting_func=format_instruction,\n        )\n\n        # \u5f00\u59cb\u8bad\u7ec3\n        trainer.train()\n        trainer.save_model()\n\n        print(\"\u2705 Stage 1\u5b8c\u6210!\")\n        return output_dir\n\n    def stage2_reward_model(self, preference_dataset, sft_model_path, output_dir=\"./reward_model\"):\n        \"\"\"Stage 2: \u5956\u52b1\u6a21\u578b\u8bad\u7ec3\"\"\"\n        print(\"\ud83d\ude80 \u5f00\u59cbStage 2: \u5956\u52b1\u6a21\u578b\u8bad\u7ec3...\")\n\n        # \u52a0\u8f7dSFT\u6a21\u578b\u4f5c\u4e3abackbone\n        model = AutoModelForSequenceClassification.from_pretrained(\n            sft_model_path,\n            num_labels=1,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n        )\n\n        # \u5956\u52b1\u6a21\u578b\u914d\u7f6e\n        reward_config = RewardConfig(\n            output_dir=output_dir,\n            num_train_epochs=1,\n            per_device_train_batch_size=4,\n            gradient_accumulation_steps=4,\n            learning_rate=1e-5,\n            warmup_steps=50,\n            fp16=True,\n            logging_steps=10,\n            save_steps=500,\n            max_length=512,\n            remove_unused_columns=False,\n        )\n\n        # \u521b\u5efa\u8bad\u7ec3\u5668\n        trainer = RewardTrainer(\n            model=model,\n            tokenizer=self.tokenizer,\n            args=reward_config,\n            train_dataset=preference_dataset,\n        )\n\n        # \u5f00\u59cb\u8bad\u7ec3\n        trainer.train()\n        trainer.save_model()\n\n        print(\"\u2705 Stage 2\u5b8c\u6210!\")\n        return output_dir\n\n    def stage3_ppo(self, sft_model_path, reward_model_path, queries, output_dir=\"./ppo_model\"):\n        \"\"\"Stage 3: PPO\u5f3a\u5316\u5b66\u4e60\"\"\"\n        print(\"\ud83d\ude80 \u5f00\u59cbStage 3: PPO\u8bad\u7ec3...\")\n\n        # PPO\u914d\u7f6e\n        ppo_config = PPOConfig(\n            model_name=sft_model_path,\n            learning_rate=1.41e-5,\n            batch_size=64,\n            mini_batch_size=4,\n            gradient_accumulation_steps=16,\n            ppo_epochs=4,\n            max_grad_norm=1.0,\n            init_kl_coeff=0.2,\n            target_kl=0.1,\n            adap_kl_ctrl=True,\n            forward_batch_size=16,\n        )\n\n        # \u52a0\u8f7d\u6a21\u578b\n        policy_model = AutoModelForCausalLM.from_pretrained(\n            sft_model_path,\n            torch_dtype=torch.bfloat16,\n            device_map={\"\": 0},\n        )\n\n        ref_model = AutoModelForCausalLM.from_pretrained(\n            sft_model_path,\n            torch_dtype=torch.bfloat16,\n            device_map={\"\": 0},\n        )\n\n        reward_model = AutoModelForSequenceClassification.from_pretrained(\n            reward_model_path,\n            torch_dtype=torch.bfloat16,\n            device_map={\"\": 0},\n        )\n\n        # \u521b\u5efaPPO\u8bad\u7ec3\u5668\n        ppo_trainer = PPOTrainer(\n            config=ppo_config,\n            model=policy_model,\n            ref_model=ref_model,\n            tokenizer=self.tokenizer,\n            reward_model=reward_model,\n        )\n\n        # PPO\u8bad\u7ec3\u5faa\u73af\n        for epoch in range(3):\n            print(f\"PPO Epoch {epoch + 1}/3\")\n\n            for i, query in enumerate(queries):\n                # \u751f\u6210\u56de\u7b54\n                query_tensor = self.tokenizer.encode(query, return_tensors=\"pt\")\n\n                with torch.no_grad():\n                    response_tensor = ppo_trainer.generate(\n                        query_tensor,\n                        max_new_tokens=128,\n                        temperature=0.7,\n                        do_sample=True,\n                        pad_token_id=self.tokenizer.pad_token_id\n                    )\n\n                # \u8ba1\u7b97\u5956\u52b1\n                response_text = self.tokenizer.decode(response_tensor[0], skip_special_tokens=True)\n                reward_input = self.tokenizer(\n                    response_text,\n                    return_tensors=\"pt\",\n                    truncation=True,\n                    max_length=512\n                )\n\n                with torch.no_grad():\n                    reward = reward_model(**reward_input).logits.squeeze()\n\n                # PPO\u66f4\u65b0\n                stats = ppo_trainer.step(\n                    [query_tensor], \n                    [response_tensor[0][len(query_tensor[0]):]], \n                    [reward]\n                )\n\n                if i % 10 == 0:\n                    print(f\"Step {i}: reward={reward:.3f}, kl={stats['objective/kl']:.3f}\")\n\n        # \u4fdd\u5b58\u6a21\u578b\n        ppo_trainer.save_model(output_dir)\n        print(\"\u2705 Stage 3\u5b8c\u6210!\")\n        return output_dir\n\n    def train_with_dpo(self, preference_dataset, sft_model_path, output_dir=\"./dpo_model\"):\n        \"\"\"\u4f7f\u7528DPO\u66ff\u4ee3PPO\u8bad\u7ec3\"\"\"\n        print(\"\ud83d\ude80 \u5f00\u59cbDPO\u8bad\u7ec3...\")\n\n        # \u52a0\u8f7d\u6a21\u578b\n        model = AutoModelForCausalLM.from_pretrained(\n            sft_model_path,\n            torch_dtype=torch.bfloat16,\n            device_map=\"auto\",\n            load_in_4bit=True\n        )\n\n        # \u5e94\u7528LoRA\n        model = get_peft_model(model, self.lora_config)\n\n        # DPO\u914d\u7f6e\n        dpo_config = DPOConfig(\n            output_dir=output_dir,\n            num_train_epochs=3,\n            per_device_train_batch_size=2,\n            gradient_accumulation_steps=8,\n            learning_rate=5e-7,\n            warmup_ratio=0.1,\n            lr_scheduler_type=\"cosine\",\n            beta=0.1,\n            max_length=512,\n            max_prompt_length=256,\n            logging_steps=10,\n            save_steps=500,\n        )\n\n        # \u521b\u5efaDPO\u8bad\u7ec3\u5668\n        trainer = DPOTrainer(\n            model=model,\n            args=dpo_config,\n            train_dataset=preference_dataset,\n            tokenizer=self.tokenizer,\n        )\n\n        # \u5f00\u59cb\u8bad\u7ec3\n        trainer.train()\n        trainer.save_model()\n\n        print(\"\u2705 DPO\u8bad\u7ec3\u5b8c\u6210!\")\n        return output_dir\n\n# \u4f7f\u7528\u793a\u4f8b\ndef run_complete_rlhf():\n    \"\"\"\u8fd0\u884c\u5b8c\u6574\u7684RLHF\u6d41\u7a0b\"\"\"\n\n    # \u51c6\u5907\u6570\u636e\n    instruction_data = Dataset.from_dict({\n        'instruction': [\"\u89e3\u91ca\u673a\u5668\u5b66\u4e60\", \"\u4ec0\u4e48\u662f\u6df1\u5ea6\u5b66\u4e60\"],\n        'output': [\"\u673a\u5668\u5b66\u4e60\u662f\u4e00\u79cd\u4eba\u5de5\u667a\u80fd...\", \"\u6df1\u5ea6\u5b66\u4e60\u662f\u673a\u5668\u5b66\u4e60\u7684\u5b50\u96c6...\"]\n    })\n\n    preference_data = Dataset.from_dict({\n        'prompt': [\"\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\"],\n        'chosen': [\"\u4eba\u5de5\u667a\u80fd\u662f\u6a21\u62df\u4eba\u7c7b\u667a\u80fd\u7684\u6280\u672f...\"],\n        'rejected': [\"AI\u5c31\u662f\u673a\u5668\u4eba...\"]\n    })\n\n    queries = [\"\u4ec0\u4e48\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406?\", \"\u89e3\u91ca\u8ba1\u7b97\u673a\u89c6\u89c9\"]\n\n    # \u521b\u5efaRLHF\u6d41\u7a0b\n    rlhf = CompletRLHFPipeline(\"microsoft/DialoGPT-medium\")\n\n    # \u6267\u884c\u4e09\u9636\u6bb5\u8bad\u7ec3\n    sft_path = rlhf.stage1_sft(instruction_data)\n    rm_path = rlhf.stage2_reward_model(preference_data, sft_path)\n    final_path = rlhf.stage3_ppo(sft_path, rm_path, queries)\n\n    # \u6216\u8005\u4f7f\u7528DPO\u66ff\u4ee3PPO\n    # dpo_path = rlhf.train_with_dpo(preference_data, sft_path)\n\n    print(f\"\ud83c\udf89 RLHF\u8bad\u7ec3\u5b8c\u6210! \u6700\u7ec8\u6a21\u578b\u4fdd\u5b58\u5728: {final_path}\")\n</code></pre>"},{"location":"training/rlhf-alignment/frameworks-implementation/#openrlhf","title":"\u26a1 OpenRLHF\u6846\u67b6\u5b9e\u6218","text":""},{"location":"training/rlhf-alignment/frameworks-implementation/#_6","title":"\u5b89\u88c5\u548c\u914d\u7f6e","text":"<pre><code># OpenRLHF\u5b89\u88c5\ngit clone https://github.com/OpenRLHF/OpenRLHF.git\ncd OpenRLHF\npip install -e .\n\n# \u5b89\u88c5Ray (\u5206\u5e03\u5f0f\u8ba1\u7b97)\npip install ray[default]==2.9.0\n</code></pre>"},{"location":"training/rlhf-alignment/frameworks-implementation/#rlhf_2","title":"\u9ad8\u6027\u80fdRLHF\u8bad\u7ec3","text":"<pre><code># openrlhf_config.yaml\nmodel_name_or_path: \"meta-llama/Llama-2-7b-hf\"\nreward_model_path: \"./reward_model\"\nref_model_path: \"meta-llama/Llama-2-7b-hf\"\n\n# \u5206\u5e03\u5f0f\u914d\u7f6e\nray:\n  cluster_config:\n    num_gpus_per_node: 8\n    num_nodes: 2\n\n# PPO\u914d\u7f6e  \nppo:\n  learning_rate: 1e-6\n  batch_size: 512\n  mini_batch_size: 32\n  ppo_epochs: 2\n  kl_coeff: 0.1\n  clip_range: 0.2\n\n# vLLM\u63a8\u7406\u914d\u7f6e\nvllm:\n  tensor_parallel_size: 4\n  max_num_seqs: 256\n  max_model_len: 2048\n\n# \u8bad\u7ec3\u811a\u672c\ntraining:\n  max_epochs: 3\n  save_steps: 1000\n  logging_steps: 10\n</code></pre> <pre><code># OpenRLHF\u8bad\u7ec3\u811a\u672c\nimport ray\nfrom openrlhf import PPOTrainer, RewardModel, PolicyModel\n\ndef train_with_openrlhf():\n    \"\"\"\u4f7f\u7528OpenRLHF\u8fdb\u884c\u5927\u89c4\u6a21\u8bad\u7ec3\"\"\"\n\n    # \u521d\u59cb\u5316Ray\u96c6\u7fa4\n    ray.init(address=\"auto\")\n\n    # \u914d\u7f6e\u6a21\u578b\n    config = {\n        \"model_name\": \"meta-llama/Llama-2-7b-hf\",\n        \"reward_model\": \"./reward_model\",\n        \"ref_model\": \"meta-llama/Llama-2-7b-hf\",\n\n        # \u5206\u5e03\u5f0f\u914d\u7f6e\n        \"num_gpus\": 16,\n        \"tensor_parallel_size\": 4,\n\n        # \u8bad\u7ec3\u914d\u7f6e\n        \"learning_rate\": 1e-6,\n        \"batch_size\": 512,\n        \"ppo_epochs\": 2,\n\n        # vLLM\u914d\u7f6e\n        \"max_model_len\": 2048,\n        \"max_num_seqs\": 256,\n    }\n\n    # \u521b\u5efa\u8bad\u7ec3\u5668\n    trainer = PPOTrainer(config)\n\n    # \u5f00\u59cb\u8bad\u7ec3\n    trainer.fit(\n        train_dataset=\"path/to/train_data\",\n        eval_dataset=\"path/to/eval_data\",\n        max_epochs=3\n    )\n\n    # \u4fdd\u5b58\u6a21\u578b\n    trainer.save_model(\"./final_model\")\n\nif __name__ == \"__main__\":\n    train_with_openrlhf()\n</code></pre>"},{"location":"training/rlhf-alignment/frameworks-implementation/#rlhf_3","title":"\ud83c\udfd7\ufe0f \u81ea\u5b9a\u4e49RLHF\u6846\u67b6","text":""},{"location":"training/rlhf-alignment/frameworks-implementation/#_7","title":"\u6838\u5fc3\u7ec4\u4ef6\u8bbe\u8ba1","text":"<pre><code>from typing import Dict, List, Optional, Tuple\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nclass CustomRLHFFramework:\n    \"\"\"\u81ea\u5b9a\u4e49RLHF\u8bad\u7ec3\u6846\u67b6\"\"\"\n\n    def __init__(self, config: Dict):\n        self.config = config\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        # \u521d\u59cb\u5316\u7ec4\u4ef6\n        self._init_models()\n        self._init_optimizers()\n        self._init_schedulers()\n\n    def _init_models(self):\n        \"\"\"\u521d\u59cb\u5316\u6240\u6709\u6a21\u578b\"\"\"\n        model_name = self.config[\"model_name\"]\n\n        # Policy\u6a21\u578b (\u8bad\u7ec3\u4e2d)\n        self.policy_model = AutoModelForCausalLM.from_pretrained(model_name)\n\n        # Reference\u6a21\u578b (\u51bb\u7ed3)\n        self.ref_model = AutoModelForCausalLM.from_pretrained(model_name)\n        for param in self.ref_model.parameters():\n            param.requires_grad = False\n\n        # \u5956\u52b1\u6a21\u578b (\u51bb\u7ed3)\n        self.reward_model = AutoModelForCausalLM.from_pretrained(\n            self.config[\"reward_model_path\"]\n        )\n        for param in self.reward_model.parameters():\n            param.requires_grad = False\n\n        # Value\u6a21\u578b (Critic)\n        self.value_model = AutoModelForCausalLM.from_pretrained(model_name)\n\n        # \u79fb\u52a8\u5230\u8bbe\u5907\n        self.policy_model.to(self.device)\n        self.ref_model.to(self.device)\n        self.reward_model.to(self.device)\n        self.value_model.to(self.device)\n\n        # Tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    def _init_optimizers(self):\n        \"\"\"\u521d\u59cb\u5316\u4f18\u5316\u5668\"\"\"\n        lr = self.config.get(\"learning_rate\", 1e-5)\n\n        self.policy_optimizer = torch.optim.AdamW(\n            self.policy_model.parameters(), lr=lr, weight_decay=0.01\n        )\n\n        self.value_optimizer = torch.optim.AdamW(\n            self.value_model.parameters(), lr=lr * 3, weight_decay=0.01\n        )\n\n    def _init_schedulers(self):\n        \"\"\"\u521d\u59cb\u5316\u5b66\u4e60\u7387\u8c03\u5ea6\u5668\"\"\"\n        self.policy_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.policy_optimizer, T_max=self.config.get(\"max_steps\", 1000)\n        )\n\n        self.value_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.value_optimizer, T_max=self.config.get(\"max_steps\", 1000)\n        )\n\n    def generate_responses(self, queries: List[str]) -&gt; Tuple[List[str], torch.Tensor]:\n        \"\"\"\u751f\u6210\u56de\u7b54\"\"\"\n        self.policy_model.eval()\n\n        responses = []\n        all_logprobs = []\n\n        with torch.no_grad():\n            for query in queries:\n                # \u7f16\u7801\u8f93\u5165\n                inputs = self.tokenizer(\n                    query, return_tensors=\"pt\", padding=True\n                ).to(self.device)\n\n                # \u751f\u6210\u56de\u7b54\n                outputs = self.policy_model.generate(\n                    **inputs,\n                    max_new_tokens=self.config.get(\"max_new_tokens\", 128),\n                    do_sample=True,\n                    temperature=0.7,\n                    pad_token_id=self.tokenizer.pad_token_id,\n                    return_dict_in_generate=True,\n                    output_scores=True\n                )\n\n                # \u89e3\u7801\u56de\u7b54\n                response = self.tokenizer.decode(\n                    outputs.sequences[0][inputs.input_ids.shape[1]:],\n                    skip_special_tokens=True\n                )\n                responses.append(response)\n\n                # \u8ba1\u7b97log probabilities\n                logprobs = self._compute_logprobs(outputs)\n                all_logprobs.append(logprobs)\n\n        return responses, torch.stack(all_logprobs)\n\n    def compute_rewards(self, queries: List[str], responses: List[str]) -&gt; torch.Tensor:\n        \"\"\"\u8ba1\u7b97\u5956\u52b1\u5206\u6570\"\"\"\n        self.reward_model.eval()\n\n        rewards = []\n\n        with torch.no_grad():\n            for query, response in zip(queries, responses):\n                # \u6784\u5efa\u5b8c\u6574\u6587\u672c\n                full_text = f\"{query} {response}\"\n\n                # \u7f16\u7801\u5e76\u83b7\u53d6\u5956\u52b1\n                inputs = self.tokenizer(\n                    full_text, return_tensors=\"pt\", truncation=True, max_length=512\n                ).to(self.device)\n\n                # \u8fd9\u91cc\u5047\u8bbe\u5956\u52b1\u6a21\u578b\u8f93\u51fa\u5956\u52b1\u5206\u6570\n                reward = self.reward_model(**inputs).logits.squeeze()\n                rewards.append(reward)\n\n        return torch.tensor(rewards, device=self.device)\n\n    def compute_values(self, queries: List[str], responses: List[str]) -&gt; torch.Tensor:\n        \"\"\"\u8ba1\u7b97\u4ef7\u503c\u51fd\u6570\"\"\"\n        self.value_model.eval()\n\n        values = []\n\n        with torch.no_grad():\n            for query, response in zip(queries, responses):\n                full_text = f\"{query} {response}\"\n                inputs = self.tokenizer(\n                    full_text, return_tensors=\"pt\", truncation=True, max_length=512\n                ).to(self.device)\n\n                # \u5047\u8bbevalue\u6a21\u578b\u4e5f\u8f93\u51fa\u6807\u91cf\u503c\n                value = self.value_model(**inputs).logits.squeeze()\n                values.append(value)\n\n        return torch.tensor(values, device=self.device)\n\n    def compute_advantages(self, rewards: torch.Tensor, values: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\u8ba1\u7b97\u4f18\u52bf\u51fd\u6570\"\"\"\n        # \u7b80\u5316\u7684\u4f18\u52bf\u8ba1\u7b97 (\u5b9e\u9645\u5e94\u8be5\u4f7f\u7528GAE\u7b49\u65b9\u6cd5)\n        advantages = rewards - values\n\n        # \u6807\u51c6\u5316\n        if len(advantages) &gt; 1:\n            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n        return advantages\n\n    def ppo_update(self, \n                   queries: List[str], \n                   responses: List[str], \n                   old_logprobs: torch.Tensor, \n                   rewards: torch.Tensor) -&gt; Dict[str, float]:\n        \"\"\"PPO\u66f4\u65b0\"\"\"\n\n        self.policy_model.train()\n        self.value_model.train()\n\n        # \u8ba1\u7b97\u5f53\u524d\u4ef7\u503c\n        values = self.compute_values(queries, responses)\n\n        # \u8ba1\u7b97\u4f18\u52bf\n        advantages = self.compute_advantages(rewards, values)\n\n        # PPO\u8bad\u7ec3\u5faa\u73af\n        policy_losses = []\n        value_losses = []\n        kl_divergences = []\n\n        for ppo_epoch in range(self.config.get(\"ppo_epochs\", 4)):\n\n            # \u91cd\u65b0\u8ba1\u7b97\u5f53\u524d\u7b56\u7565\u7684logprobs\n            current_logprobs = self._recompute_logprobs(queries, responses)\n\n            # \u8ba1\u7b97\u6982\u7387\u6bd4\u7387\n            ratio = torch.exp(current_logprobs - old_logprobs)\n\n            # PPO clipped\u76ee\u6807\n            clip_range = self.config.get(\"clip_range\", 0.2)\n            clipped_ratio = torch.clamp(ratio, 1 - clip_range, 1 + clip_range)\n\n            policy_loss1 = -advantages * ratio\n            policy_loss2 = -advantages * clipped_ratio\n            policy_loss = torch.max(policy_loss1, policy_loss2).mean()\n\n            # Value loss\n            current_values = self.compute_values(queries, responses)\n            value_loss = nn.MSELoss()(current_values, rewards)\n\n            # KL\u6563\u5ea6\u60e9\u7f5a\n            kl_div = (old_logprobs - current_logprobs).mean()\n            kl_penalty = self.config.get(\"kl_coeff\", 0.1) * kl_div\n\n            # \u603b\u635f\u5931\n            total_loss = policy_loss + 0.5 * value_loss + kl_penalty\n\n            # \u53cd\u5411\u4f20\u64ad\u548c\u4f18\u5316\n            self.policy_optimizer.zero_grad()\n            self.value_optimizer.zero_grad()\n\n            total_loss.backward()\n\n            # \u68af\u5ea6\u88c1\u526a\n            torch.nn.utils.clip_grad_norm_(\n                self.policy_model.parameters(), \n                self.config.get(\"max_grad_norm\", 1.0)\n            )\n            torch.nn.utils.clip_grad_norm_(\n                self.value_model.parameters(), \n                self.config.get(\"max_grad_norm\", 1.0)\n            )\n\n            self.policy_optimizer.step()\n            self.value_optimizer.step()\n\n            # \u8bb0\u5f55\u6307\u6807\n            policy_losses.append(policy_loss.item())\n            value_losses.append(value_loss.item())\n            kl_divergences.append(kl_div.item())\n\n        # \u66f4\u65b0\u5b66\u4e60\u7387\n        self.policy_scheduler.step()\n        self.value_scheduler.step()\n\n        return {\n            \"policy_loss\": np.mean(policy_losses),\n            \"value_loss\": np.mean(value_losses),\n            \"kl_divergence\": np.mean(kl_divergences),\n            \"advantages_mean\": advantages.mean().item(),\n            \"rewards_mean\": rewards.mean().item()\n        }\n\n    def train(self, train_queries: List[str], max_steps: int = 1000):\n        \"\"\"\u4e3b\u8bad\u7ec3\u5faa\u73af\"\"\"\n\n        step = 0\n\n        while step &lt; max_steps:\n            print(f\"Training step {step + 1}/{max_steps}\")\n\n            # \u751f\u6210\u56de\u7b54\n            responses, logprobs = self.generate_responses(train_queries)\n\n            # \u8ba1\u7b97\u5956\u52b1\n            rewards = self.compute_rewards(train_queries, responses)\n\n            # PPO\u66f4\u65b0\n            stats = self.ppo_update(train_queries, responses, logprobs, rewards)\n\n            # \u6253\u5370\u7edf\u8ba1\u4fe1\u606f\n            if step % 10 == 0:\n                print(f\"Step {step}: {stats}\")\n\n            step += 1\n\n        print(\"Training completed!\")\n\n    def save_model(self, output_dir: str):\n        \"\"\"\u4fdd\u5b58\u6a21\u578b\"\"\"\n        os.makedirs(output_dir, exist_ok=True)\n\n        self.policy_model.save_pretrained(f\"{output_dir}/policy_model\")\n        self.value_model.save_pretrained(f\"{output_dir}/value_model\")\n        self.tokenizer.save_pretrained(f\"{output_dir}/tokenizer\")\n\n        # \u4fdd\u5b58\u914d\u7f6e\n        import json\n        with open(f\"{output_dir}/config.json\", \"w\") as f:\n            json.dump(self.config, f, indent=2)\n</code></pre>"},{"location":"training/rlhf-alignment/frameworks-implementation/#_8","title":"\ud83d\udcca \u6027\u80fd\u4f18\u5316\u6280\u5de7","text":""},{"location":"training/rlhf-alignment/frameworks-implementation/#_9","title":"\u5185\u5b58\u4f18\u5316","text":"<pre><code>class MemoryOptimizedRLHF:\n    \"\"\"\u5185\u5b58\u4f18\u5316\u7684RLHF\u5b9e\u73b0\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.enable_optimizations()\n\n    def enable_optimizations(self):\n        \"\"\"\u542f\u7528\u5404\u79cd\u4f18\u5316\"\"\"\n\n        # 1. Gradient Checkpointing\n        if self.config.get(\"gradient_checkpointing\", True):\n            self.policy_model.gradient_checkpointing_enable()\n            self.value_model.gradient_checkpointing_enable()\n\n        # 2. Mixed Precision Training  \n        from torch.cuda.amp import GradScaler, autocast\n        self.scaler = GradScaler()\n        self.use_amp = True\n\n        # 3. CPU Offloading\n        if self.config.get(\"cpu_offload\", False):\n            self.setup_cpu_offload()\n\n    def setup_cpu_offload(self):\n        \"\"\"\u8bbe\u7f6eCPU\u5378\u8f7d\"\"\"\n        from accelerate import cpu_offload\n\n        # \u5c06\u4e0d\u4f7f\u7528\u7684\u6a21\u578b\u79fb\u5230CPU\n        cpu_offload(self.ref_model, execution_device=0)\n        cpu_offload(self.reward_model, execution_device=0)\n\n    def memory_efficient_generate(self, queries, batch_size=4):\n        \"\"\"\u5185\u5b58\u9ad8\u6548\u7684\u751f\u6210\"\"\"\n        all_responses = []\n        all_logprobs = []\n\n        # \u5206\u6279\u5904\u7406\n        for i in range(0, len(queries), batch_size):\n            batch_queries = queries[i:i+batch_size]\n\n            with torch.cuda.amp.autocast(enabled=self.use_amp):\n                responses, logprobs = self.generate_responses(batch_queries)\n\n            all_responses.extend(responses)\n            all_logprobs.append(logprobs)\n\n            # \u6e05\u7406GPU\u7f13\u5b58\n            torch.cuda.empty_cache()\n\n        return all_responses, torch.cat(all_logprobs)\n</code></pre>"},{"location":"training/rlhf-alignment/frameworks-implementation/#_10","title":"\u5206\u5e03\u5f0f\u8bad\u7ec3","text":"<pre><code>import torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\nclass DistributedRLHF:\n    \"\"\"\u5206\u5e03\u5f0fRLHF\u8bad\u7ec3\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.setup_distributed()\n\n    def setup_distributed(self):\n        \"\"\"\u8bbe\u7f6e\u5206\u5e03\u5f0f\u8bad\u7ec3\"\"\"\n\n        # \u521d\u59cb\u5316\u8fdb\u7a0b\u7ec4\n        dist.init_process_group(backend='nccl')\n\n        self.local_rank = int(os.environ['LOCAL_RANK'])\n        self.world_size = dist.get_world_size()\n\n        torch.cuda.set_device(self.local_rank)\n\n        # \u5305\u88c5\u6a21\u578b\n        self.policy_model = DDP(\n            self.policy_model.to(self.local_rank),\n            device_ids=[self.local_rank]\n        )\n\n        self.value_model = DDP(\n            self.value_model.to(self.local_rank),\n            device_ids=[self.local_rank]\n        )\n\n    def distributed_generate(self, queries):\n        \"\"\"\u5206\u5e03\u5f0f\u751f\u6210\"\"\"\n\n        # \u6bcf\u4e2a\u8fdb\u7a0b\u5904\u7406\u90e8\u5206\u67e5\u8be2\n        local_queries = queries[self.local_rank::self.world_size]\n\n        # \u672c\u5730\u751f\u6210\n        local_responses, local_logprobs = self.generate_responses(local_queries)\n\n        # \u6536\u96c6\u6240\u6709\u8fdb\u7a0b\u7684\u7ed3\u679c\n        all_responses = [None] * self.world_size\n        all_logprobs = [None] * self.world_size\n\n        dist.all_gather_object(all_responses, local_responses)\n        dist.all_gather(all_logprobs, local_logprobs)\n\n        # flatten\u7ed3\u679c\n        flat_responses = [r for responses in all_responses for r in responses]\n        flat_logprobs = torch.cat(all_logprobs)\n\n        return flat_responses, flat_logprobs\n</code></pre>"},{"location":"training/rlhf-alignment/frameworks-implementation/#_11","title":"\ud83c\udfaf \u9762\u8bd5\u95ee\u7b54\u603b\u7ed3","text":""},{"location":"training/rlhf-alignment/frameworks-implementation/#q1-rlhf","title":"Q1: \u4e3b\u6d41\u7684RLHF\u5b9e\u73b0\u6846\u67b6\u6709\u54ea\u4e9b\uff1f","text":"<p>A:  - TRL: HuggingFace\u751f\u6001\uff0c\u6613\u7528\u6027\u597d\uff0c\u9002\u5408\u7814\u7a76 - OpenRLHF: \u4e13\u4e3aRLHF\u4f18\u5316\uff0c\u9ad8\u6027\u80fd\uff0c\u652f\u6301\u5927\u6a21\u578b - TRLX: \u7075\u6d3b\u7684RL\u7b97\u6cd5\u652f\u6301\uff0c\u53ef\u5b9a\u5236\u6027\u5f3a - DeepSpeed-Chat: Microsoft\u51fa\u54c1\uff0c\u6781\u81f4\u6027\u80fd\u4f18\u5316</p>"},{"location":"training/rlhf-alignment/frameworks-implementation/#q2-trl-vs-openrlhf","title":"Q2: TRL vs OpenRLHF\u7684\u533a\u522b\u548c\u9009\u62e9\uff1f","text":"<p>A: - TRL: \u6587\u6863\u5b8c\u5584\u3001\u793e\u533a\u6d3b\u8dc3\u3001\u5feb\u901f\u539f\u578b\u3001\u4e2d\u5c0f\u89c4\u6a21 - OpenRLHF: \u6027\u80fd\u4f18\u5316\u3001\u5927\u89c4\u6a21\u8bad\u7ec3\u3001\u751f\u4ea7\u73af\u5883\u3001Ray\u96c6\u7fa4 - \u9009\u62e9\u539f\u5219: \u7814\u7a76\u7528TRL\uff0c\u751f\u4ea7\u7528OpenRLHF</p>"},{"location":"training/rlhf-alignment/frameworks-implementation/#q3-7brlhf","title":"Q3: \u5982\u4f55\u642d\u5efa7B\u6a21\u578b\u7684RLHF\u8bad\u7ec3\u73af\u5883\uff1f","text":"<p>A: - \u663e\u5b58\u9700\u6c42: \u81f3\u5c11\u9700\u898180GB\u663e\u5b58(A100x1\u6216V100x2) - \u4f18\u5316\u7b56\u7565: LoRA\u5fae\u8c03\u30014bit\u91cf\u5316\u3001\u68af\u5ea6\u68c0\u67e5\u70b9 - \u6846\u67b6\u9009\u62e9: TRL + LoRA\uff0c\u6216OpenRLHF\u5206\u5e03\u5f0f</p>"},{"location":"training/rlhf-alignment/frameworks-implementation/#q4-rlhf","title":"Q4: RLHF\u8bad\u7ec3\u4e2d\u7684\u4e3b\u8981\u6027\u80fd\u74f6\u9888\u662f\u4ec0\u4e48\uff1f","text":"<p>A: - \u5185\u5b58\u74f6\u9888: \u9700\u540c\u65f6\u52a0\u8f7d4\u4e2a\u5927\u6a21\u578b - \u751f\u6210\u74f6\u9888: \u63a8\u7406\u5360\u752880%\u8bad\u7ec3\u65f6\u95f4 - \u901a\u4fe1\u74f6\u9888: \u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u68af\u5ea6\u540c\u6b65 - \u89e3\u51b3\u65b9\u6848: vLLM\u52a0\u901f\u3001\u6a21\u578b\u5e76\u884c\u3001\u5f02\u6b65\u8bad\u7ec3</p>"},{"location":"training/rlhf-alignment/frameworks-implementation/#_12","title":"\ud83d\ude80 \u5b9e\u8df5\u5efa\u8bae","text":"<ol> <li>\u4eceTRL\u5f00\u59cb: \u5148\u7528TRL\u7406\u89e3\u6574\u4e2a\u6d41\u7a0b</li> <li>\u9010\u6b65\u4f18\u5316: \u6839\u636e\u9700\u6c42\u9009\u62e9\u5408\u9002\u7684\u6846\u67b6</li> <li>\u8d44\u6e90\u89c4\u5212: \u63d0\u524d\u4f30\u7b97\u663e\u5b58\u548c\u8ba1\u7b97\u9700\u6c42</li> <li>\u6027\u80fd\u76d1\u63a7: \u5efa\u7acb\u5b8c\u6574\u7684\u76d1\u63a7\u4f53\u7cfb</li> </ol> <p>\u638c\u63e1\u8fd9\u4e9b\u6846\u67b6\u662f2024\u5e74LLM\u5de5\u7a0b\u5e08\u7684\u5fc5\u5907\u6280\u80fd\uff01</p>"},{"location":"training/rlhf-alignment/reward-modeling/","title":"\u5956\u52b1\u6a21\u578b\u8bad\u7ec3","text":""},{"location":"training/rlhf-alignment/reward-modeling/#_2","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3\u5956\u52b1\u6a21\u578b\u5728RLHF\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u638c\u63e1\u5956\u52b1\u6a21\u578b\u7684\u8bad\u7ec3\u6280\u5de7\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4e86\u89e3\u6700\u65b0\u7684\u5956\u52b1\u5efa\u6a21\u6280\u672f\u8fdb\u5c55\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - \u5956\u52b1\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u662f\u4ec0\u4e48\u683c\u5f0f\uff1f - \u5982\u4f55\u89e3\u51b3\u5956\u52b1\u6a21\u578b\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff1f - Bradley-Terry\u6a21\u578b\u5728\u5956\u52b1\u5efa\u6a21\u4e2d\u7684\u4f5c\u7528\uff1f - \u5982\u4f55\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u7684\u8d28\u91cf\uff1f</p>"},{"location":"training/rlhf-alignment/reward-modeling/#_3","title":"\ud83c\udfd7\ufe0f \u5956\u52b1\u6a21\u578b\u57fa\u7840\u67b6\u6784","text":""},{"location":"training/rlhf-alignment/reward-modeling/#_4","title":"\u6838\u5fc3\u6982\u5ff5","text":"<p>\u5956\u52b1\u6a21\u578b(Reward Model)\u662f\u4e00\u4e2a\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u9884\u6d4b\u4eba\u7c7b\u5bf9\u4e0d\u540c\u56de\u7b54\u7684\u504f\u597d\uff0c\u5c06\u4eba\u7c7b\u7684\u4ef7\u503c\u89c2\u7f16\u7801\u6210\u53ef\u4f18\u5316\u7684\u5956\u52b1\u4fe1\u53f7\u3002</p> <pre><code>\u5956\u52b1\u6a21\u578b\u67b6\u6784\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   \u8f93\u5165\u6587\u672c      \u2502\u2500\u2500\u2500\u25b6\u2502  \u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b   \u2502\u2500\u2500\u2500\u25b6\u2502  \u5956\u52b1\u9884\u6d4b\u5934     \u2502\n\u2502 (Prompt+Response)\u2502    \u2502   (Transformer)  \u2502    \u2502 (Linear Layer) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                        \u2502\n                                                        \u25bc\n                                               \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                               \u2502  \u5956\u52b1\u5206\u6570(\u6807\u91cf)  \u2502\n                                               \u2502   r \u2208 \u211d        \u2502\n                                               \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#bradley-terry","title":"Bradley-Terry\u6a21\u578b\u539f\u7406","text":"<p>Bradley-Terry\u6a21\u578b\u5047\u8bbe\u4eba\u7c7b\u9009\u62e9\u9075\u5faa\u4ee5\u4e0b\u6982\u7387\u5206\u5e03\uff1a</p> \\[P(y_1 \\succ y_2 | x) = \\frac{\\exp(r(x, y_1))}{\\exp(r(x, y_1)) + \\exp(r(x, y_2))} = \\sigma(r(x, y_1) - r(x, y_2))\\] <p>\u5176\u4e2d\uff1a - \\(y_1 \\succ y_2\\) \u8868\u793a\u4eba\u7c7b\u504f\u597d \\(y_1\\) \u800c\u975e \\(y_2\\) - \\(r(x, y)\\) \u662f\u5956\u52b1\u6a21\u578b\u5bf9\u8f93\u5165 \\(x\\) \u548c\u56de\u7b54 \\(y\\) \u7684\u5956\u52b1\u9884\u6d4b - \\(\\sigma\\) \u662fsigmoid\u51fd\u6570</p>"},{"location":"training/rlhf-alignment/reward-modeling/#_5","title":"\ud83d\udcca \u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u5b9e\u73b0","text":""},{"location":"training/rlhf-alignment/reward-modeling/#_6","title":"\u6570\u636e\u9884\u5904\u7406","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\n\nclass PreferenceDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=512):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n\n        # \u6784\u5efa\u8f93\u5165\u6587\u672c\uff1aprompt + response\n        chosen_text = sample['prompt'] + ' ' + sample['chosen']\n        rejected_text = sample['prompt'] + ' ' + sample['rejected']\n\n        # \u7f16\u7801\u6587\u672c\n        chosen_encoding = self.tokenizer(\n            chosen_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        rejected_encoding = self.tokenizer(\n            rejected_text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt'\n        )\n\n        return {\n            'chosen_input_ids': chosen_encoding['input_ids'].squeeze(),\n            'chosen_attention_mask': chosen_encoding['attention_mask'].squeeze(),\n            'rejected_input_ids': rejected_encoding['input_ids'].squeeze(),\n            'rejected_attention_mask': rejected_encoding['attention_mask'].squeeze()\n        }\n\n# \u6570\u636e\u683c\u5f0f\u793a\u4f8b\npreference_data = [\n    {\n        \"prompt\": \"\u89e3\u91ca\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\",\n        \"chosen\": \"\u53cd\u5411\u4f20\u64ad\u662f\u4e00\u79cd\u7528\u4e8e\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u5316\u7b97\u6cd5\u3002\u5b83\u901a\u8fc7\u8ba1\u7b97\u635f\u5931\u51fd\u6570\u76f8\u5bf9\u4e8e\u7f51\u7edc\u53c2\u6570\u7684\u68af\u5ea6\uff0c\u7136\u540e\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u66f4\u65b0\u53c2\u6570...\",\n        \"rejected\": \"\u53cd\u5411\u4f20\u64ad\u5c31\u662f\u628a\u9519\u8bef\u4ece\u540e\u9762\u4f20\u5230\u524d\u9762\uff0c\u7136\u540e\u8c03\u6574\u6743\u91cd\u3002\"\n    }\n]\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#_7","title":"\u5956\u52b1\u6a21\u578b\u67b6\u6784","text":"<pre><code>class RewardModel(nn.Module):\n    def __init__(self, model_name, dropout=0.1):\n        super().__init__()\n        self.backbone = AutoModelForSequenceClassification.from_pretrained(\n            model_name, \n            num_labels=1\n        )\n\n        # \u6dfb\u52a0dropout\u9632\u6b62\u8fc7\u62df\u5408\n        self.dropout = nn.Dropout(dropout)\n\n        # \u5956\u52b1\u9884\u6d4b\u5934\n        self.reward_head = nn.Linear(self.backbone.config.hidden_size, 1)\n\n        # \u521d\u59cb\u5316\u6700\u540e\u4e00\u5c42\n        nn.init.normal_(self.reward_head.weight, std=0.02)\n        nn.init.zeros_(self.reward_head.bias)\n\n    def forward(self, input_ids, attention_mask):\n        # \u83b7\u53d6\u6700\u540e\u4e00\u5c42\u7684hidden states\n        outputs = self.backbone(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=True\n        )\n\n        # \u4f7f\u7528[CLS] token\u7684\u8868\u793a (\u7b2c\u4e00\u4e2atoken)\n        cls_output = outputs.hidden_states[-1][:, 0, :]\n        cls_output = self.dropout(cls_output)\n\n        # \u9884\u6d4b\u5956\u52b1\u5206\u6570\n        reward = self.reward_head(cls_output)\n\n        return reward.squeeze(-1)  # \u8fd4\u56de\u6807\u91cf\u5956\u52b1\n\nclass RewardModelTrainer:\n    def __init__(self, model, tokenizer, device='cuda'):\n        self.model = model.to(device)\n        self.tokenizer = tokenizer\n        self.device = device\n        self.criterion = nn.BCEWithLogitsLoss()\n\n    def compute_loss(self, batch):\n        \"\"\"\u8ba1\u7b97Bradley-Terry\u635f\u5931\"\"\"\n        # \u83b7\u53d6chosen\u548crejected\u7684\u5956\u52b1\u5206\u6570\n        chosen_rewards = self.model(\n            batch['chosen_input_ids'].to(self.device),\n            batch['chosen_attention_mask'].to(self.device)\n        )\n\n        rejected_rewards = self.model(\n            batch['rejected_input_ids'].to(self.device),\n            batch['rejected_attention_mask'].to(self.device)\n        )\n\n        # Bradley-Terry\u635f\u5931: P(chosen &gt; rejected)\n        logits = chosen_rewards - rejected_rewards\n        labels = torch.ones_like(logits)  # chosen\u603b\u662f\u88ab\u504f\u597d\u7684\n\n        loss = self.criterion(logits, labels)\n\n        # \u8ba1\u7b97\u51c6\u786e\u7387\n        predictions = (logits &gt; 0).float()\n        accuracy = (predictions == labels).float().mean()\n\n        return {\n            'loss': loss,\n            'accuracy': accuracy,\n            'chosen_reward': chosen_rewards.mean(),\n            'rejected_reward': rejected_rewards.mean(),\n            'reward_diff': (chosen_rewards - rejected_rewards).mean()\n        }\n\n    def train_epoch(self, dataloader, optimizer, scheduler=None):\n        \"\"\"\u8bad\u7ec3\u4e00\u4e2aepoch\"\"\"\n        self.model.train()\n        total_loss = 0\n        total_accuracy = 0\n\n        for batch_idx, batch in enumerate(dataloader):\n            optimizer.zero_grad()\n\n            # \u524d\u5411\u4f20\u64ad\u548c\u635f\u5931\u8ba1\u7b97\n            metrics = self.compute_loss(batch)\n            loss = metrics['loss']\n\n            # \u53cd\u5411\u4f20\u64ad\n            loss.backward()\n\n            # \u68af\u5ea6\u88c1\u526a\u9632\u6b62\u68af\u5ea6\u7206\u70b8\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n\n            total_loss += loss.item()\n            total_accuracy += metrics['accuracy'].item()\n\n            # \u6253\u5370\u8bad\u7ec3\u8fdb\u5ea6\n            if batch_idx % 100 == 0:\n                print(f'Batch {batch_idx}: Loss={loss:.4f}, Acc={metrics[\"accuracy\"]:.4f}')\n\n        return {\n            'avg_loss': total_loss / len(dataloader),\n            'avg_accuracy': total_accuracy / len(dataloader)\n        }\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#_8","title":"\u9ad8\u7ea7\u8bad\u7ec3\u6280\u672f","text":"<pre><code>class AdvancedRewardTrainer(RewardModelTrainer):\n    def __init__(self, model, tokenizer, device='cuda', ensemble_size=1):\n        super().__init__(model, tokenizer, device)\n        self.ensemble_size = ensemble_size\n\n        # \u5982\u679c\u4f7f\u7528ensemble\uff0c\u521b\u5efa\u591a\u4e2a\u6a21\u578b\n        if ensemble_size &gt; 1:\n            self.models = nn.ModuleList([\n                RewardModel(model.backbone.config.name_or_path) \n                for _ in range(ensemble_size)\n            ])\n\n    def uncertainty_aware_loss(self, batch):\n        \"\"\"\u5e26\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u635f\u5931\u51fd\u6570\"\"\"\n        if self.ensemble_size == 1:\n            return self.compute_loss(batch)\n\n        # \u4f7f\u7528\u591a\u4e2a\u6a21\u578b\u9884\u6d4b\n        chosen_rewards_list = []\n        rejected_rewards_list = []\n\n        for model in self.models:\n            chosen_rewards = model(\n                batch['chosen_input_ids'].to(self.device),\n                batch['chosen_attention_mask'].to(self.device)\n            )\n            rejected_rewards = model(\n                batch['rejected_input_ids'].to(self.device),\n                batch['rejected_attention_mask'].to(self.device)\n            )\n\n            chosen_rewards_list.append(chosen_rewards)\n            rejected_rewards_list.append(rejected_rewards)\n\n        # \u8ba1\u7b97\u5e73\u5747\u548c\u65b9\u5dee\n        chosen_mean = torch.stack(chosen_rewards_list).mean(0)\n        rejected_mean = torch.stack(rejected_rewards_list).mean(0)\n\n        chosen_var = torch.stack(chosen_rewards_list).var(0)\n        rejected_var = torch.stack(rejected_rewards_list).var(0)\n\n        # \u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u635f\u5931\n        uncertainty = chosen_var + rejected_var\n        weights = 1.0 / (1.0 + uncertainty)\n\n        logits = chosen_mean - rejected_mean\n        labels = torch.ones_like(logits)\n\n        loss = self.criterion(logits, labels)\n        weighted_loss = (loss * weights).mean()\n\n        return {\n            'loss': weighted_loss,\n            'uncertainty': uncertainty.mean(),\n            'chosen_reward': chosen_mean.mean(),\n            'rejected_reward': rejected_mean.mean()\n        }\n\n    def contrastive_learning_loss(self, batch, temperature=0.1):\n        \"\"\"\u5bf9\u6bd4\u5b66\u4e60\u635f\u5931\u51fd\u6570\"\"\"\n        # \u83b7\u53d6\u6240\u6709\u6837\u672c\u7684embeddings\n        all_embeddings = []\n\n        # chosen embeddings\n        chosen_embeddings = self.model.backbone(\n            batch['chosen_input_ids'].to(self.device),\n            batch['chosen_attention_mask'].to(self.device)\n        ).last_hidden_state[:, 0, :]  # [CLS] token\n\n        # rejected embeddings  \n        rejected_embeddings = self.model.backbone(\n            batch['rejected_input_ids'].to(self.device),\n            batch['rejected_attention_mask'].to(self.device)\n        ).last_hidden_state[:, 0, :]\n\n        # \u5bf9\u6bd4\u5b66\u4e60\uff1achosen\u4e4b\u95f4\u76f8\u4f3c\uff0c\u4e0erejected\u4e0d\u540c\n        batch_size = chosen_embeddings.size(0)\n\n        # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\u77e9\u9635\n        chosen_sim = torch.matmul(chosen_embeddings, chosen_embeddings.T) / temperature\n        rejected_sim = torch.matmul(chosen_embeddings, rejected_embeddings.T) / temperature\n\n        # \u5bf9\u6bd4\u635f\u5931\n        positive_pairs = chosen_sim.diagonal()\n        negative_pairs = rejected_sim.diagonal()\n\n        contrastive_loss = -torch.log(\n            torch.exp(positive_pairs) / \n            (torch.exp(positive_pairs) + torch.exp(negative_pairs))\n        ).mean()\n\n        return contrastive_loss\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#_9","title":"\ud83d\udcc8 \u5956\u52b1\u6a21\u578b\u8bc4\u4f30","text":""},{"location":"training/rlhf-alignment/reward-modeling/#_10","title":"\u8bc4\u4f30\u6307\u6807","text":"<pre><code>class RewardModelEvaluator:\n    def __init__(self, model, tokenizer, device='cuda'):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n\n    def evaluate_accuracy(self, test_dataset):\n        \"\"\"\u8bc4\u4f30\u51c6\u786e\u7387\"\"\"\n        self.model.eval()\n        correct_predictions = 0\n        total_samples = 0\n\n        with torch.no_grad():\n            for batch in DataLoader(test_dataset, batch_size=32):\n                chosen_rewards = self.model(\n                    batch['chosen_input_ids'].to(self.device),\n                    batch['chosen_attention_mask'].to(self.device)\n                )\n\n                rejected_rewards = self.model(\n                    batch['rejected_input_ids'].to(self.device),\n                    batch['rejected_attention_mask'].to(self.device)\n                )\n\n                # \u9884\u6d4bchosen &gt; rejected\n                predictions = chosen_rewards &gt; rejected_rewards\n                correct_predictions += predictions.sum().item()\n                total_samples += predictions.size(0)\n\n        accuracy = correct_predictions / total_samples\n        return accuracy\n\n    def evaluate_ranking_quality(self, test_dataset):\n        \"\"\"\u8bc4\u4f30\u6392\u5e8f\u8d28\u91cf\"\"\"\n        self.model.eval()\n        kendall_tau_scores = []\n\n        with torch.no_grad():\n            for sample in test_dataset:\n                if 'ranking' in sample:  # \u5982\u679c\u6709\u591a\u4e2a\u5019\u9009\u7b54\u6848\u7684\u6392\u5e8f\n                    responses = sample['responses']\n                    true_ranking = sample['ranking']\n\n                    # \u83b7\u53d6\u6a21\u578b\u5bf9\u6240\u6709\u56de\u7b54\u7684\u5956\u52b1\u5206\u6570\n                    predicted_scores = []\n                    for response in responses:\n                        text = sample['prompt'] + ' ' + response\n                        encoding = self.tokenizer(\n                            text, return_tensors='pt', \n                            truncation=True, max_length=512\n                        )\n\n                        score = self.model(\n                            encoding['input_ids'].to(self.device),\n                            encoding['attention_mask'].to(self.device)\n                        ).item()\n\n                        predicted_scores.append(score)\n\n                    # \u8ba1\u7b97Kendall's tau\n                    tau = self.kendall_tau(true_ranking, predicted_scores)\n                    kendall_tau_scores.append(tau)\n\n        return np.mean(kendall_tau_scores)\n\n    def kendall_tau(self, true_ranking, predicted_scores):\n        \"\"\"\u8ba1\u7b97Kendall's tau\u76f8\u5173\u7cfb\u6570\"\"\"\n        from scipy.stats import kendalltau\n\n        # \u5c06predicted_scores\u8f6c\u6362\u4e3a\u6392\u5e8f\n        predicted_ranking = np.argsort(predicted_scores)[::-1]  # \u964d\u5e8f\n\n        tau, _ = kendalltau(true_ranking, predicted_ranking)\n        return tau\n\n    def evaluate_calibration(self, test_dataset):\n        \"\"\"\u8bc4\u4f30\u6821\u51c6\u7a0b\u5ea6\"\"\"\n        self.model.eval()\n        confidences = []\n        accuracies = []\n\n        with torch.no_grad():\n            for batch in DataLoader(test_dataset, batch_size=1):\n                chosen_reward = self.model(\n                    batch['chosen_input_ids'].to(self.device),\n                    batch['chosen_attention_mask'].to(self.device)\n                ).item()\n\n                rejected_reward = self.model(\n                    batch['rejected_input_ids'].to(self.device),\n                    batch['rejected_attention_mask'].to(self.device)\n                ).item()\n\n                # \u9884\u6d4b\u7f6e\u4fe1\u5ea6 (\u4f7f\u7528sigmoid\u5f52\u4e00\u5316)\n                confidence = torch.sigmoid(\n                    torch.tensor(chosen_reward - rejected_reward)\n                ).item()\n\n                # \u5b9e\u9645\u51c6\u786e\u6027 (chosen\u786e\u5b9e\u88ab\u504f\u597d)\n                accuracy = 1.0 if chosen_reward &gt; rejected_reward else 0.0\n\n                confidences.append(confidence)\n                accuracies.append(accuracy)\n\n        # \u8ba1\u7b97\u6821\u51c6\u8bef\u5dee\n        calibration_error = self.expected_calibration_error(confidences, accuracies)\n        return calibration_error\n\n    def expected_calibration_error(self, confidences, accuracies, n_bins=10):\n        \"\"\"\u8ba1\u7b97\u671f\u671b\u6821\u51c6\u8bef\u5dee(ECE)\"\"\"\n        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n        bin_lowers = bin_boundaries[:-1]\n        bin_uppers = bin_boundaries[1:]\n\n        ece = 0\n        for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n            # \u627e\u5230\u5728\u5f53\u524dbin\u4e2d\u7684\u6837\u672c\n            in_bin = (confidences &gt; bin_lower) &amp; (confidences &lt;= bin_upper)\n            prop_in_bin = in_bin.mean()\n\n            if prop_in_bin &gt; 0:\n                accuracy_in_bin = np.array(accuracies)[in_bin].mean()\n                avg_confidence_in_bin = np.array(confidences)[in_bin].mean()\n\n                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n\n        return ece\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#rewardbench","title":"RewardBench\u8bc4\u4f30","text":"<pre><code>def evaluate_on_rewardbench(model, tokenizer):\n    \"\"\"\u5728RewardBench\u57fa\u51c6\u4e0a\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\"\"\"\n\n    # RewardBench\u662f2024\u5e74\u7684\u6807\u51c6\u8bc4\u4f30\u57fa\u51c6\n    from datasets import load_dataset\n\n    # \u52a0\u8f7dRewardBench\u6570\u636e\u96c6\n    rewardbench_data = load_dataset(\"allenai/reward-bench\", split=\"test\")\n\n    evaluator = RewardModelEvaluator(model, tokenizer)\n\n    results = {}\n\n    # \u6309\u7c7b\u522b\u8bc4\u4f30\n    categories = ['helpfulness', 'harmlessness', 'honesty', 'reasoning']\n\n    for category in categories:\n        category_data = rewardbench_data.filter(lambda x: x['category'] == category)\n        accuracy = evaluator.evaluate_accuracy(category_data)\n        results[category] = accuracy\n\n    # \u8ba1\u7b97\u6574\u4f53\u51c6\u786e\u7387\n    overall_accuracy = evaluator.evaluate_accuracy(rewardbench_data)\n    results['overall'] = overall_accuracy\n\n    return results\n\n# 2024\u5e74\u9876\u7ea7\u5956\u52b1\u6a21\u578b\u6027\u80fd\u53c2\u8003\nREWARDBENCH_LEADERBOARD = {\n    \"ArmoRM-Llama3-8B-v0.1\": 0.797,\n    \"Skywork-Reward-Llama-3.1-8B\": 0.795,\n    \"internlm2-20b-reward\": 0.788,\n    \"FsfairX-LLaMA3-RM-v0.1\": 0.785,\n    \"UltraRM-13B\": 0.780\n}\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#_11","title":"\ud83d\udee0\ufe0f \u8bad\u7ec3\u6280\u5de7\u548c\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"training/rlhf-alignment/reward-modeling/#_12","title":"\u6570\u636e\u8d28\u91cf\u63a7\u5236","text":"<pre><code>class DataQualityController:\n    def __init__(self):\n        self.quality_filters = [\n            self.length_filter,\n            self.diversity_filter,\n            self.agreement_filter\n        ]\n\n    def length_filter(self, sample):\n        \"\"\"\u8fc7\u6ee4\u957f\u5ea6\u4e0d\u5408\u9002\u7684\u6837\u672c\"\"\"\n        chosen_len = len(sample['chosen'].split())\n        rejected_len = len(sample['rejected'].split())\n\n        # \u56de\u7b54\u4e0d\u80fd\u592a\u77ed\u6216\u592a\u957f\n        if chosen_len &lt; 10 or rejected_len &lt; 10:\n            return False\n        if chosen_len &gt; 500 or rejected_len &gt; 500:\n            return False\n\n        return True\n\n    def diversity_filter(self, sample):\n        \"\"\"\u786e\u4fddchosen\u548crejected\u6709\u8db3\u591f\u5dee\u5f02\"\"\"\n        from difflib import SequenceMatcher\n\n        similarity = SequenceMatcher(\n            None, sample['chosen'], sample['rejected']\n        ).ratio()\n\n        # \u76f8\u4f3c\u5ea6\u4e0d\u80fd\u592a\u9ad8\n        return similarity &lt; 0.8\n\n    def agreement_filter(self, sample):\n        \"\"\"\u8fc7\u6ee4\u6807\u6ce8\u8005\u5206\u6b67\u8fc7\u5927\u7684\u6837\u672c\"\"\"\n        if 'annotator_agreement' in sample:\n            return sample['annotator_agreement'] &gt; 0.6\n        return True\n\n    def filter_dataset(self, dataset):\n        \"\"\"\u5e94\u7528\u6240\u6709\u8d28\u91cf\u8fc7\u6ee4\u5668\"\"\"\n        filtered_data = []\n\n        for sample in dataset:\n            if all(filter_func(sample) for filter_func in self.quality_filters):\n                filtered_data.append(sample)\n\n        return filtered_data\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#_13","title":"\u8bad\u7ec3\u7a33\u5b9a\u6027\u6280\u5de7","text":"<pre><code>def stable_reward_training(model, train_loader, num_epochs=3):\n    \"\"\"\u7a33\u5b9a\u7684\u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u6d41\u7a0b\"\"\"\n\n    # \u4f7f\u7528\u8f83\u5c0f\u7684\u5b66\u4e60\u7387\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n\n    # \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n        optimizer, T_max=num_epochs * len(train_loader)\n    )\n\n    # \u65e9\u505c\u673a\u5236\n    best_val_loss = float('inf')\n    patience = 3\n    patience_counter = 0\n\n    trainer = RewardModelTrainer(model, None)\n\n    for epoch in range(num_epochs):\n        # \u8bad\u7ec3\n        train_metrics = trainer.train_epoch(train_loader, optimizer, scheduler)\n\n        # \u9a8c\u8bc1 (\u5047\u8bbe\u6709\u9a8c\u8bc1\u96c6)\n        val_metrics = trainer.evaluate(val_loader)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_metrics['avg_loss']:.4f}\")\n        print(f\"Val Loss: {val_metrics['avg_loss']:.4f}\")\n\n        # \u65e9\u505c\u68c0\u67e5\n        if val_metrics['avg_loss'] &lt; best_val_loss:\n            best_val_loss = val_metrics['avg_loss']\n            patience_counter = 0\n            # \u4fdd\u5b58\u6700\u4f73\u6a21\u578b\n            torch.save(model.state_dict(), 'best_reward_model.pt')\n        else:\n            patience_counter += 1\n            if patience_counter &gt;= patience:\n                print(\"Early stopping triggered\")\n                break\n\n    # \u52a0\u8f7d\u6700\u4f73\u6a21\u578b\n    model.load_state_dict(torch.load('best_reward_model.pt'))\n    return model\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#_14","title":"\ud83d\udd0d \u524d\u6cbf\u7814\u7a76\u8d8b\u52bf","text":""},{"location":"training/rlhf-alignment/reward-modeling/#self-rewarding","title":"Self-Rewarding\u6a21\u578b","text":"<pre><code>class SelfRewardingModel:\n    \"\"\"\u81ea\u5956\u52b1\u6a21\u578b - 2024\u5e74\u524d\u6cbf\u6280\u672f\"\"\"\n\n    def __init__(self, base_model):\n        self.base_model = base_model\n        self.judge_prompt = \"\"\"\n        \u8bf7\u4f5c\u4e3a\u4e00\u4e2aAI\u52a9\u624b\u8bc4\u5224\u8005\uff0c\u8bc4\u4f30\u4ee5\u4e0b\u56de\u7b54\u7684\u8d28\u91cf\u3002\n        \u4ece1-10\u5206\u8bc4\u5206\uff0c\u8003\u8651\u6709\u7528\u6027\u3001\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002\n\n        \u7528\u6237\u95ee\u9898: {prompt}\n        AI\u56de\u7b54: {response}\n\n        \u8bc4\u5206\u548c\u7406\u7531:\n        \"\"\"\n\n    def generate_and_judge(self, prompt):\n        \"\"\"\u751f\u6210\u56de\u7b54\u5e76\u81ea\u6211\u8bc4\u5224\"\"\"\n\n        # 1. \u751f\u6210\u56de\u7b54\n        response = self.base_model.generate(prompt)\n\n        # 2. \u81ea\u6211\u8bc4\u5224\n        judge_input = self.judge_prompt.format(\n            prompt=prompt, response=response\n        )\n\n        judgment = self.base_model.generate(judge_input)\n        score = self.extract_score(judgment)\n\n        return {\n            'response': response,\n            'judgment': judgment,\n            'score': score\n        }\n\n    def iterative_dpo_training(self, prompts):\n        \"\"\"\u8fed\u4ee3DPO\u8bad\u7ec3\"\"\"\n        training_data = []\n\n        for prompt in prompts:\n            # \u751f\u6210\u591a\u4e2a\u5019\u9009\u56de\u7b54\n            candidates = []\n            for _ in range(4):\n                result = self.generate_and_judge(prompt)\n                candidates.append(result)\n\n            # \u9009\u62e9\u6700\u4f73\u548c\u6700\u5dee\u56de\u7b54\n            candidates.sort(key=lambda x: x['score'], reverse=True)\n            best = candidates[0]\n            worst = candidates[-1]\n\n            # \u6784\u9020DPO\u8bad\u7ec3\u6837\u672c\n            if best['score'] &gt; worst['score']:\n                training_data.append({\n                    'prompt': prompt,\n                    'chosen': best['response'],\n                    'rejected': worst['response']\n                })\n\n        return training_data\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#cloud","title":"CLoud\u5956\u52b1\u6a21\u578b","text":"<pre><code>class CLoudRewardModel:\n    \"\"\"Critique-out-Loud\u5956\u52b1\u6a21\u578b - 2024\u5e74\u65b0\u6280\u672f\"\"\"\n\n    def __init__(self, model):\n        self.model = model\n\n    def critique_then_score(self, prompt, response):\n        \"\"\"\u5148\u6279\u8bc4\u540e\u8bc4\u5206\u7684\u65b9\u6cd5\"\"\"\n\n        # 1. \u751f\u6210\u6279\u8bc4\n        critique_prompt = f\"\"\"\n        \u8bf7\u4ed4\u7ec6\u5206\u6790\u4ee5\u4e0bAI\u56de\u7b54\u7684\u4f18\u7f3a\u70b9\uff1a\n\n        \u7528\u6237\u95ee\u9898: {prompt}\n        AI\u56de\u7b54: {response}\n\n        \u5206\u6790:\n        \"\"\"\n\n        critique = self.model.generate(critique_prompt)\n\n        # 2. \u57fa\u4e8e\u6279\u8bc4\u7ed9\u51fa\u5206\u6570\n        scoring_prompt = f\"\"\"\n        \u57fa\u4e8e\u4ee5\u4e0b\u5206\u6790\uff0c\u7ed9\u56de\u7b54\u6253\u5206(1-10):\n\n        \u95ee\u9898: {prompt}\n        \u56de\u7b54: {response}\n        \u5206\u6790: {critique}\n\n        \u7efc\u5408\u8bc4\u5206:\n        \"\"\"\n\n        score_output = self.model.generate(scoring_prompt)\n        score = self.extract_numerical_score(score_output)\n\n        return {\n            'critique': critique,\n            'score': score,\n            'reasoning': score_output\n        }\n</code></pre>"},{"location":"training/rlhf-alignment/reward-modeling/#_15","title":"\ud83c\udfaf \u9762\u8bd5\u95ee\u7b54\u603b\u7ed3","text":""},{"location":"training/rlhf-alignment/reward-modeling/#q1","title":"Q1: \u5956\u52b1\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u662f\u4ec0\u4e48\u683c\u5f0f\uff1f","text":"<p>A: \u504f\u597d\u6570\u636e\u683c\u5f0f\uff0c\u5305\u542bprompt\u3001chosen(\u88ab\u504f\u597d\u7684\u56de\u7b54)\u3001rejected(\u4e0d\u88ab\u504f\u597d\u7684\u56de\u7b54)\u4e09\u5143\u7ec4\uff0c\u7528\u4e8e\u8bad\u7ec3Bradley-Terry\u6a21\u578b\u9884\u6d4b\u4eba\u7c7b\u504f\u597d\u3002</p>"},{"location":"training/rlhf-alignment/reward-modeling/#q2","title":"Q2: \u5982\u4f55\u89e3\u51b3\u5956\u52b1\u6a21\u578b\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff1f","text":"<p>A:  - \u4f7f\u7528dropout\u548c\u6743\u91cd\u8870\u51cf - \u6570\u636e\u589e\u5f3a\u548c\u591a\u6837\u5316 - \u65e9\u505c\u673a\u5236 - \u96c6\u6210\u591a\u4e2a\u6a21\u578b - \u6b63\u5219\u5316\u6280\u672f</p>"},{"location":"training/rlhf-alignment/reward-modeling/#q3-bradley-terry","title":"Q3: Bradley-Terry\u6a21\u578b\u5728\u5956\u52b1\u5efa\u6a21\u4e2d\u7684\u4f5c\u7528\uff1f","text":"<p>A: Bradley-Terry\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u4eba\u7c7b\u504f\u597d\u5efa\u6a21\u4e3a\u6982\u7387\u5206\u5e03\uff0c\u4f7f\u5f97\u5956\u52b1\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u5f0f\u8bad\u7ec3\u3002</p>"},{"location":"training/rlhf-alignment/reward-modeling/#q4","title":"Q4: \u5982\u4f55\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u7684\u8d28\u91cf\uff1f","text":"<p>A: - \u51c6\u786e\u7387\uff1a\u5728\u6d4b\u8bd5\u96c6\u4e0a\u9884\u6d4b\u504f\u597d\u7684\u6b63\u786e\u7387 - \u6392\u5e8f\u8d28\u91cf\uff1aKendall's tau\u76f8\u5173\u7cfb\u6570 - \u6821\u51c6\u7a0b\u5ea6\uff1a\u671f\u671b\u6821\u51c6\u8bef\u5dee(ECE) - RewardBench\u57fa\u51c6\u6d4b\u8bd5</p>"},{"location":"training/rlhf-alignment/reward-modeling/#_16","title":"\ud83d\ude80 \u5b66\u4e60\u5efa\u8bae","text":"<ol> <li>\u7406\u8bba\u57fa\u7840: \u6df1\u5165\u7406\u89e3Bradley-Terry\u6a21\u578b</li> <li>\u5b9e\u8df5\u7ecf\u9a8c: \u8bad\u7ec3\u771f\u5b9e\u7684\u5956\u52b1\u6a21\u578b</li> <li>\u8d28\u91cf\u63a7\u5236: \u5b66\u4f1a\u6570\u636e\u6e05\u6d17\u548c\u8d28\u91cf\u8bc4\u4f30</li> <li>\u524d\u6cbf\u8ddf\u8fdb: \u5173\u6ce8Self-Rewarding\u7b49\u65b0\u6280\u672f</li> </ol> <p>\u5956\u52b1\u5efa\u6a21\u662fRLHF\u7684\u6838\u5fc3\u73af\u8282\uff0c\u4e5f\u662f2024\u5e74\u7814\u7a76\u7684\u70ed\u70b9\u9886\u57df\uff01</p>"},{"location":"training/rlhf-alignment/rlhf-core/","title":"RLHF\u6838\u5fc3\u6280\u672f","text":""},{"location":"training/rlhf-alignment/rlhf-core/#_1","title":"\ud83c\udfaf \u5b66\u4e60\u76ee\u6807","text":"<p>\u6df1\u5165\u7406\u89e3\u5f3a\u5316\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988(RLHF)\u7684\u6838\u5fc3\u539f\u7406\uff0c\u638c\u63e1PPO\u7b97\u6cd5\u5728LLM\u5bf9\u9f50\u4e2d\u7684\u5e94\u7528\uff0c\u4e86\u89e3\u6574\u4e2aRLHF\u8bad\u7ec3\u6d41\u7a0b\u3002</p> <p>\u91cd\u70b9\u9762\u8bd5\u95ee\u9898\u9884\u89c8\uff1a - RLHF\u7684\u4e09\u4e2a\u8bad\u7ec3\u9636\u6bb5\u5206\u522b\u662f\u4ec0\u4e48\uff1f - \u4e3a\u4ec0\u4e48\u9009\u62e9PPO\u7b97\u6cd5\u800c\u4e0d\u662f\u5176\u4ed6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff1f - KL\u6563\u5ea6\u5728RLHF\u4e2d\u8d77\u4ec0\u4e48\u4f5c\u7528\uff1f - \u5982\u4f55\u89e3\u51b3\u5956\u52b1\u9ed1\u5ba2(reward hacking)\u95ee\u9898\uff1f</p>"},{"location":"training/rlhf-alignment/rlhf-core/#rlhf_1","title":"\ud83c\udfd7\ufe0f RLHF\u6280\u672f\u67b6\u6784","text":""},{"location":"training/rlhf-alignment/rlhf-core/#_2","title":"\u6838\u5fc3\u6982\u5ff5","text":"<p>RLHF(Reinforcement Learning from Human Feedback)\u662f\u4e00\u79cd\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u76ee\u6807\u662f\u8ba9\u6a21\u578b\u8f93\u51fa\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002</p> <pre><code>RLHF\u8bad\u7ec3\u6d41\u7a0b\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Stage 1: SFT   \u2502\u2500\u2500\u2500\u25b6\u2502Stage 2: RM\u8bad\u7ec3  \u2502\u2500\u2500\u2500\u25b6\u2502 Stage 3: PPO\u4f18\u5316\u2502\n\u2502 \u76d1\u7763\u5fae\u8c03        \u2502    \u2502 \u5956\u52b1\u6a21\u578b\u8bad\u7ec3    \u2502    \u2502 \u5f3a\u5316\u5b66\u4e60\u4f18\u5316    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u25b2                       \u25b2                       \u25b2\n         \u2502                       \u2502                       \u2502\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502\u6307\u4ee4\u6570\u636e\u96c6\u2502           \u2502\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\u2502        \u2502PPO\u7b97\u6cd5\u4f18\u5316  \u2502\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"training/rlhf-alignment/rlhf-core/#_3","title":"\ud83d\udcca \u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u8be6\u89e3","text":""},{"location":"training/rlhf-alignment/rlhf-core/#stage-1-sft","title":"Stage 1: \u76d1\u7763\u5fae\u8c03(SFT)","text":"<p>\u76ee\u6807: \u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u5b66\u4f1a\u9075\u5faa\u6307\u4ee4</p> <pre><code># SFT\u8bad\u7ec3\u793a\u4f8b\u4ee3\u7801\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import SFTTrainer, SFTConfig\n\n# \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\nmodel = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# SFT\u914d\u7f6e\nsft_config = SFTConfig(\n    output_dir=\"./sft_model\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    learning_rate=2e-5,\n    max_seq_length=512,\n)\n\n# \u8bad\u7ec3\u5668\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    args=sft_config,\n    train_dataset=instruction_dataset,\n    formatting_func=format_instruction,\n)\n\ntrainer.train()\n</code></pre> <p>\u5173\u952e\u8981\u70b9\uff1a - \u4f7f\u7528\u9ad8\u8d28\u91cf\u7684\u6307\u4ee4-\u56de\u7b54\u5bf9\u6570\u636e - \u901a\u5e38\u9700\u8981\u51e0\u4e07\u5230\u51e0\u5341\u4e07\u4e2a\u6837\u672c - \u5b66\u4e60\u7387\u4e00\u822c\u8bbe\u7f6e\u4e3a2e-5\u52305e-5</p>"},{"location":"training/rlhf-alignment/rlhf-core/#stage-2-rm","title":"Stage 2: \u5956\u52b1\u6a21\u578b\u8bad\u7ec3(RM)","text":"<p>\u76ee\u6807: \u8bad\u7ec3\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30\u56de\u7b54\u8d28\u91cf\u7684\u5956\u52b1\u6a21\u578b</p> <pre><code># \u5956\u52b1\u6a21\u578b\u8bad\u7ec3\u4ee3\u7801\nfrom transformers import AutoModelForSequenceClassification\nfrom trl import RewardTrainer, RewardConfig\n\n# \u52a0\u8f7dSFT\u6a21\u578b\u4f5c\u4e3abackbone\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n    \"./sft_model\", \n    num_labels=1\n)\n\n# \u5956\u52b1\u6a21\u578b\u914d\u7f6e\nreward_config = RewardConfig(\n    output_dir=\"./reward_model\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    learning_rate=1e-5,\n    max_length=512,\n)\n\n# \u8bad\u7ec3\u5668\nreward_trainer = RewardTrainer(\n    model=reward_model,\n    tokenizer=tokenizer,\n    args=reward_config,\n    train_dataset=preference_dataset,\n)\n\nreward_trainer.train()\n</code></pre> <p>\u6570\u636e\u683c\u5f0f\uff1a <pre><code>{\n  \"prompt\": \"\u89e3\u91ca\u91cf\u5b50\u8ba1\u7b97\u7684\u57fa\u672c\u539f\u7406\",\n  \"chosen\": \"\u91cf\u5b50\u8ba1\u7b97\u5229\u7528\u91cf\u5b50\u529b\u5b66\u539f\u7406...(\u9ad8\u8d28\u91cf\u56de\u7b54)\",\n  \"rejected\": \"\u91cf\u5b50\u8ba1\u7b97\u5c31\u662f\u5f88\u5feb\u7684\u8ba1\u7b97...(\u4f4e\u8d28\u91cf\u56de\u7b54)\"\n}\n</code></pre></p> <p>\u5173\u952e\u6280\u672f\u70b9\uff1a - \u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u5956\u52b1\u6a21\u578b - \u6570\u636e\u8d28\u91cf\u6bd4\u6570\u91cf\u66f4\u91cd\u8981 - \u9700\u8981\u5904\u7406\u6807\u6ce8\u8005\u4e4b\u95f4\u7684\u5206\u6b67</p>"},{"location":"training/rlhf-alignment/rlhf-core/#stage-3-ppo","title":"Stage 3: PPO\u5f3a\u5316\u5b66\u4e60\u4f18\u5316","text":"<p>\u76ee\u6807: \u4f7f\u7528\u5956\u52b1\u6a21\u578b\u6307\u5bfc\u7b56\u7565\u6a21\u578b\u4f18\u5316</p> <pre><code># PPO\u8bad\u7ec3\u4ee3\u7801\nfrom trl import PPOTrainer, PPOConfig\nimport torch\n\n# PPO\u914d\u7f6e\nppo_config = PPOConfig(\n    model_name=\"./sft_model\",\n    learning_rate=1.41e-5,\n    batch_size=64,\n    mini_batch_size=4,\n    gradient_accumulation_steps=16,\n    ppo_epochs=4,\n    max_grad_norm=1.0,\n    kl_penalty=\"kl\",\n    init_kl_coeff=0.2,\n)\n\n# \u521b\u5efaPPO\u8bad\u7ec3\u5668\nppo_trainer = PPOTrainer(\n    config=ppo_config,\n    model=policy_model,\n    ref_model=reference_model,\n    tokenizer=tokenizer,\n    reward_model=reward_model,\n)\n\n# PPO\u8bad\u7ec3\u5faa\u73af\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # \u751f\u6210\u56de\u7b54\n        query_tensors = batch[\"input_ids\"]\n        response_tensors = ppo_trainer.generate(\n            query_tensors, \n            max_new_tokens=128,\n            temperature=0.7\n        )\n\n        # \u8ba1\u7b97\u5956\u52b1\n        rewards = []\n        for query, response in zip(query_tensors, response_tensors):\n            reward = reward_model(\n                torch.cat([query, response])\n            ).logits.squeeze()\n            rewards.append(reward)\n\n        # PPO\u66f4\u65b0\n        stats = ppo_trainer.step(\n            query_tensors, \n            response_tensors, \n            rewards\n        )\n\n        print(f\"Reward mean: {torch.tensor(rewards).mean():.2f}\")\n</code></pre>"},{"location":"training/rlhf-alignment/rlhf-core/#ppo","title":"\ud83c\udfae PPO\u7b97\u6cd5\u6838\u5fc3\u539f\u7406","text":""},{"location":"training/rlhf-alignment/rlhf-core/#ppo_1","title":"\u4e3a\u4ec0\u4e48\u9009\u62e9PPO\uff1f","text":"<ol> <li>\u7a33\u5b9a\u6027: \u907f\u514d\u7b56\u7565\u66f4\u65b0\u8fc7\u5927\u5bfc\u81f4\u7684\u6027\u80fd\u5d29\u584c</li> <li>\u6548\u7387: \u76f8\u6bd4TRPO\u66f4\u7b80\u5355\uff0c\u8ba1\u7b97\u5f00\u9500\u66f4\u5c0f</li> <li>\u53ef\u8c03\u8282\u6027: \u901a\u8fc7clip\u53c2\u6570\u63a7\u5236\u66f4\u65b0\u5e45\u5ea6</li> </ol>"},{"location":"training/rlhf-alignment/rlhf-core/#ppo_2","title":"PPO\u76ee\u6807\u51fd\u6570","text":"\\[L^{CLIP}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]\\] <p>\u5176\u4e2d\uff1a - \\(r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}\\) \u662f\u6982\u7387\u6bd4\u7387 - \\(\\hat{A}_t\\) \u662f\u4f18\u52bf\u51fd\u6570\u4f30\u8ba1 - \\(\\epsilon\\) \u662fclip\u53c2\u6570(\u901a\u5e38\u8bbe\u4e3a0.2)</p>"},{"location":"training/rlhf-alignment/rlhf-core/#kl","title":"KL\u6563\u5ea6\u7ea6\u675f","text":"<p>\u4e3a\u4e86\u9632\u6b62\u7b56\u7565\u504f\u79bb\u53c2\u8003\u6a21\u578b\u8fc7\u8fdc\uff0c\u6dfb\u52a0KL\u60e9\u7f5a\uff1a</p> \\[L_{total} = L_{PPO} - \\beta \\cdot KL(\\pi_\\theta || \\pi_{ref})\\] <pre><code># KL\u6563\u5ea6\u8ba1\u7b97\u793a\u4f8b\ndef compute_kl_penalty(logprobs_new, logprobs_old, kl_coeff):\n    \"\"\"\u8ba1\u7b97KL\u6563\u5ea6\u60e9\u7f5a\"\"\"\n    kl = logprobs_old - logprobs_new\n    kl_penalty = -kl_coeff * kl\n    return kl_penalty\n\n# \u5728\u8bad\u7ec3\u4e2d\u5e94\u7528\ntotal_loss = ppo_loss + kl_penalty\n</code></pre>"},{"location":"training/rlhf-alignment/rlhf-core/#rlhf_2","title":"\u26a0\ufe0f RLHF\u4e2d\u7684\u5173\u952e\u6311\u6218","text":""},{"location":"training/rlhf-alignment/rlhf-core/#1-reward-hacking","title":"1. \u5956\u52b1\u9ed1\u5ba2(Reward Hacking)","text":"<p>\u95ee\u9898: \u6a21\u578b\u5b66\u4f1a\u5229\u7528\u5956\u52b1\u6a21\u578b\u7684\u6f0f\u6d1e\u83b7\u5f97\u9ad8\u5206</p> <p>\u89e3\u51b3\u65b9\u6848\uff1a - \u4f7f\u7528KL\u6563\u5ea6\u7ea6\u675f\u9650\u5236\u504f\u79fb - \u5b9a\u671f\u66f4\u65b0\u5956\u52b1\u6a21\u578b - \u4f7f\u7528\u591a\u4e2a\u5956\u52b1\u6a21\u578b\u7684ensemble</p> <pre><code># \u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u7684\u4ee3\u7801\u793a\u4f8b\ndef compute_reward_with_kl_penalty(response, query, reward_model, ref_model, kl_coeff=0.1):\n    # \u8ba1\u7b97\u57fa\u7840\u5956\u52b1\n    reward = reward_model(response).logits.squeeze()\n\n    # \u8ba1\u7b97KL\u60e9\u7f5a\n    current_logprobs = model(torch.cat([query, response])).log_softmax(dim=-1)\n    ref_logprobs = ref_model(torch.cat([query, response])).log_softmax(dim=-1)\n    kl_penalty = kl_coeff * (current_logprobs - ref_logprobs).sum()\n\n    return reward - kl_penalty\n</code></pre>"},{"location":"training/rlhf-alignment/rlhf-core/#2","title":"2. \u8bad\u7ec3\u4e0d\u7a33\u5b9a","text":"<p>\u95ee\u9898: PPO\u8bad\u7ec3\u5bb9\u6613\u53d1\u6563\u6216\u9677\u5165\u5c40\u90e8\u6700\u4f18</p> <p>\u89e3\u51b3\u65b9\u6848\uff1a - \u9002\u5f53\u7684\u5b66\u4e60\u7387\u8c03\u5ea6 - \u68af\u5ea6\u88c1\u526a - \u65e9\u505c\u7b56\u7565</p>"},{"location":"training/rlhf-alignment/rlhf-core/#3","title":"3. \u8ba1\u7b97\u8d44\u6e90\u9700\u6c42","text":"<p>\u95ee\u9898: \u9700\u8981\u540c\u65f6\u7ef4\u62a4\u591a\u4e2a\u5927\u6a21\u578b</p> <p>\u8d44\u6e90\u9700\u6c42\u5206\u6790\uff1a <pre><code>70B\u6a21\u578b\u7684RLHF\u8bad\u7ec3\u9700\u6c42\uff1a\n\u251c\u2500\u2500 Policy Model (\u8bad\u7ec3\u4e2d): ~280GB\u663e\u5b58\n\u251c\u2500\u2500 Reference Model (\u63a8\u7406): ~140GB\u663e\u5b58  \n\u251c\u2500\u2500 Reward Model (\u63a8\u7406): ~140GB\u663e\u5b58\n\u251c\u2500\u2500 Value Model (\u8bad\u7ec3\u4e2d): ~280GB\u663e\u5b58\n\u2514\u2500\u2500 \u603b\u8ba1: ~840GB\u663e\u5b58 (\u7ea6\u9700A100 80G \u00d7 12\u5f20)\n</code></pre></p>"},{"location":"training/rlhf-alignment/rlhf-core/#_4","title":"\ud83d\udcc8 \u6027\u80fd\u8bc4\u4f30\u6307\u6807","text":""},{"location":"training/rlhf-alignment/rlhf-core/#1","title":"1. \u5956\u52b1\u6a21\u578b\u51c6\u786e\u7387","text":"<pre><code>def evaluate_reward_model(reward_model, test_dataset):\n    \"\"\"\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u5728\u504f\u597d\u9884\u6d4b\u4e0a\u7684\u51c6\u786e\u7387\"\"\"\n    correct = 0\n    total = 0\n\n    for batch in test_dataset:\n        chosen_rewards = reward_model(batch[\"chosen\"]).logits.squeeze()\n        rejected_rewards = reward_model(batch[\"rejected\"]).logits.squeeze()\n\n        # \u7edf\u8ba1\u5956\u52b1\u6a21\u578b\u662f\u5426\u6b63\u786e\u9884\u6d4b\u4eba\u7c7b\u504f\u597d\n        correct += (chosen_rewards &gt; rejected_rewards).sum().item()\n        total += len(batch[\"chosen\"])\n\n    accuracy = correct / total\n    return accuracy\n</code></pre>"},{"location":"training/rlhf-alignment/rlhf-core/#2-kl","title":"2. KL\u6563\u5ea6\u76d1\u63a7","text":"<pre><code>def monitor_kl_divergence(policy_model, reference_model, eval_prompts):\n    \"\"\"\u76d1\u63a7\u7b56\u7565\u6a21\u578b\u4e0e\u53c2\u8003\u6a21\u578b\u7684KL\u6563\u5ea6\"\"\"\n    kl_divs = []\n\n    for prompt in eval_prompts:\n        policy_probs = F.softmax(policy_model(prompt).logits, dim=-1)\n        ref_probs = F.softmax(reference_model(prompt).logits, dim=-1)\n\n        kl_div = F.kl_div(policy_probs.log(), ref_probs, reduction='sum')\n        kl_divs.append(kl_div.item())\n\n    return np.mean(kl_divs)\n</code></pre>"},{"location":"training/rlhf-alignment/rlhf-core/#3_1","title":"3. \u4eba\u7c7b\u8bc4\u4f30\u6307\u6807","text":"<ul> <li>\u6709\u7528\u6027(Helpfulness): \u56de\u7b54\u662f\u5426\u89e3\u51b3\u4e86\u7528\u6237\u95ee\u9898</li> <li>\u65e0\u5bb3\u6027(Harmlessness): \u56de\u7b54\u662f\u5426\u907f\u514d\u6709\u5bb3\u5185\u5bb9</li> <li>\u8bda\u5b9e\u6027(Honesty): \u56de\u7b54\u662f\u5426\u51c6\u786e\u53ef\u4fe1</li> </ul>"},{"location":"training/rlhf-alignment/rlhf-core/#_5","title":"\ud83d\udd27 \u5b9e\u6218\u7ecf\u9a8c\u4e0e\u6700\u4f73\u5b9e\u8df5","text":""},{"location":"training/rlhf-alignment/rlhf-core/#1_1","title":"1. \u8d85\u53c2\u6570\u8c03\u4f18\u5efa\u8bae","text":"<pre><code># \u63a8\u8350\u7684RLHF\u8d85\u53c2\u6570\nRLHF_CONFIG = {\n    # SFT\u9636\u6bb5\n    \"sft_learning_rate\": 2e-5,\n    \"sft_epochs\": 3,\n    \"sft_batch_size\": 4,\n\n    # \u5956\u52b1\u6a21\u578b\u9636\u6bb5  \n    \"rm_learning_rate\": 1e-5,\n    \"rm_epochs\": 1,\n    \"rm_batch_size\": 4,\n\n    # PPO\u9636\u6bb5\n    \"ppo_learning_rate\": 1.41e-5,\n    \"ppo_batch_size\": 64,\n    \"ppo_epochs\": 4,\n    \"init_kl_coeff\": 0.2,\n    \"clip_range\": 0.2,\n}\n</code></pre>"},{"location":"training/rlhf-alignment/rlhf-core/#2_1","title":"2. \u8bad\u7ec3\u76d1\u63a7\u8981\u70b9","text":"<ul> <li>\u5956\u52b1\u5206\u6570\u7684\u53d8\u5316\u8d8b\u52bf</li> <li>KL\u6563\u5ea6\u662f\u5426\u5728\u5408\u7406\u8303\u56f4\u5185</li> <li>\u751f\u6210\u8d28\u91cf\u7684\u4eba\u5de5\u8bc4\u4f30</li> <li>\u8bad\u7ec3\u635f\u5931\u7684\u6536\u655b\u60c5\u51b5</li> </ul>"},{"location":"training/rlhf-alignment/rlhf-core/#3_2","title":"3. \u5e38\u89c1\u95ee\u9898\u6392\u67e5","text":"<pre><code>def diagnose_rlhf_training(stats):\n    \"\"\"\u8bca\u65adRLHF\u8bad\u7ec3\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\"\"\"\n\n    # \u68c0\u67e5\u5956\u52b1\u9ed1\u5ba2\n    if stats['reward_mean'] &gt; stats['reward_threshold']:\n        print(\"\u8b66\u544a\uff1a\u53ef\u80fd\u5b58\u5728\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\")\n        print(f\"\u5efa\u8bae\uff1a\u589e\u52a0KL\u7cfb\u6570\u4ece {stats['kl_coeff']} \u5230 {stats['kl_coeff'] * 1.5}\")\n\n    # \u68c0\u67e5\u8bad\u7ec3\u53d1\u6563\n    if stats['kl_div'] &gt; 10.0:\n        print(\"\u8b66\u544a\uff1aKL\u6563\u5ea6\u8fc7\u5927\uff0c\u8bad\u7ec3\u53ef\u80fd\u53d1\u6563\")\n        print(\"\u5efa\u8bae\uff1a\u964d\u4f4e\u5b66\u4e60\u7387\u6216\u589e\u52a0KL\u7cfb\u6570\")\n\n    # \u68c0\u67e5\u8bad\u7ec3\u505c\u6ede\n    if stats['reward_std'] &lt; 0.1:\n        print(\"\u8b66\u544a\uff1a\u5956\u52b1\u5206\u5e03\u8fc7\u4e8e\u96c6\u4e2d\uff0c\u53ef\u80fd\u9677\u5165\u5c40\u90e8\u6700\u4f18\")\n        print(\"\u5efa\u8bae\uff1a\u589e\u52a0\u63a2\u7d22\u6027\u6216\u8c03\u6574temperature\")\n</code></pre>"},{"location":"training/rlhf-alignment/rlhf-core/#_6","title":"\ud83c\udfaf \u9762\u8bd5\u95ee\u7b54\u603b\u7ed3","text":""},{"location":"training/rlhf-alignment/rlhf-core/#q1-rlhf","title":"Q1: RLHF\u7684\u4e09\u4e2a\u8bad\u7ec3\u9636\u6bb5\u5206\u522b\u662f\u4ec0\u4e48\uff1f","text":"<p>A:  1. SFT(\u76d1\u7763\u5fae\u8c03): \u8ba9\u9884\u8bad\u7ec3\u6a21\u578b\u5b66\u4f1a\u9075\u5faa\u6307\u4ee4 2. RM(\u5956\u52b1\u6a21\u578b\u8bad\u7ec3): \u8bad\u7ec3\u4e00\u4e2a\u8bc4\u4f30\u56de\u7b54\u8d28\u91cf\u7684\u6a21\u578b 3. PPO(\u5f3a\u5316\u5b66\u4e60\u4f18\u5316): \u4f7f\u7528\u5956\u52b1\u6a21\u578b\u6307\u5bfc\u7b56\u7565\u4f18\u5316</p>"},{"location":"training/rlhf-alignment/rlhf-core/#q2-ppo","title":"Q2: \u4e3a\u4ec0\u4e48\u9009\u62e9PPO\u7b97\u6cd5\uff1f","text":"<p>A:  - \u7a33\u5b9a\u6027: \u901a\u8fc7clip\u673a\u5236\u907f\u514d\u7b56\u7565\u66f4\u65b0\u8fc7\u5927 - \u7b80\u5355\u6027: \u76f8\u6bd4TRPO\u66f4\u5bb9\u6613\u5b9e\u73b0\u548c\u8c03\u4f18 - \u6709\u6548\u6027: \u5728LLM\u5bf9\u9f50\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d</p>"},{"location":"training/rlhf-alignment/rlhf-core/#q3-klrlhf","title":"Q3: KL\u6563\u5ea6\u5728RLHF\u4e2d\u8d77\u4ec0\u4e48\u4f5c\u7528\uff1f","text":"<p>A:  - \u7ea6\u675f\u4f5c\u7528: \u9632\u6b62\u7b56\u7565\u6a21\u578b\u504f\u79bb\u53c2\u8003\u6a21\u578b\u8fc7\u8fdc - \u7a33\u5b9a\u8bad\u7ec3: \u907f\u514d\u5956\u52b1\u9ed1\u5ba2\u548c\u8bad\u7ec3\u53d1\u6563 - \u4fdd\u6301\u80fd\u529b: \u786e\u4fdd\u6a21\u578b\u4e0d\u4f1a\u5931\u53bb\u539f\u6709\u7684\u8bed\u8a00\u80fd\u529b</p>"},{"location":"training/rlhf-alignment/rlhf-core/#q4","title":"Q4: \u5982\u4f55\u89e3\u51b3\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff1f","text":"<p>A: - \u4f7f\u7528KL\u6563\u5ea6\u60e9\u7f5a\u9650\u5236\u6a21\u578b\u504f\u79fb - \u5b9a\u671f\u66f4\u65b0\u548c\u9a8c\u8bc1\u5956\u52b1\u6a21\u578b - \u4f7f\u7528\u591a\u6837\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u4eba\u5de5\u68c0\u67e5</p>"},{"location":"training/rlhf-alignment/rlhf-core/#_7","title":"\ud83d\ude80 \u5b66\u4e60\u5efa\u8bae","text":"<ol> <li>\u7406\u8bba\u5148\u884c: \u5148\u7406\u89e3\u5f3a\u5316\u5b66\u4e60\u57fa\u7840\u6982\u5ff5</li> <li>\u4ee3\u7801\u5b9e\u8df5: \u8dd1\u901a\u5b8c\u6574\u7684RLHF\u6d41\u7a0b</li> <li>\u53c2\u6570\u8c03\u4f18: \u4e86\u89e3\u5404\u4e2a\u8d85\u53c2\u6570\u7684\u4f5c\u7528</li> <li>\u95ee\u9898\u8bca\u65ad: \u5b66\u4f1a\u8bc6\u522b\u548c\u89e3\u51b3\u5e38\u89c1\u95ee\u9898</li> </ol> <p>\u8fd9\u4e00\u6280\u672f\u662f\u73b0\u4ee3LLM\u5bf9\u9f50\u7684\u6838\u5fc3\uff0c\u638c\u63e1\u597d\u4e86\u5bf9\u9762\u8bd5\u548c\u5b9e\u9645\u5de5\u4f5c\u90fd\u5f88\u6709\u5e2e\u52a9\uff01</p>"},{"location":"tutorials/first-document/","title":"\u7f16\u5199\u7b2c\u4e00\u7bc7\u6587\u6863","text":"<p>\u5b66\u4e60\u5982\u4f55\u521b\u5efa\u548c\u7f16\u8f91\u60a8\u7684\u7b2c\u4e00\u7bc7 Markdown \u6587\u6863\u3002</p>"},{"location":"tutorials/first-document/#_2","title":"\u521b\u5efa\u65b0\u9875\u9762","text":"<ol> <li>\u5728 <code>docs/</code> \u76ee\u5f55\u4e0b\u521b\u5efa\u4e00\u4e2a\u65b0\u7684 <code>.md</code> \u6587\u4ef6</li> <li>\u4f7f\u7528 Markdown \u8bed\u6cd5\u7f16\u5199\u5185\u5bb9</li> <li>\u4fdd\u5b58\u6587\u4ef6\u540e\uff0c\u5f00\u53d1\u670d\u52a1\u5668\u4f1a\u81ea\u52a8\u5237\u65b0</li> </ol>"},{"location":"tutorials/first-document/#markdown","title":"\u57fa\u7840 Markdown \u8bed\u6cd5","text":""},{"location":"tutorials/first-document/#_3","title":"\u6807\u9898","text":"<pre><code># \u4e00\u7ea7\u6807\u9898\n## \u4e8c\u7ea7\u6807\u9898\n### \u4e09\u7ea7\u6807\u9898\n</code></pre>"},{"location":"tutorials/first-document/#_4","title":"\u6587\u672c\u683c\u5f0f","text":"<pre><code>**\u7c97\u4f53\u6587\u672c**\n*\u659c\u4f53\u6587\u672c*\n`\u884c\u5185\u4ee3\u7801`\n</code></pre>"},{"location":"tutorials/first-document/#_5","title":"\u5217\u8868","text":"<pre><code>- \u65e0\u5e8f\u5217\u8868\u9879 1\n- \u65e0\u5e8f\u5217\u8868\u9879 2\n\n1. \u6709\u5e8f\u5217\u8868\u9879 1\n2. \u6709\u5e8f\u5217\u8868\u9879 2\n</code></pre>"},{"location":"tutorials/first-document/#_6","title":"\u94fe\u63a5","text":"<pre><code>[\u94fe\u63a5\u6587\u672c](URL)\n[\u5185\u90e8\u94fe\u63a5](../reference/markdown.md)\n</code></pre>"},{"location":"tutorials/first-document/#_7","title":"\u4ee3\u7801\u5757","text":"<pre><code>```python\ndef hello_world():\n    print(\"Hello, World!\")\n```\n</code></pre>"},{"location":"tutorials/first-document/#_8","title":"\u5b9e\u8df5\u7ec3\u4e60","text":"<p>\u5c1d\u8bd5\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u4ee5\u4e0b\u5185\u5bb9\u7684\u65b0\u9875\u9762\uff1a</p> <ul> <li>[ ] \u6807\u9898\u548c\u5b50\u6807\u9898</li> <li>[ ] \u4e00\u4e9b\u683c\u5f0f\u5316\u6587\u672c</li> <li>[ ] \u4e00\u4e2a\u5217\u8868</li> <li>[ ] \u4e00\u4e2a\u4ee3\u7801\u793a\u4f8b</li> </ul>"},{"location":"tutorials/first-document/#_9","title":"\u63d0\u793a","text":"<p>!!! tip \"\u4e13\u4e1a\u63d0\u793a\"     \u4f7f\u7528 MkDocs Material \u7684\u544a\u793a\u6846\u529f\u80fd\u6765\u7a81\u51fa\u91cd\u8981\u4fe1\u606f\uff01</p> <p>!!! warning \"\u6ce8\u610f\"     \u8bb0\u5f97\u4fdd\u5b58\u60a8\u7684\u66f4\u6539\uff0c\u5f00\u53d1\u670d\u52a1\u5668\u4f1a\u81ea\u52a8\u91cd\u65b0\u52a0\u8f7d\u3002</p>"}]}