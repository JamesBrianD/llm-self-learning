
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="大语言模型与AI学习资源汇总">
      
      
      
        <link rel="canonical" href="https://jamesbriand.github.io/llm-self-learning/fundamentals/transformer/encoder-decoder/">
      
      
        <link rel="prev" href="../ffn/">
      
      
        <link rel="next" href="../language-models/">
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.16">
    
    
      
        <title>编码器-解码器架构 - LLM 自学指南</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto+Slab:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto Slab";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#-" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="LLM 自学指南" class="md-header__button md-logo" aria-label="LLM 自学指南" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLM 自学指南
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              编码器-解码器架构
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="light-blue" data-md-color-accent="deep-purple"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="cyan" data-md-color-accent="deep-purple"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="LLM 自学指南" class="md-nav__button md-logo" aria-label="LLM 自学指南" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LLM 自学指南
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    首页
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../getting-started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    学习指南
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    模型基础
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            模型基础
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    第1节 Transformer基础
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            第1节 Transformer基础
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    学习概览
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../attention/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Attention机制
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ffn/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    前馈神经网络
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    编码器-解码器架构
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    编码器-解码器架构
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 本节目标
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      📝 知识总结
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📝 知识总结">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      整体架构概览
    </span>
  </a>
  
    <nav class="md-nav" aria-label="整体架构概览">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      核心组件
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#encoder" class="md-nav__link">
    <span class="md-ellipsis">
      编码器 (Encoder) 详解
    </span>
  </a>
  
    <nav class="md-nav" aria-label="编码器 (Encoder) 详解">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      结构组成
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      数学表示
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      核心特征
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoder" class="md-nav__link">
    <span class="md-ellipsis">
      解码器 (Decoder) 详解
    </span>
  </a>
  
    <nav class="md-nav" aria-label="解码器 (Decoder) 详解">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      结构组成
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      数学表示
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      核心特征
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      三种注意力机制详解
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三种注意力机制详解">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-encoder-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      1. 编码器自注意力 (Encoder Self-Attention)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-masked-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      2. 解码器掩码自注意力 (Masked Self-Attention)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-cross-attention" class="md-nav__link">
    <span class="md-ellipsis">
      3. 编码器-解码器交叉注意力 (Cross-Attention)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      架构优势与应用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="架构优势与应用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      优势特点
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      典型应用
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      现代发展趋势
    </span>
  </a>
  
    <nav class="md-nav" aria-label="现代发展趋势">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      架构演进
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      💬 面试问题解答
    </span>
  </a>
  
    <nav class="md-nav" aria-label="💬 面试问题解答">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q1" class="md-nav__link">
    <span class="md-ellipsis">
      Q1: 编码器和解码器的主要区别是什么？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q2" class="md-nav__link">
    <span class="md-ellipsis">
      Q2: 交叉注意力的工作原理是什么？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q3" class="md-nav__link">
    <span class="md-ellipsis">
      Q3: 为什么解码器需要掩码自注意力？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q4-decoder-only" class="md-nav__link">
    <span class="md-ellipsis">
      Q4: 为什么现在更流行Decoder-Only架构？
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      💻 代码实现
    </span>
  </a>
  
    <nav class="md-nav" aria-label="💻 代码实现">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#encoder-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      完整Encoder-Decoder实现
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 学习检验
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      🔗 相关链接
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../language-models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    语言模型架构
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tokenizer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Tokenizer技术
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    第2节 Attention升级
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            第2节 Attention升级
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attention-advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    学习概览
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attention-advanced/mha-variants/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    多头注意力变体
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attention-advanced/kv-cache/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    KV Cache技术
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attention-advanced/normalization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    归一化技术
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../attention-advanced/positional-encoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    位置编码
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
        
          
          <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    第3节 LLM升级技术
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_3">
            <span class="md-nav__icon md-icon"></span>
            第3节 LLM升级技术
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llm-advanced/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    学习概览
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llm-advanced/moe/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MOE架构
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../llm-advanced/distributed/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    分布式训练
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_4" >
        
          
          <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    第4节 DeepSeek优化技术
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_4">
            <span class="md-nav__icon md-icon"></span>
            第4节 DeepSeek优化技术
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deepseek-innovations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    学习概览
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deepseek-innovations/mla/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MLA核心技术
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deepseek-innovations/deepseek-moe/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DeepSeek MoE创新
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../deepseek-innovations/mtp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MTP多token预测
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    模型训练
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            模型训练
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_1" >
        
          
          <label class="md-nav__link" for="__nav_4_1" id="__nav_4_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    第8节 强化学习与对齐
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_1">
            <span class="md-nav__icon md-icon"></span>
            第8节 强化学习与对齐
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/rlhf-alignment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    学习概览
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/rlhf-alignment/rlhf-core/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RLHF核心技术
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/rlhf-alignment/dpo-constitutional/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DPO与Constitutional AI
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/rlhf-alignment/reward-modeling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    奖励模型训练
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../training/rlhf-alignment/frameworks-implementation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    框架与实现
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    大模型应用
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            大模型应用
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
        
          
          <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    第5节 Context Engineering
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_1">
            <span class="md-nav__icon md-icon"></span>
            第5节 Context Engineering
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../applications/context-engineering/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    学习概览
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../applications/context-engineering/practices/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    上下文工程实践
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    第6节 RAG与Agent
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            第6节 RAG与Agent
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../applications/rag-agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    学习概览
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../applications/rag-agent/rag/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RAG技术
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../applications/rag-agent/agent/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    AI Agent
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    第7节 CoT与评测
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            第7节 CoT与评测
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../applications/cot-evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    学习概览
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../applications/cot-evaluation/cot/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    思维链技术
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../applications/cot-evaluation/langchain/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LangChain框架
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../applications/cot-evaluation/evaluation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    模型评测
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../interview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    面试题库
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../code-examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    代码实现
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      🎯 本节目标
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      📝 知识总结
    </span>
  </a>
  
    <nav class="md-nav" aria-label="📝 知识总结">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      整体架构概览
    </span>
  </a>
  
    <nav class="md-nav" aria-label="整体架构概览">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      核心组件
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#encoder" class="md-nav__link">
    <span class="md-ellipsis">
      编码器 (Encoder) 详解
    </span>
  </a>
  
    <nav class="md-nav" aria-label="编码器 (Encoder) 详解">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      结构组成
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      数学表示
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      核心特征
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoder" class="md-nav__link">
    <span class="md-ellipsis">
      解码器 (Decoder) 详解
    </span>
  </a>
  
    <nav class="md-nav" aria-label="解码器 (Decoder) 详解">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      结构组成
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      数学表示
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      核心特征
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      三种注意力机制详解
    </span>
  </a>
  
    <nav class="md-nav" aria-label="三种注意力机制详解">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-encoder-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      1. 编码器自注意力 (Encoder Self-Attention)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-masked-self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      2. 解码器掩码自注意力 (Masked Self-Attention)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-cross-attention" class="md-nav__link">
    <span class="md-ellipsis">
      3. 编码器-解码器交叉注意力 (Cross-Attention)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      架构优势与应用
    </span>
  </a>
  
    <nav class="md-nav" aria-label="架构优势与应用">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      优势特点
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      典型应用
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      现代发展趋势
    </span>
  </a>
  
    <nav class="md-nav" aria-label="现代发展趋势">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      架构演进
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      💬 面试问题解答
    </span>
  </a>
  
    <nav class="md-nav" aria-label="💬 面试问题解答">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q1" class="md-nav__link">
    <span class="md-ellipsis">
      Q1: 编码器和解码器的主要区别是什么？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q2" class="md-nav__link">
    <span class="md-ellipsis">
      Q2: 交叉注意力的工作原理是什么？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q3" class="md-nav__link">
    <span class="md-ellipsis">
      Q3: 为什么解码器需要掩码自注意力？
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q4-decoder-only" class="md-nav__link">
    <span class="md-ellipsis">
      Q4: 为什么现在更流行Decoder-Only架构？
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_18" class="md-nav__link">
    <span class="md-ellipsis">
      💻 代码实现
    </span>
  </a>
  
    <nav class="md-nav" aria-label="💻 代码实现">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#encoder-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      完整Encoder-Decoder实现
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_19" class="md-nav__link">
    <span class="md-ellipsis">
      ✅ 学习检验
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_20" class="md-nav__link">
    <span class="md-ellipsis">
      🔗 相关链接
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="-">编码器-解码器架构<a class="headerlink" href="#-" title="Permanent link">&para;</a></h1>
<h2 id="_1">🎯 本节目标<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h2>
<p>深入理解原始Transformer的编码器-解码器架构，掌握三种注意力机制的区别和作用。</p>
<h2 id="_2">📝 知识总结<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h2>
<h3 id="_3">整体架构概览<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>原始Transformer采用<strong>编码器-解码器(Encoder-Decoder)</strong>架构，专门用于序列到序列的转换任务。</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>输入序列 → 编码器 → 编码表示 → 解码器 → 输出序列
</code></pre></div>
<h4 id="_4">核心组件<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h4>
<ol>
<li><strong>编码器(Encoder)</strong>: 将输入序列编码为高层语义表示</li>
<li><strong>解码器(Decoder)</strong>: 基于编码表示自回归生成输出序列</li>
<li><strong>三种注意力机制</strong>: 自注意力、掩码自注意力、交叉注意力</li>
</ol>
<h3 id="encoder">编码器 (Encoder) 详解<a class="headerlink" href="#encoder" title="Permanent link">&para;</a></h3>
<h4 id="_5">结构组成<a class="headerlink" href="#_5" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>N层相同的层</strong>(原论文N=6)</li>
<li>每层包含两个子层：</li>
<li>多头自注意力机制</li>
<li>前馈神经网络</li>
<li>每个子层使用残差连接和层归一化</li>
</ul>
<h4 id="_6">数学表示<a class="headerlink" href="#_6" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a># 编码器层的计算过程
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>def encoder_layer(x):
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>    # 多头自注意力
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>    attn_output = multi_head_attention(x, x, x)
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>    x = layer_norm(x + attn_output)  # 残差连接 + 层归一化
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>    # 前馈网络
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>    ffn_output = feed_forward(x)
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    x = layer_norm(x + ffn_output)   # 残差连接 + 层归一化
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>    return x
</code></pre></div>
<h4 id="_7">核心特征<a class="headerlink" href="#_7" title="Permanent link">&para;</a></h4>
<p><strong>1. 并行处理</strong>
- 可以同时处理整个输入序列
- 每个位置都能看到所有其他位置
- 训练效率高，无序列计算依赖</p>
<p><strong>2. 双向上下文</strong>
- 自注意力机制允许每个位置关注整个序列
- 能够捕获全局的上下文信息
- 适合理解类任务</p>
<p><strong>3. 语义提升</strong>
- 将低阶词向量转换为高阶语义表示
- 多层堆叠逐步抽象语义信息
- 最终输出包含丰富的上下文信息</p>
<h3 id="decoder">解码器 (Decoder) 详解<a class="headerlink" href="#decoder" title="Permanent link">&para;</a></h3>
<h4 id="_8">结构组成<a class="headerlink" href="#_8" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>N层相同的层</strong>(原论文N=6)</li>
<li>每层包含三个子层：</li>
<li>掩码多头自注意力</li>
<li>编码器-解码器交叉注意力</li>
<li>前馈神经网络</li>
<li>每个子层使用残差连接和层归一化</li>
</ul>
<h4 id="_9">数学表示<a class="headerlink" href="#_9" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a># 解码器层的计算过程
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>def decoder_layer(x, encoder_output):
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>    # 1. 掩码自注意力
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>    masked_attn = masked_multi_head_attention(x, x, x)
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>    x = layer_norm(x + masked_attn)
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    # 2. 交叉注意力
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    cross_attn = multi_head_attention(
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>        query=x, 
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>        key=encoder_output, 
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>        value=encoder_output
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>    )
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>    x = layer_norm(x + cross_attn)
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>    # 3. 前馈网络
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>    ffn_output = feed_forward(x)
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>    x = layer_norm(x + ffn_output)
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>    return x
</code></pre></div>
<h4 id="_10">核心特征<a class="headerlink" href="#_10" title="Permanent link">&para;</a></h4>
<p><strong>1. 自回归生成</strong>
- 逐步生成输出序列
- 当前位置只能看到之前的位置
- 使用掩码机制防止信息泄露</p>
<p><strong>2. 双输入机制</strong>
- 输入1：解码器之前的输出(自回归)
- 输入2：编码器的输出表示(交叉注意力)
- 结合自身历史和源序列信息</p>
<h3 id="_11">三种注意力机制详解<a class="headerlink" href="#_11" title="Permanent link">&para;</a></h3>
<h4 id="1-encoder-self-attention">1. 编码器自注意力 (Encoder Self-Attention)<a class="headerlink" href="#1-encoder-self-attention" title="Permanent link">&para;</a></h4>
<p><strong>作用</strong>: 让编码器的每个位置关注输入序列的所有位置</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="c1"># 编码器自注意力</span>
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="n">Q</span> <span class="o">=</span> <span class="n">K</span> <span class="o">=</span> <span class="n">V</span> <span class="o">=</span> <span class="n">encoder_input</span>  <span class="c1"># 都来自输入序列</span>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="n">attention_output</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div>
<p><strong>特点</strong>:
- 无掩码限制，可以看到全序列
- 建立输入序列内部的依赖关系
- 捕获长距离依赖</p>
<h4 id="2-masked-self-attention">2. 解码器掩码自注意力 (Masked Self-Attention)<a class="headerlink" href="#2-masked-self-attention" title="Permanent link">&para;</a></h4>
<p><strong>作用</strong>: 让解码器的每个位置只关注之前的位置</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="c1"># 掩码自注意力</span>
<a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="n">Q</span> <span class="o">=</span> <span class="n">K</span> <span class="o">=</span> <span class="n">V</span> <span class="o">=</span> <span class="n">decoder_input</span>  <span class="c1"># 都来自解码器输入</span>
<a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">mask</span> <span class="o">=</span> <span class="n">create_causal_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span>  <span class="c1"># 下三角掩码</span>
<a id="__codelineno-4-4" name="__codelineno-4-4" href="#__codelineno-4-4"></a><span class="n">attention_output</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
</code></pre></div>
<p><strong>掩码机制</strong>:
<div class="highlight"><pre><span></span><code><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a>位置:  0  1  2  3
<a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>掩码: [1  0  0  0]  # 位置0只能看自己
<a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a>      [1  1  0  0]  # 位置1能看0,1
<a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>      [1  1  1  0]  # 位置2能看0,1,2
<a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a>      [1  1  1  1]  # 位置3能看0,1,2,3
</code></pre></div></p>
<h4 id="3-cross-attention">3. 编码器-解码器交叉注意力 (Cross-Attention)<a class="headerlink" href="#3-cross-attention" title="Permanent link">&para;</a></h4>
<p><strong>作用</strong>: 让解码器关注编码器的输出，实现序列对齐</p>
<div class="highlight"><pre><span></span><code><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="c1"># 交叉注意力</span>
<a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="n">Q</span> <span class="o">=</span> <span class="n">decoder_hidden</span>        <span class="c1"># 查询来自解码器</span>
<a id="__codelineno-6-3" name="__codelineno-6-3" href="#__codelineno-6-3"></a><span class="n">K</span> <span class="o">=</span> <span class="n">V</span> <span class="o">=</span> <span class="n">encoder_output</span>    <span class="c1"># 键值来自编码器</span>
<a id="__codelineno-6-4" name="__codelineno-6-4" href="#__codelineno-6-4"></a><span class="n">attention_output</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</code></pre></div>
<p><strong>工作原理</strong>:
- Query：解码器想要什么信息
- Key：编码器有什么信息
- Value：编码器提供的具体信息
- 实现源序列和目标序列的对齐</p>
<h3 id="_12">架构优势与应用<a class="headerlink" href="#_12" title="Permanent link">&para;</a></h3>
<h4 id="_13">优势特点<a class="headerlink" href="#_13" title="Permanent link">&para;</a></h4>
<p><strong>1. 并行训练</strong>
- 编码器可以并行处理整个输入
- 解码器在训练时也可以并行(Teacher Forcing)
- 相比RNN训练速度大幅提升</p>
<p><strong>2. 长距离依赖</strong>
- 注意力机制直接连接任意两个位置
- 避免了RNN的梯度传播问题
- 更好地捕获长距离依赖关系</p>
<p><strong>3. 可解释性</strong>
- 注意力权重提供模型决策的可视化
- 可以看到模型关注的输入部分
- 便于分析和调试</p>
<h4 id="_14">典型应用<a class="headerlink" href="#_14" title="Permanent link">&para;</a></h4>
<p><strong>1. 机器翻译</strong>
- 原始Transformer的设计目标
- 编码器理解源语言，解码器生成目标语言
- 通过交叉注意力实现语言对齐</p>
<p><strong>2. 文本摘要</strong>
- 编码器理解原文，解码器生成摘要
- 交叉注意力选择重要信息
- 控制摘要长度和内容</p>
<p><strong>3. 对话系统</strong>
- 编码器理解用户输入
- 解码器生成回复
- 维持对话上下文</p>
<h3 id="_15">现代发展趋势<a class="headerlink" href="#_15" title="Permanent link">&para;</a></h3>
<h4 id="_16">架构演进<a class="headerlink" href="#_16" title="Permanent link">&para;</a></h4>
<p><strong>Encoder-Only</strong>:
- 代表: BERT, RoBERTa
- 擅长: 理解任务(分类、阅读理解)
- 特点: 双向注意力，并行训练</p>
<p><strong>Decoder-Only</strong>:
- 代表: GPT, LLaMA, ChatGPT
- 擅长: 生成任务(对话、写作)
- 特点: 因果注意力，统一范式</p>
<p><strong>为什么Decoder-Only成为主流？</strong>
1. <strong>任务统一</strong>: 所有任务都可以表述为生成问题
2. <strong>扩展性好</strong>: 更容易扩展到大规模
3. <strong>涌现能力</strong>: 大规模后展现强大的few-shot能力
4. <strong>工程简化</strong>: 架构更简单，易于优化</p>
<h2 id="_17">💬 面试问题解答<a class="headerlink" href="#_17" title="Permanent link">&para;</a></h2>
<h3 id="q1">Q1: 编码器和解码器的主要区别是什么？<a class="headerlink" href="#q1" title="Permanent link">&para;</a></h3>
<p><strong>核心区别</strong>:</p>
<table>
<thead>
<tr>
<th>维度</th>
<th>编码器</th>
<th>解码器</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>注意力类型</strong></td>
<td>双向自注意力</td>
<td>单向掩码自注意力 + 交叉注意力</td>
</tr>
<tr>
<td><strong>处理方式</strong></td>
<td>并行处理</td>
<td>自回归生成</td>
</tr>
<tr>
<td><strong>输入来源</strong></td>
<td>原始输入序列</td>
<td>前一步输出 + 编码器输出</td>
</tr>
<tr>
<td><strong>主要功能</strong></td>
<td>理解和编码</td>
<td>生成和解码</td>
</tr>
</tbody>
</table>
<h3 id="q2">Q2: 交叉注意力的工作原理是什么？<a class="headerlink" href="#q2" title="Permanent link">&para;</a></h3>
<p><strong>工作机制</strong>:
1. <strong>Query来自解码器</strong>: 表示"我想要什么信息"
2. <strong>Key/Value来自编码器</strong>: 表示"可以提供什么信息"
3. <strong>注意力计算</strong>: 计算解码器对编码器每个位置的关注度
4. <strong>信息融合</strong>: 根据注意力权重聚合编码器信息</p>
<p><strong>数学过程</strong>:
<span class="arithmatex">\(<span class="arithmatex">\(CrossAttention = Attention(Q_{decoder}, K_{encoder}, V_{encoder})\)</span>\)</span></p>
<h3 id="q3">Q3: 为什么解码器需要掩码自注意力？<a class="headerlink" href="#q3" title="Permanent link">&para;</a></h3>
<p><strong>核心原因</strong>: 防止信息泄露</p>
<p><strong>详细解释</strong>:
1. <strong>训练时</strong>: 使用Teacher Forcing，模型能看到完整目标序列
2. <strong>推理时</strong>: 只能看到已生成的部分
3. <strong>一致性要求</strong>: 训练和推理的可见信息必须一致
4. <strong>掩码作用</strong>: 在训练时人为限制可见范围</p>
<p><strong>代码示例</strong>:
<div class="highlight"><pre><span></span><code><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">create_causal_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
<a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
<a id="__codelineno-7-3" name="__codelineno-7-3" href="#__codelineno-7-3"></a>    <span class="k">return</span> <span class="n">mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
</code></pre></div></p>
<h3 id="q4-decoder-only">Q4: 为什么现在更流行Decoder-Only架构？<a class="headerlink" href="#q4-decoder-only" title="Permanent link">&para;</a></h3>
<p><strong>主要原因</strong>:</p>
<ol>
<li><strong>统一性</strong>: </li>
<li>所有任务都可以转化为生成任务</li>
<li>分类 → 生成类别标签</li>
<li>
<p>问答 → 生成答案</p>
</li>
<li>
<p><strong>扩展性</strong>:</p>
</li>
<li>架构简单，易于扩大规模</li>
<li>
<p>训练更稳定，参数利用率高</p>
</li>
<li>
<p><strong>涌现能力</strong>:</p>
</li>
<li>大规模训练后展现强大的zero/few-shot能力</li>
<li>
<p>指令跟随、上下文学习等能力</p>
</li>
<li>
<p><strong>工程优势</strong>:</p>
</li>
<li>实现简单，优化成熟</li>
<li>推理效率高(KV Cache等技术)</li>
</ol>
<h2 id="_18">💻 代码实现<a class="headerlink" href="#_18" title="Permanent link">&para;</a></h2>
<h3 id="encoder-decoder">完整Encoder-Decoder实现<a class="headerlink" href="#encoder-decoder" title="Permanent link">&para;</a></h3>
<div class="highlight"><pre><span></span><code><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a>
<a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a><span class="k">class</span><span class="w"> </span><span class="nc">EncoderDecoderTransformer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;完整的Encoder-Decoder Transformer实现&quot;&quot;&quot;</span>
<a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a>
<a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> 
<a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a>                 <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<a id="__codelineno-8-11" name="__codelineno-8-11" href="#__codelineno-8-11"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-8-12" name="__codelineno-8-12" href="#__codelineno-8-12"></a>
<a id="__codelineno-8-13" name="__codelineno-8-13" href="#__codelineno-8-13"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
<a id="__codelineno-8-14" name="__codelineno-8-14" href="#__codelineno-8-14"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len</span> <span class="o">=</span> <span class="n">max_seq_len</span>
<a id="__codelineno-8-15" name="__codelineno-8-15" href="#__codelineno-8-15"></a>
<a id="__codelineno-8-16" name="__codelineno-8-16" href="#__codelineno-8-16"></a>        <span class="c1"># 词嵌入层</span>
<a id="__codelineno-8-17" name="__codelineno-8-17" href="#__codelineno-8-17"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">src_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-8-18" name="__codelineno-8-18" href="#__codelineno-8-18"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-8-19" name="__codelineno-8-19" href="#__codelineno-8-19"></a>
<a id="__codelineno-8-20" name="__codelineno-8-20" href="#__codelineno-8-20"></a>        <span class="c1"># 位置编码</span>
<a id="__codelineno-8-21" name="__codelineno-8-21" href="#__codelineno-8-21"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span> <span class="o">=</span> <span class="n">PositionalEncoding</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="p">)</span>
<a id="__codelineno-8-22" name="__codelineno-8-22" href="#__codelineno-8-22"></a>
<a id="__codelineno-8-23" name="__codelineno-8-23" href="#__codelineno-8-23"></a>        <span class="c1"># 编码器和解码器</span>
<a id="__codelineno-8-24" name="__codelineno-8-24" href="#__codelineno-8-24"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<a id="__codelineno-8-25" name="__codelineno-8-25" href="#__codelineno-8-25"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">Decoder</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<a id="__codelineno-8-26" name="__codelineno-8-26" href="#__codelineno-8-26"></a>
<a id="__codelineno-8-27" name="__codelineno-8-27" href="#__codelineno-8-27"></a>        <span class="c1"># 输出投影层</span>
<a id="__codelineno-8-28" name="__codelineno-8-28" href="#__codelineno-8-28"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">)</span>
<a id="__codelineno-8-29" name="__codelineno-8-29" href="#__codelineno-8-29"></a>
<a id="__codelineno-8-30" name="__codelineno-8-30" href="#__codelineno-8-30"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<a id="__codelineno-8-31" name="__codelineno-8-31" href="#__codelineno-8-31"></a>
<a id="__codelineno-8-32" name="__codelineno-8-32" href="#__codelineno-8-32"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<a id="__codelineno-8-33" name="__codelineno-8-33" href="#__codelineno-8-33"></a>        <span class="c1"># 源序列编码</span>
<a id="__codelineno-8-34" name="__codelineno-8-34" href="#__codelineno-8-34"></a>        <span class="n">src_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">src_embedding</span><span class="p">(</span><span class="n">src</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-8-35" name="__codelineno-8-35" href="#__codelineno-8-35"></a>        <span class="n">src_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">(</span><span class="n">src_emb</span><span class="p">)</span>
<a id="__codelineno-8-36" name="__codelineno-8-36" href="#__codelineno-8-36"></a>        <span class="n">src_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">src_emb</span><span class="p">)</span>
<a id="__codelineno-8-37" name="__codelineno-8-37" href="#__codelineno-8-37"></a>
<a id="__codelineno-8-38" name="__codelineno-8-38" href="#__codelineno-8-38"></a>        <span class="c1"># 目标序列编码</span>
<a id="__codelineno-8-39" name="__codelineno-8-39" href="#__codelineno-8-39"></a>        <span class="n">tgt_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tgt_embedding</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-8-40" name="__codelineno-8-40" href="#__codelineno-8-40"></a>        <span class="n">tgt_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_encoding</span><span class="p">(</span><span class="n">tgt_emb</span><span class="p">)</span>
<a id="__codelineno-8-41" name="__codelineno-8-41" href="#__codelineno-8-41"></a>        <span class="n">tgt_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">tgt_emb</span><span class="p">)</span>
<a id="__codelineno-8-42" name="__codelineno-8-42" href="#__codelineno-8-42"></a>
<a id="__codelineno-8-43" name="__codelineno-8-43" href="#__codelineno-8-43"></a>        <span class="c1"># 编码器</span>
<a id="__codelineno-8-44" name="__codelineno-8-44" href="#__codelineno-8-44"></a>        <span class="n">encoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">src_emb</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
<a id="__codelineno-8-45" name="__codelineno-8-45" href="#__codelineno-8-45"></a>
<a id="__codelineno-8-46" name="__codelineno-8-46" href="#__codelineno-8-46"></a>        <span class="c1"># 解码器</span>
<a id="__codelineno-8-47" name="__codelineno-8-47" href="#__codelineno-8-47"></a>        <span class="n">decoder_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">tgt_emb</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
<a id="__codelineno-8-48" name="__codelineno-8-48" href="#__codelineno-8-48"></a>
<a id="__codelineno-8-49" name="__codelineno-8-49" href="#__codelineno-8-49"></a>        <span class="c1"># 输出投影</span>
<a id="__codelineno-8-50" name="__codelineno-8-50" href="#__codelineno-8-50"></a>        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_projection</span><span class="p">(</span><span class="n">decoder_output</span><span class="p">)</span>
<a id="__codelineno-8-51" name="__codelineno-8-51" href="#__codelineno-8-51"></a>
<a id="__codelineno-8-52" name="__codelineno-8-52" href="#__codelineno-8-52"></a>        <span class="k">return</span> <span class="n">output</span>
<a id="__codelineno-8-53" name="__codelineno-8-53" href="#__codelineno-8-53"></a>
<a id="__codelineno-8-54" name="__codelineno-8-54" href="#__codelineno-8-54"></a><span class="k">class</span><span class="w"> </span><span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-8-55" name="__codelineno-8-55" href="#__codelineno-8-55"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Transformer编码器&quot;&quot;&quot;</span>
<a id="__codelineno-8-56" name="__codelineno-8-56" href="#__codelineno-8-56"></a>
<a id="__codelineno-8-57" name="__codelineno-8-57" href="#__codelineno-8-57"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
<a id="__codelineno-8-58" name="__codelineno-8-58" href="#__codelineno-8-58"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-8-59" name="__codelineno-8-59" href="#__codelineno-8-59"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
<a id="__codelineno-8-60" name="__codelineno-8-60" href="#__codelineno-8-60"></a>            <span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<a id="__codelineno-8-61" name="__codelineno-8-61" href="#__codelineno-8-61"></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
<a id="__codelineno-8-62" name="__codelineno-8-62" href="#__codelineno-8-62"></a>        <span class="p">])</span>
<a id="__codelineno-8-63" name="__codelineno-8-63" href="#__codelineno-8-63"></a>
<a id="__codelineno-8-64" name="__codelineno-8-64" href="#__codelineno-8-64"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<a id="__codelineno-8-65" name="__codelineno-8-65" href="#__codelineno-8-65"></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
<a id="__codelineno-8-66" name="__codelineno-8-66" href="#__codelineno-8-66"></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<a id="__codelineno-8-67" name="__codelineno-8-67" href="#__codelineno-8-67"></a>        <span class="k">return</span> <span class="n">x</span>
<a id="__codelineno-8-68" name="__codelineno-8-68" href="#__codelineno-8-68"></a>
<a id="__codelineno-8-69" name="__codelineno-8-69" href="#__codelineno-8-69"></a><span class="k">class</span><span class="w"> </span><span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-8-70" name="__codelineno-8-70" href="#__codelineno-8-70"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;编码器层&quot;&quot;&quot;</span>
<a id="__codelineno-8-71" name="__codelineno-8-71" href="#__codelineno-8-71"></a>
<a id="__codelineno-8-72" name="__codelineno-8-72" href="#__codelineno-8-72"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
<a id="__codelineno-8-73" name="__codelineno-8-73" href="#__codelineno-8-73"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-8-74" name="__codelineno-8-74" href="#__codelineno-8-74"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
<a id="__codelineno-8-75" name="__codelineno-8-75" href="#__codelineno-8-75"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<a id="__codelineno-8-76" name="__codelineno-8-76" href="#__codelineno-8-76"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-8-77" name="__codelineno-8-77" href="#__codelineno-8-77"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-8-78" name="__codelineno-8-78" href="#__codelineno-8-78"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<a id="__codelineno-8-79" name="__codelineno-8-79" href="#__codelineno-8-79"></a>
<a id="__codelineno-8-80" name="__codelineno-8-80" href="#__codelineno-8-80"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<a id="__codelineno-8-81" name="__codelineno-8-81" href="#__codelineno-8-81"></a>        <span class="c1"># 自注意力子层</span>
<a id="__codelineno-8-82" name="__codelineno-8-82" href="#__codelineno-8-82"></a>        <span class="n">attn_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<a id="__codelineno-8-83" name="__codelineno-8-83" href="#__codelineno-8-83"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">attn_output</span><span class="p">))</span>
<a id="__codelineno-8-84" name="__codelineno-8-84" href="#__codelineno-8-84"></a>
<a id="__codelineno-8-85" name="__codelineno-8-85" href="#__codelineno-8-85"></a>        <span class="c1"># 前馈网络子层</span>
<a id="__codelineno-8-86" name="__codelineno-8-86" href="#__codelineno-8-86"></a>        <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-8-87" name="__codelineno-8-87" href="#__codelineno-8-87"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">))</span>
<a id="__codelineno-8-88" name="__codelineno-8-88" href="#__codelineno-8-88"></a>
<a id="__codelineno-8-89" name="__codelineno-8-89" href="#__codelineno-8-89"></a>        <span class="k">return</span> <span class="n">x</span>
<a id="__codelineno-8-90" name="__codelineno-8-90" href="#__codelineno-8-90"></a>
<a id="__codelineno-8-91" name="__codelineno-8-91" href="#__codelineno-8-91"></a><span class="k">class</span><span class="w"> </span><span class="nc">Decoder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-8-92" name="__codelineno-8-92" href="#__codelineno-8-92"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Transformer解码器&quot;&quot;&quot;</span>
<a id="__codelineno-8-93" name="__codelineno-8-93" href="#__codelineno-8-93"></a>
<a id="__codelineno-8-94" name="__codelineno-8-94" href="#__codelineno-8-94"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
<a id="__codelineno-8-95" name="__codelineno-8-95" href="#__codelineno-8-95"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-8-96" name="__codelineno-8-96" href="#__codelineno-8-96"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span>
<a id="__codelineno-8-97" name="__codelineno-8-97" href="#__codelineno-8-97"></a>            <span class="n">DecoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<a id="__codelineno-8-98" name="__codelineno-8-98" href="#__codelineno-8-98"></a>            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">)</span>
<a id="__codelineno-8-99" name="__codelineno-8-99" href="#__codelineno-8-99"></a>        <span class="p">])</span>
<a id="__codelineno-8-100" name="__codelineno-8-100" href="#__codelineno-8-100"></a>
<a id="__codelineno-8-101" name="__codelineno-8-101" href="#__codelineno-8-101"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<a id="__codelineno-8-102" name="__codelineno-8-102" href="#__codelineno-8-102"></a>        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
<a id="__codelineno-8-103" name="__codelineno-8-103" href="#__codelineno-8-103"></a>            <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
<a id="__codelineno-8-104" name="__codelineno-8-104" href="#__codelineno-8-104"></a>        <span class="k">return</span> <span class="n">x</span>
<a id="__codelineno-8-105" name="__codelineno-8-105" href="#__codelineno-8-105"></a>
<a id="__codelineno-8-106" name="__codelineno-8-106" href="#__codelineno-8-106"></a><span class="k">class</span><span class="w"> </span><span class="nc">DecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<a id="__codelineno-8-107" name="__codelineno-8-107" href="#__codelineno-8-107"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;解码器层&quot;&quot;&quot;</span>
<a id="__codelineno-8-108" name="__codelineno-8-108" href="#__codelineno-8-108"></a>
<a id="__codelineno-8-109" name="__codelineno-8-109" href="#__codelineno-8-109"></a>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
<a id="__codelineno-8-110" name="__codelineno-8-110" href="#__codelineno-8-110"></a>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<a id="__codelineno-8-111" name="__codelineno-8-111" href="#__codelineno-8-111"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">masked_self_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
<a id="__codelineno-8-112" name="__codelineno-8-112" href="#__codelineno-8-112"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
<a id="__codelineno-8-113" name="__codelineno-8-113" href="#__codelineno-8-113"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span> <span class="o">=</span> <span class="n">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
<a id="__codelineno-8-114" name="__codelineno-8-114" href="#__codelineno-8-114"></a>
<a id="__codelineno-8-115" name="__codelineno-8-115" href="#__codelineno-8-115"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-8-116" name="__codelineno-8-116" href="#__codelineno-8-116"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-8-117" name="__codelineno-8-117" href="#__codelineno-8-117"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-8-118" name="__codelineno-8-118" href="#__codelineno-8-118"></a>        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<a id="__codelineno-8-119" name="__codelineno-8-119" href="#__codelineno-8-119"></a>
<a id="__codelineno-8-120" name="__codelineno-8-120" href="#__codelineno-8-120"></a>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">src_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<a id="__codelineno-8-121" name="__codelineno-8-121" href="#__codelineno-8-121"></a>        <span class="c1"># 掩码自注意力子层</span>
<a id="__codelineno-8-122" name="__codelineno-8-122" href="#__codelineno-8-122"></a>        <span class="n">masked_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">masked_self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
<a id="__codelineno-8-123" name="__codelineno-8-123" href="#__codelineno-8-123"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm1</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">masked_attn</span><span class="p">))</span>
<a id="__codelineno-8-124" name="__codelineno-8-124" href="#__codelineno-8-124"></a>
<a id="__codelineno-8-125" name="__codelineno-8-125" href="#__codelineno-8-125"></a>        <span class="c1"># 交叉注意力子层</span>
<a id="__codelineno-8-126" name="__codelineno-8-126" href="#__codelineno-8-126"></a>        <span class="n">cross_attn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cross_attention</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">encoder_output</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
<a id="__codelineno-8-127" name="__codelineno-8-127" href="#__codelineno-8-127"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm2</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">cross_attn</span><span class="p">))</span>
<a id="__codelineno-8-128" name="__codelineno-8-128" href="#__codelineno-8-128"></a>
<a id="__codelineno-8-129" name="__codelineno-8-129" href="#__codelineno-8-129"></a>        <span class="c1"># 前馈网络子层</span>
<a id="__codelineno-8-130" name="__codelineno-8-130" href="#__codelineno-8-130"></a>        <span class="n">ff_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<a id="__codelineno-8-131" name="__codelineno-8-131" href="#__codelineno-8-131"></a>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm3</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">ff_output</span><span class="p">))</span>
<a id="__codelineno-8-132" name="__codelineno-8-132" href="#__codelineno-8-132"></a>
<a id="__codelineno-8-133" name="__codelineno-8-133" href="#__codelineno-8-133"></a>        <span class="k">return</span> <span class="n">x</span>
<a id="__codelineno-8-134" name="__codelineno-8-134" href="#__codelineno-8-134"></a>
<a id="__codelineno-8-135" name="__codelineno-8-135" href="#__codelineno-8-135"></a><span class="k">def</span><span class="w"> </span><span class="nf">create_causal_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
<a id="__codelineno-8-136" name="__codelineno-8-136" href="#__codelineno-8-136"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;创建因果掩码(下三角矩阵)&quot;&quot;&quot;</span>
<a id="__codelineno-8-137" name="__codelineno-8-137" href="#__codelineno-8-137"></a>    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
<a id="__codelineno-8-138" name="__codelineno-8-138" href="#__codelineno-8-138"></a>    <span class="k">return</span> <span class="n">mask</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
<a id="__codelineno-8-139" name="__codelineno-8-139" href="#__codelineno-8-139"></a>
<a id="__codelineno-8-140" name="__codelineno-8-140" href="#__codelineno-8-140"></a><span class="k">def</span><span class="w"> </span><span class="nf">create_padding_mask</span><span class="p">(</span><span class="n">seq</span><span class="p">,</span> <span class="n">pad_token</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<a id="__codelineno-8-141" name="__codelineno-8-141" href="#__codelineno-8-141"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;创建填充掩码&quot;&quot;&quot;</span>
<a id="__codelineno-8-142" name="__codelineno-8-142" href="#__codelineno-8-142"></a>    <span class="k">return</span> <span class="p">(</span><span class="n">seq</span> <span class="o">!=</span> <span class="n">pad_token</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<a id="__codelineno-8-143" name="__codelineno-8-143" href="#__codelineno-8-143"></a>
<a id="__codelineno-8-144" name="__codelineno-8-144" href="#__codelineno-8-144"></a><span class="c1"># 使用示例</span>
<a id="__codelineno-8-145" name="__codelineno-8-145" href="#__codelineno-8-145"></a><span class="k">def</span><span class="w"> </span><span class="nf">demo_encoder_decoder</span><span class="p">():</span>
<a id="__codelineno-8-146" name="__codelineno-8-146" href="#__codelineno-8-146"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;演示编码器-解码器的使用&quot;&quot;&quot;</span>
<a id="__codelineno-8-147" name="__codelineno-8-147" href="#__codelineno-8-147"></a>
<a id="__codelineno-8-148" name="__codelineno-8-148" href="#__codelineno-8-148"></a>    <span class="c1"># 模型参数</span>
<a id="__codelineno-8-149" name="__codelineno-8-149" href="#__codelineno-8-149"></a>    <span class="n">src_vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
<a id="__codelineno-8-150" name="__codelineno-8-150" href="#__codelineno-8-150"></a>    <span class="n">tgt_vocab_size</span> <span class="o">=</span> <span class="mi">1000</span>
<a id="__codelineno-8-151" name="__codelineno-8-151" href="#__codelineno-8-151"></a>    <span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<a id="__codelineno-8-152" name="__codelineno-8-152" href="#__codelineno-8-152"></a>    <span class="n">seq_len</span> <span class="o">=</span> <span class="mi">20</span>
<a id="__codelineno-8-153" name="__codelineno-8-153" href="#__codelineno-8-153"></a>    <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">2</span>
<a id="__codelineno-8-154" name="__codelineno-8-154" href="#__codelineno-8-154"></a>
<a id="__codelineno-8-155" name="__codelineno-8-155" href="#__codelineno-8-155"></a>    <span class="c1"># 创建模型</span>
<a id="__codelineno-8-156" name="__codelineno-8-156" href="#__codelineno-8-156"></a>    <span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoderTransformer</span><span class="p">(</span>
<a id="__codelineno-8-157" name="__codelineno-8-157" href="#__codelineno-8-157"></a>        <span class="n">src_vocab_size</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="n">d_model</span>
<a id="__codelineno-8-158" name="__codelineno-8-158" href="#__codelineno-8-158"></a>    <span class="p">)</span>
<a id="__codelineno-8-159" name="__codelineno-8-159" href="#__codelineno-8-159"></a>
<a id="__codelineno-8-160" name="__codelineno-8-160" href="#__codelineno-8-160"></a>    <span class="c1"># 模拟数据</span>
<a id="__codelineno-8-161" name="__codelineno-8-161" href="#__codelineno-8-161"></a>    <span class="n">src</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">src_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
<a id="__codelineno-8-162" name="__codelineno-8-162" href="#__codelineno-8-162"></a>    <span class="n">tgt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">tgt_vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
<a id="__codelineno-8-163" name="__codelineno-8-163" href="#__codelineno-8-163"></a>
<a id="__codelineno-8-164" name="__codelineno-8-164" href="#__codelineno-8-164"></a>    <span class="c1"># 创建掩码</span>
<a id="__codelineno-8-165" name="__codelineno-8-165" href="#__codelineno-8-165"></a>    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">create_causal_mask</span><span class="p">(</span><span class="n">seq_len</span><span class="p">)</span>
<a id="__codelineno-8-166" name="__codelineno-8-166" href="#__codelineno-8-166"></a>    <span class="n">src_mask</span> <span class="o">=</span> <span class="n">create_padding_mask</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
<a id="__codelineno-8-167" name="__codelineno-8-167" href="#__codelineno-8-167"></a>
<a id="__codelineno-8-168" name="__codelineno-8-168" href="#__codelineno-8-168"></a>    <span class="c1"># 前向传播</span>
<a id="__codelineno-8-169" name="__codelineno-8-169" href="#__codelineno-8-169"></a>    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">tgt_mask</span><span class="p">)</span>
<a id="__codelineno-8-170" name="__codelineno-8-170" href="#__codelineno-8-170"></a>
<a id="__codelineno-8-171" name="__codelineno-8-171" href="#__codelineno-8-171"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;输入形状: </span><span class="si">{</span><span class="n">src</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-8-172" name="__codelineno-8-172" href="#__codelineno-8-172"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;输出形状: </span><span class="si">{</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-8-173" name="__codelineno-8-173" href="#__codelineno-8-173"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;参数量: </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-8-174" name="__codelineno-8-174" href="#__codelineno-8-174"></a>
<a id="__codelineno-8-175" name="__codelineno-8-175" href="#__codelineno-8-175"></a><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<a id="__codelineno-8-176" name="__codelineno-8-176" href="#__codelineno-8-176"></a>    <span class="n">demo_encoder_decoder</span><span class="p">()</span>
</code></pre></div>
<h2 id="_19">✅ 学习检验<a class="headerlink" href="#_19" title="Permanent link">&para;</a></h2>
<ul>
<li>[ ] 理解编码器-解码器架构的整体设计</li>
<li>[ ] 掌握三种注意力机制的区别和作用</li>
<li>[ ] 理解掩码机制的必要性和实现</li>
<li>[ ] 能够实现完整的Encoder-Decoder模型</li>
<li>[ ] 理解为什么现代模型偏向Decoder-Only</li>
</ul>
<h2 id="_20">🔗 相关链接<a class="headerlink" href="#_20" title="Permanent link">&para;</a></h2>
<ul>
<li><a href="../ffn/">上一节：前馈神经网络</a></li>
<li><a href="../../attention-advanced/">下一节：Attention升级技术</a></li>
<li><a href="../">返回：Transformer基础概览</a></li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["header.autohide", "navigation.tracking", "navigation.top", "search.highlight", "search.share", "search.suggest", "content.code.annotate"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.50899def.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>